2019-07-22 00:49:56,509	WARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
2019-07-22 00:49:56,511	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-22_00-49-56_510275_63049/logs.
2019-07-22 00:49:56,624	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-22 00:49:56,746	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-22 00:49:56,751	INFO services.py:806 -- Starting Redis shard with 1.72 GB max memory.
2019-07-22 00:49:56,803	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-22_00-49-56_510275_63049/logs.
2019-07-22 00:49:56,806	INFO services.py:1446 -- Starting the Plasma object store with 2.58 GB memory using /tmp.
2019-07-22 00:49:57,510	INFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()
2019-07-22 00:49:57,511	INFO tune.py:233 -- Starting a new experiment.
2019-07-22 00:49:57,573	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-22 00:49:57,812	WARNING util.py:64 -- The `start_trial` operation took 0.24917197227478027 seconds to complete, which may be a performance bottleneck.
2019-07-22 00:50:02,911	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=63081, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 101, in _init
    before_init(self)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 196, in setup_ddpg_exploration
    trainer.exploration0 = make_exploration_schedule(trainer.config, -1)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 174, in make_exploration_schedule
    assert config["num_workers"] > 1, "This requires multiple workers"
AssertionError: This requires multiple workers

2019-07-22 00:50:02,915	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-22 00:50:02,924	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-22 00:50:02,929	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-22 00:50:06,127	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=63126, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 101, in _init
    before_init(self)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 196, in setup_ddpg_exploration
    trainer.exploration0 = make_exploration_schedule(trainer.config, -1)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 174, in make_exploration_schedule
    assert config["num_workers"] > 1, "This requires multiple workers"
AssertionError: This requires multiple workers

2019-07-22 00:50:06,129	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-22 00:50:06,136	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-22 00:50:06,140	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-22 00:50:09,027	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=63079, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 101, in _init
    before_init(self)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 196, in setup_ddpg_exploration
    trainer.exploration0 = make_exploration_schedule(trainer.config, -1)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 174, in make_exploration_schedule
    assert config["num_workers"] > 1, "This requires multiple workers"
AssertionError: This requires multiple workers

2019-07-22 00:50:09,029	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-22 00:50:09,032	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-22 00:50:09,041	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-22 00:50:12,002	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=63078, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 101, in _init
    before_init(self)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 196, in setup_ddpg_exploration
    trainer.exploration0 = make_exploration_schedule(trainer.config, -1)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/ddpg/ddpg.py", line 174, in make_exploration_schedule
    assert config["num_workers"] > 1, "This requires multiple workers"
AssertionError: This requires multiple workers

2019-07-22 00:50:12,003	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[32m [     0.93755s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.93842s,  INFO] Experiment configs: 
 {
  "gym-reacher-apex-ddpg": {
    "env": "RoboschoolReacher-v1",
    "run": "APEX_DDPG",
    "local_dir": "~/kayray_results/local",
    "checkpoint_freq": 100,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 21,
      "timesteps_total": 10000000
    },
    "config": {
      "env_config": {
        "env_type": "openai"
      },
      "use_huber": true,
      "clip_rewards": false,
      "num_gpus": 0,
      "num_workers": 1,
      "n_step": 3,
      "exploration_ou_noise_scale": 1.0,
      "target_network_update_freq": 50000,
      "tau": 1.0,
      "evaluation_interval": 5,
      "evaluation_num_episodes": 10
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.9/8.6 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.9/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0:	RUNNING

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.8/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg/APEX_DDPG_RoboschoolReacher-v1_0_2019-07-22_00-49-57n0z3vu07/error_2019-07-22_00-50-02.txt

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.8/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg/APEX_DDPG_RoboschoolReacher-v1_0_2019-07-22_00-49-57n0z3vu07/error_2019-07-22_00-50-09.txt

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.8/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg
Number of trials: 1 ({'ERROR': 1})
ERROR trials:
 - APEX_DDPG_RoboschoolReacher-v1_0:	ERROR, 4 failures: /Users/amrmkayid/kayray_results/local/gym-reacher-apex-ddpg/APEX_DDPG_RoboschoolReacher-v1_0_2019-07-22_00-49-57n0z3vu07/error_2019-07-22_00-50-12.txt

Traceback (most recent call last):
  File "train.py", line 181, in <module>
    run(args, parser, dot_dict)
  File "train.py", line 170, in run
    resume=args.resume)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/tune.py", line 333, in run_experiments
    raise_on_failed_trial=raise_on_failed_trial)
  File "/Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/tune.py", line 273, in run
    raise TuneError("Trials did not complete", errored_trials)
ray.tune.error.TuneError: ('Trials did not complete', [APEX_DDPG_RoboschoolReacher-v1_0])
