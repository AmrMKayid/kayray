2019-07-15 03:06:47,648	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-15_03-06-47_600506_58889/logs.
2019-07-15 03:06:47,827	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:47,941	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,054	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,170	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,283	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,399	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,515	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,628	WARNING services.py:763 -- Redis failed to start, retrying now.
2019-07-15 03:06:48,740	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:36537 to respond...
2019-07-15 03:06:48,867	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:61367 to respond...
2019-07-15 03:06:48,871	INFO services.py:806 -- Starting Redis shard with 1.72 GB max memory.
2019-07-15 03:06:48,904	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-15_03-06-47_600506_58889/logs.
2019-07-15 03:06:48,906	INFO services.py:1446 -- Starting the Plasma object store with 2.58 GB memory using /tmp.
2019-07-15 03:06:49,145	INFO tune.py:65 -- Did not find checkpoint file in /Users/amrmkayid/kayray_results/local/gym-reacher-ppo.
2019-07-15 03:06:49,145	INFO tune.py:233 -- Starting a new experiment.
2019-07-15 03:06:54,923	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-15 03:06:55,184	WARNING util.py:64 -- The `start_trial` operation took 0.27106499671936035 seconds to complete, which may be a performance bottleneck.
[32m [     0.20456s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.20576s,  INFO] Experiment configs: 
 {
  "gym-reacher-ppo": {
    "env": "RoboschoolReacher-v1",
    "run": "PPO",
    "local_dir": "~/kayray_results/local",
    "checkpoint_freq": 100,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 21,
      "training_iteration": 500
    },
    "config": {
      "gamma": 0.995,
      "kl_coeff": 1.0,
      "num_sgd_iter": 20,
      "lr": 0.0001,
      "sgd_minibatch_size": 1000,
      "train_batch_size": 25000,
      "model": {
        "free_log_std": true
      },
      "num_gpus": 0,
      "num_workers": 0,
      "batch_mode": "complete_episodes",
      "observation_filter": "MeanStdFilter"
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.4/8.6 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING

[2m[36m(pid=58925)[0m 2019-07-15 03:07:08,503	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=58925)[0m [32m [     0.39690s,  INFO] TimeLimit:
[2m[36m(pid=58925)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=58925)[0m - action_space = Box(2,)
[2m[36m(pid=58925)[0m - observation_space = Box(9,)
[2m[36m(pid=58925)[0m - reward_range = (-inf, inf)
[2m[36m(pid=58925)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=58925)[0m - _max_episode_steps = 150
[2m[36m(pid=58925)[0m - _elapsed_steps = None [0m
[2m[36m(pid=58925)[0m 2019-07-15 03:07:08,793	INFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=58925)[0m 2019-07-15 03:07:08.793815: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=58925)[0m 2019-07-15 03:07:10,060	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=58925)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=58925)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=58925)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=58925)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=58925)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=58925)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m /Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=58925)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=58925)[0m 2019-07-15 03:07:18,523	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x1d37c3cf8>}
[2m[36m(pid=58925)[0m 2019-07-15 03:07:18,529	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x1d32f9cf8>}
[2m[36m(pid=58925)[0m 2019-07-15 03:07:18,529	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=58925)[0m 2019-07-15 03:07:18,545	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,182	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,216	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.786, max=0.383, mean=-0.138)}}
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,216	INFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,217	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.786, max=0.383, mean=-0.138)
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,217	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,220	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=58925)[0m                                   'env_id': 0,
[2m[36m(pid=58925)[0m                                   'info': None,
[2m[36m(pid=58925)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=58925)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=58925)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=58925)[0m                                   'rnn_state': []},
[2m[36m(pid=58925)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,222	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=58925)[0m 2019-07-15 03:07:39,691	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-1.55, max=1.269, mean=-0.14),
[2m[36m(pid=58925)[0m                       [],
[2m[36m(pid=58925)[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.021, max=0.021, mean=0.021),
[2m[36m(pid=58925)[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=58925)[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m 2019-07-15 03:07:40,531	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.003, max=0.157, mean=0.08),
[2m[36m(pid=58925)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-2.746, max=2.045, mean=-0.007),
[2m[36m(pid=58925)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-16.705, max=5.085, mean=-5.676),
[2m[36m(pid=58925)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=58925)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.006, max=0.008, mean=0.0),
[2m[36m(pid=58925)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=58925)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=171062137.0, max=171062137.0, mean=171062137.0),
[2m[36m(pid=58925)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=58925)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-2.525, max=3.209, mean=0.209),
[2m[36m(pid=58925)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-2.525, max=3.209, mean=0.21),
[2m[36m(pid=58925)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-2.746, max=2.045, mean=-0.006),
[2m[36m(pid=58925)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-2.804, max=2.781, mean=-0.065),
[2m[36m(pid=58925)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-2.804, max=2.781, mean=-0.062),
[2m[36m(pid=58925)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=58925)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=58925)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-16.698, max=5.094, mean=-5.677),
[2m[36m(pid=58925)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.012, max=0.009, mean=-0.0)},
[2m[36m(pid=58925)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m 2019-07-15 03:07:41,773	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m { 'data': { 'action_prob': np.ndarray((300,), dtype=float32, min=0.0, max=0.158, mean=0.08),
[2m[36m(pid=58925)[0m             'actions': np.ndarray((300, 2), dtype=float32, min=-2.842, max=2.392, mean=0.024),
[2m[36m(pid=58925)[0m             'advantages': np.ndarray((300,), dtype=float32, min=-35.018, max=5.085, mean=-13.259),
[2m[36m(pid=58925)[0m             'agent_index': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=58925)[0m             'behaviour_logits': np.ndarray((300, 4), dtype=float32, min=-0.008, max=0.009, mean=0.001),
[2m[36m(pid=58925)[0m             'dones': np.ndarray((300,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=58925)[0m             'eps_id': np.ndarray((300,), dtype=int64, min=171062137.0, max=1932256588.0, mean=1051659362.5),
[2m[36m(pid=58925)[0m             'infos': np.ndarray((300,), dtype=object, head={}),
[2m[36m(pid=58925)[0m             'new_obs': np.ndarray((300, 9), dtype=float32, min=-8.661, max=8.661, mean=0.186),
[2m[36m(pid=58925)[0m             'obs': np.ndarray((300, 9), dtype=float32, min=-12.248, max=12.248, mean=0.186),
[2m[36m(pid=58925)[0m             'prev_actions': np.ndarray((300, 2), dtype=float32, min=-2.842, max=2.392, mean=0.025),
[2m[36m(pid=58925)[0m             'prev_rewards': np.ndarray((300,), dtype=float32, min=-2.804, max=2.781, mean=-0.149),
[2m[36m(pid=58925)[0m             'rewards': np.ndarray((300,), dtype=float32, min=-2.804, max=2.781, mean=-0.146),
[2m[36m(pid=58925)[0m             't': np.ndarray((300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=58925)[0m             'unroll_id': np.ndarray((300,), dtype=int64, min=0.0, max=1.0, mean=0.5),
[2m[36m(pid=58925)[0m             'value_targets': np.ndarray((300,), dtype=float32, min=-35.022, max=5.094, mean=-13.263),
[2m[36m(pid=58925)[0m             'vf_preds': np.ndarray((300,), dtype=float32, min=-0.014, max=0.009, mean=-0.004)},
[2m[36m(pid=58925)[0m   'type': 'SampleBatch'}
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m 2019-07-15 03:11:07,633	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m { 'inputs': [ np.ndarray((25200, 2), dtype=float32, min=-3.817, max=4.017, mean=0.0),
[2m[36m(pid=58925)[0m               np.ndarray((25200,), dtype=float32, min=-14.875, max=13.752, mean=-0.1),
[2m[36m(pid=58925)[0m               np.ndarray((25200, 9), dtype=float32, min=-12.248, max=13.037, mean=-0.004),
[2m[36m(pid=58925)[0m               np.ndarray((25200, 2), dtype=float32, min=-3.817, max=4.017, mean=0.0),
[2m[36m(pid=58925)[0m               np.ndarray((25200,), dtype=float32, min=-5.042, max=3.588, mean=-0.0),
[2m[36m(pid=58925)[0m               np.ndarray((25200, 4), dtype=float32, min=-0.013, max=0.014, mean=-0.0),
[2m[36m(pid=58925)[0m               np.ndarray((25200,), dtype=float32, min=-60.12, max=31.269, mean=-6.731),
[2m[36m(pid=58925)[0m               np.ndarray((25200,), dtype=float32, min=-0.016, max=0.015, mean=0.0)],
[2m[36m(pid=58925)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=58925)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=58925)[0m   'state_inputs': []}
[2m[36m(pid=58925)[0m 
[2m[36m(pid=58925)[0m 2019-07-15 03:11:07,635	INFO multi_gpu_impl.py:191 -- Divided 25200 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_03-11-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.574951417425495
  episode_reward_mean: -15.241524165769404
  episode_reward_min: -55.39455141272646
  episodes_this_iter: 168
  episodes_total: 168
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 50629.327
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.8322646617889404
        kl: 0.0008299094624817371
        policy_loss: -0.0023403181694447994
        total_loss: 97.39031219482422
        vf_explained_var: 0.14070290327072144
        vf_loss: 97.3918228149414
    load_time_ms: 697.6
    num_steps_sampled: 25200
    num_steps_trained: 25000
    sample_time_ms: 208419.154
    update_time_ms: 0.004
  iterations_since_restore: 1
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.2235736370105594
    mean_inference_ms: 5.41917640374514
    mean_processing_ms: 1.5832627890086461
  time_since_restore: 260.7867498397827
  time_this_iter_s: 260.7867498397827
  time_total_s: 260.7867498397827
  timestamp: 1563153119
  timesteps_since_restore: 25200
  timesteps_this_iter: 25200
  timesteps_total: 25200
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 7.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 260 s, 1 iter, 25200 ts, -15.2 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_03-18-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.072298573561703
  episode_reward_mean: -16.0955011964942
  episode_reward_min: -51.61657152899346
  episodes_this_iter: 168
  episodes_total: 336
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 41539.968
    learner:
      default_policy:
        cur_kl_coeff: 0.5
        cur_lr: 9.999999747378752e-05
        entropy: 2.830134391784668
        kl: 0.002767470432445407
        policy_loss: -0.005580849479883909
        total_loss: 77.16849517822266
        vf_explained_var: 0.3047698140144348
        vf_loss: 77.17268371582031
    load_time_ms: 426.893
    num_steps_sampled: 50400
    num_steps_trained: 50000
    sample_time_ms: 294749.038
    update_time_ms: 0.005
  iterations_since_restore: 2
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6802060684749347
    mean_inference_ms: 7.715179963669216
    mean_processing_ms: 2.234630496163082
  time_since_restore: 674.8519017696381
  time_this_iter_s: 414.06515192985535
  time_total_s: 674.8519017696381
  timestamp: 1563153534
  timesteps_since_restore: 50400
  timesteps_this_iter: 25200
  timesteps_total: 50400
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.9/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 674 s, 2 iter, 50400 ts, -16.1 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_03-25-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 11.956668877274836
  episode_reward_mean: -13.424713506244794
  episode_reward_min: -47.83218618084259
  episodes_this_iter: 168
  episodes_total: 504
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 38500.895
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.823845863342285
        kl: 0.004215538967400789
        policy_loss: -0.00504621397703886
        total_loss: 61.54606628417969
        vf_explained_var: 0.33756759762763977
        vf_loss: 61.550052642822266
    load_time_ms: 290.683
    num_steps_sampled: 75600
    num_steps_trained: 75000
    sample_time_ms: 314533.809
    update_time_ms: 0.005
  iterations_since_restore: 3
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7887546978899276
    mean_inference_ms: 8.195178076488672
    mean_processing_ms: 2.422656109862188
  time_since_restore: 1061.6138327121735
  time_this_iter_s: 386.7619309425354
  time_total_s: 1061.6138327121735
  timestamp: 1563153921
  timesteps_since_restore: 75600
  timesteps_this_iter: 25200
  timesteps_total: 75600
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 1061 s, 3 iter, 75600 ts, -13.4 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_03-30-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.00382315147895
  episode_reward_mean: -12.553638110286718
  episode_reward_min: -43.22087925323916
  episodes_this_iter: 168
  episodes_total: 672
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 36854.633
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8165194988250732
        kl: 0.007314838003367186
        policy_loss: -0.007049052510410547
        total_loss: 51.62715148925781
        vf_explained_var: 0.3934023380279541
        vf_loss: 51.63328552246094
    load_time_ms: 218.5
    num_steps_sampled: 100800
    num_steps_trained: 100000
    sample_time_ms: 305724.577
    update_time_ms: 0.005
  iterations_since_restore: 4
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7067432720506641
    mean_inference_ms: 8.02453190957202
    mean_processing_ms: 2.3257865412467216
  time_since_restore: 1373.0008444786072
  time_this_iter_s: 311.3870117664337
  time_total_s: 1373.0008444786072
  timestamp: 1563154232
  timesteps_since_restore: 100800
  timesteps_this_iter: 25200
  timesteps_total: 100800
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 1373 s, 4 iter, 100800 ts, -12.6 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_03-35-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 14.89210982777388
  episode_reward_mean: -9.964503279636011
  episode_reward_min: -52.63389804110905
  episodes_this_iter: 168
  episodes_total: 840
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 35787.315
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8041832447052
        kl: 0.008833414874970913
        policy_loss: -0.009432760067284107
        total_loss: 52.9969482421875
        vf_explained_var: 0.3254571259021759
        vf_loss: 53.005271911621094
    load_time_ms: 175.852
    num_steps_sampled: 126000
    num_steps_trained: 125000
    sample_time_ms: 301486.54
    update_time_ms: 0.005
  iterations_since_restore: 5
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6310382020472436
    mean_inference_ms: 7.914594515513928
    mean_processing_ms: 2.34087893630231
  time_since_restore: 1689.2438995838165
  time_this_iter_s: 316.24305510520935
  time_total_s: 1689.2438995838165
  timestamp: 1563154549
  timesteps_since_restore: 126000
  timesteps_this_iter: 25200
  timesteps_total: 126000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 1689 s, 5 iter, 126000 ts, -9.96 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_03-40-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.46541760503753
  episode_reward_mean: -8.928545093909866
  episode_reward_min: -50.87381230776629
  episodes_this_iter: 168
  episodes_total: 1008
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 34948.283
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.796773910522461
        kl: 0.006860858295112848
        policy_loss: -0.006897628307342529
        total_loss: 38.550323486328125
        vf_explained_var: 0.39729446172714233
        vf_loss: 38.55636215209961
    load_time_ms: 146.87
    num_steps_sampled: 151200
    num_steps_trained: 150000
    sample_time_ms: 295939.783
    update_time_ms: 0.004
  iterations_since_restore: 6
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.577168240870336
    mean_inference_ms: 7.791630285968738
    mean_processing_ms: 2.2962750308798303
  time_since_restore: 1988.380307674408
  time_this_iter_s: 299.13640809059143
  time_total_s: 1988.380307674408
  timestamp: 1563154848
  timesteps_since_restore: 151200
  timesteps_this_iter: 25200
  timesteps_total: 151200
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 1988 s, 6 iter, 151200 ts, -8.93 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_04-17-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.058638817326873
  episode_reward_mean: -7.803706591820471
  episode_reward_min: -33.377914915441295
  episodes_this_iter: 168
  episodes_total: 1176
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 39039.925
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.784139394760132
        kl: 0.0064079370349645615
        policy_loss: -0.006963776890188456
        total_loss: 29.851755142211914
        vf_explained_var: 0.44378772377967834
        vf_loss: 29.85791778564453
    load_time_ms: 127.19
    num_steps_sampled: 176400
    num_steps_trained: 175000
    sample_time_ms: 562745.761
    update_time_ms: 0.004
  iterations_since_restore: 7
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.583763399111765
    mean_inference_ms: 18.013916983566507
    mean_processing_ms: 2.647190356036839
  time_since_restore: 4215.620012760162
  time_this_iter_s: 2227.2397050857544
  time_total_s: 4215.620012760162
  timestamp: 1563157075
  timesteps_since_restore: 176400
  timesteps_this_iter: 25200
  timesteps_total: 176400
  training_iteration: 7
  2019-07-15 08:05:18,066	WARNING util.py:64 -- The `experiment_checkpoint` operation took 0.12705183029174805 seconds to complete, which may be a performance bottleneck.
2019-07-15 08:32:27,109	WARNING util.py:64 -- The `process_trial` operation took 0.1369490623474121 seconds to complete, which may be a performance bottleneck.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.8/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 4215 s, 7 iter, 176400 ts, -7.8 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_04-24-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.196318860296763
  episode_reward_mean: -5.925206118072019
  episode_reward_min: -35.537848909084886
  episodes_this_iter: 168
  episodes_total: 1344
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 40633.432
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7743966579437256
        kl: 0.008086592890322208
        policy_loss: -0.009132974781095982
        total_loss: 24.540122985839844
        vf_explained_var: 0.5036687254905701
        vf_loss: 24.548242568969727
    load_time_ms: 111.553
    num_steps_sampled: 201600
    num_steps_trained: 200000
    sample_time_ms: 535223.985
    update_time_ms: 0.004
  iterations_since_restore: 8
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6034925959230786
    mean_inference_ms: 16.87284063982705
    mean_processing_ms: 2.6775771561920445
  time_since_restore: 4610.060966968536
  time_this_iter_s: 394.440954208374
  time_total_s: 4610.060966968536
  timestamp: 1563157470
  timesteps_since_restore: 201600
  timesteps_this_iter: 25200
  timesteps_total: 201600
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.8/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 4610 s, 8 iter, 201600 ts, -5.93 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_05-13-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.5000237887557
  episode_reward_mean: -5.116702175830177
  episode_reward_min: -31.46369081080961
  episodes_this_iter: 168
  episodes_total: 1512
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 44100.925
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7648229598999023
        kl: 0.00765610858798027
        policy_loss: -0.008960303850471973
        total_loss: 24.443071365356445
        vf_explained_var: 0.5151889324188232
        vf_loss: 24.451074600219727
    load_time_ms: 100.852
    num_steps_sampled: 226800
    num_steps_trained: 225000
    sample_time_ms: 791231.843
    update_time_ms: 0.004
  iterations_since_restore: 9
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8208445073273651
    mean_inference_ms: 16.81364418701828
    mean_processing_ms: 12.677959235030785
  time_since_restore: 7521.845931768417
  time_this_iter_s: 2911.784964799881
  time_total_s: 7521.845931768417
  timestamp: 1563160381
  timesteps_since_restore: 226800
  timesteps_this_iter: 25200
  timesteps_total: 226800
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 7.1/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 7521 s, 9 iter, 226800 ts, -5.12 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_06-01-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.25524316623669
  episode_reward_mean: -3.29314163423843
  episode_reward_min: -32.8589508575708
  episodes_this_iter: 168
  episodes_total: 1680
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 42812.377
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7573630809783936
        kl: 0.008207916282117367
        policy_loss: -0.00899279210716486
        total_loss: 23.892595291137695
        vf_explained_var: 0.5140259861946106
        vf_loss: 23.900558471679688
    load_time_ms: 91.736
    num_steps_sampled: 252000
    num_steps_trained: 250000
    sample_time_ms: 997586.547
    update_time_ms: 0.004
  iterations_since_restore: 10
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7969174065118123
    mean_inference_ms: 26.058527268696245
    mean_processing_ms: 11.645247809745516
  time_since_restore: 10407.94300699234
  time_this_iter_s: 2886.0970752239227
  time_total_s: 10407.94300699234
  timestamp: 1563163268
  timesteps_since_restore: 252000
  timesteps_this_iter: 25200
  timesteps_total: 252000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 10407 s, 10 iter, 252000 ts, -3.29 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_06-05-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.921265836952514
  episode_reward_mean: -2.3793464972469422
  episode_reward_min: -38.28000775736918
  episodes_this_iter: 168
  episodes_total: 1848
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 41028.782
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7484989166259766
        kl: 0.006979044526815414
        policy_loss: -0.00954682007431984
        total_loss: 19.994783401489258
        vf_explained_var: 0.5564130544662476
        vf_loss: 20.00345802307129
    load_time_ms: 22.232
    num_steps_sampled: 277200
    num_steps_trained: 275000
    sample_time_ms: 997174.572
    update_time_ms: 0.005
  iterations_since_restore: 11
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7416183206454596
    mean_inference_ms: 24.176188730329066
    mean_processing_ms: 10.725799206133601
  time_since_restore: 10645.088194131851
  time_this_iter_s: 237.1451871395111
  time_total_s: 10645.088194131851
  timestamp: 1563163505
  timesteps_since_restore: 277200
  timesteps_this_iter: 25200
  timesteps_total: 277200
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 10645 s, 11 iter, 277200 ts, -2.38 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_07-49-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.096856291927384
  episode_reward_mean: -0.5400130608947071
  episode_reward_min: -29.280904343880806
  episodes_this_iter: 168
  episodes_total: 2016
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 44232.232
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7397682666778564
        kl: 0.008410414680838585
        policy_loss: -0.010708454996347427
        total_loss: 22.626014709472656
        vf_explained_var: 0.5766996741294861
        vf_loss: 22.63567352294922
    load_time_ms: 6.809
    num_steps_sampled: 302400
    num_steps_trained: 300000
    sample_time_ms: 1577420.892
    update_time_ms: 0.005
  iterations_since_restore: 12
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7699964613327608
    mean_inference_ms: 42.1704283580607
    mean_processing_ms: 10.08906323868896
  time_since_restore: 16893.404940128326
  time_this_iter_s: 6248.316745996475
  time_total_s: 16893.404940128326
  timestamp: 1563169753
  timesteps_since_restore: 302400
  timesteps_this_iter: 25200
  timesteps_total: 302400
  training_iteration: 12
  2019-07-15 08:49:39,335	WARNING util.py:64 -- The `process_trial` operation took 1.094728946685791 seconds to complete, which may be a performance bottleneck.
2019-07-15 09:08:51,774	WARNING util.py:64 -- The `process_trial` operation took 0.11033797264099121 seconds to complete, which may be a performance bottleneck.
2019-07-15 09:33:02,674	WARNING util.py:64 -- The `process_trial` operation took 0.2325761318206787 seconds to complete, which may be a performance bottleneck.
2019-07-15 09:56:44,510	WARNING util.py:64 -- The `process_trial` operation took 0.5390121936798096 seconds to complete, which may be a performance bottleneck.
2019-07-15 10:17:32,507	WARNING util.py:64 -- The `process_trial` operation took 0.24453401565551758 seconds to complete, which may be a performance bottleneck.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 16893 s, 12 iter, 302400 ts, -0.54 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_07-56-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.967229464654103
  episode_reward_mean: -1.6091433850252461
  episode_reward_min: -26.250548512398904
  episodes_this_iter: 168
  episodes_total: 2184
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 46504.483
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.722878932952881
        kl: 0.008414128795266151
        policy_loss: -0.010300547815859318
        total_loss: 16.2193660736084
        vf_explained_var: 0.6396248936653137
        vf_loss: 16.228614807128906
    load_time_ms: 5.477
    num_steps_sampled: 327600
    num_steps_trained: 325000
    sample_time_ms: 1580908.411
    update_time_ms: 0.005
  iterations_since_restore: 13
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7868446826572384
    mean_inference_ms: 39.69645058644307
    mean_processing_ms: 9.570850096739516
  time_since_restore: 17337.669063091278
  time_this_iter_s: 444.26412296295166
  time_total_s: 17337.669063091278
  timestamp: 1563170198
  timesteps_since_restore: 327600
  timesteps_this_iter: 25200
  timesteps_total: 327600
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.5/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 17337 s, 13 iter, 327600 ts, -1.61 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_08-05-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.14990681851847
  episode_reward_mean: 1.4076463769996121
  episode_reward_min: -28.57025564394511
  episodes_this_iter: 168
  episodes_total: 2352
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 53941.89
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7028861045837402
        kl: 0.009476090781390667
        policy_loss: -0.012219425290822983
        total_loss: 18.248332977294922
        vf_explained_var: 0.6377938985824585
        vf_loss: 18.259368896484375
    load_time_ms: 18.871
    num_steps_sampled: 352800
    num_steps_trained: 350000
    sample_time_ms: 1594265.116
    update_time_ms: 0.005
  iterations_since_restore: 14
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7887425729949125
    mean_inference_ms: 37.64097143484821
    mean_processing_ms: 9.142551788639363
  time_since_restore: 17857.255738973618
  time_this_iter_s: 519.5866758823395
  time_total_s: 17857.255738973618
  timestamp: 1563170717
  timesteps_since_restore: 352800
  timesteps_this_iter: 25200
  timesteps_total: 352800
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 17857 s, 14 iter, 352800 ts, 1.41 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_08-17-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.88463825769406
  episode_reward_mean: 4.294006012508963
  episode_reward_min: -22.15633345643776
  episodes_this_iter: 168
  episodes_total: 2520
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 61326.091
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6919054985046387
        kl: 0.007618565112352371
        policy_loss: -0.010756099596619606
        total_loss: 12.635457992553711
        vf_explained_var: 0.7386125922203064
        vf_loss: 12.64526081085205
    load_time_ms: 20.022
    num_steps_sampled: 378000
    num_steps_trained: 375000
    sample_time_ms: 1630710.632
    update_time_ms: 0.005
  iterations_since_restore: 15
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8712884164030024
    mean_inference_ms: 36.277914702827495
    mean_processing_ms: 8.89455474781198
  time_since_restore: 18612.04206609726
  time_this_iter_s: 754.786327123642
  time_total_s: 18612.04206609726
  timestamp: 1563171472
  timesteps_since_restore: 378000
  timesteps_this_iter: 25200
  timesteps_total: 378000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 18612 s, 15 iter, 378000 ts, 4.29 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_08-32-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.08958747000921
  episode_reward_mean: 4.827905478402672
  episode_reward_min: -19.597954598579467
  episodes_this_iter: 168
  episodes_total: 2688
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 70118.466
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6788523197174072
        kl: 0.008505613543093204
        policy_loss: -0.01122843474149704
        total_loss: 12.435232162475586
        vf_explained_var: 0.7282630205154419
        vf_loss: 12.445396423339844
    load_time_ms: 20.686
    num_steps_sampled: 403200
    num_steps_trained: 400000
    sample_time_ms: 1679361.79
    update_time_ms: 0.005
  iterations_since_restore: 16
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9992022942088317
    mean_inference_ms: 35.204813045353056
    mean_processing_ms: 8.764096887405767
  time_since_restore: 19485.754095315933
  time_this_iter_s: 873.7120292186737
  time_total_s: 19485.754095315933
  timestamp: 1563172346
  timesteps_since_restore: 403200
  timesteps_this_iter: 25200
  timesteps_total: 403200
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 19485 s, 16 iter, 403200 ts, 4.83 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_08-49-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.519895693689076
  episode_reward_mean: 5.955983592723145
  episode_reward_min: -18.224798798233934
  episodes_this_iter: 168
  episodes_total: 2856
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 77882.635
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6675450801849365
        kl: 0.008705410175025463
        policy_loss: -0.010278310626745224
        total_loss: 8.697916984558105
        vf_explained_var: 0.7668574452400208
        vf_loss: 8.707106590270996
    load_time_ms: 20.714
    num_steps_sampled: 428400
    num_steps_trained: 425000
    sample_time_ms: 1551939.323
    update_time_ms: 0.006
  iterations_since_restore: 17
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.191477053572277
    mean_inference_ms: 34.4223941285262
    mean_processing_ms: 8.717183676911132
  time_since_restore: 20516.813313245773
  time_this_iter_s: 1031.05921792984
  time_total_s: 20516.813313245773
  timestamp: 1563173378
  timesteps_since_restore: 428400
  timesteps_this_iter: 25200
  timesteps_total: 428400
  training_iteration: 17
  2019-07-15 10:40:11,414	WARNING util.py:64 -- The `process_trial` operation took 0.3245809078216553 seconds to complete, which may be a performance bottleneck.
2019-07-15 11:06:57,025	WARNING util.py:64 -- The `process_trial` operation took 0.3872799873352051 seconds to complete, which may be a performance bottleneck.
2019-07-15 11:06:57,361	WARNING util.py:64 -- The `experiment_checkpoint` operation took 0.3352329730987549 seconds to complete, which may be a performance bottleneck.
2019-07-15 11:31:29,514	WARNING util.py:64 -- The `experiment_checkpoint` operation took 0.10733294486999512 seconds to complete, which may be a performance bottleneck.
2019-07-15 11:59:38,445	WARNING util.py:64 -- The `process_trial` operation took 0.17941784858703613 seconds to complete, which may be a performance bottleneck.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 20516 s, 17 iter, 428400 ts, 5.96 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_09-08-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.971289796851508
  episode_reward_mean: 6.499249047727449
  episode_reward_min: -23.06103102339862
  episodes_this_iter: 168
  episodes_total: 3024
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 87354.305
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6507444381713867
        kl: 0.008548939600586891
        policy_loss: -0.011183981783688068
        total_loss: 10.88629150390625
        vf_explained_var: 0.746671736240387
        vf_loss: 10.896407127380371
    load_time_ms: 34.111
    num_steps_sampled: 453600
    num_steps_trained: 450000
    sample_time_ms: 1618193.776
    update_time_ms: 0.006
  iterations_since_restore: 18
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.400310444436745
    mean_inference_ms: 33.87931165286665
    mean_processing_ms: 8.73924355813634
  time_since_restore: 21669.10858440399
  time_this_iter_s: 1152.2952711582184
  time_total_s: 21669.10858440399
  timestamp: 1563174531
  timesteps_since_restore: 453600
  timesteps_this_iter: 25200
  timesteps_total: 453600
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 21669 s, 18 iter, 453600 ts, 6.5 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_09-33-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.960738576738997
  episode_reward_mean: 9.638626130139
  episode_reward_min: -23.018217420236752
  episodes_this_iter: 168
  episodes_total: 3192
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 96262.696
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6337857246398926
        kl: 0.008568878285586834
        policy_loss: -0.012010612525045872
        total_loss: 4.886789798736572
        vf_explained_var: 0.8648930191993713
        vf_loss: 4.897729396820068
    load_time_ms: 33.697
    num_steps_sampled: 478800
    num_steps_trained: 475000
    sample_time_ms: 1463127.36
    update_time_ms: 0.007
  iterations_since_restore: 19
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.6623880256363894
    mean_inference_ms: 33.77396633786261
    mean_processing_ms: 8.88829158900228
  time_since_restore: 23119.620599269867
  time_this_iter_s: 1450.5120148658752
  time_total_s: 23119.620599269867
  timestamp: 1563175982
  timesteps_since_restore: 478800
  timesteps_this_iter: 25200
  timesteps_total: 478800
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 23119 s, 19 iter, 478800 ts, 9.64 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_09-56-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.531137631197737
  episode_reward_mean: 8.175448433719545
  episode_reward_min: -12.961868734456218
  episodes_this_iter: 168
  episodes_total: 3360
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 111937.52
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.621230363845825
        kl: 0.008854447863996029
        policy_loss: -0.012567729689180851
        total_loss: 4.006645679473877
        vf_explained_var: 0.8866759538650513
        vf_loss: 4.018106460571289
    load_time_ms: 35.002
    num_steps_sampled: 504000
    num_steps_trained: 500000
    sample_time_ms: 1300838.696
    update_time_ms: 0.007
  iterations_since_restore: 20
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.8835604264012256
    mean_inference_ms: 33.62932461518581
    mean_processing_ms: 8.978339899207414
  time_since_restore: 24540.88051223755
  time_this_iter_s: 1421.2599129676819
  time_total_s: 24540.88051223755
  timestamp: 1563177403
  timesteps_since_restore: 504000
  timesteps_this_iter: 25200
  timesteps_total: 504000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 24540 s, 20 iter, 504000 ts, 8.18 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_10-17-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.31516330499679
  episode_reward_mean: 8.58043352496961
  episode_reward_min: -15.133978597626555
  episodes_this_iter: 168
  episodes_total: 3528
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 124028.69
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6020331382751465
        kl: 0.008765589445829391
        policy_loss: -0.010988663882017136
        total_loss: 3.644871234893799
        vf_explained_var: 0.8853117227554321
        vf_loss: 3.654763698577881
    load_time_ms: 35.84
    num_steps_sampled: 529200
    num_steps_trained: 525000
    sample_time_ms: 1389765.466
    update_time_ms: 0.007
  iterations_since_restore: 21
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.043119936316408
    mean_inference_ms: 33.31569934280054
    mean_processing_ms: 9.02110704612238
  time_since_restore: 25788.554109096527
  time_this_iter_s: 1247.6735968589783
  time_total_s: 25788.554109096527
  timestamp: 1563178652
  timesteps_since_restore: 529200
  timesteps_this_iter: 25200
  timesteps_total: 529200
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=58925], 25788 s, 21 iter, 529200 ts, 8.58 rew

Result for PPO_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-15_10-40-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.35942308693406
  episode_reward_mean: 11.185091052556558
  episode_reward_min: -15.998926104926324
  episodes_this_iter: 168
  episodes_total: 3696
  experiment_id: 2d9bd4e793bb40209594a7bc06a3a988
  hostname: KayidmacOS
  info:
    grad_time_ms: 136373.332
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5914394855499268
        kl: 0.00825320091098547
        policy_loss: -0.010616514831781387
        total_loss: 2.819542646408081
        vf_explained_var: 0.920086145401001
        vf_loss: 2.829127550125122
    load_time_ms: 36.139
    num_steps_sampled: 554400
    num_steps_trained: 550000
    sample_time_ms: 888397.149
    update_time_ms: 0.007
  iterations_since_restore: 22
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 58925
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.210797626794446
    mean_inference_ms: 33.13049352127688
    mean_processing_ms: 9.07256951402852
  time_since_restore: 27147.109546899796
  time_this_iter_s: 1358.5554378032684
  time_total_s: 27147.109546899796
  timestamp: 1563180011
  timesteps_since_restore: 554400
  timesteps_this_iter: 25200
  timesteps_total: 554400
  training_iteration: 22
  2019-07-15 13:11:27,635	WARNING util.py:64 -- The `process_trial` operation took 0.11215782165527344 seconds to complete, which may be a performance bottleneck.
