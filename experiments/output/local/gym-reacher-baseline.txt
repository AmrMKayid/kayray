2019-07-22 00:41:07,968	WARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
2019-07-22 00:41:07,970	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-22_00-41-07_969353_62465/logs.
2019-07-22 00:41:08,083	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-22 00:41:08,218	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-22 00:41:08,223	INFO services.py:806 -- Starting Redis shard with 1.72 GB max memory.
2019-07-22 00:41:08,271	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-22_00-41-07_969353_62465/logs.
2019-07-22 00:41:08,275	INFO services.py:1446 -- Starting the Plasma object store with 2.58 GB memory using /tmp.
2019-07-22 00:41:08,889	INFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()
2019-07-22 00:41:08,889	INFO tune.py:233 -- Starting a new experiment.
2019-07-22 00:41:08,929	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
[32m [     0.81595s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.81652s,  INFO] Experiment configs: 
 {
  "gym-reacher-ppo-baseline": {
    "env": "RoboschoolReacher-v1",
    "run": "PPO",
    "local_dir": "~/kayray_results/local",
    "checkpoint_freq": 100,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 21,
      "training_iteration": 500
    },
    "config": {
      "env_config": {
        "env_type": "openai"
      },
      "gamma": 0.995,
      "kl_coeff": 1.0,
      "num_sgd_iter": 20,
      "lr": 0.0001,
      "sgd_minibatch_size": 1000,
      "train_batch_size": 25000,
      "model": {
        "free_log_std": true
      },
      "num_gpus": 0,
      "num_workers": 3,
      "batch_mode": "complete_episodes",
      "observation_filter": "MeanStdFilter"
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.5/8.6 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 4/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.5/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/local/gym-reacher-ppo-baseline
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING

[2m[36m(pid=62484)[0m [32m [     0.07272s,  INFO] TimeLimit:
[2m[36m(pid=62484)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=62484)[0m - action_space = Box(2,)
[2m[36m(pid=62484)[0m - observation_space = Box(9,)
[2m[36m(pid=62484)[0m - reward_range = (-inf, inf)
[2m[36m(pid=62484)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=62484)[0m - _max_episode_steps = 150
[2m[36m(pid=62484)[0m - _elapsed_steps = None [0m
[2m[36m(pid=62484)[0m 2019-07-22 00:41:14,475	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=62484)[0m 2019-07-22 00:41:14,497	INFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=62484)[0m 2019-07-22 00:41:14.505255: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=62484)[0m 2019-07-22 00:41:14,773	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=62484)[0m 
[2m[36m(pid=62484)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62484)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=62484)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62484)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=62484)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=62484)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=62484)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=62484)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=62484)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62484)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62484)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62484)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=62484)[0m 
[2m[36m(pid=62484)[0m /Users/amrmkayid/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=62484)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=62484)[0m 2019-07-22 00:41:16,348	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x1dd676e48>}
[2m[36m(pid=62484)[0m 2019-07-22 00:41:16,348	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x1dd1fecf8>}
[2m[36m(pid=62484)[0m 2019-07-22 00:41:16,350	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=62484)[0m 2019-07-22 00:41:16,401	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']
[2m[36m(pid=62530)[0m [32m [     0.10096s,  INFO] TimeLimit:
[2m[36m(pid=62530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=62530)[0m - action_space = Box(2,)
[2m[36m(pid=62530)[0m - observation_space = Box(9,)
[2m[36m(pid=62530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=62530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=62530)[0m - _max_episode_steps = 150
[2m[36m(pid=62530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=62531)[0m [32m [     0.10006s,  INFO] TimeLimit:
[2m[36m(pid=62531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=62531)[0m - action_space = Box(2,)
[2m[36m(pid=62531)[0m - observation_space = Box(9,)
[2m[36m(pid=62531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=62531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=62531)[0m - _max_episode_steps = 150
[2m[36m(pid=62531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=62531)[0m 2019-07-22 00:41:23,435	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=62531)[0m 2019-07-22 00:41:23.436650: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=62530)[0m 2019-07-22 00:41:23,440	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=62530)[0m 2019-07-22 00:41:23.440790: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=62533)[0m 2019-07-22 00:41:23,435	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=62533)[0m 2019-07-22 00:41:23.436568: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=62533)[0m [32m [     0.09984s,  INFO] TimeLimit:
[2m[36m(pid=62533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=62533)[0m - action_space = Box(2,)
[2m[36m(pid=62533)[0m - observation_space = Box(9,)
[2m[36m(pid=62533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=62533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=62533)[0m - _max_episode_steps = 150
[2m[36m(pid=62533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=62533)[0m 2019-07-22 00:41:23,776	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=62533)[0m 
[2m[36m(pid=62533)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62533)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=62533)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=62533)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=62533)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
