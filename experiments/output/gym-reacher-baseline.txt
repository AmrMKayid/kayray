2019-07-14 01:29:53,571	WARNING worker.py:1337 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
2019-07-14 01:29:53,573	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-14_01-29-53_571454_20331/logs.
2019-07-14 01:29:53,680	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:31161 to respond...
2019-07-14 01:29:53,797	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:18695 to respond...
2019-07-14 01:29:53,800	INFO services.py:806 -- Starting Redis shard with 1.72 GB max memory.
2019-07-14 01:29:53,836	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-14_01-29-53_571454_20331/logs.
2019-07-14 01:29:53,843	INFO services.py:1446 -- Starting the Plasma object store with 2.58 GB memory using /tmp.
2019-07-14 01:29:54,038	INFO tune.py:65 -- Did not find checkpoint file in /Users/amrmkayid/kayray_results/gym-reacher-ppo-baseline.
2019-07-14 01:29:54,039	INFO tune.py:233 -- Starting a new experiment.
2019-07-14 01:29:55,813	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-14 01:29:58,147	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=20353, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 106, in _init
    self.config["num_workers"])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 483, in _make_workers
    logdir=self.logdir)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 214, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 253, in __init__
    self.env = _validate_env(env_creator(env_context))
  File "train.py", line 145, in <lambda>
    lambda config: make_unity_env(env_name=env_name))
  File "/Users/amrmkayid/Desktop/kayray/kayray/envs/unity.py", line 14, in make_unity_env
    env = UnityEnv(environment_filename=env_name, worker_id=worker_id, no_graphics=no_graphics, multiagent=multiagent) #, use_visual=True)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/gym_unity/envs/unity_env.py", line 51, in __init__
    environment_filename, worker_id, no_graphics=no_graphics
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 58, in __init__
    self.executable_launcher(file_name, docker_training, no_graphics)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 172, in executable_launcher
    .format(true_filename))
mlagents.envs.exception.UnityEnvironmentException: Couldn't launch the RoboschoolReacher-v1 environment. Provided filename does not match any environments.

2019-07-14 01:29:58,150	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-14 01:29:58,153	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-14 01:29:58,159	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-14 01:30:00,470	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=20352, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 106, in _init
    self.config["num_workers"])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 483, in _make_workers
    logdir=self.logdir)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 214, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 253, in __init__
    self.env = _validate_env(env_creator(env_context))
  File "train.py", line 145, in <lambda>
    lambda config: make_unity_env(env_name=env_name))
  File "/Users/amrmkayid/Desktop/kayray/kayray/envs/unity.py", line 14, in make_unity_env
    env = UnityEnv(environment_filename=env_name, worker_id=worker_id, no_graphics=no_graphics, multiagent=multiagent) #, use_visual=True)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/gym_unity/envs/unity_env.py", line 51, in __init__
    environment_filename, worker_id, no_graphics=no_graphics
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 58, in __init__
    self.executable_launcher(file_name, docker_training, no_graphics)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 172, in executable_launcher
    .format(true_filename))
mlagents.envs.exception.UnityEnvironmentException: Couldn't launch the RoboschoolReacher-v1 environment. Provided filename does not match any environments.

2019-07-14 01:30:00,471	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-14 01:30:00,474	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-14 01:30:00,478	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-14 01:30:02,797	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=20354, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 106, in _init
    self.config["num_workers"])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 483, in _make_workers
    logdir=self.logdir)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 214, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 253, in __init__
    self.env = _validate_env(env_creator(env_context))
  File "train.py", line 145, in <lambda>
    lambda config: make_unity_env(env_name=env_name))
  File "/Users/amrmkayid/Desktop/kayray/kayray/envs/unity.py", line 14, in make_unity_env
    env = UnityEnv(environment_filename=env_name, worker_id=worker_id, no_graphics=no_graphics, multiagent=multiagent) #, use_visual=True)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/gym_unity/envs/unity_env.py", line 51, in __init__
    environment_filename, worker_id, no_graphics=no_graphics
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 58, in __init__
    self.executable_launcher(file_name, docker_training, no_graphics)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 172, in executable_launcher
    .format(true_filename))
mlagents.envs.exception.UnityEnvironmentException: Couldn't launch the RoboschoolReacher-v1 environment. Provided filename does not match any environments.

2019-07-14 01:30:02,798	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-14 01:30:02,801	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-14 01:30:02,805	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-14 01:30:05,111	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_worker[39m (pid=20351, host=KayidmacOS)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 87, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 323, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/trainable.py", line 87, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 438, in _setup
    self._init(self.config, self.env_creator)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 106, in _init
    self.config["num_workers"])
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 483, in _make_workers
    logdir=self.logdir)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 214, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 253, in __init__
    self.env = _validate_env(env_creator(env_context))
  File "train.py", line 145, in <lambda>
    lambda config: make_unity_env(env_name=env_name))
  File "/Users/amrmkayid/Desktop/kayray/kayray/envs/unity.py", line 14, in make_unity_env
    env = UnityEnv(environment_filename=env_name, worker_id=worker_id, no_graphics=no_graphics, multiagent=multiagent) #, use_visual=True)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/gym_unity/envs/unity_env.py", line 51, in __init__
    environment_filename, worker_id, no_graphics=no_graphics
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 58, in __init__
    self.executable_launcher(file_name, docker_training, no_graphics)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/mlagents/envs/environment.py", line 172, in executable_launcher
    .format(true_filename))
mlagents.envs.exception.UnityEnvironmentException: Couldn't launch the RoboschoolReacher-v1 environment. Provided filename does not match any environments.

2019-07-14 01:30:05,113	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[32m [     0.03789s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.03848s,  INFO] Experiment configs: 
 {
  "gym-reacher-ppo-baseline": {
    "env": "RoboschoolReacher-v1",
    "run": "PPO",
    "local_dir": "~/kayray_results",
    "checkpoint_freq": 100,
    "checkpoint_at_end": true,
    "stop": {
      "time_total_s": 3600,
      "episode_reward_mean": 21,
      "training_iteration": 1000
    },
    "config": {
      "gamma": 0.995,
      "kl_coeff": 1.0,
      "num_sgd_iter": 20,
      "lr": 0.0001,
      "sgd_minibatch_size": 1000,
      "train_batch_size": 25000,
      "model": {
        "free_log_std": true
      },
      "batch_mode": "complete_episodes",
      "observation_filter": "MeanStdFilter"
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.7/8.6 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 3/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ppo-baseline
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING

[2m[36m(pid=20353)[0m [32m [     0.02975s,  INFO] Starting env: RoboschoolReacher-v1 [0m
[2m[36m(pid=20353)[0m 2019-07-14 01:29:58,110	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=20352)[0m 2019-07-14 01:30:00,451	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=20352)[0m [32m [     0.03163s,  INFO] Starting env: RoboschoolReacher-v1 [0m
[2m[36m(pid=20354)[0m [32m [     0.02680s,  INFO] Starting env: RoboschoolReacher-v1 [0m
[2m[36m(pid=20354)[0m 2019-07-14 01:30:02,782	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 3/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ppo-baseline
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /Users/amrmkayid/kayray_results/gym-reacher-ppo-baseline/PPO_RoboschoolReacher-v1_0_2019-07-14_01-29-55an1j1fx5/error_2019-07-14_01-30-02.txt

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ppo-baseline
Number of trials: 1 ({'ERROR': 1})
ERROR trials:
 - PPO_RoboschoolReacher-v1_0:	ERROR, 4 failures: /Users/amrmkayid/kayray_results/gym-reacher-ppo-baseline/PPO_RoboschoolReacher-v1_0_2019-07-14_01-29-55an1j1fx5/error_2019-07-14_01-30-05.txt

Traceback (most recent call last):
  File "train.py", line 181, in <module>
    run(args, parser, dot_dict)
  File "train.py", line 170, in run
    resume=args.resume)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/tune.py", line 333, in run_experiments
    raise_on_failed_trial=raise_on_failed_trial)
  File "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.6/site-packages/ray/tune/tune.py", line 273, in run
    raise TuneError("Trials did not complete", errored_trials)
ray.tune.error.TuneError: ('Trials did not complete', [PPO_RoboschoolReacher-v1_0])
[2m[36m(pid=20351)[0m [32m [     0.03134s,  INFO] Starting env: RoboschoolReacher-v1 [0m
[2m[36m(pid=20351)[0m 2019-07-14 01:30:05,090	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
