2019-07-24 03:40:46,251	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-24_03-40-46_251624_1506/logs.
2019-07-24 03:40:46,357	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-24 03:40:46,470	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-24 03:40:46,475	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-24 03:40:46,507	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-24_03-40-46_251624_1506/logs.
2019-07-24 03:40:46,508	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-24 03:40:46,649	INFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()
2019-07-24 03:40:46,649	INFO tune.py:233 -- Starting a new experiment.
2019-07-24 03:40:46,660	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-24 03:40:46,790	WARNING util.py:64 -- The `start_trial` operation took 0.13431668281555176 seconds to complete, which may be a performance bottleneck.
[32m [     0.20857s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.20888s,  INFO] Experiment configs: 
 {
  "gym-reacher-impala": {
    "env": "RoboschoolReacher-v1",
    "run": "IMPALA",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 50,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 21,
      "timesteps_total": 10000000
    },
    "config": {
      "env_config": {
        "env_type": "openai"
      },
      "sample_batch_size": 50,
      "train_batch_size": 500,
      "lr": 1e-05,
      "num_gpus": 1,
      "num_workers": 11,
      "num_envs_per_worker": 16
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 1.1/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 1.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING

[2m[36m(pid=1565)[0m [32m [     0.01539s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m 2019-07-24 03:40:48.569843: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1565)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1565)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1565)[0m 2019-07-24 03:40:49,131	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.impala.vtrace_policy.VTraceTFPolicy object at 0x7fd6547c5d68>}
[2m[36m(pid=1565)[0m 2019-07-24 03:40:49,131	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd654de0908>}
[2m[36m(pid=1565)[0m 2019-07-24 03:40:49,131	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fd654dd7b38>}
[2m[36m(pid=1565)[0m [32m [     0.57873s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.57913s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.57950s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.57987s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58024s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58067s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58115s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58155s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58193s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58232s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58271s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58308s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58346s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58383s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m [32m [     0.58420s,  INFO] TimeLimit:
[2m[36m(pid=1565)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1565)[0m - action_space = Box(2,)
[2m[36m(pid=1565)[0m - observation_space = Box(9,)
[2m[36m(pid=1565)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1565)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1565)[0m - _max_episode_steps = 150
[2m[36m(pid=1565)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     0.03516s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m 2019-07-24 03:40:51,492	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1763)[0m 2019-07-24 03:40:51.492964: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1765)[0m [32m [     0.03593s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m 2019-07-24 03:40:51,476	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1765)[0m 2019-07-24 03:40:51.477195: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1555)[0m [32m [     0.03671s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m 2019-07-24 03:40:51,545	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1555)[0m 2019-07-24 03:40:51.546431: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1560)[0m [32m [     0.03467s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m 2019-07-24 03:40:51,567	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1560)[0m 2019-07-24 03:40:51.568154: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1564)[0m [32m [     0.03753s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m 2019-07-24 03:40:51,552	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1564)[0m 2019-07-24 03:40:51.553357: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1562)[0m 2019-07-24 03:40:51,606	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1562)[0m 2019-07-24 03:40:51.607112: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1562)[0m [32m [     0.03625s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m 2019-07-24 03:40:51,591	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1563)[0m 2019-07-24 03:40:51.592264: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1563)[0m [32m [     0.03508s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     0.03702s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m 2019-07-24 03:40:51,632	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1556)[0m 2019-07-24 03:40:51.632831: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1561)[0m 2019-07-24 03:40:51,646	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1561)[0m 2019-07-24 03:40:51.647411: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1561)[0m [32m [     0.03581s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m 2019-07-24 03:40:51,671	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1558)[0m 2019-07-24 03:40:51.671671: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1558)[0m [32m [     0.02955s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     0.03813s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m 2019-07-24 03:40:51,713	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=1557)[0m 2019-07-24 03:40:51.714129: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=1765)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1765)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1763)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1763)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1563)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1563)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1555)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1555)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1560)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1560)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1564)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1564)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1562)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1562)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1556)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1556)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1561)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1561)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1558)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1558)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1557)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=1557)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=1765)[0m [32m [     1.09118s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09178s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09250s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09321s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09382s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09458s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09519s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09576s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09632s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09686s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09741s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09796s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09851s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09908s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1765)[0m [32m [     1.09964s,  INFO] TimeLimit:
[2m[36m(pid=1765)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1765)[0m - action_space = Box(2,)
[2m[36m(pid=1765)[0m - observation_space = Box(9,)
[2m[36m(pid=1765)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1765)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1765)[0m - _max_episode_steps = 150
[2m[36m(pid=1765)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39309s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39403s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39487s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39580s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39655s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39735s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39818s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39900s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.39981s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.40062s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.40141s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.40223s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.40301s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.40389s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m [32m [     1.40464s,  INFO] TimeLimit:
[2m[36m(pid=1763)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1763)[0m - action_space = Box(2,)
[2m[36m(pid=1763)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1763)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1763)[0m - _max_episode_steps = 150
[2m[36m(pid=1763)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35299s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35389s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35478s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35576s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35664s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35751s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m 2019-07-24 03:40:52,913	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=1563)[0m [32m [     1.35849s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.35936s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36021s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36106s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36194s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36280s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36366s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36455s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1563)[0m [32m [     1.36544s,  INFO] TimeLimit:
[2m[36m(pid=1563)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1563)[0m - action_space = Box(2,)
[2m[36m(pid=1563)[0m - observation_space = Box(9,)
[2m[36m(pid=1563)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1563)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1563)[0m - _max_episode_steps = 150
[2m[36m(pid=1563)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.38874s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.38934s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39003s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39080s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39146s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39205s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39275s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39332s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39394s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39464s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39527s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39585s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39654s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m [32m [     1.43219s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43311s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43401s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43496s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43582s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43669s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43768s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43859s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.43951s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.44044s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.44133s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.44219s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.44303s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m [32m [     1.42908s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43005s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43095s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43203s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43293s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43399s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43498s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43584s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43676s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43764s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43850s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.43932s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.44016s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m [32m [     1.41217s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41307s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41393s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41519s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41618s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41711s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41801s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41887s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.41975s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.42063s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.42150s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.42234s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.42328s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39708s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1562)[0m [32m [     1.39775s,  INFO] TimeLimit:
[2m[36m(pid=1562)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1562)[0m - action_space = Box(2,)
[2m[36m(pid=1562)[0m - observation_space = Box(9,)
[2m[36m(pid=1562)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1562)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1562)[0m - _max_episode_steps = 150
[2m[36m(pid=1562)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.44389s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1555)[0m [32m [     1.44478s,  INFO] TimeLimit:
[2m[36m(pid=1555)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1555)[0m - action_space = Box(2,)
[2m[36m(pid=1555)[0m - observation_space = Box(9,)
[2m[36m(pid=1555)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1555)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1555)[0m - _max_episode_steps = 150
[2m[36m(pid=1555)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.44102s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1560)[0m [32m [     1.44195s,  INFO] TimeLimit:
[2m[36m(pid=1560)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1560)[0m - action_space = Box(2,)
[2m[36m(pid=1560)[0m - observation_space = Box(9,)
[2m[36m(pid=1560)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1560)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1560)[0m - _max_episode_steps = 150
[2m[36m(pid=1560)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.42426s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1564)[0m [32m [     1.42529s,  INFO] TimeLimit:
[2m[36m(pid=1564)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1564)[0m - action_space = Box(2,)
[2m[36m(pid=1564)[0m - observation_space = Box(9,)
[2m[36m(pid=1564)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1564)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1564)[0m - _max_episode_steps = 150
[2m[36m(pid=1564)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1565)[0m 2019-07-24 03:40:52,998	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=1565)[0m 
[2m[36m(pid=1565)[0m { 'data': { 'action_prob': np.ndarray((800,), dtype=float32, min=0.0, max=0.16, mean=0.082),
[2m[36m(pid=1565)[0m             'actions': np.ndarray((800, 2), dtype=float32, min=-2.84, max=3.649, mean=0.006),
[2m[36m(pid=1565)[0m             'agent_index': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1565)[0m             'behaviour_logits': np.ndarray((800, 4), dtype=float32, min=-0.01, max=0.008, mean=-0.001),
[2m[36m(pid=1565)[0m             'dones': np.ndarray((800,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1565)[0m             'eps_id': np.ndarray((800,), dtype=int64, min=127981059.0, max=1828383422.0, mean=929215053.688),
[2m[36m(pid=1565)[0m             'infos': np.ndarray((800,), dtype=object, head={}),
[2m[36m(pid=1565)[0m             'obs': np.ndarray((800, 9), dtype=float32, min=-2.994, max=3.424, mean=-0.015),
[2m[36m(pid=1565)[0m             'prev_actions': np.ndarray((800, 2), dtype=float32, min=-2.84, max=3.649, mean=0.005),
[2m[36m(pid=1565)[0m             'prev_rewards': np.ndarray((800,), dtype=float32, min=-3.319, max=3.132, mean=-0.082),
[2m[36m(pid=1565)[0m             'rewards': np.ndarray((800,), dtype=float32, min=-3.823, max=3.132, mean=-0.087),
[2m[36m(pid=1565)[0m             't': np.ndarray((800,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=1565)[0m             'unroll_id': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=1565)[0m   'type': 'SampleBatch'}
[2m[36m(pid=1565)[0m 
[2m[36m(pid=1565)[0m 2019-07-24 03:40:52,998	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=1556)[0m [32m [     1.42372s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.42474s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.42578s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.42695s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.42785s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.42876s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.42970s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43056s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43159s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43255s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43359s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43457s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43577s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43688s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1556)[0m [32m [     1.43781s,  INFO] TimeLimit:
[2m[36m(pid=1556)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1556)[0m - action_space = Box(2,)
[2m[36m(pid=1556)[0m - observation_space = Box(9,)
[2m[36m(pid=1556)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1556)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1556)[0m - _max_episode_steps = 150
[2m[36m(pid=1556)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.44880s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.44989s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45088s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45207s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45302s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45422s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45516s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45609s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45701s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45792s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45881s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.45984s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.46085s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m [32m [     1.42922s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43027s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43130s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43248s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43330s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43419s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43509s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43594s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43681s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43775s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43864s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.43952s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.44041s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,067	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.978, max=0.209, mean=-0.115)},
[2m[36m(pid=1763)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.106, max=0.949, mean=0.251)},
[2m[36m(pid=1763)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.725, max=0.252, mean=-0.243)},
[2m[36m(pid=1763)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.906, max=0.159, mean=-0.216)},
[2m[36m(pid=1763)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.426, max=0.967, mean=0.028)},
[2m[36m(pid=1763)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.096, max=0.906, mean=0.169)},
[2m[36m(pid=1763)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.681, max=0.732, mean=0.02)},
[2m[36m(pid=1763)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.859, max=0.325, mean=-0.144)},
[2m[36m(pid=1763)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.161, max=0.955, mean=0.205)},
[2m[36m(pid=1763)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.995, max=0.31, mean=-0.116)},
[2m[36m(pid=1763)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.257, max=0.991, mean=0.126)},
[2m[36m(pid=1763)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.807, max=0.195, mean=-0.228)},
[2m[36m(pid=1763)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.06, max=0.987, mean=0.146)},
[2m[36m(pid=1763)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.951, max=0.871, mean=-0.04)},
[2m[36m(pid=1763)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.989, max=0.261, mean=-0.2)},
[2m[36m(pid=1763)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.985, max=0.709, mean=-0.003)}}
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,067	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=1763)[0m   1: {'agent0': None},
[2m[36m(pid=1763)[0m   2: {'agent0': None},
[2m[36m(pid=1763)[0m   3: {'agent0': None},
[2m[36m(pid=1763)[0m   4: {'agent0': None},
[2m[36m(pid=1763)[0m   5: {'agent0': None},
[2m[36m(pid=1763)[0m   6: {'agent0': None},
[2m[36m(pid=1763)[0m   7: {'agent0': None},
[2m[36m(pid=1763)[0m   8: {'agent0': None},
[2m[36m(pid=1763)[0m   9: {'agent0': None},
[2m[36m(pid=1763)[0m   10: {'agent0': None},
[2m[36m(pid=1763)[0m   11: {'agent0': None},
[2m[36m(pid=1763)[0m   12: {'agent0': None},
[2m[36m(pid=1763)[0m   13: {'agent0': None},
[2m[36m(pid=1763)[0m   14: {'agent0': None},
[2m[36m(pid=1763)[0m   15: {'agent0': None}}
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,068	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.978, max=0.209, mean=-0.115)
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,068	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.978, max=0.209, mean=-0.115)
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,078	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1763)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 0,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.978, max=0.209, mean=-0.115),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 1,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.106, max=0.949, mean=0.251),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 2,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.725, max=0.252, mean=-0.243),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 3,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.906, max=0.159, mean=-0.216),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 4,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.426, max=0.967, mean=0.028),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 5,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.096, max=0.906, mean=0.169),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 6,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.681, max=0.732, mean=0.02),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 7,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.859, max=0.325, mean=-0.144),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.46183s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1561)[0m [32m [     1.46281s,  INFO] TimeLimit:
[2m[36m(pid=1561)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1561)[0m - action_space = Box(2,)
[2m[36m(pid=1561)[0m - observation_space = Box(9,)
[2m[36m(pid=1561)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1561)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1561)[0m - _max_episode_steps = 150
[2m[36m(pid=1561)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.44136s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1558)[0m [32m [     1.44220s,  INFO] TimeLimit:
[2m[36m(pid=1558)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1558)[0m - action_space = Box(2,)
[2m[36m(pid=1558)[0m - observation_space = Box(9,)
[2m[36m(pid=1558)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1558)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1558)[0m - _max_episode_steps = 150
[2m[36m(pid=1558)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 8,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.161, max=0.955, mean=0.205),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 9,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.995, max=0.31, mean=-0.116),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 10,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.257, max=0.991, mean=0.126),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 11,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.807, max=0.195, mean=-0.228),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 12,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.06, max=0.987, mean=0.146),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 13,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.951, max=0.871, mean=-0.04),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 14,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.989, max=0.261, mean=-0.2),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=1763)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=1763)[0m                                   'env_id': 15,
[2m[36m(pid=1763)[0m                                   'info': None,
[2m[36m(pid=1763)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.985, max=0.709, mean=-0.003),
[2m[36m(pid=1763)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=1763)[0m                                   'rnn_state': []},
[2m[36m(pid=1763)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,079	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=1557)[0m [32m [     1.46020s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46138s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46246s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46365s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46470s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46567s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46678s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,109	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1763)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-2.323, max=1.827, mean=-0.139),
[2m[36m(pid=1763)[0m                       [],
[2m[36m(pid=1763)[0m                       { 'action_prob': np.ndarray((16,), dtype=float32, min=0.01, max=0.155, mean=0.076),
[2m[36m(pid=1763)[0m                         'behaviour_logits': np.ndarray((16, 4), dtype=float32, min=-0.004, max=0.005, mean=-0.0)})}
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1557)[0m [32m [     1.46778s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46880s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.46973s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.47071s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.47180s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.47283s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.47380s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1557)[0m [32m [     1.47473s,  INFO] TimeLimit:
[2m[36m(pid=1557)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=1557)[0m - action_space = Box(2,)
[2m[36m(pid=1557)[0m - observation_space = Box(9,)
[2m[36m(pid=1557)[0m - reward_range = (-inf, inf)
[2m[36m(pid=1557)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=1557)[0m - _max_episode_steps = 150
[2m[36m(pid=1557)[0m - _elapsed_steps = None [0m
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,538	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1763)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.0, max=0.158, mean=0.071),
[2m[36m(pid=1763)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-3.384, max=2.651, mean=0.178),
[2m[36m(pid=1763)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                         'behaviour_logits': np.ndarray((50, 4), dtype=float32, min=-0.008, max=0.009, mean=0.001),
[2m[36m(pid=1763)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=1320985722.0, max=1320985722.0, mean=1320985722.0),
[2m[36m(pid=1763)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=1763)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-2.033, max=2.677, mean=0.174),
[2m[36m(pid=1763)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-3.384, max=2.651, mean=0.169),
[2m[36m(pid=1763)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-1.742, max=1.517, mean=-0.041),
[2m[36m(pid=1763)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-1.742, max=1.517, mean=-0.07),
[2m[36m(pid=1763)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=1763)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=1763)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1763)[0m 2019-07-24 03:40:53,566	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1763)[0m { 'data': { 'action_prob': np.ndarray((800,), dtype=float32, min=0.0, max=0.159, mean=0.078),
[2m[36m(pid=1763)[0m             'actions': np.ndarray((800, 2), dtype=float32, min=-4.02, max=3.717, mean=-0.015),
[2m[36m(pid=1763)[0m             'agent_index': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m             'behaviour_logits': np.ndarray((800, 4), dtype=float32, min=-0.009, max=0.009, mean=0.0),
[2m[36m(pid=1763)[0m             'dones': np.ndarray((800,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=1763)[0m             'eps_id': np.ndarray((800,), dtype=int64, min=8805.0, max=1778216101.0, mean=1019601180.125),
[2m[36m(pid=1763)[0m             'infos': np.ndarray((800,), dtype=object, head={}),
[2m[36m(pid=1763)[0m             'obs': np.ndarray((800, 9), dtype=float32, min=-3.359, max=4.136, mean=0.004),
[2m[36m(pid=1763)[0m             'prev_actions': np.ndarray((800, 2), dtype=float32, min=-4.02, max=3.717, mean=-0.016),
[2m[36m(pid=1763)[0m             'prev_rewards': np.ndarray((800,), dtype=float32, min=-3.526, max=2.969, mean=-0.086),
[2m[36m(pid=1763)[0m             'rewards': np.ndarray((800,), dtype=float32, min=-3.526, max=2.969, mean=-0.096),
[2m[36m(pid=1763)[0m             't': np.ndarray((800,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=1763)[0m             'unroll_id': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=1763)[0m   'type': 'SampleBatch'}
[2m[36m(pid=1763)[0m 
[2m[36m(pid=1565)[0m 2019-07-24 03:40:53,791	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=1565)[0m 
[2m[36m(pid=1565)[0m { 'learner_stats': { 'cur_lr': 9.999999747378752e-06,
[2m[36m(pid=1565)[0m                      'entropy': 2224.4258,
[2m[36m(pid=1565)[0m                      'grad_gnorm': 40.0,
[2m[36m(pid=1565)[0m                      'model': {},
[2m[36m(pid=1565)[0m                      'policy_loss': -2754.8525,
[2m[36m(pid=1565)[0m                      'var_gnorm': 22.64951,
[2m[36m(pid=1565)[0m                      'vf_explained_var': -0.008560061,
[2m[36m(pid=1565)[0m                      'vf_loss': 27649.576}}
[2m[36m(pid=1565)[0m 
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-40-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.52665191986555
  episode_reward_mean: -15.5841552413362
  episode_reward_min: -73.35316830247223
  episodes_this_iter: 896
  episodes_total: 896
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2224.649658203125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -5383.86181640625
      var_gnorm: 22.649425506591797
      vf_explained_var: -0.0002903938293457031
      vf_loss: 40562.6484375
    learner_queue:
      size_count: 161
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    num_steps_replayed: 0
    num_steps_sampled: 128800
    num_steps_trained: 128800
    num_weight_syncs: 161
    sample_throughput: 11980.656
    timing_breakdown:
      learner_dequeue_time_ms: 25.037
      learner_grad_time_ms: 11.811
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 38.358
    train_throughput: 11980.656
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.722041028463489
    mean_inference_ms: 1.5600261594787719
    mean_processing_ms: 1.7424430531713426
  time_since_restore: 10.592484951019287
  time_this_iter_s: 10.592484951019287
  time_total_s: 10.592484951019287
  timestamp: 1563932459
  timesteps_since_restore: 128800
  timesteps_this_iter: 128800
  timesteps_total: 128800
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 10 s, 1 iter, 128800 ts, -15.6 rew

[2m[36m(pid=1565)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
[2m[36m(pid=1565)[0m   out=out, **kwargs)
[2m[36m(pid=1565)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
[2m[36m(pid=1565)[0m   ret = ret.dtype.type(ret / rcount)
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-41-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.55404652854767
  episode_reward_mean: -14.786199829264444
  episode_reward_min: -66.46071574064871
  episodes_this_iter: 1600
  episodes_total: 2496
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2223.6162109375
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3092.672119140625
      var_gnorm: 22.649770736694336
      vf_explained_var: -0.0016647577285766602
      vf_loss: 35510.203125
    learner_queue:
      size_count: 461
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 368800
    num_steps_trained: 368800
    num_weight_syncs: 461
    sample_throughput: 22545.606
    timing_breakdown:
      learner_dequeue_time_ms: 22.937
      learner_grad_time_ms: 11.817
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 41.873
    train_throughput: 22545.607
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.549352556252189
    mean_inference_ms: 1.4360445692598198
    mean_processing_ms: 1.7124350739560117
  time_since_restore: 21.22965359687805
  time_this_iter_s: 10.637168645858765
  time_total_s: 21.22965359687805
  timestamp: 1563932470
  timesteps_since_restore: 368800
  timesteps_this_iter: 240000
  timesteps_total: 368800
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 21 s, 2 iter, 368800 ts, -14.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-41-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.50500822414858
  episode_reward_mean: -14.423356004189593
  episode_reward_min: -55.89347184670136
  episodes_this_iter: 1584
  episodes_total: 4080
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2224.57763671875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -5455.0625
      var_gnorm: 22.65010643005371
      vf_explained_var: -7.367134094238281e-05
      vf_loss: 34439.12890625
    learner_queue:
      size_count: 758
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 606400
    num_steps_trained: 606400
    num_weight_syncs: 758
    sample_throughput: 22451.241
    timing_breakdown:
      learner_dequeue_time_ms: 25.042
      learner_grad_time_ms: 12.358
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 36.844
    train_throughput: 22451.242
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.5157785527471095
    mean_inference_ms: 1.4026575974292093
    mean_processing_ms: 1.7179907237646277
  time_since_restore: 31.805662393569946
  time_this_iter_s: 10.576008796691895
  time_total_s: 31.805662393569946
  timestamp: 1563932481
  timesteps_since_restore: 606400
  timesteps_this_iter: 237600
  timesteps_total: 606400
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 31 s, 3 iter, 606400 ts, -14.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-41-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.942502547665864
  episode_reward_mean: -13.228135186897525
  episode_reward_min: -52.90031622522298
  episodes_this_iter: 1600
  episodes_total: 5680
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2224.330322265625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -9027.19140625
      var_gnorm: 22.650911331176758
      vf_explained_var: 0.0012763738632202148
      vf_loss: 27997.021484375
    learner_queue:
      size_count: 1056
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 844800
    num_steps_trained: 844800
    num_weight_syncs: 1056
    sample_throughput: 22384.305
    timing_breakdown:
      learner_dequeue_time_ms: 20.283
      learner_grad_time_ms: 9.877
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 47.86
    train_throughput: 22384.306
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.499371374287036
    mean_inference_ms: 1.3910833007768262
    mean_processing_ms: 1.722384527368423
  time_since_restore: 42.448967695236206
  time_this_iter_s: 10.64330530166626
  time_total_s: 42.448967695236206
  timestamp: 1563932491
  timesteps_since_restore: 844800
  timesteps_this_iter: 238400
  timesteps_total: 844800
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 42 s, 4 iter, 844800 ts, -13.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-41-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.170480182008237
  episode_reward_mean: -12.715862579285123
  episode_reward_min: -54.31255617679465
  episodes_this_iter: 1600
  episodes_total: 7280
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2222.212890625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -10540.1455078125
      var_gnorm: 22.65237808227539
      vf_explained_var: 0.0022240877151489258
      vf_loss: 44724.203125
    learner_queue:
      size_count: 1356
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1084800
    num_steps_trained: 1084800
    num_weight_syncs: 1356
    sample_throughput: 22483.585
    timing_breakdown:
      learner_dequeue_time_ms: 17.211
      learner_grad_time_ms: 11.826
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 38.614
    train_throughput: 22483.586
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.490641165129736
    mean_inference_ms: 1.3820483102320469
    mean_processing_ms: 1.7266244024364468
  time_since_restore: 53.11550259590149
  time_this_iter_s: 10.666534900665283
  time_total_s: 53.11550259590149
  timestamp: 1563932502
  timesteps_since_restore: 1084800
  timesteps_this_iter: 240000
  timesteps_total: 1084800
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 53 s, 5 iter, 1084800 ts, -12.7 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-41-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.94517118503814
  episode_reward_mean: -11.939005924258717
  episode_reward_min: -53.44190460108432
  episodes_this_iter: 1584
  episodes_total: 8864
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2222.086181640625
      grad_gnorm: 39.99999237060547
      model: {}
      policy_loss: -1639.7239990234375
      var_gnorm: 22.653789520263672
      vf_explained_var: -0.0038480758666992188
      vf_loss: 23714.65234375
    learner_queue:
      size_count: 1653
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 1322400
    num_steps_trained: 1322400
    num_weight_syncs: 1653
    sample_throughput: 22419.452
    timing_breakdown:
      learner_dequeue_time_ms: 23.764
      learner_grad_time_ms: 11.533
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 36.1
    train_throughput: 22419.452
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.484373583356029
    mean_inference_ms: 1.3778101049732787
    mean_processing_ms: 1.7283778304128152
  time_since_restore: 63.7061927318573
  time_this_iter_s: 10.59069013595581
  time_total_s: 63.7061927318573
  timestamp: 1563932513
  timesteps_since_restore: 1322400
  timesteps_this_iter: 237600
  timesteps_total: 1322400
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 63 s, 6 iter, 1322400 ts, -11.9 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-42-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.16976075339339
  episode_reward_mean: -11.712142240111563
  episode_reward_min: -55.70207064775232
  episodes_this_iter: 1584
  episodes_total: 10448
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2223.66796875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -1887.607666015625
      var_gnorm: 22.655813217163086
      vf_explained_var: 0.0037323832511901855
      vf_loss: 14499.736328125
    learner_queue:
      size_count: 1951
      size_mean: 0.04
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.19595917942265428
    num_steps_replayed: 0
    num_steps_sampled: 1560800
    num_steps_trained: 1560800
    num_weight_syncs: 1951
    sample_throughput: 22365.213
    timing_breakdown:
      learner_dequeue_time_ms: 24.441
      learner_grad_time_ms: 11.003
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 54.319
    train_throughput: 22365.214
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.480761831197109
    mean_inference_ms: 1.3764953974573035
    mean_processing_ms: 1.7301094769792067
  time_since_restore: 74.35863375663757
  time_this_iter_s: 10.652441024780273
  time_total_s: 74.35863375663757
  timestamp: 1563932523
  timesteps_since_restore: 1560800
  timesteps_this_iter: 238400
  timesteps_total: 1560800
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 74 s, 7 iter, 1560800 ts, -11.7 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-42-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.130203832390016
  episode_reward_mean: -11.479851138182212
  episode_reward_min: -58.02001949077815
  episodes_this_iter: 1600
  episodes_total: 12048
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2221.60498046875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -7801.0166015625
      var_gnorm: 22.658308029174805
      vf_explained_var: -3.2067298889160156e-05
      vf_loss: 25769.70703125
    learner_queue:
      size_count: 2248
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.417612260356422
    num_steps_replayed: 0
    num_steps_sampled: 1798400
    num_steps_trained: 1797600
    num_weight_syncs: 2248
    sample_throughput: 22402.807
    timing_breakdown:
      learner_dequeue_time_ms: 22.21
      learner_grad_time_ms: 12.058
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 48.618
    train_throughput: 22327.378
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.478309339975868
    mean_inference_ms: 1.3724090142726844
    mean_processing_ms: 1.729076475694647
  time_since_restore: 84.95766282081604
  time_this_iter_s: 10.599029064178467
  time_total_s: 84.95766282081604
  timestamp: 1563932534
  timesteps_since_restore: 1798400
  timesteps_this_iter: 237600
  timesteps_total: 1798400
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 84 s, 8 iter, 1798400 ts, -11.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-42-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.048158228173683
  episode_reward_mean: -11.099320833351806
  episode_reward_min: -51.99569883794488
  episodes_this_iter: 1584
  episodes_total: 13632
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2223.3466796875
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 1639.1849365234375
      var_gnorm: 22.661760330200195
      vf_explained_var: -0.003995776176452637
      vf_loss: 9841.169921875
    learner_queue:
      size_count: 2545
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2036000
    num_steps_trained: 2036000
    num_weight_syncs: 2545
    sample_throughput: 22222.624
    timing_breakdown:
      learner_dequeue_time_ms: 25.379
      learner_grad_time_ms: 11.017
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 48.29
    train_throughput: 22297.449
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.475902273148828
    mean_inference_ms: 1.3733849019817177
    mean_processing_ms: 1.7335802292097073
  time_since_restore: 95.64267063140869
  time_this_iter_s: 10.685007810592651
  time_total_s: 95.64267063140869
  timestamp: 1563932545
  timesteps_since_restore: 2036000
  timesteps_this_iter: 237600
  timesteps_total: 2036000
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 95 s, 9 iter, 2036000 ts, -11.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-42-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.36980146710064
  episode_reward_mean: -10.136492558643313
  episode_reward_min: -51.76025331710097
  episodes_this_iter: 1568
  episodes_total: 15200
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2224.88037109375
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: -4630.744140625
      var_gnorm: 22.665565490722656
      vf_explained_var: -0.0061103105545043945
      vf_loss: 10512.9169921875
    learner_queue:
      size_count: 2840
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2272000
    num_steps_trained: 2272000
    num_weight_syncs: 2840
    sample_throughput: 22346.866
    timing_breakdown:
      learner_dequeue_time_ms: 25.265
      learner_grad_time_ms: 11.611
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 39.395
    train_throughput: 22346.867
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.475904854723517
    mean_inference_ms: 1.3749561032520907
    mean_processing_ms: 1.731895397482069
  time_since_restore: 106.19643783569336
  time_this_iter_s: 10.553767204284668
  time_total_s: 106.19643783569336
  timestamp: 1563932555
  timesteps_since_restore: 2272000
  timesteps_this_iter: 236000
  timesteps_total: 2272000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 106 s, 10 iter, 2272000 ts, -10.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-42-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.385496401557177
  episode_reward_mean: -10.244324542121943
  episode_reward_min: -52.31553467986718
  episodes_this_iter: 1584
  episodes_total: 16784
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2216.922607421875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -2569.0634765625
      var_gnorm: 22.669729232788086
      vf_explained_var: -0.00033605098724365234
      vf_loss: 5727.5546875
    learner_queue:
      size_count: 3138
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 2510400
    num_steps_trained: 2510400
    num_weight_syncs: 3138
    sample_throughput: 22353.925
    timing_breakdown:
      learner_dequeue_time_ms: 26.89
      learner_grad_time_ms: 10.93
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 38.451
    train_throughput: 22353.926
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.476680659312639
    mean_inference_ms: 1.3722180627623035
    mean_processing_ms: 1.7320202564142535
  time_since_restore: 116.85423564910889
  time_this_iter_s: 10.657797813415527
  time_total_s: 116.85423564910889
  timestamp: 1563932566
  timesteps_since_restore: 2510400
  timesteps_this_iter: 238400
  timesteps_total: 2510400
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 116 s, 11 iter, 2510400 ts, -10.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-42-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.182355944045117
  episode_reward_mean: -10.05601301357856
  episode_reward_min: -46.08937487084938
  episodes_this_iter: 1584
  episodes_total: 18368
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2221.97802734375
      grad_gnorm: 40.0
      model: {}
      policy_loss: -2039.919189453125
      var_gnorm: 22.674022674560547
      vf_explained_var: 0.010302722454071045
      vf_loss: 5745.5546875
    learner_queue:
      size_count: 3436
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 2748800
    num_steps_trained: 2748800
    num_weight_syncs: 3436
    sample_throughput: 22279.289
    timing_breakdown:
      learner_dequeue_time_ms: 25.894
      learner_grad_time_ms: 10.306
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 52.02
    train_throughput: 22279.29
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.476675854647208
    mean_inference_ms: 1.3713996406704225
    mean_processing_ms: 1.7323805950596578
  time_since_restore: 127.54677820205688
  time_this_iter_s: 10.692542552947998
  time_total_s: 127.54677820205688
  timestamp: 1563932576
  timesteps_since_restore: 2748800
  timesteps_this_iter: 238400
  timesteps_total: 2748800
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 127 s, 12 iter, 2748800 ts, -10.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-43-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.272927145893973
  episode_reward_mean: -10.029987782984612
  episode_reward_min: -51.326498873543215
  episodes_this_iter: 1600
  episodes_total: 19968
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2223.64501953125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -910.746826171875
      var_gnorm: 22.67943000793457
      vf_explained_var: 0.016520142555236816
      vf_loss: 11430.896484375
    learner_queue:
      size_count: 3734
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    num_steps_replayed: 0
    num_steps_sampled: 2987200
    num_steps_trained: 2986400
    num_weight_syncs: 3734
    sample_throughput: 22361.768
    timing_breakdown:
      learner_dequeue_time_ms: 17.166
      learner_grad_time_ms: 10.608
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 38.574
    train_throughput: 22286.729
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4761224776932025
    mean_inference_ms: 1.3703081037967184
    mean_processing_ms: 1.7327582898564209
  time_since_restore: 138.2007806301117
  time_this_iter_s: 10.65400242805481
  time_total_s: 138.2007806301117
  timestamp: 1563932587
  timesteps_since_restore: 2987200
  timesteps_this_iter: 238400
  timesteps_total: 2987200
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 138 s, 13 iter, 2987200 ts, -10 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-43-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.19459198256709
  episode_reward_mean: -10.105926040396325
  episode_reward_min: -46.62647295752996
  episodes_this_iter: 1600
  episodes_total: 21568
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2213.66162109375
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -3088.546875
      var_gnorm: 22.684814453125
      vf_explained_var: 0.03635966777801514
      vf_loss: 4769.41015625
    learner_queue:
      size_count: 4031
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 3224800
    num_steps_trained: 3224800
    num_weight_syncs: 4031
    sample_throughput: 22308.498
    timing_breakdown:
      learner_dequeue_time_ms: 21.151
      learner_grad_time_ms: 11.663
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 32.881
    train_throughput: 22383.611
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.477159562164937
    mean_inference_ms: 1.3696426944450986
    mean_processing_ms: 1.7323883653840784
  time_since_restore: 148.84413838386536
  time_this_iter_s: 10.643357753753662
  time_total_s: 148.84413838386536
  timestamp: 1563932598
  timesteps_since_restore: 3224800
  timesteps_this_iter: 237600
  timesteps_total: 3224800
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 148 s, 14 iter, 3224800 ts, -10.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-43-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.4245636444869
  episode_reward_mean: -9.82496177221956
  episode_reward_min: -46.37893500588768
  episodes_this_iter: 1584
  episodes_total: 23152
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2220.36376953125
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -1660.4132080078125
      var_gnorm: 22.690959930419922
      vf_explained_var: 0.019719958305358887
      vf_loss: 8359.033203125
    learner_queue:
      size_count: 4328
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4000000000000001
    num_steps_replayed: 0
    num_steps_sampled: 3462400
    num_steps_trained: 3461600
    num_weight_syncs: 4328
    sample_throughput: 22333.52
    timing_breakdown:
      learner_dequeue_time_ms: 27.682
      learner_grad_time_ms: 10.859
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 37.361
    train_throughput: 22258.324
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.477281995531466
    mean_inference_ms: 1.3688971437397086
    mean_processing_ms: 1.7324672630951647
  time_since_restore: 159.4760513305664
  time_this_iter_s: 10.63191294670105
  time_total_s: 159.4760513305664
  timestamp: 1563932608
  timesteps_since_restore: 3462400
  timesteps_this_iter: 237600
  timesteps_total: 3462400
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 159 s, 15 iter, 3462400 ts, -9.82 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-43-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.73332859007694
  episode_reward_mean: -9.062853051818427
  episode_reward_min: -37.462061304428985
  episodes_this_iter: 1568
  episodes_total: 24720
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2216.485107421875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1348.9853515625
      var_gnorm: 22.698171615600586
      vf_explained_var: 0.02739429473876953
      vf_loss: 5790.0986328125
    learner_queue:
      size_count: 4624
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 3699200
    num_steps_trained: 3699200
    num_weight_syncs: 4624
    sample_throughput: 22298.941
    timing_breakdown:
      learner_dequeue_time_ms: 25.633
      learner_grad_time_ms: 10.236
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 36.844
    train_throughput: 22374.276
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4777313411862005
    mean_inference_ms: 1.3686275532393903
    mean_processing_ms: 1.73161495896123
  time_since_restore: 170.0883641242981
  time_this_iter_s: 10.61231279373169
  time_total_s: 170.0883641242981
  timestamp: 1563932619
  timesteps_since_restore: 3699200
  timesteps_this_iter: 236800
  timesteps_total: 3699200
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 170 s, 16 iter, 3699200 ts, -9.06 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-43-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.305817842471143
  episode_reward_mean: -9.54398446082977
  episode_reward_min: -47.2003443903336
  episodes_this_iter: 1584
  episodes_total: 26304
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2212.4453125
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -4379.966796875
      var_gnorm: 22.705942153930664
      vf_explained_var: 0.018505632877349854
      vf_loss: 7349.8818359375
    learner_queue:
      size_count: 4921
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 3936800
    num_steps_trained: 3936000
    num_weight_syncs: 4921
    sample_throughput: 22330.172
    timing_breakdown:
      learner_dequeue_time_ms: 22.624
      learner_grad_time_ms: 13.139
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 55.431
    train_throughput: 22254.987
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.477282037344393
    mean_inference_ms: 1.3679821723308851
    mean_processing_ms: 1.7322062629308947
  time_since_restore: 180.7215211391449
  time_this_iter_s: 10.633157014846802
  time_total_s: 180.7215211391449
  timestamp: 1563932630
  timesteps_since_restore: 3936800
  timesteps_this_iter: 237600
  timesteps_total: 3936800
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 180 s, 17 iter, 3936800 ts, -9.54 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-44-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.086765859299646
  episode_reward_mean: -9.298583398501437
  episode_reward_min: -44.42517153331856
  episodes_this_iter: 1584
  episodes_total: 27888
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2214.96728515625
      grad_gnorm: 39.99999237060547
      model: {}
      policy_loss: -4178.421875
      var_gnorm: 22.71514892578125
      vf_explained_var: 0.008907556533813477
      vf_loss: 8854.2421875
    learner_queue:
      size_count: 5218
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 4174400
    num_steps_trained: 4174400
    num_weight_syncs: 5218
    sample_throughput: 22298.212
    timing_breakdown:
      learner_dequeue_time_ms: 28.388
      learner_grad_time_ms: 10.938
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 40.046
    train_throughput: 22373.291
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.477330462033497
    mean_inference_ms: 1.367295170479163
    mean_processing_ms: 1.7325774356127772
  time_since_restore: 191.3701605796814
  time_this_iter_s: 10.648639440536499
  time_total_s: 191.3701605796814
  timestamp: 1563932640
  timesteps_since_restore: 4174400
  timesteps_this_iter: 237600
  timesteps_total: 4174400
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 191 s, 18 iter, 4174400 ts, -9.3 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-44-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.57072459580509
  episode_reward_mean: -9.390357698278128
  episode_reward_min: -45.289945214228666
  episodes_this_iter: 1584
  episodes_total: 29472
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2209.68310546875
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 3121.584716796875
      var_gnorm: 22.725807189941406
      vf_explained_var: 0.0028226375579833984
      vf_loss: 13263.66015625
    learner_queue:
      size_count: 5515
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 4412000
    num_steps_trained: 4411200
    num_weight_syncs: 5515
    sample_throughput: 22288.325
    timing_breakdown:
      learner_dequeue_time_ms: 22.808
      learner_grad_time_ms: 10.522
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 36.196
    train_throughput: 22213.281
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4768746848796495
    mean_inference_ms: 1.3673928355569502
    mean_processing_ms: 1.7321732104689134
  time_since_restore: 202.0232698917389
  time_this_iter_s: 10.653109312057495
  time_total_s: 202.0232698917389
  timestamp: 1563932651
  timesteps_since_restore: 4412000
  timesteps_this_iter: 237600
  timesteps_total: 4412000
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 202 s, 19 iter, 4412000 ts, -9.39 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-44-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.407044584369782
  episode_reward_mean: -9.611912698361241
  episode_reward_min: -41.880527325819685
  episodes_this_iter: 1584
  episodes_total: 31056
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2206.8505859375
      grad_gnorm: 40.0
      model: {}
      policy_loss: -5382.0888671875
      var_gnorm: 22.73716926574707
      vf_explained_var: 0.00843268632888794
      vf_loss: 8267.380859375
    learner_queue:
      size_count: 5812
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 4649600
    num_steps_trained: 4649600
    num_weight_syncs: 5812
    sample_throughput: 22290.794
    timing_breakdown:
      learner_dequeue_time_ms: 26.754
      learner_grad_time_ms: 12.298
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 43.628
    train_throughput: 22365.848
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.478064613697565
    mean_inference_ms: 1.367079885342248
    mean_processing_ms: 1.7320325239110574
  time_since_restore: 212.67516660690308
  time_this_iter_s: 10.651896715164185
  time_total_s: 212.67516660690308
  timestamp: 1563932662
  timesteps_since_restore: 4649600
  timesteps_this_iter: 237600
  timesteps_total: 4649600
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 212 s, 20 iter, 4649600 ts, -9.61 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-44-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.041627634782337
  episode_reward_mean: -9.256325078811624
  episode_reward_min: -40.134067751263046
  episodes_this_iter: 1584
  episodes_total: 32640
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2206.89599609375
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3146.86767578125
      var_gnorm: 22.750648498535156
      vf_explained_var: -0.03940129280090332
      vf_loss: 3793.431884765625
    learner_queue:
      size_count: 6110
      size_mean: 0.36
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 3.0
      size_std: 0.7418894796396563
    num_steps_replayed: 0
    num_steps_sampled: 4888000
    num_steps_trained: 4887200
    num_weight_syncs: 6110
    sample_throughput: 22385.008
    timing_breakdown:
      learner_dequeue_time_ms: 14.741
      learner_grad_time_ms: 11.51
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 50.922
    train_throughput: 22309.891
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.478432388795665
    mean_inference_ms: 1.366933532285267
    mean_processing_ms: 1.7320307166666382
  time_since_restore: 223.31639075279236
  time_this_iter_s: 10.641224145889282
  time_total_s: 223.31639075279236
  timestamp: 1563932672
  timesteps_since_restore: 4888000
  timesteps_this_iter: 238400
  timesteps_total: 4888000
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 223 s, 21 iter, 4888000 ts, -9.26 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-44-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.35787966048708
  episode_reward_mean: -9.375828401260344
  episode_reward_min: -46.69393751846181
  episodes_this_iter: 1584
  episodes_total: 34224
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2202.717529296875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3253.4609375
      var_gnorm: 22.765151977539062
      vf_explained_var: 0.047889530658721924
      vf_loss: 4970.91015625
    learner_queue:
      size_count: 6408
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4770744176750626
    num_steps_replayed: 0
    num_steps_sampled: 5126400
    num_steps_trained: 5126400
    num_weight_syncs: 6408
    sample_throughput: 22395.01
    timing_breakdown:
      learner_dequeue_time_ms: 22.571
      learner_grad_time_ms: 10.784
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 42.578
    train_throughput: 22470.162
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4785663925092845
    mean_inference_ms: 1.3665883519512463
    mean_processing_ms: 1.732427856447896
  time_since_restore: 233.95460534095764
  time_this_iter_s: 10.638214588165283
  time_total_s: 233.95460534095764
  timestamp: 1563932683
  timesteps_since_restore: 5126400
  timesteps_this_iter: 238400
  timesteps_total: 5126400
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 233 s, 22 iter, 5126400 ts, -9.38 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-44-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.338282866486274
  episode_reward_mean: -9.600492603661856
  episode_reward_min: -41.622480167791245
  episodes_this_iter: 1584
  episodes_total: 35808
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2189.931640625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -3069.92138671875
      var_gnorm: 22.78037452697754
      vf_explained_var: 0.020459651947021484
      vf_loss: 3119.7548828125
    learner_queue:
      size_count: 6704
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 5363200
    num_steps_trained: 5362400
    num_weight_syncs: 6704
    sample_throughput: 22288.479
    timing_breakdown:
      learner_dequeue_time_ms: 26.627
      learner_grad_time_ms: 12.22
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 36.944
    train_throughput: 22213.181
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.478355068444556
    mean_inference_ms: 1.366281064142359
    mean_processing_ms: 1.7326386074408908
  time_since_restore: 244.57121181488037
  time_this_iter_s: 10.61660647392273
  time_total_s: 244.57121181488037
  timestamp: 1563932694
  timesteps_since_restore: 5363200
  timesteps_this_iter: 236800
  timesteps_total: 5363200
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 244 s, 23 iter, 5363200 ts, -9.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-45-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.31975668632966
  episode_reward_mean: -9.028139004857202
  episode_reward_min: -52.929928593842575
  episodes_this_iter: 1584
  episodes_total: 37392
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2194.43017578125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 442.6695556640625
      var_gnorm: 22.795305252075195
      vf_explained_var: 0.04696160554885864
      vf_loss: 4138.05029296875
    learner_queue:
      size_count: 7001
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 5600800
    num_steps_trained: 5600800
    num_weight_syncs: 7001
    sample_throughput: 22314.771
    timing_breakdown:
      learner_dequeue_time_ms: 26.261
      learner_grad_time_ms: 10.151
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 36.638
    train_throughput: 22389.906
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.478071515807794
    mean_inference_ms: 1.3657692068938805
    mean_processing_ms: 1.7331218890928792
  time_since_restore: 255.21149349212646
  time_this_iter_s: 10.640281677246094
  time_total_s: 255.21149349212646
  timestamp: 1563932704
  timesteps_since_restore: 5600800
  timesteps_this_iter: 237600
  timesteps_total: 5600800
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 255 s, 24 iter, 5600800 ts, -9.03 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-45-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.404805797054454
  episode_reward_mean: -9.1072369265493
  episode_reward_min: -44.32476803406025
  episodes_this_iter: 1584
  episodes_total: 38976
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2191.185791015625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -1170.304931640625
      var_gnorm: 22.810779571533203
      vf_explained_var: 0.00033402442932128906
      vf_loss: 2806.13720703125
    learner_queue:
      size_count: 7299
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 2.0
      size_std: 0.3815756805667782
    num_steps_replayed: 0
    num_steps_sampled: 5839200
    num_steps_trained: 5838400
    num_weight_syncs: 7299
    sample_throughput: 22402.604
    timing_breakdown:
      learner_dequeue_time_ms: 15.918
      learner_grad_time_ms: 12.781
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 40.249
    train_throughput: 22327.428
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.478852455890504
    mean_inference_ms: 1.3652905125361143
    mean_processing_ms: 1.7326930017552609
  time_since_restore: 265.8461227416992
  time_this_iter_s: 10.634629249572754
  time_total_s: 265.8461227416992
  timestamp: 1563932715
  timesteps_since_restore: 5839200
  timesteps_this_iter: 238400
  timesteps_total: 5839200
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 265 s, 25 iter, 5839200 ts, -9.11 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-45-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.215915980872133
  episode_reward_mean: -8.885172997962945
  episode_reward_min: -46.32947197025991
  episodes_this_iter: 1584
  episodes_total: 40560
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2179.218017578125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3141.667236328125
      var_gnorm: 22.82567596435547
      vf_explained_var: 0.02111154794692993
      vf_loss: 7822.3349609375
    learner_queue:
      size_count: 7597
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3249615361854384
    num_steps_replayed: 0
    num_steps_sampled: 6077600
    num_steps_trained: 6076800
    num_weight_syncs: 7597
    sample_throughput: 22397.132
    timing_breakdown:
      learner_dequeue_time_ms: 21.281
      learner_grad_time_ms: 10.329
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 31.104
    train_throughput: 22397.133
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4789014084338765
    mean_inference_ms: 1.3648431892564834
    mean_processing_ms: 1.7331469448772496
  time_since_restore: 276.48267221450806
  time_this_iter_s: 10.636549472808838
  time_total_s: 276.48267221450806
  timestamp: 1563932726
  timesteps_since_restore: 6077600
  timesteps_this_iter: 238400
  timesteps_total: 6077600
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 276 s, 26 iter, 6077600 ts, -8.89 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-45-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.34169130389038
  episode_reward_mean: -8.677340475674638
  episode_reward_min: -43.24439970080488
  episodes_this_iter: 1584
  episodes_total: 42144
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2178.804443359375
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: -1400.88671875
      var_gnorm: 22.839792251586914
      vf_explained_var: 0.04592078924179077
      vf_loss: 5630.1044921875
    learner_queue:
      size_count: 7893
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 6314400
    num_steps_trained: 6314400
    num_weight_syncs: 7893
    sample_throughput: 22299.599
    timing_breakdown:
      learner_dequeue_time_ms: 24.516
      learner_grad_time_ms: 14.34
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 49.683
    train_throughput: 22374.936
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479127526043656
    mean_inference_ms: 1.3646253764607876
    mean_processing_ms: 1.7333988336718957
  time_since_restore: 287.0940501689911
  time_this_iter_s: 10.611377954483032
  time_total_s: 287.0940501689911
  timestamp: 1563932736
  timesteps_since_restore: 6314400
  timesteps_this_iter: 236800
  timesteps_total: 6314400
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 287 s, 27 iter, 6314400 ts, -8.68 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-45-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.312255094542255
  episode_reward_mean: -8.395805702406491
  episode_reward_min: -37.17982193857158
  episodes_this_iter: 1600
  episodes_total: 43744
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2165.85595703125
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: 362.28106689453125
      var_gnorm: 22.85302734375
      vf_explained_var: -0.0019034147262573242
      vf_loss: 3519.05126953125
    learner_queue:
      size_count: 8191
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4004996878900157
    num_steps_replayed: 0
    num_steps_sampled: 6552800
    num_steps_trained: 6552800
    num_weight_syncs: 8191
    sample_throughput: 22389.219
    timing_breakdown:
      learner_dequeue_time_ms: 26.748
      learner_grad_time_ms: 11.173
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 40.321
    train_throughput: 22389.22
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.47875254396663
    mean_inference_ms: 1.3646979270226283
    mean_processing_ms: 1.7332625150745082
  time_since_restore: 297.7342813014984
  time_this_iter_s: 10.640231132507324
  time_total_s: 297.7342813014984
  timestamp: 1563932747
  timesteps_since_restore: 6552800
  timesteps_this_iter: 238400
  timesteps_total: 6552800
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 297 s, 28 iter, 6552800 ts, -8.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-45-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.565465893591707
  episode_reward_mean: -8.529684725895828
  episode_reward_min: -53.750551764706955
  episodes_this_iter: 1600
  episodes_total: 45344
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2161.7490234375
      grad_gnorm: 39.9999885559082
      model: {}
      policy_loss: -1552.75439453125
      var_gnorm: 22.8668155670166
      vf_explained_var: 0.0017809271812438965
      vf_loss: 4899.73828125
    learner_queue:
      size_count: 8488
      size_mean: 0.04
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.19595917942265428
    num_steps_replayed: 0
    num_steps_sampled: 6790400
    num_steps_trained: 6790400
    num_weight_syncs: 8488
    sample_throughput: 22349.296
    timing_breakdown:
      learner_dequeue_time_ms: 26.072
      learner_grad_time_ms: 10.961
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 39.031
    train_throughput: 22349.297
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479209372810923
    mean_inference_ms: 1.3645529103202865
    mean_processing_ms: 1.733144879111338
  time_since_restore: 308.357257604599
  time_this_iter_s: 10.622976303100586
  time_total_s: 308.357257604599
  timestamp: 1563932757
  timesteps_since_restore: 6790400
  timesteps_this_iter: 237600
  timesteps_total: 6790400
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 308 s, 29 iter, 6790400 ts, -8.53 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-46-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.167862563973273
  episode_reward_mean: -8.538787545520194
  episode_reward_min: -41.6584461231419
  episodes_this_iter: 1584
  episodes_total: 46928
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2157.69921875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -170.1075439453125
      var_gnorm: 22.879825592041016
      vf_explained_var: 0.0004938840866088867
      vf_loss: 4488.078125
    learner_queue:
      size_count: 8785
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 7028000
    num_steps_trained: 7027200
    num_weight_syncs: 8785
    sample_throughput: 22184.118
    timing_breakdown:
      learner_dequeue_time_ms: 22.98
      learner_grad_time_ms: 10.485
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 33.14
    train_throughput: 22109.424
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479549359279012
    mean_inference_ms: 1.3642716146628768
    mean_processing_ms: 1.733211632620159
  time_since_restore: 319.05901765823364
  time_this_iter_s: 10.701760053634644
  time_total_s: 319.05901765823364
  timestamp: 1563932768
  timesteps_since_restore: 7028000
  timesteps_this_iter: 237600
  timesteps_total: 7028000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 319 s, 30 iter, 7028000 ts, -8.54 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-46-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.205397617903888
  episode_reward_mean: -8.809663428176078
  episode_reward_min: -47.02976371913532
  episodes_this_iter: 1584
  episodes_total: 48512
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2152.01611328125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 117.95724487304688
      var_gnorm: 22.891582489013672
      vf_explained_var: -0.006776213645935059
      vf_loss: 3227.83642578125
    learner_queue:
      size_count: 9082
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    num_steps_replayed: 0
    num_steps_sampled: 7265600
    num_steps_trained: 7265600
    num_weight_syncs: 9082
    sample_throughput: 22283.527
    timing_breakdown:
      learner_dequeue_time_ms: 26.28
      learner_grad_time_ms: 10.777
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 38.08
    train_throughput: 22358.557
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4799372273381195
    mean_inference_ms: 1.3640454124853771
    mean_processing_ms: 1.7329771624891153
  time_since_restore: 329.71373105049133
  time_this_iter_s: 10.65471339225769
  time_total_s: 329.71373105049133
  timestamp: 1563932779
  timesteps_since_restore: 7265600
  timesteps_this_iter: 237600
  timesteps_total: 7265600
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 329 s, 31 iter, 7265600 ts, -8.81 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-46-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.208879057985662
  episode_reward_mean: -8.964703324389644
  episode_reward_min: -34.594094670595794
  episodes_this_iter: 1584
  episodes_total: 50096
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2143.810302734375
      grad_gnorm: 40.0
      model: {}
      policy_loss: 2080.322265625
      var_gnorm: 22.904685974121094
      vf_explained_var: 0.043951213359832764
      vf_loss: 6466.0478515625
    learner_queue:
      size_count: 9379
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 7503200
    num_steps_trained: 7502400
    num_weight_syncs: 9379
    sample_throughput: 22325.722
    timing_breakdown:
      learner_dequeue_time_ms: 24.33
      learner_grad_time_ms: 10.73
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 34.963
    train_throughput: 22250.551
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4795282816328745
    mean_inference_ms: 1.364195508645073
    mean_processing_ms: 1.7326651721038522
  time_since_restore: 340.34868597984314
  time_this_iter_s: 10.634954929351807
  time_total_s: 340.34868597984314
  timestamp: 1563932789
  timesteps_since_restore: 7503200
  timesteps_this_iter: 237600
  timesteps_total: 7503200
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 340 s, 32 iter, 7503200 ts, -8.96 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-46-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.998461437672294
  episode_reward_mean: -8.802342077442706
  episode_reward_min: -49.76266728066885
  episodes_this_iter: 1584
  episodes_total: 51680
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2142.79931640625
      grad_gnorm: 39.9999885559082
      model: {}
      policy_loss: -2259.2109375
      var_gnorm: 22.916423797607422
      vf_explained_var: 0.02426093816757202
      vf_loss: 12350.28125
    learner_queue:
      size_count: 9675
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 7740000
    num_steps_trained: 7740000
    num_weight_syncs: 9675
    sample_throughput: 22280.038
    timing_breakdown:
      learner_dequeue_time_ms: 26.712
      learner_grad_time_ms: 10.994
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 37.92
    train_throughput: 22355.308
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479584167237406
    mean_inference_ms: 1.3639684328236488
    mean_processing_ms: 1.7326602936581599
  time_since_restore: 350.96901845932007
  time_this_iter_s: 10.620332479476929
  time_total_s: 350.96901845932007
  timestamp: 1563932800
  timesteps_since_restore: 7740000
  timesteps_this_iter: 236800
  timesteps_total: 7740000
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 350 s, 33 iter, 7740000 ts, -8.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-46-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.609119988981412
  episode_reward_mean: -8.488837948938968
  episode_reward_min: -37.9956570307699
  episodes_this_iter: 1568
  episodes_total: 53248
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2130.267578125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -2990.063232421875
      var_gnorm: 22.926929473876953
      vf_explained_var: -0.08215653896331787
      vf_loss: 5129.66162109375
    learner_queue:
      size_count: 9971
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 7976800
    num_steps_trained: 7976800
    num_weight_syncs: 9971
    sample_throughput: 22323.789
    timing_breakdown:
      learner_dequeue_time_ms: 29.599
      learner_grad_time_ms: 11.037
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 41.749
    train_throughput: 22323.79
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479904500615075
    mean_inference_ms: 1.3635919447692517
    mean_processing_ms: 1.732186871095211
  time_since_restore: 361.5693633556366
  time_this_iter_s: 10.600344896316528
  time_total_s: 361.5693633556366
  timestamp: 1563932811
  timesteps_since_restore: 7976800
  timesteps_this_iter: 236800
  timesteps_total: 7976800
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 361 s, 34 iter, 7976800 ts, -8.49 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-47-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.491307752275997
  episode_reward_mean: -8.342118983373597
  episode_reward_min: -47.44331514681459
  episodes_this_iter: 1584
  episodes_total: 54832
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2130.71533203125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -928.2454833984375
      var_gnorm: 22.93575096130371
      vf_explained_var: 0.07063442468643188
      vf_loss: 5739.83154296875
    learner_queue:
      size_count: 10270
      size_mean: 0.04
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.19595917942265426
    num_steps_replayed: 0
    num_steps_sampled: 8216000
    num_steps_trained: 8215200
    num_weight_syncs: 10270
    sample_throughput: 22323.513
    timing_breakdown:
      learner_dequeue_time_ms: 27.558
      learner_grad_time_ms: 11.28
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 38.689
    train_throughput: 22248.853
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4803929113900685
    mean_inference_ms: 1.363373189084932
    mean_processing_ms: 1.7321349348867936
  time_since_restore: 372.276887178421
  time_this_iter_s: 10.707523822784424
  time_total_s: 372.276887178421
  timestamp: 1563932821
  timesteps_since_restore: 8216000
  timesteps_this_iter: 239200
  timesteps_total: 8216000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 372 s, 35 iter, 8216000 ts, -8.34 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-47-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.69109953258926
  episode_reward_mean: -8.430349737291891
  episode_reward_min: -40.397795025376006
  episodes_this_iter: 1584
  episodes_total: 56416
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2123.607421875
      grad_gnorm: 40.0
      model: {}
      policy_loss: 3927.77197265625
      var_gnorm: 22.943410873413086
      vf_explained_var: 0.03458684682846069
      vf_loss: 8226.935546875
    learner_queue:
      size_count: 10567
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    num_steps_replayed: 0
    num_steps_sampled: 8453600
    num_steps_trained: 8453600
    num_weight_syncs: 10567
    sample_throughput: 22295.914
    timing_breakdown:
      learner_dequeue_time_ms: 28.778
      learner_grad_time_ms: 10.866
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 40.89
    train_throughput: 22370.986
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4804095595713935
    mean_inference_ms: 1.3632945131830925
    mean_processing_ms: 1.7319833765222674
  time_since_restore: 382.9258677959442
  time_this_iter_s: 10.648980617523193
  time_total_s: 382.9258677959442
  timestamp: 1563932832
  timesteps_since_restore: 8453600
  timesteps_this_iter: 237600
  timesteps_total: 8453600
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 382 s, 36 iter, 8453600 ts, -8.43 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-47-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.754584656677068
  episode_reward_mean: -8.526092774253895
  episode_reward_min: -42.94883655563632
  episodes_this_iter: 1584
  episodes_total: 58000
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2095.468994140625
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -2274.840087890625
      var_gnorm: 22.951875686645508
      vf_explained_var: 0.004326164722442627
      vf_loss: 4968.615234375
    learner_queue:
      size_count: 10865
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 8692000
    num_steps_trained: 8691200
    num_weight_syncs: 10865
    sample_throughput: 22339.479
    timing_breakdown:
      learner_dequeue_time_ms: 13.285
      learner_grad_time_ms: 14.232
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 26.77
    train_throughput: 22264.514
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.480033492175409
    mean_inference_ms: 1.363207146284477
    mean_processing_ms: 1.7317501263405164
  time_since_restore: 393.5883665084839
  time_this_iter_s: 10.662498712539673
  time_total_s: 393.5883665084839
  timestamp: 1563932843
  timesteps_since_restore: 8692000
  timesteps_this_iter: 238400
  timesteps_total: 8692000
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 393 s, 37 iter, 8692000 ts, -8.53 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-47-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.185526775129038
  episode_reward_mean: -8.563828890916408
  episode_reward_min: -40.43908959944098
  episodes_this_iter: 1616
  episodes_total: 59616
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2095.77734375
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 2297.32421875
      var_gnorm: 22.958694458007812
      vf_explained_var: 0.013374626636505127
      vf_loss: 16243.1103515625
    learner_queue:
      size_count: 11161
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4004996878900157
    num_steps_replayed: 0
    num_steps_sampled: 8928800
    num_steps_trained: 8928000
    num_weight_syncs: 11161
    sample_throughput: 22266.593
    timing_breakdown:
      learner_dequeue_time_ms: 25.214
      learner_grad_time_ms: 12.438
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 40.054
    train_throughput: 22266.594
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479772836097697
    mean_inference_ms: 1.362999038918465
    mean_processing_ms: 1.7315941876925436
  time_since_restore: 404.21533370018005
  time_this_iter_s: 10.626967191696167
  time_total_s: 404.21533370018005
  timestamp: 1563932853
  timesteps_since_restore: 8928800
  timesteps_this_iter: 236800
  timesteps_total: 8928800
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 404 s, 38 iter, 8928800 ts, -8.56 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-47-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.944255353402912
  episode_reward_mean: -8.184621469092825
  episode_reward_min: -43.91523264961403
  episodes_this_iter: 1584
  episodes_total: 61200
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2092.44873046875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -906.7161865234375
      var_gnorm: 22.96466636657715
      vf_explained_var: 0.009895563125610352
      vf_loss: 8157.47265625
    learner_queue:
      size_count: 11459
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 9167200
    num_steps_trained: 9167200
    num_weight_syncs: 11459
    sample_throughput: 22322.307
    timing_breakdown:
      learner_dequeue_time_ms: 25.259
      learner_grad_time_ms: 11.823
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 37.107
    train_throughput: 22397.215
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.479813987820281
    mean_inference_ms: 1.3631371912239472
    mean_processing_ms: 1.731652667460223
  time_since_restore: 414.88702869415283
  time_this_iter_s: 10.671694993972778
  time_total_s: 414.88702869415283
  timestamp: 1563932864
  timesteps_since_restore: 9167200
  timesteps_this_iter: 238400
  timesteps_total: 9167200
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 414 s, 39 iter, 9167200 ts, -8.18 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-47-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.271256447679207
  episode_reward_mean: -8.044290253961178
  episode_reward_min: -40.95657005302902
  episodes_this_iter: 1600
  episodes_total: 62800
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2066.677490234375
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -909.841552734375
      var_gnorm: 22.969913482666016
      vf_explained_var: 0.045092761516571045
      vf_loss: 1022.976318359375
    learner_queue:
      size_count: 11757
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    num_steps_replayed: 0
    num_steps_sampled: 9405600
    num_steps_trained: 9405600
    num_weight_syncs: 11757
    sample_throughput: 22351.187
    timing_breakdown:
      learner_dequeue_time_ms: 21.907
      learner_grad_time_ms: 10.457
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 42.239
    train_throughput: 22351.188
  iterations_since_restore: 40
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.4801988503745855
    mean_inference_ms: 1.362861282655285
    mean_processing_ms: 1.7317974333510755
  time_since_restore: 425.54539728164673
  time_this_iter_s: 10.658368587493896
  time_total_s: 425.54539728164673
  timestamp: 1563932875
  timesteps_since_restore: 9405600
  timesteps_this_iter: 238400
  timesteps_total: 9405600
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 425 s, 40 iter, 9405600 ts, -8.04 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-48-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.4132869672695
  episode_reward_mean: -8.121824979574303
  episode_reward_min: -40.21505315408487
  episodes_this_iter: 1568
  episodes_total: 64368
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2076.88525390625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 210.0003662109375
      var_gnorm: 22.97310447692871
      vf_explained_var: 0.06652259826660156
      vf_loss: 3212.67041015625
    learner_queue:
      size_count: 12055
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 9644000
    num_steps_trained: 9644000
    num_weight_syncs: 12055
    sample_throughput: 22311.745
    timing_breakdown:
      learner_dequeue_time_ms: 29.025
      learner_grad_time_ms: 9.465
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 57.933
    train_throughput: 22311.746
  iterations_since_restore: 41
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.480558683353017
    mean_inference_ms: 1.3628615373230055
    mean_processing_ms: 1.731471730543605
  time_since_restore: 436.2218692302704
  time_this_iter_s: 10.676471948623657
  time_total_s: 436.2218692302704
  timestamp: 1563932885
  timesteps_since_restore: 9644000
  timesteps_this_iter: 238400
  timesteps_total: 9644000
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 436 s, 41 iter, 9644000 ts, -8.12 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-48-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.356308788103995
  episode_reward_mean: -7.950390095362031
  episode_reward_min: -45.219767382537384
  episodes_this_iter: 1584
  episodes_total: 65952
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2078.716552734375
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1827.6373291015625
      var_gnorm: 22.976398468017578
      vf_explained_var: 0.030905306339263916
      vf_loss: 7209.78515625
    learner_queue:
      size_count: 12354
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 9883200
    num_steps_trained: 9882400
    num_weight_syncs: 12354
    sample_throughput: 22319.567
    timing_breakdown:
      learner_dequeue_time_ms: 22.769
      learner_grad_time_ms: 10.632
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 34.846
    train_throughput: 22244.92
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.480471962271675
    mean_inference_ms: 1.3628515559007208
    mean_processing_ms: 1.7318648446222806
  time_since_restore: 446.9312193393707
  time_this_iter_s: 10.709350109100342
  time_total_s: 446.9312193393707
  timestamp: 1563932896
  timesteps_since_restore: 9883200
  timesteps_this_iter: 239200
  timesteps_total: 9883200
  training_iteration: 42
  2019-07-24 03:48:27,301	INFO ray_trial_executor.py:187 -- Destroying actor for trial IMPALA_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=1565], 446 s, 42 iter, 9883200 ts, -7.95 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-24_03-48-27
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 18.31979671781731
  episode_reward_mean: -8.034430427232213
  episode_reward_min: -43.15292683018875
  episodes_this_iter: 1584
  episodes_total: 67536
  experiment_id: f934fc895227404ebb108b8a5bae886a
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 9.999999747378752e-06
      entropy: 2079.21435546875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 2189.90478515625
      var_gnorm: 22.980070114135742
      vf_explained_var: 0.05855751037597656
      vf_loss: 13332.98046875
    learner_queue:
      size_count: 12650
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    num_steps_replayed: 0
    num_steps_sampled: 10120000
    num_steps_trained: 10120000
    num_weight_syncs: 12650
    sample_throughput: 22162.68
    timing_breakdown:
      learner_dequeue_time_ms: 22.131
      learner_grad_time_ms: 11.933
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 34.093
    train_throughput: 22237.555
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 1565
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.480413552220781
    mean_inference_ms: 1.3631330775522257
    mean_processing_ms: 1.7325654446078718
  time_since_restore: 457.60796427726746
  time_this_iter_s: 10.676744937896729
  time_total_s: 457.60796427726746
  timestamp: 1563932907
  timesteps_since_restore: 10120000
  timesteps_this_iter: 236800
  timesteps_total: 10120000
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 9.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'TERMINATED': 1})
TERMINATED trials:
 - IMPALA_RoboschoolReacher-v1_0:	TERMINATED, [12 CPUs, 1 GPUs], [pid=1565], 457 s, 43 iter, 10120000 ts, -8.03 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 9.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'TERMINATED': 1})
TERMINATED trials:
 - IMPALA_RoboschoolReacher-v1_0:	TERMINATED, [12 CPUs, 1 GPUs], [pid=1565], 457 s, 43 iter, 10120000 ts, -8.03 rew

[32m [   461.26092s,  INFO] Experiment took 461.05550 seconds | 7.68426 minutes | 0.12807 hours [0m
