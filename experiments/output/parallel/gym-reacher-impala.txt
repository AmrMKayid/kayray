2019-07-18 03:16:14,760	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-18_03-16-14_760471_18610/logs.
2019-07-18 03:16:14,866	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-18 03:16:14,976	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-18 03:16:14,980	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-18 03:16:15,003	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-18_03-16-14_760471_18610/logs.
2019-07-18 03:16:15,003	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-18 03:16:15,105	INFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()
2019-07-18 03:16:15,105	INFO tune.py:233 -- Starting a new experiment.
2019-07-18 03:16:15,132	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-18 03:16:15,251	WARNING util.py:64 -- The `start_trial` operation took 0.138458251953125 seconds to complete, which may be a performance bottleneck.
[32m [     0.21993s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.22025s,  INFO] Experiment configs: 
 {
  "gym-reacher-impala": {
    "env": "RoboschoolReacher-v1",
    "run": "IMPALA",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 50,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 18,
      "training_iteration": 500
    },
    "config": {
      "sample_batch_size": 50,
      "train_batch_size": 500,
      "num_gpus": 1,
      "num_workers": 11,
      "num_envs_per_worker": 5
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 2.2/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 2.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING

[2m[36m(pid=18657)[0m [32m [     0.01617s,  INFO] TimeLimit:
[2m[36m(pid=18657)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18657)[0m - action_space = Box(2,)
[2m[36m(pid=18657)[0m - observation_space = Box(9,)
[2m[36m(pid=18657)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18657)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18657)[0m - _max_episode_steps = 150
[2m[36m(pid=18657)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m 2019-07-18 03:16:17.227333: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18657)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18657)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18657)[0m [32m [     0.59136s,  INFO] TimeLimit:
[2m[36m(pid=18657)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18657)[0m - action_space = Box(2,)
[2m[36m(pid=18657)[0m - observation_space = Box(9,)
[2m[36m(pid=18657)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18657)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18657)[0m - _max_episode_steps = 150
[2m[36m(pid=18657)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m [32m [     0.59181s,  INFO] TimeLimit:
[2m[36m(pid=18657)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18657)[0m - action_space = Box(2,)
[2m[36m(pid=18657)[0m - observation_space = Box(9,)
[2m[36m(pid=18657)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18657)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18657)[0m - _max_episode_steps = 150
[2m[36m(pid=18657)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m [32m [     0.59222s,  INFO] TimeLimit:
[2m[36m(pid=18657)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18657)[0m - action_space = Box(2,)
[2m[36m(pid=18657)[0m - observation_space = Box(9,)
[2m[36m(pid=18657)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18657)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18657)[0m - _max_episode_steps = 150
[2m[36m(pid=18657)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m [32m [     0.59261s,  INFO] TimeLimit:
[2m[36m(pid=18657)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18657)[0m - action_space = Box(2,)
[2m[36m(pid=18657)[0m - observation_space = Box(9,)
[2m[36m(pid=18657)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18657)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18657)[0m - _max_episode_steps = 150
[2m[36m(pid=18657)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m 2019-07-18 03:16:17,800	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.impala.vtrace_policy.VTraceTFPolicy object at 0x7f7005b65c18>}
[2m[36m(pid=18657)[0m 2019-07-18 03:16:17,800	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7006175b00>}
[2m[36m(pid=18657)[0m 2019-07-18 03:16:17,800	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f7006175978>}
[2m[36m(pid=18863)[0m 2019-07-18 03:16:20,178	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18863)[0m 2019-07-18 03:16:20.179098: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18863)[0m [32m [     0.03765s,  INFO] TimeLimit:
[2m[36m(pid=18863)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18863)[0m - action_space = Box(2,)
[2m[36m(pid=18863)[0m - observation_space = Box(9,)
[2m[36m(pid=18863)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18863)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18863)[0m - _max_episode_steps = 150
[2m[36m(pid=18863)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18858)[0m [32m [     0.03934s,  INFO] TimeLimit:
[2m[36m(pid=18858)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18858)[0m - action_space = Box(2,)
[2m[36m(pid=18858)[0m - observation_space = Box(9,)
[2m[36m(pid=18858)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18858)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18858)[0m - _max_episode_steps = 150
[2m[36m(pid=18858)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18858)[0m 2019-07-18 03:16:20,279	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18858)[0m 2019-07-18 03:16:20.280281: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18655)[0m [32m [     0.03811s,  INFO] TimeLimit:
[2m[36m(pid=18655)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18655)[0m - action_space = Box(2,)
[2m[36m(pid=18655)[0m - observation_space = Box(9,)
[2m[36m(pid=18655)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18655)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18655)[0m - _max_episode_steps = 150
[2m[36m(pid=18655)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18655)[0m 2019-07-18 03:16:20,314	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18655)[0m 2019-07-18 03:16:20.315078: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18664)[0m 2019-07-18 03:16:20,320	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18664)[0m 2019-07-18 03:16:20.321138: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18664)[0m [32m [     0.03698s,  INFO] TimeLimit:
[2m[36m(pid=18664)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18664)[0m - action_space = Box(2,)
[2m[36m(pid=18664)[0m - observation_space = Box(9,)
[2m[36m(pid=18664)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18664)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18664)[0m - _max_episode_steps = 150
[2m[36m(pid=18664)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18856)[0m 2019-07-18 03:16:20,308	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18856)[0m 2019-07-18 03:16:20.309324: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18856)[0m [32m [     0.03793s,  INFO] TimeLimit:
[2m[36m(pid=18856)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18856)[0m - action_space = Box(2,)
[2m[36m(pid=18856)[0m - observation_space = Box(9,)
[2m[36m(pid=18856)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18856)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18856)[0m - _max_episode_steps = 150
[2m[36m(pid=18856)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18661)[0m [32m [     0.03790s,  INFO] TimeLimit:
[2m[36m(pid=18661)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18661)[0m - action_space = Box(2,)
[2m[36m(pid=18661)[0m - observation_space = Box(9,)
[2m[36m(pid=18661)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18661)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18661)[0m - _max_episode_steps = 150
[2m[36m(pid=18661)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18661)[0m 2019-07-18 03:16:20,360	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18661)[0m 2019-07-18 03:16:20.360669: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18660)[0m [32m [     0.03995s,  INFO] TimeLimit:
[2m[36m(pid=18660)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18660)[0m - action_space = Box(2,)
[2m[36m(pid=18660)[0m - observation_space = Box(9,)
[2m[36m(pid=18660)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18660)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18660)[0m - _max_episode_steps = 150
[2m[36m(pid=18660)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18660)[0m 2019-07-18 03:16:20,436	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18660)[0m 2019-07-18 03:16:20.436835: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18654)[0m 2019-07-18 03:16:20,410	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18654)[0m 2019-07-18 03:16:20.410807: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18654)[0m [32m [     0.03933s,  INFO] TimeLimit:
[2m[36m(pid=18654)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18654)[0m - action_space = Box(2,)
[2m[36m(pid=18654)[0m - observation_space = Box(9,)
[2m[36m(pid=18654)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18654)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18654)[0m - _max_episode_steps = 150
[2m[36m(pid=18654)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18663)[0m 2019-07-18 03:16:20,470	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18663)[0m 2019-07-18 03:16:20.471016: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18663)[0m [32m [     0.03991s,  INFO] TimeLimit:
[2m[36m(pid=18663)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18663)[0m - action_space = Box(2,)
[2m[36m(pid=18663)[0m - observation_space = Box(9,)
[2m[36m(pid=18663)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18663)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18663)[0m - _max_episode_steps = 150
[2m[36m(pid=18663)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18662)[0m [32m [     0.03938s,  INFO] TimeLimit:
[2m[36m(pid=18662)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18662)[0m - action_space = Box(2,)
[2m[36m(pid=18662)[0m - observation_space = Box(9,)
[2m[36m(pid=18662)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18662)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18662)[0m - _max_episode_steps = 150
[2m[36m(pid=18662)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18662)[0m 2019-07-18 03:16:20,512	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18662)[0m 2019-07-18 03:16:20.513352: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18658)[0m [32m [     0.03938s,  INFO] TimeLimit:
[2m[36m(pid=18658)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18658)[0m - action_space = Box(2,)
[2m[36m(pid=18658)[0m - observation_space = Box(9,)
[2m[36m(pid=18658)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18658)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18658)[0m - _max_episode_steps = 150
[2m[36m(pid=18658)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18658)[0m 2019-07-18 03:16:20,512	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18658)[0m 2019-07-18 03:16:20.513352: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18863)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18863)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18856)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18856)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18858)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18858)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18655)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18655)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18664)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18664)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18661)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18661)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18658)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18658)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18654)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18654)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18663)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18663)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18660)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18660)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18662)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18662)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18856)[0m [32m [     1.16308s,  INFO] TimeLimit:
[2m[36m(pid=18856)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18856)[0m - action_space = Box(2,)
[2m[36m(pid=18856)[0m - observation_space = Box(9,)
[2m[36m(pid=18856)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18856)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18856)[0m - _max_episode_steps = 150
[2m[36m(pid=18856)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18856)[0m [32m [     1.16375s,  INFO] TimeLimit:
[2m[36m(pid=18856)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18856)[0m - action_space = Box(2,)
[2m[36m(pid=18856)[0m - observation_space = Box(9,)
[2m[36m(pid=18856)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18856)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18856)[0m - _max_episode_steps = 150
[2m[36m(pid=18856)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18856)[0m [32m [     1.16439s,  INFO] TimeLimit:
[2m[36m(pid=18856)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18856)[0m - action_space = Box(2,)
[2m[36m(pid=18856)[0m - observation_space = Box(9,)
[2m[36m(pid=18856)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18856)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18856)[0m - _max_episode_steps = 150
[2m[36m(pid=18856)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18856)[0m [32m [     1.16513s,  INFO] TimeLimit:
[2m[36m(pid=18856)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18856)[0m - action_space = Box(2,)
[2m[36m(pid=18856)[0m - observation_space = Box(9,)
[2m[36m(pid=18856)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18856)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18856)[0m - _max_episode_steps = 150
[2m[36m(pid=18856)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m [32m [     1.44092s,  INFO] TimeLimit:
[2m[36m(pid=18863)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18863)[0m - action_space = Box(2,)
[2m[36m(pid=18863)[0m - observation_space = Box(9,)
[2m[36m(pid=18863)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18863)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18863)[0m - _max_episode_steps = 150
[2m[36m(pid=18863)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m [32m [     1.44188s,  INFO] TimeLimit:
[2m[36m(pid=18863)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18863)[0m - action_space = Box(2,)
[2m[36m(pid=18863)[0m - observation_space = Box(9,)
[2m[36m(pid=18863)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18863)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18863)[0m - _max_episode_steps = 150
[2m[36m(pid=18863)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m [32m [     1.44282s,  INFO] TimeLimit:
[2m[36m(pid=18863)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18863)[0m - action_space = Box(2,)
[2m[36m(pid=18863)[0m - observation_space = Box(9,)
[2m[36m(pid=18863)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18863)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18863)[0m - _max_episode_steps = 150
[2m[36m(pid=18863)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m [32m [     1.44394s,  INFO] TimeLimit:
[2m[36m(pid=18863)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18863)[0m - action_space = Box(2,)
[2m[36m(pid=18863)[0m - observation_space = Box(9,)
[2m[36m(pid=18863)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18863)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18863)[0m - _max_episode_steps = 150
[2m[36m(pid=18863)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,634	INFO rollout_worker.py:428 -- Generating sample batch of size 250
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,683	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.87, max=0.493, mean=-0.067)},
[2m[36m(pid=18863)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.991, max=0.23, mean=-0.173)},
[2m[36m(pid=18863)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.967, max=0.554, mean=-0.095)},
[2m[36m(pid=18863)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.809, max=0.587, mean=-0.097)},
[2m[36m(pid=18863)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.999, max=0.883, mean=-0.024)}}
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,683	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=18863)[0m   1: {'agent0': None},
[2m[36m(pid=18863)[0m   2: {'agent0': None},
[2m[36m(pid=18863)[0m   3: {'agent0': None},
[2m[36m(pid=18863)[0m   4: {'agent0': None}}
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,683	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.87, max=0.493, mean=-0.067)
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,684	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.87, max=0.493, mean=-0.067)
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,688	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18863)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18863)[0m                                   'env_id': 0,
[2m[36m(pid=18863)[0m                                   'info': None,
[2m[36m(pid=18863)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.87, max=0.493, mean=-0.067),
[2m[36m(pid=18863)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18863)[0m                                   'rnn_state': []},
[2m[36m(pid=18863)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18863)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18863)[0m                                   'env_id': 1,
[2m[36m(pid=18863)[0m                                   'info': None,
[2m[36m(pid=18863)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.991, max=0.23, mean=-0.173),
[2m[36m(pid=18863)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18863)[0m                                   'rnn_state': []},
[2m[36m(pid=18863)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18863)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18863)[0m                                   'env_id': 2,
[2m[36m(pid=18863)[0m                                   'info': None,
[2m[36m(pid=18863)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.967, max=0.554, mean=-0.095),
[2m[36m(pid=18863)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18863)[0m                                   'rnn_state': []},
[2m[36m(pid=18863)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18863)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18863)[0m                                   'env_id': 3,
[2m[36m(pid=18863)[0m                                   'info': None,
[2m[36m(pid=18863)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.809, max=0.587, mean=-0.097),
[2m[36m(pid=18863)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18863)[0m                                   'rnn_state': []},
[2m[36m(pid=18863)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18863)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18863)[0m                                   'env_id': 4,
[2m[36m(pid=18863)[0m                                   'info': None,
[2m[36m(pid=18863)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.999, max=0.883, mean=-0.024),
[2m[36m(pid=18863)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18863)[0m                                   'rnn_state': []},
[2m[36m(pid=18863)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,688	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=18858)[0m [32m [     1.45425s,  INFO] TimeLimit:
[2m[36m(pid=18858)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18858)[0m - action_space = Box(2,)
[2m[36m(pid=18858)[0m - observation_space = Box(9,)
[2m[36m(pid=18858)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18858)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18858)[0m - _max_episode_steps = 150
[2m[36m(pid=18858)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18858)[0m [32m [     1.45520s,  INFO] TimeLimit:
[2m[36m(pid=18858)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18858)[0m - action_space = Box(2,)
[2m[36m(pid=18858)[0m - observation_space = Box(9,)
[2m[36m(pid=18858)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18858)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18858)[0m - _max_episode_steps = 150
[2m[36m(pid=18858)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18858)[0m [32m [     1.45612s,  INFO] TimeLimit:
[2m[36m(pid=18858)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18858)[0m - action_space = Box(2,)
[2m[36m(pid=18858)[0m - observation_space = Box(9,)
[2m[36m(pid=18858)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18858)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18858)[0m - _max_episode_steps = 150
[2m[36m(pid=18858)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18858)[0m [32m [     1.45716s,  INFO] TimeLimit:
[2m[36m(pid=18858)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18858)[0m - action_space = Box(2,)
[2m[36m(pid=18858)[0m - observation_space = Box(9,)
[2m[36m(pid=18858)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18858)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18858)[0m - _max_episode_steps = 150
[2m[36m(pid=18858)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,716	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18863)[0m { 'default_policy': ( np.ndarray((5, 2), dtype=float32, min=-2.243, max=2.947, mean=-0.22),
[2m[36m(pid=18863)[0m                       [],
[2m[36m(pid=18863)[0m                       { 'action_prob': np.ndarray((5,), dtype=float32, min=0.001, max=0.101, mean=0.034),
[2m[36m(pid=18863)[0m                         'behaviour_logits': np.ndarray((5, 4), dtype=float32, min=-0.007, max=0.008, mean=-0.0)})}
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18655)[0m [32m [     1.50080s,  INFO] TimeLimit:
[2m[36m(pid=18655)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18655)[0m - action_space = Box(2,)
[2m[36m(pid=18655)[0m - observation_space = Box(9,)
[2m[36m(pid=18655)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18655)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18655)[0m - _max_episode_steps = 150
[2m[36m(pid=18655)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18655)[0m [32m [     1.50179s,  INFO] TimeLimit:
[2m[36m(pid=18655)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18655)[0m - action_space = Box(2,)
[2m[36m(pid=18655)[0m - observation_space = Box(9,)
[2m[36m(pid=18655)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18655)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18655)[0m - _max_episode_steps = 150
[2m[36m(pid=18655)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18655)[0m [32m [     1.50274s,  INFO] TimeLimit:
[2m[36m(pid=18655)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18655)[0m - action_space = Box(2,)
[2m[36m(pid=18655)[0m - observation_space = Box(9,)
[2m[36m(pid=18655)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18655)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18655)[0m - _max_episode_steps = 150
[2m[36m(pid=18655)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18655)[0m [32m [     1.50384s,  INFO] TimeLimit:
[2m[36m(pid=18655)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18655)[0m - action_space = Box(2,)
[2m[36m(pid=18655)[0m - observation_space = Box(9,)
[2m[36m(pid=18655)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18655)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18655)[0m - _max_episode_steps = 150
[2m[36m(pid=18655)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18664)[0m [32m [     1.46861s,  INFO] TimeLimit:
[2m[36m(pid=18664)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18664)[0m - action_space = Box(2,)
[2m[36m(pid=18664)[0m - observation_space = Box(9,)
[2m[36m(pid=18664)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18664)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18664)[0m - _max_episode_steps = 150
[2m[36m(pid=18664)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18664)[0m [32m [     1.46953s,  INFO] TimeLimit:
[2m[36m(pid=18664)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18664)[0m - action_space = Box(2,)
[2m[36m(pid=18664)[0m - observation_space = Box(9,)
[2m[36m(pid=18664)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18664)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18664)[0m - _max_episode_steps = 150
[2m[36m(pid=18664)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18664)[0m [32m [     1.47044s,  INFO] TimeLimit:
[2m[36m(pid=18664)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18664)[0m - action_space = Box(2,)
[2m[36m(pid=18664)[0m - observation_space = Box(9,)
[2m[36m(pid=18664)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18664)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18664)[0m - _max_episode_steps = 150
[2m[36m(pid=18664)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18664)[0m [32m [     1.47160s,  INFO] TimeLimit:
[2m[36m(pid=18664)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18664)[0m - action_space = Box(2,)
[2m[36m(pid=18664)[0m - observation_space = Box(9,)
[2m[36m(pid=18664)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18664)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18664)[0m - _max_episode_steps = 150
[2m[36m(pid=18664)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18661)[0m [32m [     1.48233s,  INFO] TimeLimit:
[2m[36m(pid=18661)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18661)[0m - action_space = Box(2,)
[2m[36m(pid=18661)[0m - observation_space = Box(9,)
[2m[36m(pid=18661)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18661)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18661)[0m - _max_episode_steps = 150
[2m[36m(pid=18661)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18661)[0m [32m [     1.48329s,  INFO] TimeLimit:
[2m[36m(pid=18661)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18661)[0m - action_space = Box(2,)
[2m[36m(pid=18661)[0m - observation_space = Box(9,)
[2m[36m(pid=18661)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18661)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18661)[0m - _max_episode_steps = 150
[2m[36m(pid=18661)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18661)[0m [32m [     1.48426s,  INFO] TimeLimit:
[2m[36m(pid=18661)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18661)[0m - action_space = Box(2,)
[2m[36m(pid=18661)[0m - observation_space = Box(9,)
[2m[36m(pid=18661)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18661)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18661)[0m - _max_episode_steps = 150
[2m[36m(pid=18661)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18661)[0m [32m [     1.48541s,  INFO] TimeLimit:
[2m[36m(pid=18661)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18661)[0m - action_space = Box(2,)
[2m[36m(pid=18661)[0m - observation_space = Box(9,)
[2m[36m(pid=18661)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18661)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18661)[0m - _max_episode_steps = 150
[2m[36m(pid=18661)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18658)[0m [32m [     1.33216s,  INFO] TimeLimit:
[2m[36m(pid=18658)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18658)[0m - action_space = Box(2,)
[2m[36m(pid=18658)[0m - observation_space = Box(9,)
[2m[36m(pid=18658)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18658)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18658)[0m - _max_episode_steps = 150
[2m[36m(pid=18658)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18658)[0m [32m [     1.33308s,  INFO] TimeLimit:
[2m[36m(pid=18658)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18658)[0m - action_space = Box(2,)
[2m[36m(pid=18658)[0m - observation_space = Box(9,)
[2m[36m(pid=18658)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18658)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18658)[0m - _max_episode_steps = 150
[2m[36m(pid=18658)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18658)[0m [32m [     1.33406s,  INFO] TimeLimit:
[2m[36m(pid=18658)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18658)[0m - action_space = Box(2,)
[2m[36m(pid=18658)[0m - observation_space = Box(9,)
[2m[36m(pid=18658)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18658)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18658)[0m - _max_episode_steps = 150
[2m[36m(pid=18658)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18658)[0m [32m [     1.33507s,  INFO] TimeLimit:
[2m[36m(pid=18658)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18658)[0m - action_space = Box(2,)
[2m[36m(pid=18658)[0m - observation_space = Box(9,)
[2m[36m(pid=18658)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18658)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18658)[0m - _max_episode_steps = 150
[2m[36m(pid=18658)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18660)[0m [32m [     1.48106s,  INFO] TimeLimit:
[2m[36m(pid=18660)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18660)[0m - action_space = Box(2,)
[2m[36m(pid=18660)[0m - observation_space = Box(9,)
[2m[36m(pid=18660)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18660)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18660)[0m - _max_episode_steps = 150
[2m[36m(pid=18660)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18660)[0m [32m [     1.48205s,  INFO] TimeLimit:
[2m[36m(pid=18660)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18660)[0m - action_space = Box(2,)
[2m[36m(pid=18660)[0m - observation_space = Box(9,)
[2m[36m(pid=18660)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18660)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18660)[0m - _max_episode_steps = 150
[2m[36m(pid=18660)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18660)[0m [32m [     1.48304s,  INFO] TimeLimit:
[2m[36m(pid=18660)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18660)[0m - action_space = Box(2,)
[2m[36m(pid=18660)[0m - observation_space = Box(9,)
[2m[36m(pid=18660)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18660)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18660)[0m - _max_episode_steps = 150
[2m[36m(pid=18660)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18660)[0m [32m [     1.48421s,  INFO] TimeLimit:
[2m[36m(pid=18660)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18660)[0m - action_space = Box(2,)
[2m[36m(pid=18660)[0m - observation_space = Box(9,)
[2m[36m(pid=18660)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18660)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18660)[0m - _max_episode_steps = 150
[2m[36m(pid=18660)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m 2019-07-18 03:16:21,882	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=18657)[0m 
[2m[36m(pid=18657)[0m { 'data': { 'action_prob': np.ndarray((500,), dtype=float32, min=0.0, max=0.161, mean=0.081),
[2m[36m(pid=18657)[0m             'actions': np.ndarray((500, 2), dtype=float32, min=-4.127, max=2.947, mean=-0.025),
[2m[36m(pid=18657)[0m             'agent_index': np.ndarray((500,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18657)[0m             'behaviour_logits': np.ndarray((500, 4), dtype=float32, min=-0.01, max=0.011, mean=-0.0),
[2m[36m(pid=18657)[0m             'dones': np.ndarray((500,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18657)[0m             'eps_id': np.ndarray((500,), dtype=int64, min=148034688.0, max=1865868879.0, mean=1011544070.3),
[2m[36m(pid=18657)[0m             'infos': np.ndarray((500,), dtype=object, head={}),
[2m[36m(pid=18657)[0m             'obs': np.ndarray((500, 9), dtype=float32, min=-3.076, max=3.193, mean=-0.088),
[2m[36m(pid=18657)[0m             'prev_actions': np.ndarray((500, 2), dtype=float32, min=-4.127, max=2.947, mean=-0.034),
[2m[36m(pid=18657)[0m             'prev_rewards': np.ndarray((500,), dtype=float32, min=-2.824, max=2.302, mean=0.01),
[2m[36m(pid=18657)[0m             'rewards': np.ndarray((500,), dtype=float32, min=-2.824, max=2.302, mean=0.002),
[2m[36m(pid=18657)[0m             't': np.ndarray((500,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=18657)[0m             'unroll_id': np.ndarray((500,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=18657)[0m   'type': 'SampleBatch'}
[2m[36m(pid=18657)[0m 
[2m[36m(pid=18657)[0m 2019-07-18 03:16:21,882	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=18654)[0m [32m [     1.49325s,  INFO] TimeLimit:
[2m[36m(pid=18654)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18654)[0m - action_space = Box(2,)
[2m[36m(pid=18654)[0m - observation_space = Box(9,)
[2m[36m(pid=18654)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18654)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18654)[0m - _max_episode_steps = 150
[2m[36m(pid=18654)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18654)[0m [32m [     1.49424s,  INFO] TimeLimit:
[2m[36m(pid=18654)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18654)[0m - action_space = Box(2,)
[2m[36m(pid=18654)[0m - observation_space = Box(9,)
[2m[36m(pid=18654)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18654)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18654)[0m - _max_episode_steps = 150
[2m[36m(pid=18654)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18654)[0m [32m [     1.49519s,  INFO] TimeLimit:
[2m[36m(pid=18654)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18654)[0m - action_space = Box(2,)
[2m[36m(pid=18654)[0m - observation_space = Box(9,)
[2m[36m(pid=18654)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18654)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18654)[0m - _max_episode_steps = 150
[2m[36m(pid=18654)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18654)[0m [32m [     1.49638s,  INFO] TimeLimit:
[2m[36m(pid=18654)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18654)[0m - action_space = Box(2,)
[2m[36m(pid=18654)[0m - observation_space = Box(9,)
[2m[36m(pid=18654)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18654)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18654)[0m - _max_episode_steps = 150
[2m[36m(pid=18654)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,867	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18863)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.001, max=0.157, mean=0.087),
[2m[36m(pid=18863)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-2.915, max=2.289, mean=-0.087),
[2m[36m(pid=18863)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                         'behaviour_logits': np.ndarray((50, 4), dtype=float32, min=-0.007, max=0.008, mean=0.001),
[2m[36m(pid=18863)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=1808298289.0, max=1808298289.0, mean=1808298289.0),
[2m[36m(pid=18863)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=18863)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-2.352, max=1.593, mean=-0.188),
[2m[36m(pid=18863)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.915, max=1.66, mean=-0.107),
[2m[36m(pid=18863)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-2.818, max=1.045, mean=-0.308),
[2m[36m(pid=18863)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-2.818, max=1.045, mean=-0.305),
[2m[36m(pid=18863)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=18863)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=18863)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18863)[0m 2019-07-18 03:16:21,875	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18863)[0m { 'data': { 'action_prob': np.ndarray((250,), dtype=float32, min=0.001, max=0.161, mean=0.079),
[2m[36m(pid=18863)[0m             'actions': np.ndarray((250, 2), dtype=float32, min=-2.915, max=2.947, mean=-0.022),
[2m[36m(pid=18863)[0m             'agent_index': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m             'behaviour_logits': np.ndarray((250, 4), dtype=float32, min=-0.009, max=0.011, mean=0.0),
[2m[36m(pid=18863)[0m             'dones': np.ndarray((250,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18863)[0m             'eps_id': np.ndarray((250,), dtype=int64, min=411753589.0, max=1808298289.0, mean=910336661.4),
[2m[36m(pid=18863)[0m             'infos': np.ndarray((250,), dtype=object, head={}),
[2m[36m(pid=18863)[0m             'obs': np.ndarray((250, 9), dtype=float32, min=-3.076, max=3.193, mean=-0.102),
[2m[36m(pid=18863)[0m             'prev_actions': np.ndarray((250, 2), dtype=float32, min=-2.915, max=2.947, mean=-0.039),
[2m[36m(pid=18863)[0m             'prev_rewards': np.ndarray((250,), dtype=float32, min=-2.824, max=2.192, mean=0.024),
[2m[36m(pid=18863)[0m             'rewards': np.ndarray((250,), dtype=float32, min=-2.824, max=2.192, mean=0.007),
[2m[36m(pid=18863)[0m             't': np.ndarray((250,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=18863)[0m             'unroll_id': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=18863)[0m   'type': 'SampleBatch'}
[2m[36m(pid=18863)[0m 
[2m[36m(pid=18663)[0m [32m [     1.49857s,  INFO] TimeLimit:
[2m[36m(pid=18663)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18663)[0m - action_space = Box(2,)
[2m[36m(pid=18663)[0m - observation_space = Box(9,)
[2m[36m(pid=18663)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18663)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18663)[0m - _max_episode_steps = 150
[2m[36m(pid=18663)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18663)[0m [32m [     1.49956s,  INFO] TimeLimit:
[2m[36m(pid=18663)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18663)[0m - action_space = Box(2,)
[2m[36m(pid=18663)[0m - observation_space = Box(9,)
[2m[36m(pid=18663)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18663)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18663)[0m - _max_episode_steps = 150
[2m[36m(pid=18663)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18663)[0m [32m [     1.50054s,  INFO] TimeLimit:
[2m[36m(pid=18663)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18663)[0m - action_space = Box(2,)
[2m[36m(pid=18663)[0m - observation_space = Box(9,)
[2m[36m(pid=18663)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18663)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18663)[0m - _max_episode_steps = 150
[2m[36m(pid=18663)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18663)[0m [32m [     1.50165s,  INFO] TimeLimit:
[2m[36m(pid=18663)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18663)[0m - action_space = Box(2,)
[2m[36m(pid=18663)[0m - observation_space = Box(9,)
[2m[36m(pid=18663)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18663)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18663)[0m - _max_episode_steps = 150
[2m[36m(pid=18663)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18662)[0m [32m [     1.48301s,  INFO] TimeLimit:
[2m[36m(pid=18662)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18662)[0m - action_space = Box(2,)
[2m[36m(pid=18662)[0m - observation_space = Box(9,)
[2m[36m(pid=18662)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18662)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18662)[0m - _max_episode_steps = 150
[2m[36m(pid=18662)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18662)[0m [32m [     1.48406s,  INFO] TimeLimit:
[2m[36m(pid=18662)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18662)[0m - action_space = Box(2,)
[2m[36m(pid=18662)[0m - observation_space = Box(9,)
[2m[36m(pid=18662)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18662)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18662)[0m - _max_episode_steps = 150
[2m[36m(pid=18662)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18662)[0m [32m [     1.48505s,  INFO] TimeLimit:
[2m[36m(pid=18662)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18662)[0m - action_space = Box(2,)
[2m[36m(pid=18662)[0m - observation_space = Box(9,)
[2m[36m(pid=18662)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18662)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18662)[0m - _max_episode_steps = 150
[2m[36m(pid=18662)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18662)[0m [32m [     1.48649s,  INFO] TimeLimit:
[2m[36m(pid=18662)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18662)[0m - action_space = Box(2,)
[2m[36m(pid=18662)[0m - observation_space = Box(9,)
[2m[36m(pid=18662)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18662)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18662)[0m - _max_episode_steps = 150
[2m[36m(pid=18662)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18657)[0m 2019-07-18 03:16:22,506	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=18657)[0m 
[2m[36m(pid=18657)[0m { 'learner_stats': { 'cur_lr': 0.0005000000237487257,
[2m[36m(pid=18657)[0m                      'entropy': 1390.339,
[2m[36m(pid=18657)[0m                      'grad_gnorm': 40.0,
[2m[36m(pid=18657)[0m                      'model': {},
[2m[36m(pid=18657)[0m                      'policy_loss': -2986.729,
[2m[36m(pid=18657)[0m                      'var_gnorm': 22.649506,
[2m[36m(pid=18657)[0m                      'vf_explained_var': 0.0039896965,
[2m[36m(pid=18657)[0m                      'vf_loss': 17776.643}}
[2m[36m(pid=18657)[0m 
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-16-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.680735920278867
  episode_reward_mean: -17.5282596509228
  episode_reward_min: -99.21794166685916
  episodes_this_iter: 565
  episodes_total: 565
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1789.881591796875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -391.62066650390625
      var_gnorm: 22.99424934387207
      vf_explained_var: 0.9251495599746704
      vf_loss: 1071.9305419921875
    learner_queue:
      size_count: 164
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 82000
    num_steps_trained: 81500
    num_weight_syncs: 328
    sample_throughput: 7852.012
    timing_breakdown:
      learner_dequeue_time_ms: 23.873
      learner_grad_time_ms: 8.979
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.488
    train_throughput: 7804.134
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6353995899612273
    mean_inference_ms: 1.5379457085614552
    mean_processing_ms: 0.6713058839403042
  time_since_restore: 10.298677206039429
  time_this_iter_s: 10.298677206039429
  time_total_s: 10.298677206039429
  timestamp: 1563412588
  timesteps_since_restore: 82000
  timesteps_this_iter: 82000
  timesteps_total: 82000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 10 s, 1 iter, 82000 ts, -17.5 rew

[2m[36m(pid=18657)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
[2m[36m(pid=18657)[0m   out=out, **kwargs)
[2m[36m(pid=18657)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
[2m[36m(pid=18657)[0m   ret = ret.dtype.type(ret / rcount)
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-16-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.200022043163457
  episode_reward_mean: -33.40137137449625
  episode_reward_min: -163.80358394610641
  episodes_this_iter: 1035
  episodes_total: 1600
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 601.9577026367188
      grad_gnorm: 40.0
      model: {}
      policy_loss: 3016.08642578125
      var_gnorm: 23.390708923339844
      vf_explained_var: 0.6635032892227173
      vf_loss: 1472.141357421875
    learner_queue:
      size_count: 475
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 237500
    num_steps_trained: 237500
    num_weight_syncs: 951
    sample_throughput: 15056.983
    timing_breakdown:
      learner_dequeue_time_ms: 21.312
      learner_grad_time_ms: 8.386
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.338
    train_throughput: 15105.399
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.578598403097986
    mean_inference_ms: 1.360135066031444
    mean_processing_ms: 0.6540640054456409
  time_since_restore: 20.61777091026306
  time_this_iter_s: 10.319093704223633
  time_total_s: 20.61777091026306
  timestamp: 1563412598
  timesteps_since_restore: 237500
  timesteps_this_iter: 155500
  timesteps_total: 237500
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 20 s, 2 iter, 237500 ts, -33.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-16-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.00067255066908
  episode_reward_mean: -24.944399454325946
  episode_reward_min: -161.53610026529813
  episodes_this_iter: 1050
  episodes_total: 2650
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2361.014404296875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 4416.50390625
      var_gnorm: 23.79604148864746
      vf_explained_var: 0.5020865201950073
      vf_loss: 5259.46533203125
    learner_queue:
      size_count: 786
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 393000
    num_steps_trained: 393000
    num_weight_syncs: 1573
    sample_throughput: 15094.78
    timing_breakdown:
      learner_dequeue_time_ms: 23.207
      learner_grad_time_ms: 9.543
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 16.988
    train_throughput: 15094.78
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5659528748954081
    mean_inference_ms: 1.3214198819800522
    mean_processing_ms: 0.6510049766271842
  time_since_restore: 30.911834716796875
  time_this_iter_s: 10.294063806533813
  time_total_s: 30.911834716796875
  timestamp: 1563412608
  timesteps_since_restore: 393000
  timesteps_this_iter: 155500
  timesteps_total: 393000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 30 s, 3 iter, 393000 ts, -24.9 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-16-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 3.6369477430738986
  episode_reward_mean: -35.233009597493265
  episode_reward_min: -149.24970546704554
  episodes_this_iter: 1030
  episodes_total: 3680
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 4260.27685546875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 3630.092529296875
      var_gnorm: 24.068622589111328
      vf_explained_var: 0.6077219843864441
      vf_loss: 3442.284423828125
    learner_queue:
      size_count: 1098
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 549000
    num_steps_trained: 549000
    num_weight_syncs: 2196
    sample_throughput: 15136.466
    timing_breakdown:
      learner_dequeue_time_ms: 23.982
      learner_grad_time_ms: 8.366
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.707
    train_throughput: 15136.466
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5596084410887068
    mean_inference_ms: 1.3045220732974747
    mean_processing_ms: 0.6485877590880968
  time_since_restore: 41.21094584465027
  time_this_iter_s: 10.299111127853394
  time_total_s: 41.21094584465027
  timestamp: 1563412619
  timesteps_since_restore: 549000
  timesteps_this_iter: 156000
  timesteps_total: 549000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 41 s, 4 iter, 549000 ts, -35.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-17-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 9.122557237801395
  episode_reward_mean: -34.298448287455834
  episode_reward_min: -146.95094898607292
  episodes_this_iter: 1040
  episodes_total: 4720
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 4538.60107421875
      grad_gnorm: 40.0
      model: {}
      policy_loss: 5464.3232421875
      var_gnorm: 24.4928035736084
      vf_explained_var: 0.5792735815048218
      vf_loss: 2931.5234375
    learner_queue:
      size_count: 1409
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 704500
    num_steps_trained: 704500
    num_weight_syncs: 2819
    sample_throughput: 15066.11
    timing_breakdown:
      learner_dequeue_time_ms: 25.885
      learner_grad_time_ms: 8.057
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.569
    train_throughput: 15066.111
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5561036939751074
    mean_inference_ms: 1.2960387042647732
    mean_processing_ms: 0.6480138280460995
  time_since_restore: 51.52488899230957
  time_this_iter_s: 10.313943147659302
  time_total_s: 51.52488899230957
  timestamp: 1563412629
  timesteps_since_restore: 704500
  timesteps_this_iter: 155500
  timesteps_total: 704500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 51 s, 5 iter, 704500 ts, -34.3 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-17-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.841480450788579
  episode_reward_mean: -17.55120264085889
  episode_reward_min: -112.05674864024355
  episodes_this_iter: 1035
  episodes_total: 5755
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 225.9416961669922
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 38.37589645385742
      var_gnorm: 24.578649520874023
      vf_explained_var: 0.9923146367073059
      vf_loss: 19.796100616455078
    learner_queue:
      size_count: 1720
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 860000
    num_steps_trained: 860000
    num_weight_syncs: 3441
    sample_throughput: 15091.619
    timing_breakdown:
      learner_dequeue_time_ms: 19.134
      learner_grad_time_ms: 7.776
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 26.506
    train_throughput: 15091.619
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5541902484678654
    mean_inference_ms: 1.2888520526897276
    mean_processing_ms: 0.6473996722019335
  time_since_restore: 61.82103943824768
  time_this_iter_s: 10.29615044593811
  time_total_s: 61.82103943824768
  timestamp: 1563412639
  timesteps_since_restore: 860000
  timesteps_this_iter: 155500
  timesteps_total: 860000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 61 s, 6 iter, 860000 ts, -17.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-17-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.07757166865902
  episode_reward_mean: -8.33765790888512
  episode_reward_min: -60.91212653209075
  episodes_this_iter: 1040
  episodes_total: 6795
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1224.2691650390625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -232.70350646972656
      var_gnorm: 24.672565460205078
      vf_explained_var: 0.9493482112884521
      vf_loss: 99.37751007080078
    learner_queue:
      size_count: 2031
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1015500
    num_steps_trained: 1015500
    num_weight_syncs: 4063
    sample_throughput: 15079.54
    timing_breakdown:
      learner_dequeue_time_ms: 25.258
      learner_grad_time_ms: 7.502
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 15.746
    train_throughput: 15079.541
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5521429049162676
    mean_inference_ms: 1.2847732297474417
    mean_processing_ms: 0.6472415544637349
  time_since_restore: 72.125173330307
  time_this_iter_s: 10.304133892059326
  time_total_s: 72.125173330307
  timestamp: 1563412650
  timesteps_since_restore: 1015500
  timesteps_this_iter: 155500
  timesteps_total: 1015500
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 72 s, 7 iter, 1015500 ts, -8.34 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-17-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.967526868793108
  episode_reward_mean: -11.20550953297128
  episode_reward_min: -97.35132298198667
  episodes_this_iter: 1025
  episodes_total: 7820
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1582.8243408203125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3659.78271484375
      var_gnorm: 25.072784423828125
      vf_explained_var: 0.8532150387763977
      vf_loss: 1812.7681884765625
    learner_queue:
      size_count: 2342
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 1171000
    num_steps_trained: 1171000
    num_weight_syncs: 4684
    sample_throughput: 15110.56
    timing_breakdown:
      learner_dequeue_time_ms: 23.02
      learner_grad_time_ms: 8.384
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.819
    train_throughput: 15110.56
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5516771483544005
    mean_inference_ms: 1.2816358661843719
    mean_processing_ms: 0.647208761156514
  time_since_restore: 82.4075083732605
  time_this_iter_s: 10.282335042953491
  time_total_s: 82.4075083732605
  timestamp: 1563412660
  timesteps_since_restore: 1171000
  timesteps_this_iter: 155500
  timesteps_total: 1171000
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 82 s, 8 iter, 1171000 ts, -11.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-17-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.68561600469631
  episode_reward_mean: -15.113660883076186
  episode_reward_min: -89.29938130163026
  episodes_this_iter: 1040
  episodes_total: 8860
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1737.4830322265625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 1264.107421875
      var_gnorm: 25.40228843688965
      vf_explained_var: 0.8507595658302307
      vf_loss: 2067.85498046875
    learner_queue:
      size_count: 2653
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1326500
    num_steps_trained: 1326500
    num_weight_syncs: 5306
    sample_throughput: 15117.892
    timing_breakdown:
      learner_dequeue_time_ms: 25.504
      learner_grad_time_ms: 8.218
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.099
    train_throughput: 15117.892
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5500286294003296
    mean_inference_ms: 1.2788316187972795
    mean_processing_ms: 0.6469996038027118
  time_since_restore: 92.68637704849243
  time_this_iter_s: 10.278868675231934
  time_total_s: 92.68637704849243
  timestamp: 1563412670
  timesteps_since_restore: 1326500
  timesteps_this_iter: 155500
  timesteps_total: 1326500
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 92 s, 9 iter, 1326500 ts, -15.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-18-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.88579731908379
  episode_reward_mean: -15.799688869833286
  episode_reward_min: -77.00519493082479
  episodes_this_iter: 1040
  episodes_total: 9900
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1982.8739013671875
      grad_gnorm: 40.0
      model: {}
      policy_loss: 1341.8614501953125
      var_gnorm: 25.643388748168945
      vf_explained_var: 0.9325117468833923
      vf_loss: 501.3147888183594
    learner_queue:
      size_count: 2964
      size_mean: 0.04
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.19595917942265428
    num_steps_replayed: 0
    num_steps_sampled: 1482000
    num_steps_trained: 1482000
    num_weight_syncs: 5928
    sample_throughput: 15102.95
    timing_breakdown:
      learner_dequeue_time_ms: 21.909
      learner_grad_time_ms: 9.403
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 23.223
    train_throughput: 15102.95
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5487481960482228
    mean_inference_ms: 1.2778399528099937
    mean_processing_ms: 0.6465968545217056
  time_since_restore: 102.97474527359009
  time_this_iter_s: 10.288368225097656
  time_total_s: 102.97474527359009
  timestamp: 1563412681
  timesteps_since_restore: 1482000
  timesteps_this_iter: 155500
  timesteps_total: 1482000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 102 s, 10 iter, 1482000 ts, -15.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-18-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.306679059458265
  episode_reward_mean: -14.37827873062554
  episode_reward_min: -109.0303310270774
  episodes_this_iter: 1035
  episodes_total: 10935
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1628.99267578125
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -2571.03466796875
      var_gnorm: 26.048147201538086
      vf_explained_var: 0.8357011675834656
      vf_loss: 1081.469482421875
    learner_queue:
      size_count: 3276
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1638000
    num_steps_trained: 1637500
    num_weight_syncs: 6552
    sample_throughput: 15118.477
    timing_breakdown:
      learner_dequeue_time_ms: 22.482
      learner_grad_time_ms: 9.729
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 15.084
    train_throughput: 15070.021
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.547705979116193
    mean_inference_ms: 1.276811614337358
    mean_processing_ms: 0.6463047924769203
  time_since_restore: 113.28572845458984
  time_this_iter_s: 10.310983180999756
  time_total_s: 113.28572845458984
  timestamp: 1563412691
  timesteps_since_restore: 1638000
  timesteps_this_iter: 156000
  timesteps_total: 1638000
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 113 s, 11 iter, 1638000 ts, -14.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-18-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.451542982058587
  episode_reward_mean: -13.3327036431963
  episode_reward_min: -103.61999510943019
  episodes_this_iter: 1035
  episodes_total: 11970
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1600.9090576171875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3526.265380859375
      var_gnorm: 26.386566162109375
      vf_explained_var: 0.9116874933242798
      vf_loss: 1269.8763427734375
    learner_queue:
      size_count: 3587
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 1793500
    num_steps_trained: 1793000
    num_weight_syncs: 7174
    sample_throughput: 15103.748
    timing_breakdown:
      learner_dequeue_time_ms: 25.606
      learner_grad_time_ms: 8.701
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 16.851
    train_throughput: 15103.748
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.546355736697903
    mean_inference_ms: 1.2755113014829744
    mean_processing_ms: 0.6462594474861622
  time_since_restore: 123.57348775863647
  time_this_iter_s: 10.28775930404663
  time_total_s: 123.57348775863647
  timestamp: 1563412701
  timesteps_since_restore: 1793500
  timesteps_this_iter: 155500
  timesteps_total: 1793500
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 123 s, 12 iter, 1793500 ts, -13.3 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-18-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.708204449753058
  episode_reward_mean: -9.832548765592644
  episode_reward_min: -133.0641139254986
  episodes_this_iter: 1035
  episodes_total: 13005
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 692.5391845703125
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 1415.9893798828125
      var_gnorm: 26.61813735961914
      vf_explained_var: 0.789100170135498
      vf_loss: 1079.4813232421875
    learner_queue:
      size_count: 3898
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 1949000
    num_steps_trained: 1949000
    num_weight_syncs: 7796
    sample_throughput: 15127.613
    timing_breakdown:
      learner_dequeue_time_ms: 25.764
      learner_grad_time_ms: 8.394
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.065
    train_throughput: 15176.256
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5457174984818456
    mean_inference_ms: 1.2746049135608812
    mean_processing_ms: 0.6459099102847993
  time_since_restore: 133.84509563446045
  time_this_iter_s: 10.271607875823975
  time_total_s: 133.84509563446045
  timestamp: 1563412711
  timesteps_since_restore: 1949000
  timesteps_this_iter: 155500
  timesteps_total: 1949000
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 133 s, 13 iter, 1949000 ts, -9.83 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-18-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.182952939362963
  episode_reward_mean: -12.39782782626487
  episode_reward_min: -113.57675916904299
  episodes_this_iter: 1045
  episodes_total: 14050
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1149.3646240234375
      grad_gnorm: 40.0
      model: {}
      policy_loss: 2810.447021484375
      var_gnorm: 26.8915958404541
      vf_explained_var: 0.8138852715492249
      vf_loss: 1212.6219482421875
    learner_queue:
      size_count: 4208
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2104000
    num_steps_trained: 2104000
    num_weight_syncs: 8417
    sample_throughput: 15063.87
    timing_breakdown:
      learner_dequeue_time_ms: 24.235
      learner_grad_time_ms: 9.176
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.346
    train_throughput: 15063.87
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5450776334257337
    mean_inference_ms: 1.274135847265951
    mean_processing_ms: 0.6459026347970371
  time_since_restore: 144.12666702270508
  time_this_iter_s: 10.281571388244629
  time_total_s: 144.12666702270508
  timestamp: 1563412722
  timesteps_since_restore: 2104000
  timesteps_this_iter: 155000
  timesteps_total: 2104000
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 144 s, 14 iter, 2104000 ts, -12.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-18-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.776826016584433
  episode_reward_mean: -11.65540797737678
  episode_reward_min: -93.19263445364246
  episodes_this_iter: 1035
  episodes_total: 15085
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 3159.993408203125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -3846.306884765625
      var_gnorm: 27.046112060546875
      vf_explained_var: 0.9621331691741943
      vf_loss: 853.0344848632812
    learner_queue:
      size_count: 4520
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2260000
    num_steps_trained: 2260000
    num_weight_syncs: 9040
    sample_throughput: 15143.772
    timing_breakdown:
      learner_dequeue_time_ms: 24.102
      learner_grad_time_ms: 8.202
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.581
    train_throughput: 15143.773
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5444005274349657
    mean_inference_ms: 1.273177425595536
    mean_processing_ms: 0.6456120029804112
  time_since_restore: 154.42073774337769
  time_this_iter_s: 10.294070720672607
  time_total_s: 154.42073774337769
  timestamp: 1563412732
  timesteps_since_restore: 2260000
  timesteps_this_iter: 156000
  timesteps_total: 2260000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 154 s, 15 iter, 2260000 ts, -11.7 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-19-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.796383566272567
  episode_reward_mean: -24.957519126035816
  episode_reward_min: -128.73299740710598
  episodes_this_iter: 1040
  episodes_total: 16125
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 13071.6494140625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1242.9075927734375
      var_gnorm: 28.313629150390625
      vf_explained_var: 0.9370549917221069
      vf_loss: 494.044189453125
    learner_queue:
      size_count: 4832
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2416000
    num_steps_trained: 2416000
    num_weight_syncs: 9664
    sample_throughput: 15084.601
    timing_breakdown:
      learner_dequeue_time_ms: 24.51
      learner_grad_time_ms: 7.771
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 26.17
    train_throughput: 15084.602
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5441911721841775
    mean_inference_ms: 1.272713325847471
    mean_processing_ms: 0.6456879520570327
  time_since_restore: 164.75516438484192
  time_this_iter_s: 10.334426641464233
  time_total_s: 164.75516438484192
  timestamp: 1563412742
  timesteps_since_restore: 2416000
  timesteps_this_iter: 156000
  timesteps_total: 2416000
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 164 s, 16 iter, 2416000 ts, -25 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-19-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.306840286713598
  episode_reward_mean: -22.916120301040316
  episode_reward_min: -104.7517594022062
  episodes_this_iter: 1035
  episodes_total: 17160
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 3524.6826171875
      grad_gnorm: 39.99999237060547
      model: {}
      policy_loss: 5562.546875
      var_gnorm: 28.885601043701172
      vf_explained_var: 0.908912181854248
      vf_loss: 662.11865234375
    learner_queue:
      size_count: 5143
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2571500
    num_steps_trained: 2571500
    num_weight_syncs: 10286
    sample_throughput: 15094.351
    timing_breakdown:
      learner_dequeue_time_ms: 24.175
      learner_grad_time_ms: 8.151
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.084
    train_throughput: 15094.351
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5440443728240494
    mean_inference_ms: 1.2722707279198218
    mean_processing_ms: 0.645310961248781
  time_since_restore: 175.04947710037231
  time_this_iter_s: 10.294312715530396
  time_total_s: 175.04947710037231
  timestamp: 1563412753
  timesteps_since_restore: 2571500
  timesteps_this_iter: 155500
  timesteps_total: 2571500
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 175 s, 17 iter, 2571500 ts, -22.9 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-19-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.386084584266005
  episode_reward_mean: -24.48477377428747
  episode_reward_min: -126.46212994591787
  episodes_this_iter: 1030
  episodes_total: 18190
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8131.8388671875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -2851.0791015625
      var_gnorm: 29.48235321044922
      vf_explained_var: 0.7140116691589355
      vf_loss: 1892.1080322265625
    learner_queue:
      size_count: 5454
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2727000
    num_steps_trained: 2727000
    num_weight_syncs: 10909
    sample_throughput: 15093.793
    timing_breakdown:
      learner_dequeue_time_ms: 25.49
      learner_grad_time_ms: 9.127
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.836
    train_throughput: 15093.794
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.543767292095176
    mean_inference_ms: 1.271568486146116
    mean_processing_ms: 0.6451621318986082
  time_since_restore: 185.34469389915466
  time_this_iter_s: 10.295216798782349
  time_total_s: 185.34469389915466
  timestamp: 1563412763
  timesteps_since_restore: 2727000
  timesteps_this_iter: 155500
  timesteps_total: 2727000
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 185 s, 18 iter, 2727000 ts, -24.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-19-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.64132257405202
  episode_reward_mean: -23.623893330283927
  episode_reward_min: -135.11081853198417
  episodes_this_iter: 1050
  episodes_total: 19240
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 12833.626953125
      grad_gnorm: 39.9999885559082
      model: {}
      policy_loss: -3016.36669921875
      var_gnorm: 30.343059539794922
      vf_explained_var: 0.9670045971870422
      vf_loss: 435.7752990722656
    learner_queue:
      size_count: 5765
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2882500
    num_steps_trained: 2882500
    num_weight_syncs: 11530
    sample_throughput: 15092.449
    timing_breakdown:
      learner_dequeue_time_ms: 24.805
      learner_grad_time_ms: 7.88
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 23.918
    train_throughput: 15092.449
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.543844845837579
    mean_inference_ms: 1.2712504291972417
    mean_processing_ms: 0.645387820231587
  time_since_restore: 195.6399872303009
  time_this_iter_s: 10.29529333114624
  time_total_s: 195.6399872303009
  timestamp: 1563412773
  timesteps_since_restore: 2882500
  timesteps_this_iter: 155500
  timesteps_total: 2882500
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 195 s, 19 iter, 2882500 ts, -23.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-19-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.030394651255204
  episode_reward_mean: -25.797632529870164
  episode_reward_min: -117.63269277121199
  episodes_this_iter: 1030
  episodes_total: 20270
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2086.48486328125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1261.5499267578125
      var_gnorm: 31.128326416015625
      vf_explained_var: 0.9545242190361023
      vf_loss: 166.58180236816406
    learner_queue:
      size_count: 6076
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 3038000
    num_steps_trained: 3038000
    num_weight_syncs: 12152
    sample_throughput: 15085.419
    timing_breakdown:
      learner_dequeue_time_ms: 24.213
      learner_grad_time_ms: 8.065
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 15.462
    train_throughput: 15085.419
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5435369386462863
    mean_inference_ms: 1.2710675995694927
    mean_processing_ms: 0.64541006912164
  time_since_restore: 205.9407675266266
  time_this_iter_s: 10.300780296325684
  time_total_s: 205.9407675266266
  timestamp: 1563412784
  timesteps_since_restore: 3038000
  timesteps_this_iter: 155500
  timesteps_total: 3038000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 205 s, 20 iter, 3038000 ts, -25.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-19-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 13.337630493005408
  episode_reward_mean: -24.663316628460862
  episode_reward_min: -135.42790020280924
  episodes_this_iter: 1030
  episodes_total: 21300
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 10568.078125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 8985.970703125
      var_gnorm: 31.585058212280273
      vf_explained_var: 0.8736363649368286
      vf_loss: 1969.5693359375
    learner_queue:
      size_count: 6384
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 3192000
    num_steps_trained: 3192000
    num_weight_syncs: 12769
    sample_throughput: 14934.572
    timing_breakdown:
      learner_dequeue_time_ms: 29.733
      learner_grad_time_ms: 8.574
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.053
    train_throughput: 14934.572
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5437064016962223
    mean_inference_ms: 1.2713380476330858
    mean_processing_ms: 0.6460009857396288
  time_since_restore: 216.2452163696289
  time_this_iter_s: 10.30444884300232
  time_total_s: 216.2452163696289
  timestamp: 1563412794
  timesteps_since_restore: 3192000
  timesteps_this_iter: 154000
  timesteps_total: 3192000
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 216 s, 21 iter, 3192000 ts, -24.7 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-20-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 13.060979813586348
  episode_reward_mean: -30.358080094568415
  episode_reward_min: -151.41336148607488
  episodes_this_iter: 1030
  episodes_total: 22330
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 14812.884765625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 6457.18408203125
      var_gnorm: 32.193336486816406
      vf_explained_var: 0.9000150561332703
      vf_loss: 1996.836669921875
    learner_queue:
      size_count: 6694
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 3347000
    num_steps_trained: 3347000
    num_weight_syncs: 13388
    sample_throughput: 15043.815
    timing_breakdown:
      learner_dequeue_time_ms: 24.624
      learner_grad_time_ms: 8.772
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 22.144
    train_throughput: 15043.816
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.543930971250195
    mean_inference_ms: 1.2710964421298439
    mean_processing_ms: 0.6459242844544386
  time_since_restore: 226.53939199447632
  time_this_iter_s: 10.294175624847412
  time_total_s: 226.53939199447632
  timestamp: 1563412804
  timesteps_since_restore: 3347000
  timesteps_this_iter: 155000
  timesteps_total: 3347000
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 226 s, 22 iter, 3347000 ts, -30.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-20-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 14.364179868502815
  episode_reward_mean: -27.967106669511047
  episode_reward_min: -134.39012806226032
  episodes_this_iter: 1040
  episodes_total: 23370
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 9397.322265625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1394.6754150390625
      var_gnorm: 32.650638580322266
      vf_explained_var: 0.9622077941894531
      vf_loss: 398.07891845703125
    learner_queue:
      size_count: 7004
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 3502000
    num_steps_trained: 3501500
    num_weight_syncs: 14008
    sample_throughput: 15025.428
    timing_breakdown:
      learner_dequeue_time_ms: 24.586
      learner_grad_time_ms: 9.71
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 16.694
    train_throughput: 14976.959
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5441962012618735
    mean_inference_ms: 1.2708868543976302
    mean_processing_ms: 0.6459576186304737
  time_since_restore: 236.84781503677368
  time_this_iter_s: 10.308423042297363
  time_total_s: 236.84781503677368
  timestamp: 1563412815
  timesteps_since_restore: 3502000
  timesteps_this_iter: 155000
  timesteps_total: 3502000
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 236 s, 23 iter, 3502000 ts, -28 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-20-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 14.24560715676598
  episode_reward_mean: -23.898548927160515
  episode_reward_min: -153.9576957087644
  episodes_this_iter: 1035
  episodes_total: 24405
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 9396.369140625
      grad_gnorm: 40.0
      model: {}
      policy_loss: 193.61460876464844
      var_gnorm: 33.019447326660156
      vf_explained_var: 0.8392760157585144
      vf_loss: 835.1856689453125
    learner_queue:
      size_count: 7314
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 3657000
    num_steps_trained: 3657000
    num_weight_syncs: 14628
    sample_throughput: 15022.955
    timing_breakdown:
      learner_dequeue_time_ms: 25.046
      learner_grad_time_ms: 8.356
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.403
    train_throughput: 15071.417
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5445301107864997
    mean_inference_ms: 1.2708398139070418
    mean_processing_ms: 0.6460658251426583
  time_since_restore: 247.15812635421753
  time_this_iter_s: 10.310311317443848
  time_total_s: 247.15812635421753
  timestamp: 1563412825
  timesteps_since_restore: 3657000
  timesteps_this_iter: 155000
  timesteps_total: 3657000
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 247 s, 24 iter, 3657000 ts, -23.9 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-20-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.453291803566035
  episode_reward_mean: -26.968735465071624
  episode_reward_min: -153.25210854783228
  episodes_this_iter: 1030
  episodes_total: 25435
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 10833.6923828125
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -11082.6845703125
      var_gnorm: 33.336212158203125
      vf_explained_var: 0.9395462870597839
      vf_loss: 853.1168212890625
    learner_queue:
      size_count: 7624
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 3812000
    num_steps_trained: 3812000
    num_weight_syncs: 15248
    sample_throughput: 15032.335
    timing_breakdown:
      learner_dequeue_time_ms: 23.326
      learner_grad_time_ms: 8.799
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 22.654
    train_throughput: 15032.336
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5447435534711518
    mean_inference_ms: 1.2707261629560478
    mean_processing_ms: 0.6461927537057113
  time_since_restore: 257.46193504333496
  time_this_iter_s: 10.303808689117432
  time_total_s: 257.46193504333496
  timestamp: 1563412835
  timesteps_since_restore: 3812000
  timesteps_this_iter: 155000
  timesteps_total: 3812000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 257 s, 25 iter, 3812000 ts, -27 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-20-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.820735244312406
  episode_reward_mean: -26.461055700793455
  episode_reward_min: -92.74498767581798
  episodes_this_iter: 1025
  episodes_total: 26460
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 14042.943359375
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: 5991.76611328125
      var_gnorm: 33.659767150878906
      vf_explained_var: 0.9298990368843079
      vf_loss: 713.2459716796875
    learner_queue:
      size_count: 7934
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 3967000
    num_steps_trained: 3967000
    num_weight_syncs: 15868
    sample_throughput: 15043.173
    timing_breakdown:
      learner_dequeue_time_ms: 25.836
      learner_grad_time_ms: 9.303
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.923
    train_throughput: 15043.174
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.544741331353402
    mean_inference_ms: 1.2705200537699544
    mean_processing_ms: 0.6462751862015007
  time_since_restore: 267.75809478759766
  time_this_iter_s: 10.296159744262695
  time_total_s: 267.75809478759766
  timestamp: 1563412845
  timesteps_since_restore: 3967000
  timesteps_this_iter: 155000
  timesteps_total: 3967000
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 267 s, 26 iter, 3967000 ts, -26.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-20-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.585473583355785
  episode_reward_mean: -28.19705098666884
  episode_reward_min: -122.30958692826522
  episodes_this_iter: 1035
  episodes_total: 27495
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8623.4951171875
      grad_gnorm: 40.0
      model: {}
      policy_loss: -10039.65625
      var_gnorm: 34.024261474609375
      vf_explained_var: 0.9325829148292542
      vf_loss: 901.8995971679688
    learner_queue:
      size_count: 8243
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 4121500
    num_steps_trained: 4121500
    num_weight_syncs: 16486
    sample_throughput: 15032.64
    timing_breakdown:
      learner_dequeue_time_ms: 25.354
      learner_grad_time_ms: 8.546
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.744
    train_throughput: 15032.641
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.545289056683822
    mean_inference_ms: 1.2702122834118126
    mean_processing_ms: 0.6463374768308078
  time_since_restore: 278.0278844833374
  time_this_iter_s: 10.269789695739746
  time_total_s: 278.0278844833374
  timestamp: 1563412856
  timesteps_since_restore: 4121500
  timesteps_this_iter: 154500
  timesteps_total: 4121500
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 278 s, 27 iter, 4121500 ts, -28.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-21-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.820491892072038
  episode_reward_mean: -30.509729902560913
  episode_reward_min: -130.76355825154513
  episodes_this_iter: 1035
  episodes_total: 28530
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 10501.45703125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 3229.810546875
      var_gnorm: 34.34614562988281
      vf_explained_var: 0.9736008048057556
      vf_loss: 354.22314453125
    learner_queue:
      size_count: 8553
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 4276500
    num_steps_trained: 4276000
    num_weight_syncs: 17106
    sample_throughput: 15006.738
    timing_breakdown:
      learner_dequeue_time_ms: 21.44
      learner_grad_time_ms: 9.31
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.169
    train_throughput: 14958.33
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.545401208875055
    mean_inference_ms: 1.2703516388861313
    mean_processing_ms: 0.6463990147326212
  time_since_restore: 288.3481683731079
  time_this_iter_s: 10.320283889770508
  time_total_s: 288.3481683731079
  timestamp: 1563412866
  timesteps_since_restore: 4276500
  timesteps_this_iter: 155000
  timesteps_total: 4276500
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 288 s, 28 iter, 4276500 ts, -30.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-21-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.978902101606566
  episode_reward_mean: -30.63689335579071
  episode_reward_min: -124.48252701498897
  episodes_this_iter: 1030
  episodes_total: 29560
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 11897.1689453125
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 7312.271484375
      var_gnorm: 34.55776596069336
      vf_explained_var: 0.9660043716430664
      vf_loss: 412.3405456542969
    learner_queue:
      size_count: 8863
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 4431500
    num_steps_trained: 4431500
    num_weight_syncs: 17726
    sample_throughput: 15044.33
    timing_breakdown:
      learner_dequeue_time_ms: 24.648
      learner_grad_time_ms: 8.619
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.32
    train_throughput: 15092.861
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5455435867383196
    mean_inference_ms: 1.2701396656906965
    mean_processing_ms: 0.6464821978872258
  time_since_restore: 298.64323592185974
  time_this_iter_s: 10.295067548751831
  time_total_s: 298.64323592185974
  timestamp: 1563412876
  timesteps_since_restore: 4431500
  timesteps_this_iter: 155000
  timesteps_total: 4431500
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 298 s, 29 iter, 4431500 ts, -30.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-21-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 11.124130422174545
  episode_reward_mean: -29.91546547278471
  episode_reward_min: -110.64361852750056
  episodes_this_iter: 1035
  episodes_total: 30595
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 16786.671875
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -3470.239990234375
      var_gnorm: 34.850006103515625
      vf_explained_var: 0.968181848526001
      vf_loss: 482.22381591796875
    learner_queue:
      size_count: 9173
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 4586500
    num_steps_trained: 4586000
    num_weight_syncs: 18346
    sample_throughput: 15027.944
    timing_breakdown:
      learner_dequeue_time_ms: 21.929
      learner_grad_time_ms: 10.407
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.295
    train_throughput: 14979.468
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5453901210270335
    mean_inference_ms: 1.2699426917383891
    mean_processing_ms: 0.646560635492267
  time_since_restore: 308.9489142894745
  time_this_iter_s: 10.305678367614746
  time_total_s: 308.9489142894745
  timestamp: 1563412887
  timesteps_since_restore: 4586500
  timesteps_this_iter: 155000
  timesteps_total: 4586500
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 308 s, 30 iter, 4586500 ts, -29.9 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-21-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.728624674828946
  episode_reward_mean: -30.209245575008854
  episode_reward_min: -128.58079555402435
  episodes_this_iter: 1030
  episodes_total: 31625
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 14394.134765625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -2686.672607421875
      var_gnorm: 35.2077751159668
      vf_explained_var: 0.9874942898750305
      vf_loss: 209.57968139648438
    learner_queue:
      size_count: 9483
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 4741500
    num_steps_trained: 4741500
    num_weight_syncs: 18966
    sample_throughput: 15026.608
    timing_breakdown:
      learner_dequeue_time_ms: 24.563
      learner_grad_time_ms: 9.372
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.37
    train_throughput: 15075.081
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5454071674433574
    mean_inference_ms: 1.2700433150152306
    mean_processing_ms: 0.6466183321311609
  time_since_restore: 319.2543852329254
  time_this_iter_s: 10.305470943450928
  time_total_s: 319.2543852329254
  timestamp: 1563412897
  timesteps_since_restore: 4741500
  timesteps_this_iter: 155000
  timesteps_total: 4741500
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 319 s, 31 iter, 4741500 ts, -30.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-21-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.887025659931467
  episode_reward_mean: -32.272946875385536
  episode_reward_min: -127.88452845876864
  episodes_this_iter: 1030
  episodes_total: 32655
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 20018.423828125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 2865.986572265625
      var_gnorm: 35.51002883911133
      vf_explained_var: 0.9798550605773926
      vf_loss: 271.0079650878906
    learner_queue:
      size_count: 9793
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 4896500
    num_steps_trained: 4896000
    num_weight_syncs: 19586
    sample_throughput: 14999.644
    timing_breakdown:
      learner_dequeue_time_ms: 25.346
      learner_grad_time_ms: 8.006
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 25.671
    train_throughput: 14951.259
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5455509294079905
    mean_inference_ms: 1.2697290699560253
    mean_processing_ms: 0.6467375255163647
  time_since_restore: 329.57933950424194
  time_this_iter_s: 10.324954271316528
  time_total_s: 329.57933950424194
  timestamp: 1563412907
  timesteps_since_restore: 4896500
  timesteps_this_iter: 155000
  timesteps_total: 4896500
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 329 s, 32 iter, 4896500 ts, -32.3 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-21-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.125143266769502
  episode_reward_mean: -31.734256912278482
  episode_reward_min: -146.74212332500963
  episodes_this_iter: 1045
  episodes_total: 33700
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 19325.89453125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -13264.607421875
      var_gnorm: 35.80462646484375
      vf_explained_var: 0.9739042520523071
      vf_loss: 425.5389404296875
    learner_queue:
      size_count: 10103
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 5051500
    num_steps_trained: 5051500
    num_weight_syncs: 20206
    sample_throughput: 15042.42
    timing_breakdown:
      learner_dequeue_time_ms: 24.446
      learner_grad_time_ms: 8.359
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 16.37
    train_throughput: 15090.944
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5455674831701844
    mean_inference_ms: 1.2697188770215933
    mean_processing_ms: 0.6469694992409339
  time_since_restore: 339.875447511673
  time_this_iter_s: 10.29610800743103
  time_total_s: 339.875447511673
  timestamp: 1563412918
  timesteps_since_restore: 5051500
  timesteps_this_iter: 155000
  timesteps_total: 5051500
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 339 s, 33 iter, 5051500 ts, -31.7 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-22-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 9.329764087591604
  episode_reward_mean: -31.77214502865909
  episode_reward_min: -137.36021935867382
  episodes_this_iter: 1040
  episodes_total: 34740
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 24576.640625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -8795.578125
      var_gnorm: 36.15340805053711
      vf_explained_var: 0.973487377166748
      vf_loss: 362.67596435546875
    learner_queue:
      size_count: 10414
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 5207000
    num_steps_trained: 5206500
    num_weight_syncs: 20828
    sample_throughput: 15030.315
    timing_breakdown:
      learner_dequeue_time_ms: 22.071
      learner_grad_time_ms: 8.428
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 14.282
    train_throughput: 14981.987
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.54542423239578
    mean_inference_ms: 1.269674176081312
    mean_processing_ms: 0.6470047114245163
  time_since_restore: 350.2124297618866
  time_this_iter_s: 10.336982250213623
  time_total_s: 350.2124297618866
  timestamp: 1563412928
  timesteps_since_restore: 5207000
  timesteps_this_iter: 155500
  timesteps_total: 5207000
  training_iteration: 34
  2019-07-18 03:22:20,353	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_IMPALA:train()[39m (pid=18657, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 364, in train
    raise e
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 353, in train
    result = Trainable.train(self)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 150, in train
    result = self._train()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 126, in _train
    fetches = self.optimizer.step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 131, in step
    sample_timesteps, train_timesteps = self._step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 173, in _step
    for train_batch in self.aggregator.iter_train_batches():
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 103, in iter_train_batches
    blocking_wait=True, max_yield=max_yield)):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 151, in _augment_with_replay
    sample_batch = ray_get_and_free(sample_batch)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError: [36mray_RolloutWorker:sample()[39m (pid=18664, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 430, in sample
    batches = [self.input_reader.next()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 60, in next
    batches = [self.get_data()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 101, in get_data
    item = next(self.rollout_provider)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 338, in _env_runner
    base_env.send_actions(actions_to_send)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/base_env.py", line 332, in send_actions
    self.vector_env.vector_step(action_vector)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/vector_env.py", line 110, in vector_step
    obs, r, done, info = self.envs[i].step(actions[i])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/gym/wrappers/time_limit.py", line 15, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 57, in step
    self.apply_action(a)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 30, in apply_action
    assert( np.isfinite(a).all() )
AssertionError


2019-07-18 03:22:20,356	INFO ray_trial_executor.py:187 -- Destroying actor for trial IMPALA_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 03:22:20,358	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-18 03:22:20,364	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 350 s, 34 iter, 5207000 ts, -31.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-22-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 9.70212422946102
  episode_reward_mean: -31.226622861260548
  episode_reward_min: -139.99095658349825
  episodes_this_iter: 1025
  episodes_total: 35765
  experiment_id: 70ed9f2dd581452fa8d84c25ab8467ac
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 26055.806640625
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 8451.8125
      var_gnorm: 36.40243911743164
      vf_explained_var: 0.9886037111282349
      vf_loss: 188.48162841796875
    learner_queue:
      size_count: 10724
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 5362000
    num_steps_trained: 5362000
    num_weight_syncs: 21449
    sample_throughput: 15030.698
    timing_breakdown:
      learner_dequeue_time_ms: 23.293
      learner_grad_time_ms: 8.36
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 26.146
    train_throughput: 15079.185
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18657
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5456309870500597
    mean_inference_ms: 1.2695821103672393
    mean_processing_ms: 0.6469789305330659
  time_since_restore: 360.51640701293945
  time_this_iter_s: 10.303977251052856
  time_total_s: 360.51640701293945
  timestamp: 1563412938
  timesteps_since_restore: 5362000
  timesteps_this_iter: 155000
  timesteps_total: 5362000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, [12 CPUs, 1 GPUs], [pid=18657], 360 s, 35 iter, 5362000 ts, -31.2 rew

[2m[36m(pid=18657)[0m 2019-07-18 03:22:20,349	INFO trainer.py:361 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.
[2m[36m(pid=18656)[0m [32m [     0.04810s,  INFO] TimeLimit:
[2m[36m(pid=18656)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18656)[0m - action_space = Box(2,)
[2m[36m(pid=18656)[0m - observation_space = Box(9,)
[2m[36m(pid=18656)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18656)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18656)[0m - _max_episode_steps = 150
[2m[36m(pid=18656)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18656)[0m 2019-07-18 03:22:20.506931: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18656)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18656)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18656)[0m [32m [     1.04537s,  INFO] TimeLimit:
[2m[36m(pid=18656)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18656)[0m - action_space = Box(2,)
[2m[36m(pid=18656)[0m - observation_space = Box(9,)
[2m[36m(pid=18656)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18656)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18656)[0m - _max_episode_steps = 150
[2m[36m(pid=18656)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18656)[0m [32m [     1.04579s,  INFO] TimeLimit:
[2m[36m(pid=18656)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18656)[0m - action_space = Box(2,)
[2m[36m(pid=18656)[0m - observation_space = Box(9,)
[2m[36m(pid=18656)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18656)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18656)[0m - _max_episode_steps = 150
[2m[36m(pid=18656)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18656)[0m [32m [     1.04619s,  INFO] TimeLimit:
[2m[36m(pid=18656)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18656)[0m - action_space = Box(2,)
[2m[36m(pid=18656)[0m - observation_space = Box(9,)
[2m[36m(pid=18656)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18656)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18656)[0m - _max_episode_steps = 150
[2m[36m(pid=18656)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18656)[0m [32m [     1.04659s,  INFO] TimeLimit:
[2m[36m(pid=18656)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18656)[0m - action_space = Box(2,)
[2m[36m(pid=18656)[0m - observation_space = Box(9,)
[2m[36m(pid=18656)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18656)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18656)[0m - _max_episode_steps = 150
[2m[36m(pid=18656)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18656)[0m 2019-07-18 03:22:21,501	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.impala.vtrace_policy.VTraceTFPolicy object at 0x7faae47dd7f0>}
[2m[36m(pid=18656)[0m 2019-07-18 03:22:21,501	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7faae4de8748>}
[2m[36m(pid=18656)[0m 2019-07-18 03:22:21,501	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7faae4de85c0>}
[2m[36m(pid=18659)[0m [32m [     0.03062s,  INFO] TimeLimit:
[2m[36m(pid=18659)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18659)[0m - action_space = Box(2,)
[2m[36m(pid=18659)[0m - observation_space = Box(9,)
[2m[36m(pid=18659)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18659)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18659)[0m - _max_episode_steps = 150
[2m[36m(pid=18659)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18659)[0m 2019-07-18 03:22:21,570	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18659)[0m 2019-07-18 03:22:21.571361: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18653)[0m [32m [     0.08983s,  INFO] TimeLimit:
[2m[36m(pid=18653)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18653)[0m - action_space = Box(2,)
[2m[36m(pid=18653)[0m - observation_space = Box(9,)
[2m[36m(pid=18653)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18653)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18653)[0m - _max_episode_steps = 150
[2m[36m(pid=18653)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18653)[0m 2019-07-18 03:22:21,651	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=18653)[0m 2019-07-18 03:22:21.652306: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=18659)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18659)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18653)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=18653)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=18659)[0m [32m [     2.72481s,  INFO] TimeLimit:
[2m[36m(pid=18659)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18659)[0m - action_space = Box(2,)
[2m[36m(pid=18659)[0m - observation_space = Box(9,)
[2m[36m(pid=18659)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18659)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18659)[0m - _max_episode_steps = 150
[2m[36m(pid=18659)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18659)[0m [32m [     2.72581s,  INFO] TimeLimit:
[2m[36m(pid=18659)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18659)[0m - action_space = Box(2,)
[2m[36m(pid=18659)[0m - observation_space = Box(9,)
[2m[36m(pid=18659)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18659)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18659)[0m - _max_episode_steps = 150
[2m[36m(pid=18659)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18659)[0m [32m [     2.72688s,  INFO] TimeLimit:
[2m[36m(pid=18659)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18659)[0m - action_space = Box(2,)
[2m[36m(pid=18659)[0m - observation_space = Box(9,)
[2m[36m(pid=18659)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18659)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18659)[0m - _max_episode_steps = 150
[2m[36m(pid=18659)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18659)[0m [32m [     2.72801s,  INFO] TimeLimit:
[2m[36m(pid=18659)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18659)[0m - action_space = Box(2,)
[2m[36m(pid=18659)[0m - observation_space = Box(9,)
[2m[36m(pid=18659)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18659)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18659)[0m - _max_episode_steps = 150
[2m[36m(pid=18659)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,321	INFO rollout_worker.py:428 -- Generating sample batch of size 250
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,371	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.584, max=0.812, mean=0.079)},
[2m[36m(pid=18659)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.995, max=0.161, mean=-0.133)},
[2m[36m(pid=18659)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.972, max=0.234, mean=-0.169)},
[2m[36m(pid=18659)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.997, max=0.131, mean=-0.181)},
[2m[36m(pid=18659)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.0, max=0.423, mean=-0.067)}}
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,372	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=18659)[0m   1: {'agent0': None},
[2m[36m(pid=18659)[0m   2: {'agent0': None},
[2m[36m(pid=18659)[0m   3: {'agent0': None},
[2m[36m(pid=18659)[0m   4: {'agent0': None}}
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,372	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.584, max=0.812, mean=0.079)
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,372	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.584, max=0.812, mean=0.079)
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,376	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18659)[0m                                   'env_id': 0,
[2m[36m(pid=18659)[0m                                   'info': None,
[2m[36m(pid=18659)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.584, max=0.812, mean=0.079),
[2m[36m(pid=18659)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18659)[0m                                   'rnn_state': []},
[2m[36m(pid=18659)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18659)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18659)[0m                                   'env_id': 1,
[2m[36m(pid=18659)[0m                                   'info': None,
[2m[36m(pid=18659)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.995, max=0.161, mean=-0.133),
[2m[36m(pid=18659)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18659)[0m                                   'rnn_state': []},
[2m[36m(pid=18659)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18659)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18659)[0m                                   'env_id': 2,
[2m[36m(pid=18659)[0m                                   'info': None,
[2m[36m(pid=18659)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.972, max=0.234, mean=-0.169),
[2m[36m(pid=18659)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18659)[0m                                   'rnn_state': []},
[2m[36m(pid=18659)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18659)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18659)[0m                                   'env_id': 3,
[2m[36m(pid=18659)[0m                                   'info': None,
[2m[36m(pid=18659)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.997, max=0.131, mean=-0.181),
[2m[36m(pid=18659)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18659)[0m                                   'rnn_state': []},
[2m[36m(pid=18659)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=18659)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=18659)[0m                                   'env_id': 4,
[2m[36m(pid=18659)[0m                                   'info': None,
[2m[36m(pid=18659)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.0, max=0.423, mean=-0.067),
[2m[36m(pid=18659)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=18659)[0m                                   'rnn_state': []},
[2m[36m(pid=18659)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,376	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,403	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m { 'default_policy': ( np.ndarray((5, 2), dtype=float32, min=-1.268, max=1.587, mean=0.141),
[2m[36m(pid=18659)[0m                       [],
[2m[36m(pid=18659)[0m                       { 'action_prob': np.ndarray((5,), dtype=float32, min=0.02, max=0.132, mean=0.076),
[2m[36m(pid=18659)[0m                         'behaviour_logits': np.ndarray((5, 4), dtype=float32, min=-0.006, max=0.009, mean=0.001)})}
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,598	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.001, max=0.156, mean=0.073),
[2m[36m(pid=18659)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-2.885, max=2.086, mean=-0.128),
[2m[36m(pid=18659)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                         'behaviour_logits': np.ndarray((50, 4), dtype=float32, min=-0.012, max=0.009, mean=-0.0),
[2m[36m(pid=18659)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=60123521.0, max=60123521.0, mean=60123521.0),
[2m[36m(pid=18659)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=18659)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-2.331, max=0.955, mean=-0.215),
[2m[36m(pid=18659)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.885, max=2.086, mean=-0.097),
[2m[36m(pid=18659)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-1.269, max=2.686, mean=0.087),
[2m[36m(pid=18659)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-1.269, max=2.686, mean=0.085),
[2m[36m(pid=18659)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=18659)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=18659)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m 2019-07-18 03:22:24,606	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18659)[0m { 'data': { 'action_prob': np.ndarray((250,), dtype=float32, min=0.0, max=0.158, mean=0.073),
[2m[36m(pid=18659)[0m             'actions': np.ndarray((250, 2), dtype=float32, min=-3.33, max=3.269, mean=-0.001),
[2m[36m(pid=18659)[0m             'agent_index': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m             'behaviour_logits': np.ndarray((250, 4), dtype=float32, min=-0.012, max=0.011, mean=0.0),
[2m[36m(pid=18659)[0m             'dones': np.ndarray((250,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18659)[0m             'eps_id': np.ndarray((250,), dtype=int64, min=60123521.0, max=1992579007.0, mean=851869247.2),
[2m[36m(pid=18659)[0m             'infos': np.ndarray((250,), dtype=object, head={}),
[2m[36m(pid=18659)[0m             'obs': np.ndarray((250, 9), dtype=float32, min=-3.053, max=3.617, mean=-0.096),
[2m[36m(pid=18659)[0m             'prev_actions': np.ndarray((250, 2), dtype=float32, min=-3.33, max=3.269, mean=0.005),
[2m[36m(pid=18659)[0m             'prev_rewards': np.ndarray((250,), dtype=float32, min=-3.889, max=2.686, mean=-0.042),
[2m[36m(pid=18659)[0m             'rewards': np.ndarray((250,), dtype=float32, min=-3.889, max=2.686, mean=-0.038),
[2m[36m(pid=18659)[0m             't': np.ndarray((250,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=18659)[0m             'unroll_id': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=18659)[0m   'type': 'SampleBatch'}
[2m[36m(pid=18659)[0m 
[2m[36m(pid=18653)[0m [32m [     3.28271s,  INFO] TimeLimit:
[2m[36m(pid=18653)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18653)[0m - action_space = Box(2,)
[2m[36m(pid=18653)[0m - observation_space = Box(9,)
[2m[36m(pid=18653)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18653)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18653)[0m - _max_episode_steps = 150
[2m[36m(pid=18653)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18653)[0m [32m [     3.28366s,  INFO] TimeLimit:
[2m[36m(pid=18653)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18653)[0m - action_space = Box(2,)
[2m[36m(pid=18653)[0m - observation_space = Box(9,)
[2m[36m(pid=18653)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18653)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18653)[0m - _max_episode_steps = 150
[2m[36m(pid=18653)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18653)[0m [32m [     3.28458s,  INFO] TimeLimit:
[2m[36m(pid=18653)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18653)[0m - action_space = Box(2,)
[2m[36m(pid=18653)[0m - observation_space = Box(9,)
[2m[36m(pid=18653)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18653)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18653)[0m - _max_episode_steps = 150
[2m[36m(pid=18653)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18653)[0m [32m [     3.28566s,  INFO] TimeLimit:
[2m[36m(pid=18653)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=18653)[0m - action_space = Box(2,)
[2m[36m(pid=18653)[0m - observation_space = Box(9,)
[2m[36m(pid=18653)[0m - reward_range = (-inf, inf)
[2m[36m(pid=18653)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=18653)[0m - _max_episode_steps = 150
[2m[36m(pid=18653)[0m - _elapsed_steps = None [0m
[2m[36m(pid=18656)[0m 2019-07-18 03:22:24,877	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=18656)[0m 
[2m[36m(pid=18656)[0m { 'data': { 'action_prob': np.ndarray((500,), dtype=float32, min=0.0, max=0.16, mean=0.079),
[2m[36m(pid=18656)[0m             'actions': np.ndarray((500, 2), dtype=float32, min=-3.33, max=3.269, mean=-0.026),
[2m[36m(pid=18656)[0m             'agent_index': np.ndarray((500,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18656)[0m             'behaviour_logits': np.ndarray((500, 4), dtype=float32, min=-0.012, max=0.014, mean=0.0),
[2m[36m(pid=18656)[0m             'dones': np.ndarray((500,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=18656)[0m             'eps_id': np.ndarray((500,), dtype=int64, min=60123521.0, max=1992579007.0, mean=851869247.2),
[2m[36m(pid=18656)[0m             'infos': np.ndarray((500,), dtype=object, head={}),
[2m[36m(pid=18656)[0m             'obs': np.ndarray((500, 9), dtype=float32, min=-3.88, max=3.617, mean=-0.058),
[2m[36m(pid=18656)[0m             'prev_actions': np.ndarray((500, 2), dtype=float32, min=-3.33, max=3.269, mean=-0.026),
[2m[36m(pid=18656)[0m             'prev_rewards': np.ndarray((500,), dtype=float32, min=-3.889, max=2.912, mean=-0.016),
[2m[36m(pid=18656)[0m             'rewards': np.ndarray((500,), dtype=float32, min=-3.889, max=2.912, mean=-0.007),
[2m[36m(pid=18656)[0m             't': np.ndarray((500,), dtype=int64, min=0.0, max=99.0, mean=49.5),
[2m[36m(pid=18656)[0m             'unroll_id': np.ndarray((500,), dtype=int64, min=0.0, max=1.0, mean=0.5)},
[2m[36m(pid=18656)[0m   'type': 'SampleBatch'}
[2m[36m(pid=18656)[0m 
[2m[36m(pid=18656)[0m 2019-07-18 03:22:24,878	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=18656)[0m 2019-07-18 03:22:25,949	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=18656)[0m 
[2m[36m(pid=18656)[0m { 'learner_stats': { 'cur_lr': 0.0005000000237487257,
[2m[36m(pid=18656)[0m                      'entropy': 1390.6041,
[2m[36m(pid=18656)[0m                      'grad_gnorm': 39.999992,
[2m[36m(pid=18656)[0m                      'model': {},
[2m[36m(pid=18656)[0m                      'policy_loss': -1422.0979,
[2m[36m(pid=18656)[0m                      'var_gnorm': 22.649513,
[2m[36m(pid=18656)[0m                      'vf_explained_var': -0.03151822,
[2m[36m(pid=18656)[0m                      'vf_loss': 11171.84}}
[2m[36m(pid=18656)[0m 
[2m[36m(pid=19043)[0m [32m [     0.11062s,  INFO] TimeLimit:
[2m[36m(pid=19043)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19043)[0m - action_space = Box(2,)
[2m[36m(pid=19043)[0m - observation_space = Box(9,)
[2m[36m(pid=19043)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19043)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19043)[0m - _max_episode_steps = 150
[2m[36m(pid=19043)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19043)[0m 2019-07-18 03:22:27,368	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19043)[0m 2019-07-18 03:22:27.369469: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19041)[0m [32m [     0.05843s,  INFO] TimeLimit:
[2m[36m(pid=19041)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19041)[0m - action_space = Box(2,)
[2m[36m(pid=19041)[0m - observation_space = Box(9,)
[2m[36m(pid=19041)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19041)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19041)[0m - _max_episode_steps = 150
[2m[36m(pid=19041)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19041)[0m 2019-07-18 03:22:27,549	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19041)[0m 2019-07-18 03:22:27.550234: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19044)[0m [32m [     0.07892s,  INFO] TimeLimit:
[2m[36m(pid=19044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19044)[0m - action_space = Box(2,)
[2m[36m(pid=19044)[0m - observation_space = Box(9,)
[2m[36m(pid=19044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19044)[0m - _max_episode_steps = 150
[2m[36m(pid=19044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19044)[0m 2019-07-18 03:22:27,754	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19044)[0m 2019-07-18 03:22:27.754970: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19057)[0m 2019-07-18 03:22:27,831	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19057)[0m 2019-07-18 03:22:27.832082: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19057)[0m [32m [     0.03999s,  INFO] TimeLimit:
[2m[36m(pid=19057)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19057)[0m - action_space = Box(2,)
[2m[36m(pid=19057)[0m - observation_space = Box(9,)
[2m[36m(pid=19057)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19057)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19057)[0m - _max_episode_steps = 150
[2m[36m(pid=19057)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19045)[0m 2019-07-18 03:22:27,837	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19045)[0m 2019-07-18 03:22:27.838142: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19045)[0m [32m [     0.07709s,  INFO] TimeLimit:
[2m[36m(pid=19045)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19045)[0m - action_space = Box(2,)
[2m[36m(pid=19045)[0m - observation_space = Box(9,)
[2m[36m(pid=19045)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19045)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19045)[0m - _max_episode_steps = 150
[2m[36m(pid=19045)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19040)[0m 2019-07-18 03:22:28,146	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19040)[0m 2019-07-18 03:22:28.146642: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19040)[0m [32m [     0.11261s,  INFO] TimeLimit:
[2m[36m(pid=19040)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19040)[0m - action_space = Box(2,)
[2m[36m(pid=19040)[0m - observation_space = Box(9,)
[2m[36m(pid=19040)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19040)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19040)[0m - _max_episode_steps = 150
[2m[36m(pid=19040)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19073)[0m [32m [     0.05528s,  INFO] TimeLimit:
[2m[36m(pid=19073)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19073)[0m - action_space = Box(2,)
[2m[36m(pid=19073)[0m - observation_space = Box(9,)
[2m[36m(pid=19073)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19073)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19073)[0m - _max_episode_steps = 150
[2m[36m(pid=19073)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19073)[0m 2019-07-18 03:22:28,168	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19073)[0m 2019-07-18 03:22:28.169280: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19042)[0m [32m [     0.08050s,  INFO] TimeLimit:
[2m[36m(pid=19042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19042)[0m - action_space = Box(2,)
[2m[36m(pid=19042)[0m - observation_space = Box(9,)
[2m[36m(pid=19042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19042)[0m - _max_episode_steps = 150
[2m[36m(pid=19042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19042)[0m 2019-07-18 03:22:28,248	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19042)[0m 2019-07-18 03:22:28.248788: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19041)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19041)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19057)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19057)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19043)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19043)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19085)[0m 2019-07-18 03:22:29,093	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19085)[0m 2019-07-18 03:22:29.094376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19085)[0m [32m [     0.07175s,  INFO] TimeLimit:
[2m[36m(pid=19085)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19085)[0m - action_space = Box(2,)
[2m[36m(pid=19085)[0m - observation_space = Box(9,)
[2m[36m(pid=19085)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19085)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19085)[0m - _max_episode_steps = 150
[2m[36m(pid=19085)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19040)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19040)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19073)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19073)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19045)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19045)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19042)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19042)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19044)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19044)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19041)[0m [32m [     2.29082s,  INFO] TimeLimit:
[2m[36m(pid=19041)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19041)[0m - action_space = Box(2,)
[2m[36m(pid=19041)[0m - observation_space = Box(9,)
[2m[36m(pid=19041)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19041)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19041)[0m - _max_episode_steps = 150
[2m[36m(pid=19041)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19041)[0m [32m [     2.29195s,  INFO] TimeLimit:
[2m[36m(pid=19041)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19041)[0m - action_space = Box(2,)
[2m[36m(pid=19041)[0m - observation_space = Box(9,)
[2m[36m(pid=19041)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19041)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19041)[0m - _max_episode_steps = 150
[2m[36m(pid=19041)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19041)[0m [32m [     2.29292s,  INFO] TimeLimit:
[2m[36m(pid=19041)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19041)[0m - action_space = Box(2,)
[2m[36m(pid=19041)[0m - observation_space = Box(9,)
[2m[36m(pid=19041)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19041)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19041)[0m - _max_episode_steps = 150
[2m[36m(pid=19041)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19041)[0m [32m [     2.29407s,  INFO] TimeLimit:
[2m[36m(pid=19041)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19041)[0m - action_space = Box(2,)
[2m[36m(pid=19041)[0m - observation_space = Box(9,)
[2m[36m(pid=19041)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19041)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19041)[0m - _max_episode_steps = 150
[2m[36m(pid=19041)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19057)[0m [32m [     1.98654s,  INFO] TimeLimit:
[2m[36m(pid=19057)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19057)[0m - action_space = Box(2,)
[2m[36m(pid=19057)[0m - observation_space = Box(9,)
[2m[36m(pid=19057)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19057)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19057)[0m - _max_episode_steps = 150
[2m[36m(pid=19057)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19057)[0m [32m [     1.98753s,  INFO] TimeLimit:
[2m[36m(pid=19057)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19057)[0m - action_space = Box(2,)
[2m[36m(pid=19057)[0m - observation_space = Box(9,)
[2m[36m(pid=19057)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19057)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19057)[0m - _max_episode_steps = 150
[2m[36m(pid=19057)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19057)[0m [32m [     1.98847s,  INFO] TimeLimit:
[2m[36m(pid=19057)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19057)[0m - action_space = Box(2,)
[2m[36m(pid=19057)[0m - observation_space = Box(9,)
[2m[36m(pid=19057)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19057)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19057)[0m - _max_episode_steps = 150
[2m[36m(pid=19057)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19057)[0m [32m [     1.98954s,  INFO] TimeLimit:
[2m[36m(pid=19057)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19057)[0m - action_space = Box(2,)
[2m[36m(pid=19057)[0m - observation_space = Box(9,)
[2m[36m(pid=19057)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19057)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19057)[0m - _max_episode_steps = 150
[2m[36m(pid=19057)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19043)[0m [32m [     2.60375s,  INFO] TimeLimit:
[2m[36m(pid=19043)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19043)[0m - action_space = Box(2,)
[2m[36m(pid=19043)[0m - observation_space = Box(9,)
[2m[36m(pid=19043)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19043)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19043)[0m - _max_episode_steps = 150
[2m[36m(pid=19043)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19043)[0m [32m [     2.60469s,  INFO] TimeLimit:
[2m[36m(pid=19043)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19043)[0m - action_space = Box(2,)
[2m[36m(pid=19043)[0m - observation_space = Box(9,)
[2m[36m(pid=19043)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19043)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19043)[0m - _max_episode_steps = 150
[2m[36m(pid=19043)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19043)[0m [32m [     2.60561s,  INFO] TimeLimit:
[2m[36m(pid=19043)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19043)[0m - action_space = Box(2,)
[2m[36m(pid=19043)[0m - observation_space = Box(9,)
[2m[36m(pid=19043)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19043)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19043)[0m - _max_episode_steps = 150
[2m[36m(pid=19043)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19043)[0m [32m [     2.60673s,  INFO] TimeLimit:
[2m[36m(pid=19043)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19043)[0m - action_space = Box(2,)
[2m[36m(pid=19043)[0m - observation_space = Box(9,)
[2m[36m(pid=19043)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19043)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19043)[0m - _max_episode_steps = 150
[2m[36m(pid=19043)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19040)[0m [32m [     1.98030s,  INFO] TimeLimit:
[2m[36m(pid=19040)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19040)[0m - action_space = Box(2,)
[2m[36m(pid=19040)[0m - observation_space = Box(9,)
[2m[36m(pid=19040)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19040)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19040)[0m - _max_episode_steps = 150
[2m[36m(pid=19040)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19040)[0m [32m [     1.98131s,  INFO] TimeLimit:
[2m[36m(pid=19040)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19040)[0m - action_space = Box(2,)
[2m[36m(pid=19040)[0m - observation_space = Box(9,)
[2m[36m(pid=19040)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19040)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19040)[0m - _max_episode_steps = 150
[2m[36m(pid=19040)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19040)[0m [32m [     1.98229s,  INFO] TimeLimit:
[2m[36m(pid=19040)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19040)[0m - action_space = Box(2,)
[2m[36m(pid=19040)[0m - observation_space = Box(9,)
[2m[36m(pid=19040)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19040)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19040)[0m - _max_episode_steps = 150
[2m[36m(pid=19040)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19040)[0m [32m [     1.98327s,  INFO] TimeLimit:
[2m[36m(pid=19040)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19040)[0m - action_space = Box(2,)
[2m[36m(pid=19040)[0m - observation_space = Box(9,)
[2m[36m(pid=19040)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19040)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19040)[0m - _max_episode_steps = 150
[2m[36m(pid=19040)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19085)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19085)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19073)[0m [32m [     2.01641s,  INFO] TimeLimit:
[2m[36m(pid=19073)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19073)[0m - action_space = Box(2,)
[2m[36m(pid=19073)[0m - observation_space = Box(9,)
[2m[36m(pid=19073)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19073)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19073)[0m - _max_episode_steps = 150
[2m[36m(pid=19073)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19073)[0m [32m [     2.01738s,  INFO] TimeLimit:
[2m[36m(pid=19073)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19073)[0m - action_space = Box(2,)
[2m[36m(pid=19073)[0m - observation_space = Box(9,)
[2m[36m(pid=19073)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19073)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19073)[0m - _max_episode_steps = 150
[2m[36m(pid=19073)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19073)[0m [32m [     2.01832s,  INFO] TimeLimit:
[2m[36m(pid=19073)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19073)[0m - action_space = Box(2,)
[2m[36m(pid=19073)[0m - observation_space = Box(9,)
[2m[36m(pid=19073)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19073)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19073)[0m - _max_episode_steps = 150
[2m[36m(pid=19073)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19073)[0m [32m [     2.01946s,  INFO] TimeLimit:
[2m[36m(pid=19073)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19073)[0m - action_space = Box(2,)
[2m[36m(pid=19073)[0m - observation_space = Box(9,)
[2m[36m(pid=19073)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19073)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19073)[0m - _max_episode_steps = 150
[2m[36m(pid=19073)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19042)[0m [32m [     2.00266s,  INFO] TimeLimit:
[2m[36m(pid=19042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19042)[0m - action_space = Box(2,)
[2m[36m(pid=19042)[0m - observation_space = Box(9,)
[2m[36m(pid=19042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19042)[0m - _max_episode_steps = 150
[2m[36m(pid=19042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19042)[0m [32m [     2.00366s,  INFO] TimeLimit:
[2m[36m(pid=19042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19042)[0m - action_space = Box(2,)
[2m[36m(pid=19042)[0m - observation_space = Box(9,)
[2m[36m(pid=19042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19042)[0m - _max_episode_steps = 150
[2m[36m(pid=19042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19042)[0m [32m [     2.00460s,  INFO] TimeLimit:
[2m[36m(pid=19042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19042)[0m - action_space = Box(2,)
[2m[36m(pid=19042)[0m - observation_space = Box(9,)
[2m[36m(pid=19042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19042)[0m - _max_episode_steps = 150
[2m[36m(pid=19042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19042)[0m [32m [     2.00570s,  INFO] TimeLimit:
[2m[36m(pid=19042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19042)[0m - action_space = Box(2,)
[2m[36m(pid=19042)[0m - observation_space = Box(9,)
[2m[36m(pid=19042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19042)[0m - _max_episode_steps = 150
[2m[36m(pid=19042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19045)[0m [32m [     2.45971s,  INFO] TimeLimit:
[2m[36m(pid=19045)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19045)[0m - action_space = Box(2,)
[2m[36m(pid=19045)[0m - observation_space = Box(9,)
[2m[36m(pid=19045)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19045)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19045)[0m - _max_episode_steps = 150
[2m[36m(pid=19045)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19045)[0m [32m [     2.46069s,  INFO] TimeLimit:
[2m[36m(pid=19045)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19045)[0m - action_space = Box(2,)
[2m[36m(pid=19045)[0m - observation_space = Box(9,)
[2m[36m(pid=19045)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19045)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19045)[0m - _max_episode_steps = 150
[2m[36m(pid=19045)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19045)[0m [32m [     2.46164s,  INFO] TimeLimit:
[2m[36m(pid=19045)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19045)[0m - action_space = Box(2,)
[2m[36m(pid=19045)[0m - observation_space = Box(9,)
[2m[36m(pid=19045)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19045)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19045)[0m - _max_episode_steps = 150
[2m[36m(pid=19045)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19045)[0m [32m [     2.46285s,  INFO] TimeLimit:
[2m[36m(pid=19045)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19045)[0m - action_space = Box(2,)
[2m[36m(pid=19045)[0m - observation_space = Box(9,)
[2m[36m(pid=19045)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19045)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19045)[0m - _max_episode_steps = 150
[2m[36m(pid=19045)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19044)[0m [32m [     2.61101s,  INFO] TimeLimit:
[2m[36m(pid=19044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19044)[0m - action_space = Box(2,)
[2m[36m(pid=19044)[0m - observation_space = Box(9,)
[2m[36m(pid=19044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19044)[0m - _max_episode_steps = 150
[2m[36m(pid=19044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19044)[0m [32m [     2.61199s,  INFO] TimeLimit:
[2m[36m(pid=19044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19044)[0m - action_space = Box(2,)
[2m[36m(pid=19044)[0m - observation_space = Box(9,)
[2m[36m(pid=19044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19044)[0m - _max_episode_steps = 150
[2m[36m(pid=19044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19044)[0m [32m [     2.61294s,  INFO] TimeLimit:
[2m[36m(pid=19044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19044)[0m - action_space = Box(2,)
[2m[36m(pid=19044)[0m - observation_space = Box(9,)
[2m[36m(pid=19044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19044)[0m - _max_episode_steps = 150
[2m[36m(pid=19044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19044)[0m [32m [     2.61408s,  INFO] TimeLimit:
[2m[36m(pid=19044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19044)[0m - action_space = Box(2,)
[2m[36m(pid=19044)[0m - observation_space = Box(9,)
[2m[36m(pid=19044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19044)[0m - _max_episode_steps = 150
[2m[36m(pid=19044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19085)[0m [32m [     1.66474s,  INFO] TimeLimit:
[2m[36m(pid=19085)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19085)[0m - action_space = Box(2,)
[2m[36m(pid=19085)[0m - observation_space = Box(9,)
[2m[36m(pid=19085)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19085)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19085)[0m - _max_episode_steps = 150
[2m[36m(pid=19085)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19085)[0m [32m [     1.66572s,  INFO] TimeLimit:
[2m[36m(pid=19085)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19085)[0m - action_space = Box(2,)
[2m[36m(pid=19085)[0m - observation_space = Box(9,)
[2m[36m(pid=19085)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19085)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19085)[0m - _max_episode_steps = 150
[2m[36m(pid=19085)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19085)[0m [32m [     1.66678s,  INFO] TimeLimit:
[2m[36m(pid=19085)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19085)[0m - action_space = Box(2,)
[2m[36m(pid=19085)[0m - observation_space = Box(9,)
[2m[36m(pid=19085)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19085)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19085)[0m - _max_episode_steps = 150
[2m[36m(pid=19085)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19085)[0m [32m [     1.66793s,  INFO] TimeLimit:
[2m[36m(pid=19085)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19085)[0m - action_space = Box(2,)
[2m[36m(pid=19085)[0m - observation_space = Box(9,)
[2m[36m(pid=19085)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19085)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19085)[0m - _max_episode_steps = 150
[2m[36m(pid=19085)[0m - _elapsed_steps = None [0m
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-22-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.970565135808963
  episode_reward_mean: -12.33135288406492
  episode_reward_min: -44.613662334409895
  episodes_this_iter: 260
  episodes_total: 260
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1380.7916259765625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -110.45115661621094
      var_gnorm: 22.680896759033203
      vf_explained_var: 0.11452174186706543
      vf_loss: 3144.291748046875
    learner_queue:
      size_count: 73
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 36500
    num_steps_trained: 36500
    num_weight_syncs: 146
    sample_throughput: 3416.58
    timing_breakdown:
      learner_dequeue_time_ms: 24.046
      learner_grad_time_ms: 9.897
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.897
    train_throughput: 3416.58
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6279396738013818
    mean_inference_ms: 1.562283540619522
    mean_processing_ms: 0.658546531422625
  time_since_restore: 10.324702024459839
  time_this_iter_s: 10.324702024459839
  time_total_s: 10.324702024459839
  timestamp: 1563412952
  timesteps_since_restore: 36500
  timesteps_this_iter: 36500
  timesteps_total: 36500
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 10 s, 1 iter, 36500 ts, -12.3 rew

[2m[36m(pid=18656)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
[2m[36m(pid=18656)[0m   out=out, **kwargs)
[2m[36m(pid=18656)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
[2m[36m(pid=18656)[0m   ret = ret.dtype.type(ret / rcount)
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-22-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 11.082043724066944
  episode_reward_mean: -84.48018166634981
  episode_reward_min: -204.8314324271205
  episodes_this_iter: 1025
  episodes_total: 1285
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1108.9981689453125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 383.1502685546875
      var_gnorm: 22.969589233398438
      vf_explained_var: 0.9836170077323914
      vf_loss: 264.8182373046875
    learner_queue:
      size_count: 379
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 189500
    num_steps_trained: 189500
    num_weight_syncs: 759
    sample_throughput: 14857.043
    timing_breakdown:
      learner_dequeue_time_ms: 22.929
      learner_grad_time_ms: 8.546
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 16.366
    train_throughput: 14857.043
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5903875335302518
    mean_inference_ms: 1.3212405217632481
    mean_processing_ms: 0.6523906644293115
  time_since_restore: 20.615028619766235
  time_this_iter_s: 10.290326595306396
  time_total_s: 20.615028619766235
  timestamp: 1563412962
  timesteps_since_restore: 189500
  timesteps_this_iter: 153000
  timesteps_total: 189500
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 20 s, 2 iter, 189500 ts, -84.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-22-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.026428996961673
  episode_reward_mean: -124.33199354810034
  episode_reward_min: -206.15962630985723
  episodes_this_iter: 1015
  episodes_total: 2300
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1587.6605224609375
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 38.44255065917969
      var_gnorm: 23.122526168823242
      vf_explained_var: 0.6806445121765137
      vf_loss: 1524.4757080078125
    learner_queue:
      size_count: 686
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 343000
    num_steps_trained: 342500
    num_weight_syncs: 1372
    sample_throughput: 14904.355
    timing_breakdown:
      learner_dequeue_time_ms: 26.672
      learner_grad_time_ms: 8.823
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.619
    train_throughput: 14855.807
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.586928653596067
    mean_inference_ms: 1.297508160786762
    mean_processing_ms: 0.6513382283414504
  time_since_restore: 30.906733989715576
  time_this_iter_s: 10.29170536994934
  time_total_s: 30.906733989715576
  timestamp: 1563412972
  timesteps_since_restore: 343000
  timesteps_this_iter: 153500
  timesteps_total: 343000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 30 s, 3 iter, 343000 ts, -124 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-23-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 14.294470950448513
  episode_reward_mean: -136.3486991958727
  episode_reward_min: -206.67040187598383
  episodes_this_iter: 1025
  episodes_total: 3325
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2475.684326171875
      grad_gnorm: 39.99999237060547
      model: {}
      policy_loss: 216.88323974609375
      var_gnorm: 23.336225509643555
      vf_explained_var: 0.11055600643157959
      vf_loss: 1704.4642333984375
    learner_queue:
      size_count: 994
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 497000
    num_steps_trained: 497000
    num_weight_syncs: 1988
    sample_throughput: 14919.93
    timing_breakdown:
      learner_dequeue_time_ms: 17.749
      learner_grad_time_ms: 9.442
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.934
    train_throughput: 14968.372
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5858731983234513
    mean_inference_ms: 1.2871127514228682
    mean_processing_ms: 0.6513893716008831
  time_since_restore: 41.22106051445007
  time_this_iter_s: 10.314326524734497
  time_total_s: 41.22106051445007
  timestamp: 1563412983
  timesteps_since_restore: 497000
  timesteps_this_iter: 154000
  timesteps_total: 497000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 41 s, 4 iter, 497000 ts, -136 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-23-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.611538954526633
  episode_reward_mean: -60.22174294752168
  episode_reward_min: -188.60353586833298
  episodes_this_iter: 1040
  episodes_total: 4365
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1096.05126953125
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 272.8912048339844
      var_gnorm: 23.4903621673584
      vf_explained_var: 0.8279653787612915
      vf_loss: 910.3638305664062
    learner_queue:
      size_count: 1302
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 651000
    num_steps_trained: 651000
    num_weight_syncs: 2604
    sample_throughput: 14917.776
    timing_breakdown:
      learner_dequeue_time_ms: 23.498
      learner_grad_time_ms: 9.211
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.385
    train_throughput: 14917.777
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.582548450382864
    mean_inference_ms: 1.2840805575527816
    mean_processing_ms: 0.6509127252867459
  time_since_restore: 51.5368287563324
  time_this_iter_s: 10.315768241882324
  time_total_s: 51.5368287563324
  timestamp: 1563412993
  timesteps_since_restore: 651000
  timesteps_this_iter: 154000
  timesteps_total: 651000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 51 s, 5 iter, 651000 ts, -60.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-23-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.157403335006867
  episode_reward_mean: -49.6278272965697
  episode_reward_min: -184.35350283351332
  episodes_this_iter: 1015
  episodes_total: 5380
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 3070.18505859375
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -403.7505187988281
      var_gnorm: 23.953493118286133
      vf_explained_var: 0.9873753190040588
      vf_loss: 141.60104370117188
    learner_queue:
      size_count: 1608
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 804000
    num_steps_trained: 804000
    num_weight_syncs: 3216
    sample_throughput: 14838.543
    timing_breakdown:
      learner_dequeue_time_ms: 25.756
      learner_grad_time_ms: 8.338
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.865
    train_throughput: 14838.544
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5831671399324854
    mean_inference_ms: 1.2821070140518886
    mean_processing_ms: 0.6510916110636467
  time_since_restore: 61.83911156654358
  time_this_iter_s: 10.302282810211182
  time_total_s: 61.83911156654358
  timestamp: 1563413003
  timesteps_since_restore: 804000
  timesteps_this_iter: 153000
  timesteps_total: 804000
  training_iteration: 6
  2019-07-18 03:23:57,176	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_IMPALA:train()[39m (pid=18656, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 364, in train
    raise e
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 353, in train
    result = Trainable.train(self)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 150, in train
    result = self._train()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 126, in _train
    fetches = self.optimizer.step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 131, in step
    sample_timesteps, train_timesteps = self._step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 173, in _step
    for train_batch in self.aggregator.iter_train_batches():
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 103, in iter_train_batches
    blocking_wait=True, max_yield=max_yield)):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 151, in _augment_with_replay
    sample_batch = ray_get_and_free(sample_batch)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError: [36mray_RolloutWorker:sample()[39m (pid=19042, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 430, in sample
    batches = [self.input_reader.next()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 60, in next
    batches = [self.get_data()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 101, in get_data
    item = next(self.rollout_provider)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 338, in _env_runner
    base_env.send_actions(actions_to_send)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/base_env.py", line 332, in send_actions
    self.vector_env.vector_step(action_vector)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/vector_env.py", line 110, in vector_step
    obs, r, done, info = self.envs[i].step(actions[i])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/gym/wrappers/time_limit.py", line 15, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 57, in step
    self.apply_action(a)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 30, in apply_action
    assert( np.isfinite(a).all() )
AssertionError


2019-07-18 03:23:57,178	INFO ray_trial_executor.py:187 -- Destroying actor for trial IMPALA_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 03:23:57,183	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-18 03:23:57,187	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 61 s, 6 iter, 804000 ts, -49.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-23-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 9.162077948769534
  episode_reward_mean: -20.57576416777476
  episode_reward_min: -136.39046194096224
  episodes_this_iter: 1020
  episodes_total: 6400
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 9566.7314453125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 2819.127685546875
      var_gnorm: 25.07550811767578
      vf_explained_var: 0.7999658584594727
      vf_loss: 1400.6285400390625
    learner_queue:
      size_count: 1916
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 958000
    num_steps_trained: 957500
    num_weight_syncs: 3832
    sample_throughput: 14934.515
    timing_breakdown:
      learner_dequeue_time_ms: 27.038
      learner_grad_time_ms: 8.91
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.335
    train_throughput: 14886.027
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5799528766545055
    mean_inference_ms: 1.280429736525786
    mean_processing_ms: 0.6511896401520447
  time_since_restore: 72.14287281036377
  time_this_iter_s: 10.30376124382019
  time_total_s: 72.14287281036377
  timestamp: 1563413014
  timesteps_since_restore: 958000
  timesteps_this_iter: 154000
  timesteps_total: 958000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 72 s, 7 iter, 958000 ts, -20.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-23-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 11.466924266237772
  episode_reward_mean: -28.81264284854677
  episode_reward_min: -157.4725294105569
  episodes_this_iter: 1025
  episodes_total: 7425
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 9062.0361328125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 3530.099365234375
      var_gnorm: 25.491546630859375
      vf_explained_var: 0.30989891290664673
      vf_loss: 5660.28662109375
    learner_queue:
      size_count: 2224
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1112000
    num_steps_trained: 1112000
    num_weight_syncs: 4448
    sample_throughput: 14959.855
    timing_breakdown:
      learner_dequeue_time_ms: 26.848
      learner_grad_time_ms: 8.404
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.893
    train_throughput: 15008.426
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5775975363671708
    mean_inference_ms: 1.2783413064913962
    mean_processing_ms: 0.6508146365681362
  time_since_restore: 82.42986297607422
  time_this_iter_s: 10.28699016571045
  time_total_s: 82.42986297607422
  timestamp: 1563413024
  timesteps_since_restore: 1112000
  timesteps_this_iter: 154000
  timesteps_total: 1112000
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 82 s, 8 iter, 1112000 ts, -28.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-23-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 8.370288726060831
  episode_reward_mean: -27.543103463212944
  episode_reward_min: -122.44341930641178
  episodes_this_iter: 1045
  episodes_total: 8470
  experiment_id: e4b8b0a6fc534b2eba0063dc11663b73
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 13956.1328125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1429.847900390625
      var_gnorm: 26.150068283081055
      vf_explained_var: 0.8399115204811096
      vf_loss: 323.1555480957031
    learner_queue:
      size_count: 2533
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1266500
    num_steps_trained: 1266500
    num_weight_syncs: 5066
    sample_throughput: 14990.83
    timing_breakdown:
      learner_dequeue_time_ms: 23.607
      learner_grad_time_ms: 9.155
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.499
    train_throughput: 14990.83
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 18656
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5761509261945834
    mean_inference_ms: 1.2762453696483276
    mean_processing_ms: 0.6512316830097855
  time_since_restore: 92.72888469696045
  time_this_iter_s: 10.29902172088623
  time_total_s: 92.72888469696045
  timestamp: 1563413034
  timesteps_since_restore: 1266500
  timesteps_this_iter: 154500
  timesteps_total: 1266500
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 1 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-22-20.txt, [12 CPUs, 1 GPUs], [pid=18656], 92 s, 9 iter, 1266500 ts, -27.5 rew

[2m[36m(pid=18656)[0m 2019-07-18 03:23:57,172	INFO trainer.py:361 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.
[2m[36m(pid=19262)[0m 2019-07-18 03:23:57.404067: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19262)[0m [32m [     0.05251s,  INFO] TimeLimit:
[2m[36m(pid=19262)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19262)[0m - action_space = Box(2,)
[2m[36m(pid=19262)[0m - observation_space = Box(9,)
[2m[36m(pid=19262)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19262)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19262)[0m - _max_episode_steps = 150
[2m[36m(pid=19262)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19262)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19262)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19262)[0m 2019-07-18 03:23:58,341	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.impala.vtrace_policy.VTraceTFPolicy object at 0x7f468a8dd7f0>}
[2m[36m(pid=19262)[0m 2019-07-18 03:23:58,341	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f468aee96d8>}
[2m[36m(pid=19262)[0m 2019-07-18 03:23:58,341	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f468aee9588>}
[2m[36m(pid=19262)[0m [32m [     0.99200s,  INFO] TimeLimit:
[2m[36m(pid=19262)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19262)[0m - action_space = Box(2,)
[2m[36m(pid=19262)[0m - observation_space = Box(9,)
[2m[36m(pid=19262)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19262)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19262)[0m - _max_episode_steps = 150
[2m[36m(pid=19262)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19262)[0m [32m [     0.99242s,  INFO] TimeLimit:
[2m[36m(pid=19262)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19262)[0m - action_space = Box(2,)
[2m[36m(pid=19262)[0m - observation_space = Box(9,)
[2m[36m(pid=19262)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19262)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19262)[0m - _max_episode_steps = 150
[2m[36m(pid=19262)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19262)[0m [32m [     0.99282s,  INFO] TimeLimit:
[2m[36m(pid=19262)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19262)[0m - action_space = Box(2,)
[2m[36m(pid=19262)[0m - observation_space = Box(9,)
[2m[36m(pid=19262)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19262)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19262)[0m - _max_episode_steps = 150
[2m[36m(pid=19262)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19262)[0m [32m [     0.99322s,  INFO] TimeLimit:
[2m[36m(pid=19262)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19262)[0m - action_space = Box(2,)
[2m[36m(pid=19262)[0m - observation_space = Box(9,)
[2m[36m(pid=19262)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19262)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19262)[0m - _max_episode_steps = 150
[2m[36m(pid=19262)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19214)[0m [32m [     0.03692s,  INFO] TimeLimit:
[2m[36m(pid=19214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19214)[0m - action_space = Box(2,)
[2m[36m(pid=19214)[0m - observation_space = Box(9,)
[2m[36m(pid=19214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19214)[0m - _max_episode_steps = 150
[2m[36m(pid=19214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19214)[0m 2019-07-18 03:23:58,448	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19214)[0m 2019-07-18 03:23:58.449438: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19208)[0m 2019-07-18 03:23:58,431	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19208)[0m 2019-07-18 03:23:58.431388: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19208)[0m [32m [     0.03124s,  INFO] TimeLimit:
[2m[36m(pid=19208)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19208)[0m - action_space = Box(2,)
[2m[36m(pid=19208)[0m - observation_space = Box(9,)
[2m[36m(pid=19208)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19208)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19208)[0m - _max_episode_steps = 150
[2m[36m(pid=19208)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19248)[0m 2019-07-18 03:23:58,423	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19248)[0m 2019-07-18 03:23:58.423502: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19234)[0m [32m [     0.02702s,  INFO] TimeLimit:
[2m[36m(pid=19234)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19234)[0m - action_space = Box(2,)
[2m[36m(pid=19234)[0m - observation_space = Box(9,)
[2m[36m(pid=19234)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19234)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19234)[0m - _max_episode_steps = 150
[2m[36m(pid=19234)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m 2019-07-18 03:23:58,411	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19234)[0m 2019-07-18 03:23:58.411888: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19217)[0m [32m [     0.02999s,  INFO] TimeLimit:
[2m[36m(pid=19217)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19217)[0m - action_space = Box(2,)
[2m[36m(pid=19217)[0m - observation_space = Box(9,)
[2m[36m(pid=19217)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19217)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19217)[0m - _max_episode_steps = 150
[2m[36m(pid=19217)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19248)[0m [32m [     0.03088s,  INFO] TimeLimit:
[2m[36m(pid=19248)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19248)[0m - action_space = Box(2,)
[2m[36m(pid=19248)[0m - observation_space = Box(9,)
[2m[36m(pid=19248)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19248)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19248)[0m - _max_episode_steps = 150
[2m[36m(pid=19248)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19217)[0m 2019-07-18 03:23:58,423	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19217)[0m 2019-07-18 03:23:58.423693: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19221)[0m [32m [     0.03607s,  INFO] TimeLimit:
[2m[36m(pid=19221)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19221)[0m - action_space = Box(2,)
[2m[36m(pid=19221)[0m - observation_space = Box(9,)
[2m[36m(pid=19221)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19221)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19221)[0m - _max_episode_steps = 150
[2m[36m(pid=19221)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19221)[0m 2019-07-18 03:23:58,460	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19221)[0m 2019-07-18 03:23:58.461496: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19159)[0m [32m [     0.03826s,  INFO] TimeLimit:
[2m[36m(pid=19159)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19159)[0m - action_space = Box(2,)
[2m[36m(pid=19159)[0m - observation_space = Box(9,)
[2m[36m(pid=19159)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19159)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19159)[0m - _max_episode_steps = 150
[2m[36m(pid=19159)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19159)[0m 2019-07-18 03:23:58,510	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19159)[0m 2019-07-18 03:23:58.511096: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19160)[0m [32m [     0.03455s,  INFO] TimeLimit:
[2m[36m(pid=19160)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19160)[0m - action_space = Box(2,)
[2m[36m(pid=19160)[0m - observation_space = Box(9,)
[2m[36m(pid=19160)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19160)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19160)[0m - _max_episode_steps = 150
[2m[36m(pid=19160)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19158)[0m 2019-07-18 03:23:58,484	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19158)[0m 2019-07-18 03:23:58.485536: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19160)[0m 2019-07-18 03:23:58,493	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19160)[0m 2019-07-18 03:23:58.494059: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19158)[0m [32m [     0.03501s,  INFO] TimeLimit:
[2m[36m(pid=19158)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19158)[0m - action_space = Box(2,)
[2m[36m(pid=19158)[0m - observation_space = Box(9,)
[2m[36m(pid=19158)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19158)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19158)[0m - _max_episode_steps = 150
[2m[36m(pid=19158)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19205)[0m 2019-07-18 03:23:58,476	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19205)[0m 2019-07-18 03:23:58.476875: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19205)[0m [32m [     0.03444s,  INFO] TimeLimit:
[2m[36m(pid=19205)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19205)[0m - action_space = Box(2,)
[2m[36m(pid=19205)[0m - observation_space = Box(9,)
[2m[36m(pid=19205)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19205)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19205)[0m - _max_episode_steps = 150
[2m[36m(pid=19205)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19211)[0m [32m [     0.03674s,  INFO] TimeLimit:
[2m[36m(pid=19211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19211)[0m - action_space = Box(2,)
[2m[36m(pid=19211)[0m - observation_space = Box(9,)
[2m[36m(pid=19211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19211)[0m - _max_episode_steps = 150
[2m[36m(pid=19211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19211)[0m 2019-07-18 03:23:58,471	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19211)[0m 2019-07-18 03:23:58.471828: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19160)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19160)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19234)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19234)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19217)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19217)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19221)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19221)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19248)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19248)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19208)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19208)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19159)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19159)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19158)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19158)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19205)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19205)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19214)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19214)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19211)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19211)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19160)[0m [32m [     1.17690s,  INFO] TimeLimit:
[2m[36m(pid=19160)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19160)[0m - action_space = Box(2,)
[2m[36m(pid=19160)[0m - observation_space = Box(9,)
[2m[36m(pid=19160)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19160)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19160)[0m - _max_episode_steps = 150
[2m[36m(pid=19160)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19160)[0m [32m [     1.17779s,  INFO] TimeLimit:
[2m[36m(pid=19160)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19160)[0m - action_space = Box(2,)
[2m[36m(pid=19160)[0m - observation_space = Box(9,)
[2m[36m(pid=19160)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19160)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19160)[0m - _max_episode_steps = 150
[2m[36m(pid=19160)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19160)[0m [32m [     1.17866s,  INFO] TimeLimit:
[2m[36m(pid=19160)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19160)[0m - action_space = Box(2,)
[2m[36m(pid=19160)[0m - observation_space = Box(9,)
[2m[36m(pid=19160)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19160)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19160)[0m - _max_episode_steps = 150
[2m[36m(pid=19160)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19160)[0m [32m [     1.17967s,  INFO] TimeLimit:
[2m[36m(pid=19160)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19160)[0m - action_space = Box(2,)
[2m[36m(pid=19160)[0m - observation_space = Box(9,)
[2m[36m(pid=19160)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19160)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19160)[0m - _max_episode_steps = 150
[2m[36m(pid=19160)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19205)[0m [32m [     1.39253s,  INFO] TimeLimit:
[2m[36m(pid=19205)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19205)[0m - action_space = Box(2,)
[2m[36m(pid=19205)[0m - observation_space = Box(9,)
[2m[36m(pid=19205)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19205)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19205)[0m - _max_episode_steps = 150
[2m[36m(pid=19205)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19205)[0m [32m [     1.39351s,  INFO] TimeLimit:
[2m[36m(pid=19205)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19205)[0m - action_space = Box(2,)
[2m[36m(pid=19205)[0m - observation_space = Box(9,)
[2m[36m(pid=19205)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19205)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19205)[0m - _max_episode_steps = 150
[2m[36m(pid=19205)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19205)[0m [32m [     1.39451s,  INFO] TimeLimit:
[2m[36m(pid=19205)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19205)[0m - action_space = Box(2,)
[2m[36m(pid=19205)[0m - observation_space = Box(9,)
[2m[36m(pid=19205)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19205)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19205)[0m - _max_episode_steps = 150
[2m[36m(pid=19205)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19205)[0m [32m [     1.39565s,  INFO] TimeLimit:
[2m[36m(pid=19205)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19205)[0m - action_space = Box(2,)
[2m[36m(pid=19205)[0m - observation_space = Box(9,)
[2m[36m(pid=19205)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19205)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19205)[0m - _max_episode_steps = 150
[2m[36m(pid=19205)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m [32m [     1.45402s,  INFO] TimeLimit:
[2m[36m(pid=19234)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19234)[0m - action_space = Box(2,)
[2m[36m(pid=19234)[0m - observation_space = Box(9,)
[2m[36m(pid=19234)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19234)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19234)[0m - _max_episode_steps = 150
[2m[36m(pid=19234)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m [32m [     1.45508s,  INFO] TimeLimit:
[2m[36m(pid=19234)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19234)[0m - action_space = Box(2,)
[2m[36m(pid=19234)[0m - observation_space = Box(9,)
[2m[36m(pid=19234)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19234)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19234)[0m - _max_episode_steps = 150
[2m[36m(pid=19234)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m [32m [     1.45606s,  INFO] TimeLimit:
[2m[36m(pid=19234)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19234)[0m - action_space = Box(2,)
[2m[36m(pid=19234)[0m - observation_space = Box(9,)
[2m[36m(pid=19234)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19234)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19234)[0m - _max_episode_steps = 150
[2m[36m(pid=19234)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m [32m [     1.45720s,  INFO] TimeLimit:
[2m[36m(pid=19234)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19234)[0m - action_space = Box(2,)
[2m[36m(pid=19234)[0m - observation_space = Box(9,)
[2m[36m(pid=19234)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19234)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19234)[0m - _max_episode_steps = 150
[2m[36m(pid=19234)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19217)[0m [32m [     1.46931s,  INFO] TimeLimit:
[2m[36m(pid=19217)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19217)[0m - action_space = Box(2,)
[2m[36m(pid=19217)[0m - observation_space = Box(9,)
[2m[36m(pid=19217)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19217)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19217)[0m - _max_episode_steps = 150
[2m[36m(pid=19217)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19217)[0m [32m [     1.47029s,  INFO] TimeLimit:
[2m[36m(pid=19217)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19217)[0m - action_space = Box(2,)
[2m[36m(pid=19217)[0m - observation_space = Box(9,)
[2m[36m(pid=19217)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19217)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19217)[0m - _max_episode_steps = 150
[2m[36m(pid=19217)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19217)[0m [32m [     1.47123s,  INFO] TimeLimit:
[2m[36m(pid=19217)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19217)[0m - action_space = Box(2,)
[2m[36m(pid=19217)[0m - observation_space = Box(9,)
[2m[36m(pid=19217)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19217)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19217)[0m - _max_episode_steps = 150
[2m[36m(pid=19217)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19217)[0m [32m [     1.47234s,  INFO] TimeLimit:
[2m[36m(pid=19217)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19217)[0m - action_space = Box(2,)
[2m[36m(pid=19217)[0m - observation_space = Box(9,)
[2m[36m(pid=19217)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19217)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19217)[0m - _max_episode_steps = 150
[2m[36m(pid=19217)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19159)[0m [32m [     1.45633s,  INFO] TimeLimit:
[2m[36m(pid=19159)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19159)[0m - action_space = Box(2,)
[2m[36m(pid=19159)[0m - observation_space = Box(9,)
[2m[36m(pid=19159)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19159)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19159)[0m - _max_episode_steps = 150
[2m[36m(pid=19159)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19159)[0m [32m [     1.45735s,  INFO] TimeLimit:
[2m[36m(pid=19159)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19159)[0m - action_space = Box(2,)
[2m[36m(pid=19159)[0m - observation_space = Box(9,)
[2m[36m(pid=19159)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19159)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19159)[0m - _max_episode_steps = 150
[2m[36m(pid=19159)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19159)[0m [32m [     1.45829s,  INFO] TimeLimit:
[2m[36m(pid=19159)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19159)[0m - action_space = Box(2,)
[2m[36m(pid=19159)[0m - observation_space = Box(9,)
[2m[36m(pid=19159)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19159)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19159)[0m - _max_episode_steps = 150
[2m[36m(pid=19159)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19159)[0m [32m [     1.45945s,  INFO] TimeLimit:
[2m[36m(pid=19159)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19159)[0m - action_space = Box(2,)
[2m[36m(pid=19159)[0m - observation_space = Box(9,)
[2m[36m(pid=19159)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19159)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19159)[0m - _max_episode_steps = 150
[2m[36m(pid=19159)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19158)[0m [32m [     1.47379s,  INFO] TimeLimit:
[2m[36m(pid=19158)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19158)[0m - action_space = Box(2,)
[2m[36m(pid=19158)[0m - observation_space = Box(9,)
[2m[36m(pid=19158)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19158)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19158)[0m - _max_episode_steps = 150
[2m[36m(pid=19158)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19158)[0m [32m [     1.47476s,  INFO] TimeLimit:
[2m[36m(pid=19158)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19158)[0m - action_space = Box(2,)
[2m[36m(pid=19158)[0m - observation_space = Box(9,)
[2m[36m(pid=19158)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19158)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19158)[0m - _max_episode_steps = 150
[2m[36m(pid=19158)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19158)[0m [32m [     1.47572s,  INFO] TimeLimit:
[2m[36m(pid=19158)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19158)[0m - action_space = Box(2,)
[2m[36m(pid=19158)[0m - observation_space = Box(9,)
[2m[36m(pid=19158)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19158)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19158)[0m - _max_episode_steps = 150
[2m[36m(pid=19158)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19158)[0m [32m [     1.47693s,  INFO] TimeLimit:
[2m[36m(pid=19158)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19158)[0m - action_space = Box(2,)
[2m[36m(pid=19158)[0m - observation_space = Box(9,)
[2m[36m(pid=19158)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19158)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19158)[0m - _max_episode_steps = 150
[2m[36m(pid=19158)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19214)[0m [32m [     1.49625s,  INFO] TimeLimit:
[2m[36m(pid=19214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19214)[0m - action_space = Box(2,)
[2m[36m(pid=19214)[0m - observation_space = Box(9,)
[2m[36m(pid=19214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19214)[0m - _max_episode_steps = 150
[2m[36m(pid=19214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19214)[0m [32m [     1.49698s,  INFO] TimeLimit:
[2m[36m(pid=19214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19214)[0m - action_space = Box(2,)
[2m[36m(pid=19214)[0m - observation_space = Box(9,)
[2m[36m(pid=19214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19214)[0m - _max_episode_steps = 150
[2m[36m(pid=19214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19214)[0m [32m [     1.49762s,  INFO] TimeLimit:
[2m[36m(pid=19214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19214)[0m - action_space = Box(2,)
[2m[36m(pid=19214)[0m - observation_space = Box(9,)
[2m[36m(pid=19214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19214)[0m - _max_episode_steps = 150
[2m[36m(pid=19214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19214)[0m [32m [     1.49837s,  INFO] TimeLimit:
[2m[36m(pid=19214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19214)[0m - action_space = Box(2,)
[2m[36m(pid=19214)[0m - observation_space = Box(9,)
[2m[36m(pid=19214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19214)[0m - _max_episode_steps = 150
[2m[36m(pid=19214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19211)[0m [32m [     1.46882s,  INFO] TimeLimit:
[2m[36m(pid=19211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19211)[0m - action_space = Box(2,)
[2m[36m(pid=19211)[0m - observation_space = Box(9,)
[2m[36m(pid=19211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19211)[0m - _max_episode_steps = 150
[2m[36m(pid=19211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19211)[0m [32m [     1.46979s,  INFO] TimeLimit:
[2m[36m(pid=19211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19211)[0m - action_space = Box(2,)
[2m[36m(pid=19211)[0m - observation_space = Box(9,)
[2m[36m(pid=19211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19211)[0m - _max_episode_steps = 150
[2m[36m(pid=19211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19211)[0m [32m [     1.47073s,  INFO] TimeLimit:
[2m[36m(pid=19211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19211)[0m - action_space = Box(2,)
[2m[36m(pid=19211)[0m - observation_space = Box(9,)
[2m[36m(pid=19211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19211)[0m - _max_episode_steps = 150
[2m[36m(pid=19211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19211)[0m [32m [     1.47186s,  INFO] TimeLimit:
[2m[36m(pid=19211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19211)[0m - action_space = Box(2,)
[2m[36m(pid=19211)[0m - observation_space = Box(9,)
[2m[36m(pid=19211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19211)[0m - _max_episode_steps = 150
[2m[36m(pid=19211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19221)[0m [32m [     1.49028s,  INFO] TimeLimit:
[2m[36m(pid=19221)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19221)[0m - action_space = Box(2,)
[2m[36m(pid=19221)[0m - observation_space = Box(9,)
[2m[36m(pid=19221)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19221)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19221)[0m - _max_episode_steps = 150
[2m[36m(pid=19221)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19221)[0m [32m [     1.49123s,  INFO] TimeLimit:
[2m[36m(pid=19221)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19221)[0m - action_space = Box(2,)
[2m[36m(pid=19221)[0m - observation_space = Box(9,)
[2m[36m(pid=19221)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19221)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19221)[0m - _max_episode_steps = 150
[2m[36m(pid=19221)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19221)[0m [32m [     1.49215s,  INFO] TimeLimit:
[2m[36m(pid=19221)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19221)[0m - action_space = Box(2,)
[2m[36m(pid=19221)[0m - observation_space = Box(9,)
[2m[36m(pid=19221)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19221)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19221)[0m - _max_episode_steps = 150
[2m[36m(pid=19221)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19221)[0m [32m [     1.49336s,  INFO] TimeLimit:
[2m[36m(pid=19221)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19221)[0m - action_space = Box(2,)
[2m[36m(pid=19221)[0m - observation_space = Box(9,)
[2m[36m(pid=19221)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19221)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19221)[0m - _max_episode_steps = 150
[2m[36m(pid=19221)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19208)[0m [32m [     1.51493s,  INFO] TimeLimit:
[2m[36m(pid=19208)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19208)[0m - action_space = Box(2,)
[2m[36m(pid=19208)[0m - observation_space = Box(9,)
[2m[36m(pid=19208)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19208)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19208)[0m - _max_episode_steps = 150
[2m[36m(pid=19208)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19208)[0m [32m [     1.51593s,  INFO] TimeLimit:
[2m[36m(pid=19208)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19208)[0m - action_space = Box(2,)
[2m[36m(pid=19208)[0m - observation_space = Box(9,)
[2m[36m(pid=19208)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19208)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19208)[0m - _max_episode_steps = 150
[2m[36m(pid=19208)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19208)[0m [32m [     1.51690s,  INFO] TimeLimit:
[2m[36m(pid=19208)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19208)[0m - action_space = Box(2,)
[2m[36m(pid=19208)[0m - observation_space = Box(9,)
[2m[36m(pid=19208)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19208)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19208)[0m - _max_episode_steps = 150
[2m[36m(pid=19208)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19208)[0m [32m [     1.51822s,  INFO] TimeLimit:
[2m[36m(pid=19208)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19208)[0m - action_space = Box(2,)
[2m[36m(pid=19208)[0m - observation_space = Box(9,)
[2m[36m(pid=19208)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19208)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19208)[0m - _max_episode_steps = 150
[2m[36m(pid=19208)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,891	INFO rollout_worker.py:428 -- Generating sample batch of size 250
[2m[36m(pid=19248)[0m [32m [     1.50010s,  INFO] TimeLimit:
[2m[36m(pid=19248)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19248)[0m - action_space = Box(2,)
[2m[36m(pid=19248)[0m - observation_space = Box(9,)
[2m[36m(pid=19248)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19248)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19248)[0m - _max_episode_steps = 150
[2m[36m(pid=19248)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19248)[0m [32m [     1.50101s,  INFO] TimeLimit:
[2m[36m(pid=19248)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19248)[0m - action_space = Box(2,)
[2m[36m(pid=19248)[0m - observation_space = Box(9,)
[2m[36m(pid=19248)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19248)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19248)[0m - _max_episode_steps = 150
[2m[36m(pid=19248)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19248)[0m [32m [     1.50189s,  INFO] TimeLimit:
[2m[36m(pid=19248)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19248)[0m - action_space = Box(2,)
[2m[36m(pid=19248)[0m - observation_space = Box(9,)
[2m[36m(pid=19248)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19248)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19248)[0m - _max_episode_steps = 150
[2m[36m(pid=19248)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19248)[0m [32m [     1.50302s,  INFO] TimeLimit:
[2m[36m(pid=19248)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19248)[0m - action_space = Box(2,)
[2m[36m(pid=19248)[0m - observation_space = Box(9,)
[2m[36m(pid=19248)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19248)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19248)[0m - _max_episode_steps = 150
[2m[36m(pid=19248)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,940	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.819, max=0.239, mean=-0.249)},
[2m[36m(pid=19234)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.919, max=0.923, mean=-0.036)},
[2m[36m(pid=19234)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.999, max=0.149, mean=-0.227)},
[2m[36m(pid=19234)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.969, max=0.147, mean=-0.25)},
[2m[36m(pid=19234)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.222, max=1.0, mean=0.115)}}
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,940	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=19234)[0m   1: {'agent0': None},
[2m[36m(pid=19234)[0m   2: {'agent0': None},
[2m[36m(pid=19234)[0m   3: {'agent0': None},
[2m[36m(pid=19234)[0m   4: {'agent0': None}}
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,941	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.819, max=0.239, mean=-0.249)
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,941	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.819, max=0.239, mean=-0.249)
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,945	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19234)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19234)[0m                                   'env_id': 0,
[2m[36m(pid=19234)[0m                                   'info': None,
[2m[36m(pid=19234)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.819, max=0.239, mean=-0.249),
[2m[36m(pid=19234)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19234)[0m                                   'rnn_state': []},
[2m[36m(pid=19234)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19234)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19234)[0m                                   'env_id': 1,
[2m[36m(pid=19234)[0m                                   'info': None,
[2m[36m(pid=19234)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.919, max=0.923, mean=-0.036),
[2m[36m(pid=19234)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19234)[0m                                   'rnn_state': []},
[2m[36m(pid=19234)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19234)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19234)[0m                                   'env_id': 2,
[2m[36m(pid=19234)[0m                                   'info': None,
[2m[36m(pid=19234)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.999, max=0.149, mean=-0.227),
[2m[36m(pid=19234)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19234)[0m                                   'rnn_state': []},
[2m[36m(pid=19234)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19234)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19234)[0m                                   'env_id': 3,
[2m[36m(pid=19234)[0m                                   'info': None,
[2m[36m(pid=19234)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.969, max=0.147, mean=-0.25),
[2m[36m(pid=19234)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19234)[0m                                   'rnn_state': []},
[2m[36m(pid=19234)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19234)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19234)[0m                                   'env_id': 4,
[2m[36m(pid=19234)[0m                                   'info': None,
[2m[36m(pid=19234)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.222, max=1.0, mean=0.115),
[2m[36m(pid=19234)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19234)[0m                                   'rnn_state': []},
[2m[36m(pid=19234)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,945	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=19234)[0m 2019-07-18 03:23:59,974	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19234)[0m { 'default_policy': ( np.ndarray((5, 2), dtype=float32, min=-1.764, max=1.602, mean=0.132),
[2m[36m(pid=19234)[0m                       [],
[2m[36m(pid=19234)[0m                       { 'action_prob': np.ndarray((5,), dtype=float32, min=0.019, max=0.092, mean=0.056),
[2m[36m(pid=19234)[0m                         'behaviour_logits': np.ndarray((5, 4), dtype=float32, min=-0.006, max=0.006, mean=0.0)})}
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19262)[0m 2019-07-18 03:24:00,082	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=19262)[0m 
[2m[36m(pid=19262)[0m { 'data': { 'action_prob': np.ndarray((500,), dtype=float32, min=0.0, max=0.16, mean=0.081),
[2m[36m(pid=19262)[0m             'actions': np.ndarray((500, 2), dtype=float32, min=-3.06, max=3.289, mean=0.006),
[2m[36m(pid=19262)[0m             'agent_index': np.ndarray((500,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19262)[0m             'behaviour_logits': np.ndarray((500, 4), dtype=float32, min=-0.01, max=0.012, mean=0.0),
[2m[36m(pid=19262)[0m             'dones': np.ndarray((500,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19262)[0m             'eps_id': np.ndarray((500,), dtype=int64, min=110973350.0, max=1459447259.0, mean=795044497.4),
[2m[36m(pid=19262)[0m             'infos': np.ndarray((500,), dtype=object, head={}),
[2m[36m(pid=19262)[0m             'obs': np.ndarray((500, 9), dtype=float32, min=-2.829, max=3.057, mean=-0.038),
[2m[36m(pid=19262)[0m             'prev_actions': np.ndarray((500, 2), dtype=float32, min=-3.06, max=3.289, mean=0.01),
[2m[36m(pid=19262)[0m             'prev_rewards': np.ndarray((500,), dtype=float32, min=-3.111, max=3.759, mean=-0.044),
[2m[36m(pid=19262)[0m             'rewards': np.ndarray((500,), dtype=float32, min=-3.111, max=3.759, mean=-0.042),
[2m[36m(pid=19262)[0m             't': np.ndarray((500,), dtype=int64, min=0.0, max=99.0, mean=49.5),
[2m[36m(pid=19262)[0m             'unroll_id': np.ndarray((500,), dtype=int64, min=0.0, max=1.0, mean=0.5)},
[2m[36m(pid=19262)[0m   'type': 'SampleBatch'}
[2m[36m(pid=19262)[0m 
[2m[36m(pid=19262)[0m 2019-07-18 03:24:00,082	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=19234)[0m 2019-07-18 03:24:00,138	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19234)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.005, max=0.159, mean=0.086),
[2m[36m(pid=19234)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-2.107, max=2.24, mean=0.028),
[2m[36m(pid=19234)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                         'behaviour_logits': np.ndarray((50, 4), dtype=float32, min=-0.008, max=0.007, mean=-0.001),
[2m[36m(pid=19234)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=80347244.0, max=80347244.0, mean=80347244.0),
[2m[36m(pid=19234)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=19234)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-1.606, max=2.536, mean=-0.136),
[2m[36m(pid=19234)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.107, max=2.24, mean=0.014),
[2m[36m(pid=19234)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-1.948, max=0.98, mean=-0.381),
[2m[36m(pid=19234)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-1.948, max=0.98, mean=-0.373),
[2m[36m(pid=19234)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=19234)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=19234)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19234)[0m 2019-07-18 03:24:00,145	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19234)[0m { 'data': { 'action_prob': np.ndarray((250,), dtype=float32, min=0.0, max=0.159, mean=0.078),
[2m[36m(pid=19234)[0m             'actions': np.ndarray((250, 2), dtype=float32, min=-3.63, max=3.379, mean=0.02),
[2m[36m(pid=19234)[0m             'agent_index': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m             'behaviour_logits': np.ndarray((250, 4), dtype=float32, min=-0.009, max=0.011, mean=-0.0),
[2m[36m(pid=19234)[0m             'dones': np.ndarray((250,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19234)[0m             'eps_id': np.ndarray((250,), dtype=int64, min=80347244.0, max=1523375375.0, mean=725871839.6),
[2m[36m(pid=19234)[0m             'infos': np.ndarray((250,), dtype=object, head={}),
[2m[36m(pid=19234)[0m             'obs': np.ndarray((250, 9), dtype=float32, min=-3.276, max=3.559, mean=0.003),
[2m[36m(pid=19234)[0m             'prev_actions': np.ndarray((250, 2), dtype=float32, min=-3.63, max=3.379, mean=0.005),
[2m[36m(pid=19234)[0m             'prev_rewards': np.ndarray((250,), dtype=float32, min=-3.136, max=3.058, mean=-0.141),
[2m[36m(pid=19234)[0m             'rewards': np.ndarray((250,), dtype=float32, min=-3.136, max=3.058, mean=-0.138),
[2m[36m(pid=19234)[0m             't': np.ndarray((250,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=19234)[0m             'unroll_id': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=19234)[0m   'type': 'SampleBatch'}
[2m[36m(pid=19234)[0m 
[2m[36m(pid=19262)[0m 2019-07-18 03:24:00,556	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=19262)[0m 
[2m[36m(pid=19262)[0m { 'learner_stats': { 'cur_lr': 0.0005000000237487257,
[2m[36m(pid=19262)[0m                      'entropy': 1390.739,
[2m[36m(pid=19262)[0m                      'grad_gnorm': 40.000004,
[2m[36m(pid=19262)[0m                      'model': {},
[2m[36m(pid=19262)[0m                      'policy_loss': -1561.3524,
[2m[36m(pid=19262)[0m                      'var_gnorm': 22.64951,
[2m[36m(pid=19262)[0m                      'vf_explained_var': -0.014597774,
[2m[36m(pid=19262)[0m                      'vf_loss': 14379.277}}
[2m[36m(pid=19262)[0m 
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-24-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.42016592553012
  episode_reward_mean: -44.02989549265565
  episode_reward_min: -176.08625167752197
  episodes_this_iter: 870
  episodes_total: 870
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1266.596435546875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 646.1665649414062
      var_gnorm: 23.060264587402344
      vf_explained_var: 0.9661020040512085
      vf_loss: 954.369140625
    learner_queue:
      size_count: 255
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 127500
    num_steps_trained: 127500
    num_weight_syncs: 511
    sample_throughput: 12210.204
    timing_breakdown:
      learner_dequeue_time_ms: 22.858
      learner_grad_time_ms: 9.133
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.617
    train_throughput: 12210.204
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5852164415326475
    mean_inference_ms: 1.2702596519024265
    mean_processing_ms: 0.6387754411442932
  time_since_restore: 10.30542516708374
  time_this_iter_s: 10.30542516708374
  time_total_s: 10.30542516708374
  timestamp: 1563413048
  timesteps_since_restore: 127500
  timesteps_this_iter: 127500
  timesteps_total: 127500
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 10 s, 1 iter, 127500 ts, -44 rew

[2m[36m(pid=19262)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
[2m[36m(pid=19262)[0m   out=out, **kwargs)
[2m[36m(pid=19262)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
[2m[36m(pid=19262)[0m   ret = ret.dtype.type(ret / rcount)
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-24-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 7.91544454319917
  episode_reward_mean: -64.95054466066692
  episode_reward_min: -182.0262536399287
  episodes_this_iter: 1030
  episodes_total: 1900
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 360.1990661621094
      grad_gnorm: 40.0
      model: {}
      policy_loss: -887.296630859375
      var_gnorm: 23.165279388427734
      vf_explained_var: 0.8147510290145874
      vf_loss: 2001.3184814453125
    learner_queue:
      size_count: 563
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 281500
    num_steps_trained: 281500
    num_weight_syncs: 1126
    sample_throughput: 14969.109
    timing_breakdown:
      learner_dequeue_time_ms: 23.677
      learner_grad_time_ms: 8.942
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 16.835
    train_throughput: 14969.109
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5791859529674057
    mean_inference_ms: 1.2615377977674822
    mean_processing_ms: 0.6437690556958393
  time_since_restore: 20.58559012413025
  time_this_iter_s: 10.280164957046509
  time_total_s: 20.58559012413025
  timestamp: 1563413059
  timesteps_since_restore: 281500
  timesteps_this_iter: 154000
  timesteps_total: 281500
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 20 s, 2 iter, 281500 ts, -65 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-24-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.038276340396294
  episode_reward_mean: -22.09724042243348
  episode_reward_min: -154.73208969401912
  episodes_this_iter: 1025
  episodes_total: 2925
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1337.2703857421875
      grad_gnorm: 40.0
      model: {}
      policy_loss: 2304.1591796875
      var_gnorm: 23.430187225341797
      vf_explained_var: 0.2528294324874878
      vf_loss: 4705.1630859375
    learner_queue:
      size_count: 872
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 436000
    num_steps_trained: 436000
    num_weight_syncs: 1745
    sample_throughput: 15004.176
    timing_breakdown:
      learner_dequeue_time_ms: 19.984
      learner_grad_time_ms: 10.076
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 22.11
    train_throughput: 15004.177
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5720334001454808
    mean_inference_ms: 1.2597062882648393
    mean_processing_ms: 0.6446561110252619
  time_since_restore: 30.874194622039795
  time_this_iter_s: 10.288604497909546
  time_total_s: 30.874194622039795
  timestamp: 1563413069
  timesteps_since_restore: 436000
  timesteps_this_iter: 154500
  timesteps_total: 436000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 30 s, 3 iter, 436000 ts, -22.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-24-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -11.800840139281208
  episode_reward_mean: -48.80934009629925
  episode_reward_min: -87.64048679862549
  episodes_this_iter: 1035
  episodes_total: 3960
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1310.71826171875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 1327.0164794921875
      var_gnorm: 23.90247917175293
      vf_explained_var: 0.11826926469802856
      vf_loss: 7768.03076171875
    learner_queue:
      size_count: 1183
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 591500
    num_steps_trained: 591500
    num_weight_syncs: 2366
    sample_throughput: 15063.919
    timing_breakdown:
      learner_dequeue_time_ms: 27.178
      learner_grad_time_ms: 8.156
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.121
    train_throughput: 15063.919
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5671509765018645
    mean_inference_ms: 1.2596858374966469
    mean_processing_ms: 0.6448897950209289
  time_since_restore: 41.189221143722534
  time_this_iter_s: 10.31502652168274
  time_total_s: 41.189221143722534
  timestamp: 1563413079
  timesteps_since_restore: 591500
  timesteps_this_iter: 155500
  timesteps_total: 591500
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 41 s, 4 iter, 591500 ts, -48.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-24-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -18.042184043681274
  episode_reward_mean: -54.485678180756665
  episode_reward_min: -92.12585599365835
  episodes_this_iter: 1030
  episodes_total: 4990
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1415.1337890625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -760.2611694335938
      var_gnorm: 24.189029693603516
      vf_explained_var: 0.33737021684646606
      vf_loss: 4455.61083984375
    learner_queue:
      size_count: 1493
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 746500
    num_steps_trained: 746500
    num_weight_syncs: 2987
    sample_throughput: 15020.952
    timing_breakdown:
      learner_dequeue_time_ms: 21.054
      learner_grad_time_ms: 8.438
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 21.511
    train_throughput: 15020.953
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5657202430635717
    mean_inference_ms: 1.2597501642708517
    mean_processing_ms: 0.6451255926848288
  time_since_restore: 51.50061011314392
  time_this_iter_s: 10.311388969421387
  time_total_s: 51.50061011314392
  timestamp: 1563413090
  timesteps_since_restore: 746500
  timesteps_this_iter: 155000
  timesteps_total: 746500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 51 s, 5 iter, 746500 ts, -54.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-25-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.838161076229248
  episode_reward_mean: -31.789610323854127
  episode_reward_min: -158.1528949174778
  episodes_this_iter: 1035
  episodes_total: 6025
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: -31.444068908691406
      grad_gnorm: 40.0
      model: {}
      policy_loss: 1631.4735107421875
      var_gnorm: 25.092273712158203
      vf_explained_var: 0.9780340790748596
      vf_loss: 419.0606689453125
    learner_queue:
      size_count: 1803
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 901500
    num_steps_trained: 901000
    num_weight_syncs: 3606
    sample_throughput: 15069.767
    timing_breakdown:
      learner_dequeue_time_ms: 24.31
      learner_grad_time_ms: 9.138
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.914
    train_throughput: 15021.155
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5625823362529692
    mean_inference_ms: 1.2609613795718502
    mean_processing_ms: 0.6449754619818744
  time_since_restore: 61.7789044380188
  time_this_iter_s: 10.278294324874878
  time_total_s: 61.7789044380188
  timestamp: 1563413100
  timesteps_since_restore: 901500
  timesteps_this_iter: 155000
  timesteps_total: 901500
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 61 s, 6 iter, 901500 ts, -31.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-25-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.854339485253904
  episode_reward_mean: -10.501142943237557
  episode_reward_min: -45.825473335968134
  episodes_this_iter: 1045
  episodes_total: 7070
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2213.414794921875
      grad_gnorm: 40.0
      model: {}
      policy_loss: 108.64924621582031
      var_gnorm: 25.48533821105957
      vf_explained_var: 0.9964532256126404
      vf_loss: 91.92118835449219
    learner_queue:
      size_count: 2114
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1057000
    num_steps_trained: 1057000
    num_weight_syncs: 4228
    sample_throughput: 15031.63
    timing_breakdown:
      learner_dequeue_time_ms: 26.017
      learner_grad_time_ms: 8.669
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 24.19
    train_throughput: 15079.963
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5612882637513907
    mean_inference_ms: 1.2612400510216641
    mean_processing_ms: 0.6452544441798423
  time_since_restore: 72.11574363708496
  time_this_iter_s: 10.336839199066162
  time_total_s: 72.11574363708496
  timestamp: 1563413110
  timesteps_since_restore: 1057000
  timesteps_this_iter: 155500
  timesteps_total: 1057000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 72 s, 7 iter, 1057000 ts, -10.5 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-25-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.14453862508111
  episode_reward_mean: -19.58376454922985
  episode_reward_min: -66.30445910909064
  episodes_this_iter: 1030
  episodes_total: 8100
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 787.06298828125
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -379.931396484375
      var_gnorm: 25.77149200439453
      vf_explained_var: 0.9903733134269714
      vf_loss: 189.62478637695312
    learner_queue:
      size_count: 2425
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1212500
    num_steps_trained: 1212500
    num_weight_syncs: 4850
    sample_throughput: 15070.583
    timing_breakdown:
      learner_dequeue_time_ms: 23.608
      learner_grad_time_ms: 7.934
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.594
    train_throughput: 15070.583
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.560931567015485
    mean_inference_ms: 1.2608585997316812
    mean_processing_ms: 0.6452266314461323
  time_since_restore: 82.42638230323792
  time_this_iter_s: 10.310638666152954
  time_total_s: 82.42638230323792
  timestamp: 1563413121
  timesteps_since_restore: 1212500
  timesteps_this_iter: 155500
  timesteps_total: 1212500
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 82 s, 8 iter, 1212500 ts, -19.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-25-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.757687911013688
  episode_reward_mean: -16.936916891595526
  episode_reward_min: -78.14845823415598
  episodes_this_iter: 1035
  episodes_total: 9135
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2376.901123046875
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: 802.1034545898438
      var_gnorm: 25.893674850463867
      vf_explained_var: 0.9379169344902039
      vf_loss: 333.2607727050781
    learner_queue:
      size_count: 2735
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1367500
    num_steps_trained: 1367500
    num_weight_syncs: 5470
    sample_throughput: 15035.931
    timing_breakdown:
      learner_dequeue_time_ms: 25.454
      learner_grad_time_ms: 8.302
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.133
    train_throughput: 15035.932
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5597967568090576
    mean_inference_ms: 1.260722522457992
    mean_processing_ms: 0.6452845855648621
  time_since_restore: 92.72733116149902
  time_this_iter_s: 10.300948858261108
  time_total_s: 92.72733116149902
  timestamp: 1563413131
  timesteps_since_restore: 1367500
  timesteps_this_iter: 155000
  timesteps_total: 1367500
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 92 s, 9 iter, 1367500 ts, -16.9 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-25-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 11.09563648586543
  episode_reward_mean: -18.621992395079175
  episode_reward_min: -119.32288302351857
  episodes_this_iter: 1035
  episodes_total: 10170
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 4387.80419921875
      grad_gnorm: 40.0
      model: {}
      policy_loss: 2134.65380859375
      var_gnorm: 26.13701057434082
      vf_explained_var: 0.9604269862174988
      vf_loss: 223.29702758789062
    learner_queue:
      size_count: 3045
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1522500
    num_steps_trained: 1522500
    num_weight_syncs: 6091
    sample_throughput: 15019.899
    timing_breakdown:
      learner_dequeue_time_ms: 21.992
      learner_grad_time_ms: 8.977
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 13.302
    train_throughput: 15019.899
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5591454336911732
    mean_inference_ms: 1.260668494933224
    mean_processing_ms: 0.64521854158301
  time_since_restore: 103.03878140449524
  time_this_iter_s: 10.311450242996216
  time_total_s: 103.03878140449524
  timestamp: 1563413141
  timesteps_since_restore: 1522500
  timesteps_this_iter: 155000
  timesteps_total: 1522500
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 103 s, 10 iter, 1522500 ts, -18.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-25-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.032791389019778
  episode_reward_mean: -15.592334041146808
  episode_reward_min: -60.26871740874404
  episodes_this_iter: 1040
  episodes_total: 11210
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2413.786376953125
      grad_gnorm: 40.0
      model: {}
      policy_loss: 829.4970703125
      var_gnorm: 26.254289627075195
      vf_explained_var: 0.9386987686157227
      vf_loss: 614.8388061523438
    learner_queue:
      size_count: 3357
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1678500
    num_steps_trained: 1678000
    num_weight_syncs: 6714
    sample_throughput: 15112.845
    timing_breakdown:
      learner_dequeue_time_ms: 20.285
      learner_grad_time_ms: 8.863
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.744
    train_throughput: 15064.407
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5578744333124204
    mean_inference_ms: 1.2607889314216079
    mean_processing_ms: 0.6455556008908592
  time_since_restore: 113.3531424999237
  time_this_iter_s: 10.314361095428467
  time_total_s: 113.3531424999237
  timestamp: 1563413152
  timesteps_since_restore: 1678500
  timesteps_this_iter: 156000
  timesteps_total: 1678500
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 113 s, 11 iter, 1678500 ts, -15.6 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-26-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.626052094032037
  episode_reward_mean: -19.95455805779618
  episode_reward_min: -94.51301657629544
  episodes_this_iter: 1030
  episodes_total: 12240
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2576.04833984375
      grad_gnorm: 40.0
      model: {}
      policy_loss: 3222.482421875
      var_gnorm: 26.793670654296875
      vf_explained_var: 0.5138455629348755
      vf_loss: 2644.885986328125
    learner_queue:
      size_count: 3667
      size_mean: 0.04
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.19595917942265428
    num_steps_replayed: 0
    num_steps_sampled: 1833500
    num_steps_trained: 1833500
    num_weight_syncs: 7335
    sample_throughput: 15032.862
    timing_breakdown:
      learner_dequeue_time_ms: 20.54
      learner_grad_time_ms: 8.099
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.443
    train_throughput: 15081.356
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5565250918071136
    mean_inference_ms: 1.2616394645929208
    mean_processing_ms: 0.645459810499862
  time_since_restore: 123.65512681007385
  time_this_iter_s: 10.301984310150146
  time_total_s: 123.65512681007385
  timestamp: 1563413162
  timesteps_since_restore: 1833500
  timesteps_this_iter: 155000
  timesteps_total: 1833500
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 123 s, 12 iter, 1833500 ts, -20 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-26-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.584311434960785
  episode_reward_mean: -29.186063554801496
  episode_reward_min: -129.4609859217176
  episodes_this_iter: 1045
  episodes_total: 13285
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 4784.42919921875
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: 1480.1207275390625
      var_gnorm: 27.978715896606445
      vf_explained_var: 0.8099150657653809
      vf_loss: 2857.593017578125
    learner_queue:
      size_count: 3979
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 1989500
    num_steps_trained: 1989500
    num_weight_syncs: 7958
    sample_throughput: 15109.852
    timing_breakdown:
      learner_dequeue_time_ms: 18.565
      learner_grad_time_ms: 8.347
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.485
    train_throughput: 15109.853
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5557660032156277
    mean_inference_ms: 1.261412238868952
    mean_processing_ms: 0.6458262466176279
  time_since_restore: 133.97064852714539
  time_this_iter_s: 10.315521717071533
  time_total_s: 133.97064852714539
  timestamp: 1563413172
  timesteps_since_restore: 1989500
  timesteps_this_iter: 156000
  timesteps_total: 1989500
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 133 s, 13 iter, 1989500 ts, -29.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-26-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.69046516778279
  episode_reward_mean: -27.833638118402273
  episode_reward_min: -153.12418025623887
  episodes_this_iter: 1035
  episodes_total: 14320
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 5862.66455078125
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -10490.5400390625
      var_gnorm: 29.058107376098633
      vf_explained_var: 0.88776695728302
      vf_loss: 1885.215087890625
    learner_queue:
      size_count: 4290
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2145000
    num_steps_trained: 2145000
    num_weight_syncs: 8580
    sample_throughput: 15084.176
    timing_breakdown:
      learner_dequeue_time_ms: 19.776
      learner_grad_time_ms: 9.652
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.814
    train_throughput: 15084.177
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5552108677861658
    mean_inference_ms: 1.261168870447835
    mean_processing_ms: 0.6459103539399451
  time_since_restore: 144.27049255371094
  time_this_iter_s: 10.299844026565552
  time_total_s: 144.27049255371094
  timestamp: 1563413182
  timesteps_since_restore: 2145000
  timesteps_this_iter: 155500
  timesteps_total: 2145000
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 144 s, 14 iter, 2145000 ts, -27.8 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-26-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.844727344075665
  episode_reward_mean: -26.302882835774582
  episode_reward_min: -142.3996243204799
  episodes_this_iter: 1025
  episodes_total: 15345
  experiment_id: 2ec371d401284d4da3f93a9133193bec
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8329.970703125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -6243.45703125
      var_gnorm: 29.757448196411133
      vf_explained_var: 0.6132563352584839
      vf_loss: 8109.67578125
    learner_queue:
      size_count: 4598
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2299000
    num_steps_trained: 2299000
    num_weight_syncs: 9197
    sample_throughput: 14943.134
    timing_breakdown:
      learner_dequeue_time_ms: 24.339
      learner_grad_time_ms: 8.759
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.357
    train_throughput: 14943.135
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19262
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5541390542731244
    mean_inference_ms: 1.2617348511789093
    mean_processing_ms: 0.6454840596253903
  time_since_restore: 154.56733393669128
  time_this_iter_s: 10.296841382980347
  time_total_s: 154.56733393669128
  timestamp: 1563413193
  timesteps_since_restore: 2299000
  timesteps_this_iter: 154000
  timesteps_total: 2299000
  training_iteration: 15
  2019-07-18 03:26:42,952	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_IMPALA:train()[39m (pid=19262, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 364, in train
    raise e
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 353, in train
    result = Trainable.train(self)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 150, in train
    result = self._train()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 126, in _train
    fetches = self.optimizer.step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 131, in step
    sample_timesteps, train_timesteps = self._step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 173, in _step
    for train_batch in self.aggregator.iter_train_batches():
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 103, in iter_train_batches
    blocking_wait=True, max_yield=max_yield)):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 151, in _augment_with_replay
    sample_batch = ray_get_and_free(sample_batch)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError: [36mray_RolloutWorker:sample()[39m (pid=19208, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 430, in sample
    batches = [self.input_reader.next()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 60, in next
    batches = [self.get_data()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 101, in get_data
    item = next(self.rollout_provider)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 338, in _env_runner
    base_env.send_actions(actions_to_send)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/base_env.py", line 332, in send_actions
    self.vector_env.vector_step(action_vector)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/vector_env.py", line 110, in vector_step
    obs, r, done, info = self.envs[i].step(actions[i])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/gym/wrappers/time_limit.py", line 15, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 57, in step
    self.apply_action(a)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 30, in apply_action
    assert( np.isfinite(a).all() )
AssertionError


2019-07-18 03:26:42,954	INFO ray_trial_executor.py:187 -- Destroying actor for trial IMPALA_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 03:26:42,957	INFO trial_runner.py:524 -- Attempting to recover trial state from last checkpoint.
2019-07-18 03:26:42,967	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 2 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-23-57.txt, [12 CPUs, 1 GPUs], [pid=19262], 154 s, 15 iter, 2299000 ts, -26.3 rew

[2m[36m(pid=19262)[0m 2019-07-18 03:26:42,949	INFO trainer.py:361 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19262], 154 s, 15 iter, 2299000 ts, -26.3 rew

[2m[36m(pid=19585)[0m [32m [     0.03937s,  INFO] TimeLimit:
[2m[36m(pid=19585)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19585)[0m - action_space = Box(2,)
[2m[36m(pid=19585)[0m - observation_space = Box(9,)
[2m[36m(pid=19585)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19585)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19585)[0m - _max_episode_steps = 150
[2m[36m(pid=19585)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19585)[0m 2019-07-18 03:26:47.154999: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19585)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19585)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19585)[0m [32m [     0.73135s,  INFO] TimeLimit:
[2m[36m(pid=19585)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19585)[0m - action_space = Box(2,)
[2m[36m(pid=19585)[0m - observation_space = Box(9,)
[2m[36m(pid=19585)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19585)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19585)[0m - _max_episode_steps = 150
[2m[36m(pid=19585)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19585)[0m [32m [     0.73177s,  INFO] TimeLimit:
[2m[36m(pid=19585)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19585)[0m - action_space = Box(2,)
[2m[36m(pid=19585)[0m - observation_space = Box(9,)
[2m[36m(pid=19585)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19585)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19585)[0m - _max_episode_steps = 150
[2m[36m(pid=19585)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19585)[0m [32m [     0.73216s,  INFO] TimeLimit:
[2m[36m(pid=19585)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19585)[0m - action_space = Box(2,)
[2m[36m(pid=19585)[0m - observation_space = Box(9,)
[2m[36m(pid=19585)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19585)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19585)[0m - _max_episode_steps = 150
[2m[36m(pid=19585)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19585)[0m [32m [     0.73256s,  INFO] TimeLimit:
[2m[36m(pid=19585)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19585)[0m - action_space = Box(2,)
[2m[36m(pid=19585)[0m - observation_space = Box(9,)
[2m[36m(pid=19585)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19585)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19585)[0m - _max_episode_steps = 150
[2m[36m(pid=19585)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19585)[0m 2019-07-18 03:26:47,845	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.impala.vtrace_policy.VTraceTFPolicy object at 0x7f3788574898>}
[2m[36m(pid=19585)[0m 2019-07-18 03:26:47,845	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f3788b8b7f0>}
[2m[36m(pid=19585)[0m 2019-07-18 03:26:47,845	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f3788b8b668>}
[2m[36m(pid=19594)[0m 2019-07-18 03:26:47,924	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19594)[0m 2019-07-18 03:26:47.925433: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19594)[0m [32m [     0.02603s,  INFO] TimeLimit:
[2m[36m(pid=19594)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19594)[0m - action_space = Box(2,)
[2m[36m(pid=19594)[0m - observation_space = Box(9,)
[2m[36m(pid=19594)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19594)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19594)[0m - _max_episode_steps = 150
[2m[36m(pid=19594)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19593)[0m 2019-07-18 03:26:47,933	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19593)[0m 2019-07-18 03:26:47.933700: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19593)[0m [32m [     0.03646s,  INFO] TimeLimit:
[2m[36m(pid=19593)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19593)[0m - action_space = Box(2,)
[2m[36m(pid=19593)[0m - observation_space = Box(9,)
[2m[36m(pid=19593)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19593)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19593)[0m - _max_episode_steps = 150
[2m[36m(pid=19593)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m [32m [     0.04823s,  INFO] TimeLimit:
[2m[36m(pid=19739)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19739)[0m - action_space = Box(2,)
[2m[36m(pid=19739)[0m - observation_space = Box(9,)
[2m[36m(pid=19739)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19739)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19739)[0m - _max_episode_steps = 150
[2m[36m(pid=19739)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m 2019-07-18 03:26:47,936	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19584)[0m 2019-07-18 03:26:47,993	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19584)[0m 2019-07-18 03:26:47.994463: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19584)[0m [32m [     0.03882s,  INFO] TimeLimit:
[2m[36m(pid=19584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19584)[0m - action_space = Box(2,)
[2m[36m(pid=19584)[0m - observation_space = Box(9,)
[2m[36m(pid=19584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19584)[0m - _max_episode_steps = 150
[2m[36m(pid=19584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19586)[0m 2019-07-18 03:26:47,978	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19586)[0m 2019-07-18 03:26:47.979022: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19583)[0m [32m [     0.03826s,  INFO] TimeLimit:
[2m[36m(pid=19583)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19583)[0m - action_space = Box(2,)
[2m[36m(pid=19583)[0m - observation_space = Box(9,)
[2m[36m(pid=19583)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19583)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19583)[0m - _max_episode_steps = 150
[2m[36m(pid=19583)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19583)[0m 2019-07-18 03:26:47,972	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19583)[0m 2019-07-18 03:26:47.972955: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19586)[0m [32m [     0.03869s,  INFO] TimeLimit:
[2m[36m(pid=19586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19586)[0m - action_space = Box(2,)
[2m[36m(pid=19586)[0m - observation_space = Box(9,)
[2m[36m(pid=19586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19586)[0m - _max_episode_steps = 150
[2m[36m(pid=19586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19590)[0m [32m [     0.03832s,  INFO] TimeLimit:
[2m[36m(pid=19590)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19590)[0m - action_space = Box(2,)
[2m[36m(pid=19590)[0m - observation_space = Box(9,)
[2m[36m(pid=19590)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19590)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19590)[0m - _max_episode_steps = 150
[2m[36m(pid=19590)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19590)[0m 2019-07-18 03:26:47,968	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19590)[0m 2019-07-18 03:26:47.968580: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19588)[0m [32m [     0.03792s,  INFO] TimeLimit:
[2m[36m(pid=19588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19588)[0m - action_space = Box(2,)
[2m[36m(pid=19588)[0m - observation_space = Box(9,)
[2m[36m(pid=19588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19588)[0m - _max_episode_steps = 150
[2m[36m(pid=19588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19588)[0m 2019-07-18 03:26:47,955	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19588)[0m 2019-07-18 03:26:47.955959: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19592)[0m [32m [     0.03814s,  INFO] TimeLimit:
[2m[36m(pid=19592)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19592)[0m - action_space = Box(2,)
[2m[36m(pid=19592)[0m - observation_space = Box(9,)
[2m[36m(pid=19592)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19592)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19592)[0m - _max_episode_steps = 150
[2m[36m(pid=19592)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19592)[0m 2019-07-18 03:26:47,953	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19592)[0m 2019-07-18 03:26:47.954475: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19591)[0m [32m [     0.03813s,  INFO] TimeLimit:
[2m[36m(pid=19591)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19591)[0m - action_space = Box(2,)
[2m[36m(pid=19591)[0m - observation_space = Box(9,)
[2m[36m(pid=19591)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19591)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19591)[0m - _max_episode_steps = 150
[2m[36m(pid=19591)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19591)[0m 2019-07-18 03:26:47,978	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19591)[0m 2019-07-18 03:26:47.979560: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19739)[0m 2019-07-18 03:26:47.947133: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19587)[0m [32m [     0.03959s,  INFO] TimeLimit:
[2m[36m(pid=19587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19587)[0m - action_space = Box(2,)
[2m[36m(pid=19587)[0m - observation_space = Box(9,)
[2m[36m(pid=19587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19587)[0m - _max_episode_steps = 150
[2m[36m(pid=19587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19587)[0m 2019-07-18 03:26:48,006	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=19587)[0m 2019-07-18 03:26:48.006857: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=19590)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19590)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19594)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19594)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19593)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19593)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19584)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19584)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19586)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19586)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19583)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19583)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19588)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19588)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19592)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19592)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19591)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19591)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19739)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19739)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19587)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=19587)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=19590)[0m [32m [     1.19744s,  INFO] TimeLimit:
[2m[36m(pid=19590)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19590)[0m - action_space = Box(2,)
[2m[36m(pid=19590)[0m - observation_space = Box(9,)
[2m[36m(pid=19590)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19590)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19590)[0m - _max_episode_steps = 150
[2m[36m(pid=19590)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19590)[0m [32m [     1.19844s,  INFO] TimeLimit:
[2m[36m(pid=19590)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19590)[0m - action_space = Box(2,)
[2m[36m(pid=19590)[0m - observation_space = Box(9,)
[2m[36m(pid=19590)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19590)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19590)[0m - _max_episode_steps = 150
[2m[36m(pid=19590)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19590)[0m [32m [     1.19941s,  INFO] TimeLimit:
[2m[36m(pid=19590)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19590)[0m - action_space = Box(2,)
[2m[36m(pid=19590)[0m - observation_space = Box(9,)
[2m[36m(pid=19590)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19590)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19590)[0m - _max_episode_steps = 150
[2m[36m(pid=19590)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19590)[0m [32m [     1.20056s,  INFO] TimeLimit:
[2m[36m(pid=19590)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19590)[0m - action_space = Box(2,)
[2m[36m(pid=19590)[0m - observation_space = Box(9,)
[2m[36m(pid=19590)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19590)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19590)[0m - _max_episode_steps = 150
[2m[36m(pid=19590)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19593)[0m [32m [     1.29539s,  INFO] TimeLimit:
[2m[36m(pid=19593)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19593)[0m - action_space = Box(2,)
[2m[36m(pid=19593)[0m - observation_space = Box(9,)
[2m[36m(pid=19593)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19593)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19593)[0m - _max_episode_steps = 150
[2m[36m(pid=19593)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19593)[0m [32m [     1.29606s,  INFO] TimeLimit:
[2m[36m(pid=19593)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19593)[0m - action_space = Box(2,)
[2m[36m(pid=19593)[0m - observation_space = Box(9,)
[2m[36m(pid=19593)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19593)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19593)[0m - _max_episode_steps = 150
[2m[36m(pid=19593)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19593)[0m [32m [     1.29666s,  INFO] TimeLimit:
[2m[36m(pid=19593)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19593)[0m - action_space = Box(2,)
[2m[36m(pid=19593)[0m - observation_space = Box(9,)
[2m[36m(pid=19593)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19593)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19593)[0m - _max_episode_steps = 150
[2m[36m(pid=19593)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19593)[0m [32m [     1.29736s,  INFO] TimeLimit:
[2m[36m(pid=19593)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19593)[0m - action_space = Box(2,)
[2m[36m(pid=19593)[0m - observation_space = Box(9,)
[2m[36m(pid=19593)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19593)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19593)[0m - _max_episode_steps = 150
[2m[36m(pid=19593)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19594)[0m [32m [     1.36878s,  INFO] TimeLimit:
[2m[36m(pid=19594)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19594)[0m - action_space = Box(2,)
[2m[36m(pid=19594)[0m - observation_space = Box(9,)
[2m[36m(pid=19594)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19594)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19594)[0m - _max_episode_steps = 150
[2m[36m(pid=19594)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19594)[0m [32m [     1.36979s,  INFO] TimeLimit:
[2m[36m(pid=19594)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19594)[0m - action_space = Box(2,)
[2m[36m(pid=19594)[0m - observation_space = Box(9,)
[2m[36m(pid=19594)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19594)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19594)[0m - _max_episode_steps = 150
[2m[36m(pid=19594)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19594)[0m [32m [     1.37073s,  INFO] TimeLimit:
[2m[36m(pid=19594)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19594)[0m - action_space = Box(2,)
[2m[36m(pid=19594)[0m - observation_space = Box(9,)
[2m[36m(pid=19594)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19594)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19594)[0m - _max_episode_steps = 150
[2m[36m(pid=19594)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19594)[0m [32m [     1.37187s,  INFO] TimeLimit:
[2m[36m(pid=19594)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19594)[0m - action_space = Box(2,)
[2m[36m(pid=19594)[0m - observation_space = Box(9,)
[2m[36m(pid=19594)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19594)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19594)[0m - _max_episode_steps = 150
[2m[36m(pid=19594)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19586)[0m [32m [     1.48155s,  INFO] TimeLimit:
[2m[36m(pid=19586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19586)[0m - action_space = Box(2,)
[2m[36m(pid=19586)[0m - observation_space = Box(9,)
[2m[36m(pid=19586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19586)[0m - _max_episode_steps = 150
[2m[36m(pid=19586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19586)[0m [32m [     1.48248s,  INFO] TimeLimit:
[2m[36m(pid=19586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19586)[0m - action_space = Box(2,)
[2m[36m(pid=19586)[0m - observation_space = Box(9,)
[2m[36m(pid=19586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19586)[0m - _max_episode_steps = 150
[2m[36m(pid=19586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19586)[0m [32m [     1.48347s,  INFO] TimeLimit:
[2m[36m(pid=19586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19586)[0m - action_space = Box(2,)
[2m[36m(pid=19586)[0m - observation_space = Box(9,)
[2m[36m(pid=19586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19586)[0m - _max_episode_steps = 150
[2m[36m(pid=19586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19586)[0m [32m [     1.48470s,  INFO] TimeLimit:
[2m[36m(pid=19586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19586)[0m - action_space = Box(2,)
[2m[36m(pid=19586)[0m - observation_space = Box(9,)
[2m[36m(pid=19586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19586)[0m - _max_episode_steps = 150
[2m[36m(pid=19586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19588)[0m [32m [     1.49351s,  INFO] TimeLimit:
[2m[36m(pid=19588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19588)[0m - action_space = Box(2,)
[2m[36m(pid=19588)[0m - observation_space = Box(9,)
[2m[36m(pid=19588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19588)[0m - _max_episode_steps = 150
[2m[36m(pid=19588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19588)[0m [32m [     1.49447s,  INFO] TimeLimit:
[2m[36m(pid=19588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19588)[0m - action_space = Box(2,)
[2m[36m(pid=19588)[0m - observation_space = Box(9,)
[2m[36m(pid=19588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19588)[0m - _max_episode_steps = 150
[2m[36m(pid=19588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19588)[0m [32m [     1.49538s,  INFO] TimeLimit:
[2m[36m(pid=19588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19588)[0m - action_space = Box(2,)
[2m[36m(pid=19588)[0m - observation_space = Box(9,)
[2m[36m(pid=19588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19588)[0m - _max_episode_steps = 150
[2m[36m(pid=19588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19588)[0m [32m [     1.49648s,  INFO] TimeLimit:
[2m[36m(pid=19588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19588)[0m - action_space = Box(2,)
[2m[36m(pid=19588)[0m - observation_space = Box(9,)
[2m[36m(pid=19588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19588)[0m - _max_episode_steps = 150
[2m[36m(pid=19588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19592)[0m [32m [     1.49191s,  INFO] TimeLimit:
[2m[36m(pid=19592)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19592)[0m - action_space = Box(2,)
[2m[36m(pid=19592)[0m - observation_space = Box(9,)
[2m[36m(pid=19592)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19592)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19592)[0m - _max_episode_steps = 150
[2m[36m(pid=19592)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19592)[0m [32m [     1.49287s,  INFO] TimeLimit:
[2m[36m(pid=19592)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19592)[0m - action_space = Box(2,)
[2m[36m(pid=19592)[0m - observation_space = Box(9,)
[2m[36m(pid=19592)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19592)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19592)[0m - _max_episode_steps = 150
[2m[36m(pid=19592)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19592)[0m [32m [     1.49380s,  INFO] TimeLimit:
[2m[36m(pid=19592)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19592)[0m - action_space = Box(2,)
[2m[36m(pid=19592)[0m - observation_space = Box(9,)
[2m[36m(pid=19592)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19592)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19592)[0m - _max_episode_steps = 150
[2m[36m(pid=19592)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19592)[0m [32m [     1.49496s,  INFO] TimeLimit:
[2m[36m(pid=19592)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19592)[0m - action_space = Box(2,)
[2m[36m(pid=19592)[0m - observation_space = Box(9,)
[2m[36m(pid=19592)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19592)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19592)[0m - _max_episode_steps = 150
[2m[36m(pid=19592)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19591)[0m [32m [     1.48105s,  INFO] TimeLimit:
[2m[36m(pid=19591)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19591)[0m - action_space = Box(2,)
[2m[36m(pid=19591)[0m - observation_space = Box(9,)
[2m[36m(pid=19591)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19591)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19591)[0m - _max_episode_steps = 150
[2m[36m(pid=19591)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19591)[0m [32m [     1.48212s,  INFO] TimeLimit:
[2m[36m(pid=19591)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19591)[0m - action_space = Box(2,)
[2m[36m(pid=19591)[0m - observation_space = Box(9,)
[2m[36m(pid=19591)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19591)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19591)[0m - _max_episode_steps = 150
[2m[36m(pid=19591)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19591)[0m [32m [     1.48308s,  INFO] TimeLimit:
[2m[36m(pid=19591)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19591)[0m - action_space = Box(2,)
[2m[36m(pid=19591)[0m - observation_space = Box(9,)
[2m[36m(pid=19591)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19591)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19591)[0m - _max_episode_steps = 150
[2m[36m(pid=19591)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19591)[0m [32m [     1.48429s,  INFO] TimeLimit:
[2m[36m(pid=19591)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19591)[0m - action_space = Box(2,)
[2m[36m(pid=19591)[0m - observation_space = Box(9,)
[2m[36m(pid=19591)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19591)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19591)[0m - _max_episode_steps = 150
[2m[36m(pid=19591)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19584)[0m [32m [     1.47748s,  INFO] TimeLimit:
[2m[36m(pid=19584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19584)[0m - action_space = Box(2,)
[2m[36m(pid=19584)[0m - observation_space = Box(9,)
[2m[36m(pid=19584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19584)[0m - _max_episode_steps = 150
[2m[36m(pid=19584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19584)[0m [32m [     1.47839s,  INFO] TimeLimit:
[2m[36m(pid=19584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19584)[0m - action_space = Box(2,)
[2m[36m(pid=19584)[0m - observation_space = Box(9,)
[2m[36m(pid=19584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19584)[0m - _max_episode_steps = 150
[2m[36m(pid=19584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19584)[0m [32m [     1.47925s,  INFO] TimeLimit:
[2m[36m(pid=19584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19584)[0m - action_space = Box(2,)
[2m[36m(pid=19584)[0m - observation_space = Box(9,)
[2m[36m(pid=19584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19584)[0m - _max_episode_steps = 150
[2m[36m(pid=19584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19584)[0m [32m [     1.48030s,  INFO] TimeLimit:
[2m[36m(pid=19584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19584)[0m - action_space = Box(2,)
[2m[36m(pid=19584)[0m - observation_space = Box(9,)
[2m[36m(pid=19584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19584)[0m - _max_episode_steps = 150
[2m[36m(pid=19584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19583)[0m [32m [     1.50304s,  INFO] TimeLimit:
[2m[36m(pid=19583)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19583)[0m - action_space = Box(2,)
[2m[36m(pid=19583)[0m - observation_space = Box(9,)
[2m[36m(pid=19583)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19583)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19583)[0m - _max_episode_steps = 150
[2m[36m(pid=19583)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19583)[0m [32m [     1.50378s,  INFO] TimeLimit:
[2m[36m(pid=19583)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19583)[0m - action_space = Box(2,)
[2m[36m(pid=19583)[0m - observation_space = Box(9,)
[2m[36m(pid=19583)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19583)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19583)[0m - _max_episode_steps = 150
[2m[36m(pid=19583)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19583)[0m [32m [     1.50460s,  INFO] TimeLimit:
[2m[36m(pid=19583)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19583)[0m - action_space = Box(2,)
[2m[36m(pid=19583)[0m - observation_space = Box(9,)
[2m[36m(pid=19583)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19583)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19583)[0m - _max_episode_steps = 150
[2m[36m(pid=19583)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19583)[0m [32m [     1.50566s,  INFO] TimeLimit:
[2m[36m(pid=19583)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19583)[0m - action_space = Box(2,)
[2m[36m(pid=19583)[0m - observation_space = Box(9,)
[2m[36m(pid=19583)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19583)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19583)[0m - _max_episode_steps = 150
[2m[36m(pid=19583)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19585)[0m 2019-07-18 03:26:49,456	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=19585)[0m 
[2m[36m(pid=19585)[0m { 'data': { 'action_prob': np.ndarray((500,), dtype=float32, min=0.0, max=0.159, mean=0.08),
[2m[36m(pid=19585)[0m             'actions': np.ndarray((500, 2), dtype=float32, min=-3.378, max=3.539, mean=-0.013),
[2m[36m(pid=19585)[0m             'agent_index': np.ndarray((500,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19585)[0m             'behaviour_logits': np.ndarray((500, 4), dtype=float32, min=-0.008, max=0.007, mean=-0.0),
[2m[36m(pid=19585)[0m             'dones': np.ndarray((500,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19585)[0m             'eps_id': np.ndarray((500,), dtype=int64, min=210999967.0, max=1996255911.0, mean=969735550.5),
[2m[36m(pid=19585)[0m             'infos': np.ndarray((500,), dtype=object, head={}),
[2m[36m(pid=19585)[0m             'obs': np.ndarray((500, 9), dtype=float32, min=-3.763, max=3.22, mean=-0.074),
[2m[36m(pid=19585)[0m             'prev_actions': np.ndarray((500, 2), dtype=float32, min=-3.378, max=3.539, mean=-0.015),
[2m[36m(pid=19585)[0m             'prev_rewards': np.ndarray((500,), dtype=float32, min=-2.785, max=3.166, mean=-0.253),
[2m[36m(pid=19585)[0m             'rewards': np.ndarray((500,), dtype=float32, min=-2.785, max=3.166, mean=-0.276),
[2m[36m(pid=19585)[0m             't': np.ndarray((500,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=19585)[0m             'unroll_id': np.ndarray((500,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=19585)[0m   'type': 'SampleBatch'}
[2m[36m(pid=19585)[0m 
[2m[36m(pid=19585)[0m 2019-07-18 03:26:49,456	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=19587)[0m [32m [     1.50173s,  INFO] TimeLimit:
[2m[36m(pid=19587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19587)[0m - action_space = Box(2,)
[2m[36m(pid=19587)[0m - observation_space = Box(9,)
[2m[36m(pid=19587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19587)[0m - _max_episode_steps = 150
[2m[36m(pid=19587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19587)[0m [32m [     1.50272s,  INFO] TimeLimit:
[2m[36m(pid=19587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19587)[0m - action_space = Box(2,)
[2m[36m(pid=19587)[0m - observation_space = Box(9,)
[2m[36m(pid=19587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19587)[0m - _max_episode_steps = 150
[2m[36m(pid=19587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19587)[0m [32m [     1.50369s,  INFO] TimeLimit:
[2m[36m(pid=19587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19587)[0m - action_space = Box(2,)
[2m[36m(pid=19587)[0m - observation_space = Box(9,)
[2m[36m(pid=19587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19587)[0m - _max_episode_steps = 150
[2m[36m(pid=19587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19587)[0m [32m [     1.50488s,  INFO] TimeLimit:
[2m[36m(pid=19587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19587)[0m - action_space = Box(2,)
[2m[36m(pid=19587)[0m - observation_space = Box(9,)
[2m[36m(pid=19587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19587)[0m - _max_episode_steps = 150
[2m[36m(pid=19587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m [32m [     1.56600s,  INFO] TimeLimit:
[2m[36m(pid=19739)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19739)[0m - action_space = Box(2,)
[2m[36m(pid=19739)[0m - observation_space = Box(9,)
[2m[36m(pid=19739)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19739)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19739)[0m - _max_episode_steps = 150
[2m[36m(pid=19739)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m [32m [     1.56699s,  INFO] TimeLimit:
[2m[36m(pid=19739)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19739)[0m - action_space = Box(2,)
[2m[36m(pid=19739)[0m - observation_space = Box(9,)
[2m[36m(pid=19739)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19739)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19739)[0m - _max_episode_steps = 150
[2m[36m(pid=19739)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m [32m [     1.56797s,  INFO] TimeLimit:
[2m[36m(pid=19739)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19739)[0m - action_space = Box(2,)
[2m[36m(pid=19739)[0m - observation_space = Box(9,)
[2m[36m(pid=19739)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19739)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19739)[0m - _max_episode_steps = 150
[2m[36m(pid=19739)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m [32m [     1.56913s,  INFO] TimeLimit:
[2m[36m(pid=19739)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=19739)[0m - action_space = Box(2,)
[2m[36m(pid=19739)[0m - observation_space = Box(9,)
[2m[36m(pid=19739)[0m - reward_range = (-inf, inf)
[2m[36m(pid=19739)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=19739)[0m - _max_episode_steps = 150
[2m[36m(pid=19739)[0m - _elapsed_steps = None [0m
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,520	INFO rollout_worker.py:428 -- Generating sample batch of size 250
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,573	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.038, max=0.145, mean=-0.216)},
[2m[36m(pid=19739)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.124, max=0.953, mean=0.232)},
[2m[36m(pid=19739)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.999, max=0.233, mean=-0.159)},
[2m[36m(pid=19739)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.346, max=0.977, mean=0.106)},
[2m[36m(pid=19739)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.956, max=0.195, mean=-0.139)}}
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,574	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=19739)[0m   1: {'agent0': None},
[2m[36m(pid=19739)[0m   2: {'agent0': None},
[2m[36m(pid=19739)[0m   3: {'agent0': None},
[2m[36m(pid=19739)[0m   4: {'agent0': None}}
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,574	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-1.038, max=0.145, mean=-0.216)
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,574	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-1.038, max=0.145, mean=-0.216)
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,578	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19739)[0m                                   'env_id': 0,
[2m[36m(pid=19739)[0m                                   'info': None,
[2m[36m(pid=19739)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.038, max=0.145, mean=-0.216),
[2m[36m(pid=19739)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19739)[0m                                   'rnn_state': []},
[2m[36m(pid=19739)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19739)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19739)[0m                                   'env_id': 1,
[2m[36m(pid=19739)[0m                                   'info': None,
[2m[36m(pid=19739)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.124, max=0.953, mean=0.232),
[2m[36m(pid=19739)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19739)[0m                                   'rnn_state': []},
[2m[36m(pid=19739)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19739)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19739)[0m                                   'env_id': 2,
[2m[36m(pid=19739)[0m                                   'info': None,
[2m[36m(pid=19739)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.999, max=0.233, mean=-0.159),
[2m[36m(pid=19739)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19739)[0m                                   'rnn_state': []},
[2m[36m(pid=19739)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19739)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19739)[0m                                   'env_id': 3,
[2m[36m(pid=19739)[0m                                   'info': None,
[2m[36m(pid=19739)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.346, max=0.977, mean=0.106),
[2m[36m(pid=19739)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19739)[0m                                   'rnn_state': []},
[2m[36m(pid=19739)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=19739)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=19739)[0m                                   'env_id': 4,
[2m[36m(pid=19739)[0m                                   'info': None,
[2m[36m(pid=19739)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.956, max=0.195, mean=-0.139),
[2m[36m(pid=19739)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=19739)[0m                                   'rnn_state': []},
[2m[36m(pid=19739)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,579	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,607	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m { 'default_policy': ( np.ndarray((5, 2), dtype=float32, min=-1.119, max=1.304, mean=0.007),
[2m[36m(pid=19739)[0m                       [],
[2m[36m(pid=19739)[0m                       { 'action_prob': np.ndarray((5,), dtype=float32, min=0.062, max=0.154, mean=0.098),
[2m[36m(pid=19739)[0m                         'behaviour_logits': np.ndarray((5, 4), dtype=float32, min=-0.005, max=0.005, mean=-0.0)})}
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,769	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.003, max=0.154, mean=0.086),
[2m[36m(pid=19739)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-2.233, max=1.919, mean=-0.204),
[2m[36m(pid=19739)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                         'behaviour_logits': np.ndarray((50, 4), dtype=float32, min=-0.004, max=0.006, mean=0.0),
[2m[36m(pid=19739)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=760331067.0, max=760331067.0, mean=760331067.0),
[2m[36m(pid=19739)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=19739)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-1.038, max=0.524, mean=-0.239),
[2m[36m(pid=19739)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.233, max=1.919, mean=-0.204),
[2m[36m(pid=19739)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-0.67, max=0.837, mean=-0.079),
[2m[36m(pid=19739)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-0.67, max=0.837, mean=-0.082),
[2m[36m(pid=19739)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=19739)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=19739)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m 2019-07-18 03:26:49,777	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19739)[0m { 'data': { 'action_prob': np.ndarray((250,), dtype=float32, min=0.0, max=0.158, mean=0.083),
[2m[36m(pid=19739)[0m             'actions': np.ndarray((250, 2), dtype=float32, min=-2.82, max=3.549, mean=-0.015),
[2m[36m(pid=19739)[0m             'agent_index': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m             'behaviour_logits': np.ndarray((250, 4), dtype=float32, min=-0.008, max=0.007, mean=-0.0),
[2m[36m(pid=19739)[0m             'dones': np.ndarray((250,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=19739)[0m             'eps_id': np.ndarray((250,), dtype=int64, min=651289879.0, max=1975583514.0, mean=1050204079.6),
[2m[36m(pid=19739)[0m             'infos': np.ndarray((250,), dtype=object, head={}),
[2m[36m(pid=19739)[0m             'obs': np.ndarray((250, 9), dtype=float32, min=-5.238, max=3.906, mean=-0.032),
[2m[36m(pid=19739)[0m             'prev_actions': np.ndarray((250, 2), dtype=float32, min=-2.82, max=3.549, mean=-0.015),
[2m[36m(pid=19739)[0m             'prev_rewards': np.ndarray((250,), dtype=float32, min=-3.694, max=3.434, mean=0.002),
[2m[36m(pid=19739)[0m             'rewards': np.ndarray((250,), dtype=float32, min=-3.694, max=3.434, mean=-0.007),
[2m[36m(pid=19739)[0m             't': np.ndarray((250,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=19739)[0m             'unroll_id': np.ndarray((250,), dtype=int64, min=0.0, max=0.0, mean=0.0)},
[2m[36m(pid=19739)[0m   'type': 'SampleBatch'}
[2m[36m(pid=19739)[0m 
[2m[36m(pid=19585)[0m 2019-07-18 03:26:49,965	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=19585)[0m 
[2m[36m(pid=19585)[0m { 'learner_stats': { 'cur_lr': 0.0005000000237487257,
[2m[36m(pid=19585)[0m                      'entropy': 1390.8639,
[2m[36m(pid=19585)[0m                      'grad_gnorm': 40.0,
[2m[36m(pid=19585)[0m                      'model': {},
[2m[36m(pid=19585)[0m                      'policy_loss': -8958.027,
[2m[36m(pid=19585)[0m                      'var_gnorm': 22.649517,
[2m[36m(pid=19585)[0m                      'vf_explained_var': 0.006641865,
[2m[36m(pid=19585)[0m                      'vf_loss': 25697.068}}
[2m[36m(pid=19585)[0m 
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-26-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.502065465596575
  episode_reward_mean: -15.845838865684321
  episode_reward_min: -91.39859669504304
  episodes_this_iter: 880
  episodes_total: 880
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1006.059326171875
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 959.3829956054688
      var_gnorm: 23.080583572387695
      vf_explained_var: 0.7249205112457275
      vf_loss: 1223.6715087890625
    learner_queue:
      size_count: 257
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 128500
    num_steps_trained: 128500
    num_weight_syncs: 514
    sample_throughput: 12257.235
    timing_breakdown:
      learner_dequeue_time_ms: 25.427
      learner_grad_time_ms: 8.159
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.602
    train_throughput: 12257.235
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.570058443804889
    mean_inference_ms: 1.2733694401803197
    mean_processing_ms: 0.6440669852948498
  time_since_restore: 10.300618886947632
  time_this_iter_s: 10.300618886947632
  time_total_s: 10.300618886947632
  timestamp: 1563413218
  timesteps_since_restore: 128500
  timesteps_this_iter: 128500
  timesteps_total: 128500
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 10 s, 1 iter, 128500 ts, -15.8 rew

[2m[36m(pid=19585)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.
[2m[36m(pid=19585)[0m   out=out, **kwargs)
[2m[36m(pid=19585)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
[2m[36m(pid=19585)[0m   ret = ret.dtype.type(ret / rcount)
Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-27-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 8.03404697373143
  episode_reward_mean: -64.28052161293196
  episode_reward_min: -210.4206987078231
  episodes_this_iter: 1020
  episodes_total: 1900
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1480.904052734375
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 70.34774780273438
      var_gnorm: 23.486448287963867
      vf_explained_var: 0.857815146446228
      vf_loss: 4038.859130859375
    learner_queue:
      size_count: 567
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 283500
    num_steps_trained: 283500
    num_weight_syncs: 1135
    sample_throughput: 15028.895
    timing_breakdown:
      learner_dequeue_time_ms: 17.397
      learner_grad_time_ms: 8.086
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 26.429
    train_throughput: 15028.895
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.563157412334532
    mean_inference_ms: 1.267014911601945
    mean_processing_ms: 0.643260791575344
  time_since_restore: 20.60600733757019
  time_this_iter_s: 10.305388450622559
  time_total_s: 20.60600733757019
  timestamp: 1563413228
  timesteps_since_restore: 283500
  timesteps_this_iter: 155000
  timesteps_total: 283500
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 20 s, 2 iter, 283500 ts, -64.3 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-27-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 8.051615430427072
  episode_reward_mean: -93.05661318384882
  episode_reward_min: -204.93394636918373
  episodes_this_iter: 1040
  episodes_total: 2940
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1296.725830078125
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 96.96188354492188
      var_gnorm: 23.7493953704834
      vf_explained_var: 0.981600821018219
      vf_loss: 574.6360473632812
    learner_queue:
      size_count: 877
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 438500
    num_steps_trained: 438500
    num_weight_syncs: 1754
    sample_throughput: 15040.747
    timing_breakdown:
      learner_dequeue_time_ms: 20.631
      learner_grad_time_ms: 9.358
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.562
    train_throughput: 15040.748
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5631812253536772
    mean_inference_ms: 1.264910995634176
    mean_processing_ms: 0.6450833158994063
  time_since_restore: 30.90399932861328
  time_this_iter_s: 10.29799199104309
  time_total_s: 30.90399932861328
  timestamp: 1563413239
  timesteps_since_restore: 438500
  timesteps_this_iter: 155000
  timesteps_total: 438500
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 30 s, 3 iter, 438500 ts, -93.1 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-27-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.810440176068788
  episode_reward_mean: -68.99614784731064
  episode_reward_min: -204.2535787215985
  episodes_this_iter: 1030
  episodes_total: 3970
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1353.6829833984375
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 1254.6300048828125
      var_gnorm: 23.88361930847168
      vf_explained_var: 0.7360367774963379
      vf_loss: 3984.37744140625
    learner_queue:
      size_count: 1186
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 593000
    num_steps_trained: 593000
    num_weight_syncs: 2372
    sample_throughput: 15005.711
    timing_breakdown:
      learner_dequeue_time_ms: 24.108
      learner_grad_time_ms: 8.827
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.986
    train_throughput: 15005.711
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5623459219525797
    mean_inference_ms: 1.2643159536074675
    mean_processing_ms: 0.6446601373344806
  time_since_restore: 41.192519664764404
  time_this_iter_s: 10.288520336151123
  time_total_s: 41.192519664764404
  timestamp: 1563413249
  timesteps_since_restore: 593000
  timesteps_this_iter: 154500
  timesteps_total: 593000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 41 s, 4 iter, 593000 ts, -69 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-27-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.028591556568657
  episode_reward_mean: -25.181529381837365
  episode_reward_min: -98.19795606324485
  episodes_this_iter: 1030
  episodes_total: 5000
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 1090.5003662109375
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 28.419525146484375
      var_gnorm: 24.03960609436035
      vf_explained_var: 0.9570351839065552
      vf_loss: 597.2310791015625
    learner_queue:
      size_count: 1496
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    num_steps_replayed: 0
    num_steps_sampled: 748000
    num_steps_trained: 748000
    num_weight_syncs: 2992
    sample_throughput: 15057.604
    timing_breakdown:
      learner_dequeue_time_ms: 27.562
      learner_grad_time_ms: 8.63
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.792
    train_throughput: 15057.605
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.559453849343128
    mean_inference_ms: 1.2637602632960476
    mean_processing_ms: 0.6443751251261024
  time_since_restore: 51.478402853012085
  time_this_iter_s: 10.28588318824768
  time_total_s: 51.478402853012085
  timestamp: 1563413259
  timesteps_since_restore: 748000
  timesteps_this_iter: 155000
  timesteps_total: 748000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 51 s, 5 iter, 748000 ts, -25.2 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-27-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 11.667587642955795
  episode_reward_mean: -58.37013744928887
  episode_reward_min: -187.61067126617334
  episodes_this_iter: 1045
  episodes_total: 6045
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 2040.4466552734375
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: 689.2880249023438
      var_gnorm: 24.315677642822266
      vf_explained_var: 0.6279077529907227
      vf_loss: 6909.84228515625
    learner_queue:
      size_count: 1807
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 903500
    num_steps_trained: 903500
    num_weight_syncs: 3614
    sample_throughput: 15086.15
    timing_breakdown:
      learner_dequeue_time_ms: 24.168
      learner_grad_time_ms: 8.716
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 19.3
    train_throughput: 15086.151
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5580858991476163
    mean_inference_ms: 1.263232127797387
    mean_processing_ms: 0.6441156357267684
  time_since_restore: 61.77812123298645
  time_this_iter_s: 10.299718379974365
  time_total_s: 61.77812123298645
  timestamp: 1563413269
  timesteps_since_restore: 903500
  timesteps_this_iter: 155500
  timesteps_total: 903500
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 61 s, 6 iter, 903500 ts, -58.4 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-28-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -3.9337266155936446
  episode_reward_mean: -106.30628438497354
  episode_reward_min: -199.11590317974242
  episodes_this_iter: 1035
  episodes_total: 7080
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: -990.02001953125
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 1659.249267578125
      var_gnorm: 24.7360897064209
      vf_explained_var: 0.9698317646980286
      vf_loss: 2040.8272705078125
    learner_queue:
      size_count: 2116
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1058000
    num_steps_trained: 1058000
    num_weight_syncs: 4233
    sample_throughput: 14971.873
    timing_breakdown:
      learner_dequeue_time_ms: 26.394
      learner_grad_time_ms: 8.55
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.035
    train_throughput: 14971.874
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5591399271086568
    mean_inference_ms: 1.2622013752397705
    mean_processing_ms: 0.6442008446239585
  time_since_restore: 72.08940958976746
  time_this_iter_s: 10.311288356781006
  time_total_s: 72.08940958976746
  timestamp: 1563413280
  timesteps_since_restore: 1058000
  timesteps_this_iter: 154500
  timesteps_total: 1058000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 72 s, 7 iter, 1058000 ts, -106 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-28-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -2.5745072291392073
  episode_reward_mean: -102.77539123863423
  episode_reward_min: -205.26024581907026
  episodes_this_iter: 1020
  episodes_total: 8100
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 861.3453369140625
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 2591.340576171875
      var_gnorm: 25.013534545898438
      vf_explained_var: 0.9382316470146179
      vf_loss: 9633.646484375
    learner_queue:
      size_count: 2425
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1212500
    num_steps_trained: 1212500
    num_weight_syncs: 4850
    sample_throughput: 14991.761
    timing_breakdown:
      learner_dequeue_time_ms: 22.984
      learner_grad_time_ms: 9.183
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.588
    train_throughput: 14991.762
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.561221184359819
    mean_inference_ms: 1.262719014047744
    mean_processing_ms: 0.6439668815053925
  time_since_restore: 82.38728427886963
  time_this_iter_s: 10.297874689102173
  time_total_s: 82.38728427886963
  timestamp: 1563413290
  timesteps_since_restore: 1212500
  timesteps_this_iter: 154500
  timesteps_total: 1212500
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 82 s, 8 iter, 1212500 ts, -103 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-28-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -9.448606634503816
  episode_reward_mean: -113.085427023669
  episode_reward_min: -203.6538994095043
  episodes_this_iter: 1020
  episodes_total: 9120
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 3468.1572265625
      grad_gnorm: 40.0
      model: {}
      policy_loss: -1071.7935791015625
      var_gnorm: 25.688180923461914
      vf_explained_var: 0.9849861264228821
      vf_loss: 7482.50146484375
    learner_queue:
      size_count: 2733
      size_mean: 0.04
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.19595917942265423
    num_steps_replayed: 0
    num_steps_sampled: 1366500
    num_steps_trained: 1366500
    num_weight_syncs: 5466
    sample_throughput: 14979.322
    timing_breakdown:
      learner_dequeue_time_ms: 25.67
      learner_grad_time_ms: 7.791
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.531
    train_throughput: 14979.322
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.561983168862632
    mean_inference_ms: 1.262174377922376
    mean_processing_ms: 0.643909133261344
  time_since_restore: 92.66064643859863
  time_this_iter_s: 10.273362159729004
  time_total_s: 92.66064643859863
  timestamp: 1563413300
  timesteps_since_restore: 1366500
  timesteps_this_iter: 154000
  timesteps_total: 1366500
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 92 s, 9 iter, 1366500 ts, -113 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-28-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -0.22234778874968697
  episode_reward_mean: -125.52510639294933
  episode_reward_min: -204.96085416434204
  episodes_this_iter: 1035
  episodes_total: 10155
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 5951.38671875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -1642.063232421875
      var_gnorm: 26.142915725708008
      vf_explained_var: 0.9957705140113831
      vf_loss: 741.5577392578125
    learner_queue:
      size_count: 3042
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1521000
    num_steps_trained: 1520500
    num_weight_syncs: 6084
    sample_throughput: 14993.197
    timing_breakdown:
      learner_dequeue_time_ms: 26.362
      learner_grad_time_ms: 9.31
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 17.027
    train_throughput: 14944.676
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5622706114192395
    mean_inference_ms: 1.2619964847434375
    mean_processing_ms: 0.6441944663724133
  time_since_restore: 102.95772194862366
  time_this_iter_s: 10.297075510025024
  time_total_s: 102.95772194862366
  timestamp: 1563413311
  timesteps_since_restore: 1521000
  timesteps_this_iter: 154500
  timesteps_total: 1521000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 102 s, 10 iter, 1521000 ts, -126 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-28-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -11.171424050887481
  episode_reward_mean: -136.58628885108232
  episode_reward_min: -212.46259531219874
  episodes_this_iter: 1030
  episodes_total: 11185
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 6696.44921875
      grad_gnorm: 39.999996185302734
      model: {}
      policy_loss: -2220.188232421875
      var_gnorm: 26.4476261138916
      vf_explained_var: 0.9763563871383667
      vf_loss: 5542.4033203125
    learner_queue:
      size_count: 3350
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1675000
    num_steps_trained: 1675000
    num_weight_syncs: 6700
    sample_throughput: 14909.587
    timing_breakdown:
      learner_dequeue_time_ms: 23.415
      learner_grad_time_ms: 9.005
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 22.426
    train_throughput: 14957.995
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5620266590430854
    mean_inference_ms: 1.2620655274187609
    mean_processing_ms: 0.644017800240291
  time_since_restore: 113.27776145935059
  time_this_iter_s: 10.320039510726929
  time_total_s: 113.27776145935059
  timestamp: 1563413321
  timesteps_since_restore: 1675000
  timesteps_this_iter: 154000
  timesteps_total: 1675000
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 113 s, 11 iter, 1675000 ts, -137 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-28-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -10.617455058551219
  episode_reward_mean: -145.42883849774003
  episode_reward_min: -209.2860854445666
  episodes_this_iter: 1030
  episodes_total: 12215
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8344.5126953125
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: -2114.33203125
      var_gnorm: 27.107664108276367
      vf_explained_var: 0.5310461521148682
      vf_loss: 3869.260986328125
    learner_queue:
      size_count: 3658
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1829000
    num_steps_trained: 1829000
    num_weight_syncs: 7316
    sample_throughput: 14959.245
    timing_breakdown:
      learner_dequeue_time_ms: 27.035
      learner_grad_time_ms: 8.523
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 28.115
    train_throughput: 14959.245
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5623734512766392
    mean_inference_ms: 1.262344789257148
    mean_processing_ms: 0.6439805047182953
  time_since_restore: 123.56456160545349
  time_this_iter_s: 10.286800146102905
  time_total_s: 123.56456160545349
  timestamp: 1563413331
  timesteps_since_restore: 1829000
  timesteps_this_iter: 154000
  timesteps_total: 1829000
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 123 s, 12 iter, 1829000 ts, -145 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-29-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -21.05082815968398
  episode_reward_mean: -154.33058322715883
  episode_reward_min: -211.95681645261556
  episodes_this_iter: 1025
  episodes_total: 13240
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8534.3388671875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: 2166.01708984375
      var_gnorm: 27.448362350463867
      vf_explained_var: 0.1830577254295349
      vf_loss: 14081.5673828125
    learner_queue:
      size_count: 3967
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 1983500
    num_steps_trained: 1983500
    num_weight_syncs: 7934
    sample_throughput: 15009.971
    timing_breakdown:
      learner_dequeue_time_ms: 25.806
      learner_grad_time_ms: 9.164
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 25.326
    train_throughput: 15009.972
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.562865619966463
    mean_inference_ms: 1.2618527589014708
    mean_processing_ms: 0.6440835863119292
  time_since_restore: 133.84991002082825
  time_this_iter_s: 10.285348415374756
  time_total_s: 133.84991002082825
  timestamp: 1563413342
  timesteps_since_restore: 1983500
  timesteps_this_iter: 154500
  timesteps_total: 1983500
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 133 s, 13 iter, 1983500 ts, -154 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-29-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -11.230083694444357
  episode_reward_mean: -144.21800359380018
  episode_reward_min: -207.04385467819634
  episodes_this_iter: 1035
  episodes_total: 14275
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8773.796875
      grad_gnorm: 40.000003814697266
      model: {}
      policy_loss: -230.51133728027344
      var_gnorm: 28.107152938842773
      vf_explained_var: 0.9412855505943298
      vf_loss: 8572.6806640625
    learner_queue:
      size_count: 4276
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2138000
    num_steps_trained: 2138000
    num_weight_syncs: 8552
    sample_throughput: 14978.539
    timing_breakdown:
      learner_dequeue_time_ms: 23.282
      learner_grad_time_ms: 8.563
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.636
    train_throughput: 14978.54
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5630238421371812
    mean_inference_ms: 1.2619320291563032
    mean_processing_ms: 0.6440336919596313
  time_since_restore: 144.15624594688416
  time_this_iter_s: 10.306335926055908
  time_total_s: 144.15624594688416
  timestamp: 1563413352
  timesteps_since_restore: 2138000
  timesteps_this_iter: 154500
  timesteps_total: 2138000
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 144 s, 14 iter, 2138000 ts, -144 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-29-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -3.90079865722356
  episode_reward_mean: -147.67088175171645
  episode_reward_min: -211.16070403958412
  episodes_this_iter: 1030
  episodes_total: 15305
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 9403.9814453125
      grad_gnorm: 40.0
      model: {}
      policy_loss: -9510.92578125
      var_gnorm: 28.839195251464844
      vf_explained_var: 0.9744132161140442
      vf_loss: 6035.93603515625
    learner_queue:
      size_count: 4586
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2293000
    num_steps_trained: 2293000
    num_weight_syncs: 9172
    sample_throughput: 15020.889
    timing_breakdown:
      learner_dequeue_time_ms: 19.185
      learner_grad_time_ms: 9.04
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.36
    train_throughput: 15020.889
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.563275176481124
    mean_inference_ms: 1.2616868470869034
    mean_processing_ms: 0.6442077782225079
  time_since_restore: 154.46658658981323
  time_this_iter_s: 10.310340642929077
  time_total_s: 154.46658658981323
  timestamp: 1563413362
  timesteps_since_restore: 2293000
  timesteps_this_iter: 155000
  timesteps_total: 2293000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 154 s, 15 iter, 2293000 ts, -148 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-29-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -13.065774498809057
  episode_reward_mean: -144.38900168580525
  episode_reward_min: -213.1797570685175
  episodes_this_iter: 1030
  episodes_total: 16335
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 9925.2822265625
      grad_gnorm: 40.0
      model: {}
      policy_loss: 25755.4453125
      var_gnorm: 29.385313034057617
      vf_explained_var: 0.7133541107177734
      vf_loss: 7445.28076171875
    learner_queue:
      size_count: 4895
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    num_steps_replayed: 0
    num_steps_sampled: 2447500
    num_steps_trained: 2447500
    num_weight_syncs: 9790
    sample_throughput: 14997.027
    timing_breakdown:
      learner_dequeue_time_ms: 22.287
      learner_grad_time_ms: 8.59
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 20.851
    train_throughput: 14997.027
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5634674171028606
    mean_inference_ms: 1.2616191874225111
    mean_processing_ms: 0.644003426779341
  time_since_restore: 164.7596173286438
  time_this_iter_s: 10.293030738830566
  time_total_s: 164.7596173286438
  timestamp: 1563413372
  timesteps_since_restore: 2447500
  timesteps_this_iter: 154500
  timesteps_total: 2447500
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 164 s, 16 iter, 2447500 ts, -144 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-29-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -10.800110421118974
  episode_reward_mean: -144.09269730895667
  episode_reward_min: -207.6786389207375
  episodes_this_iter: 1030
  episodes_total: 17365
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 8427.3916015625
      grad_gnorm: 40.00000762939453
      model: {}
      policy_loss: -2269.379150390625
      var_gnorm: 29.631975173950195
      vf_explained_var: 0.6829023361206055
      vf_loss: 3062.55126953125
    learner_queue:
      size_count: 5205
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2602500
    num_steps_trained: 2602500
    num_weight_syncs: 10410
    sample_throughput: 15003.919
    timing_breakdown:
      learner_dequeue_time_ms: 24.438
      learner_grad_time_ms: 8.711
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 15.941
    train_throughput: 15003.92
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5638219638065967
    mean_inference_ms: 1.261459770385873
    mean_processing_ms: 0.643871395304643
  time_since_restore: 175.08127856254578
  time_this_iter_s: 10.321661233901978
  time_total_s: 175.08127856254578
  timestamp: 1563413383
  timesteps_since_restore: 2602500
  timesteps_this_iter: 155000
  timesteps_total: 2602500
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 175 s, 17 iter, 2602500 ts, -144 rew

Result for IMPALA_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-18_03-29-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -13.747863455553295
  episode_reward_mean: -147.26704066972687
  episode_reward_min: -210.73036096555919
  episodes_this_iter: 1035
  episodes_total: 18400
  experiment_id: 133284c7add44b16bb83b688fa0aa8f7
  hostname: navel-notebook-1
  info:
    learner:
      cur_lr: 0.0005000000237487257
      entropy: 10964.9755859375
      grad_gnorm: 40.0
      model: {}
      policy_loss: -12714.861328125
      var_gnorm: 30.034608840942383
      vf_explained_var: 0.7774538993835449
      vf_loss: 4484.79736328125
    learner_queue:
      size_count: 5514
      size_mean: 0.0
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      size_std: 0.0
    num_steps_replayed: 0
    num_steps_sampled: 2757000
    num_steps_trained: 2757000
    num_weight_syncs: 11028
    sample_throughput: 15005.553
    timing_breakdown:
      learner_dequeue_time_ms: 21.536
      learner_grad_time_ms: 9.614
      learner_load_time_ms: .nan
      learner_load_wait_time_ms: .nan
      optimizer_step_time_ms: 18.41
    train_throughput: 15005.554
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 19585
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5641713202289182
    mean_inference_ms: 1.2614787943201886
    mean_processing_ms: 0.6438500857939552
  time_since_restore: 185.36874532699585
  time_this_iter_s: 10.287466764450073
  time_total_s: 185.36874532699585
  timestamp: 1563413393
  timesteps_since_restore: 2757000
  timesteps_this_iter: 154500
  timesteps_total: 2757000
  training_iteration: 18
  2019-07-18 03:29:55,264	ERROR trial_runner.py:487 -- Error processing event.
Traceback (most recent call last):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 436, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 323, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/worker.py", line 2195, in get
    raise value
ray.exceptions.RayTaskError: [36mray_IMPALA:train()[39m (pid=19585, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 364, in train
    raise e
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 353, in train
    result = Trainable.train(self)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/trainable.py", line 150, in train
    result = self._train()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 126, in _train
    fetches = self.optimizer.step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 131, in step
    sample_timesteps, train_timesteps = self._step()
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 173, in _step
    for train_batch in self.aggregator.iter_train_batches():
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 103, in iter_train_batches
    blocking_wait=True, max_yield=max_yield)):
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 151, in _augment_with_replay
    sample_batch = ray_get_and_free(sample_batch)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError: [36mray_RolloutWorker:sample()[39m (pid=19591, host=navel-notebook-1)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 430, in sample
    batches = [self.input_reader.next()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 60, in next
    batches = [self.get_data()]
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 101, in get_data
    item = next(self.rollout_provider)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py", line 338, in _env_runner
    base_env.send_actions(actions_to_send)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/base_env.py", line 332, in send_actions
    self.vector_env.vector_step(action_vector)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/env/vector_env.py", line 110, in vector_step
    obs, r, done, info = self.envs[i].step(actions[i])
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/gym/wrappers/time_limit.py", line 15, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 57, in step
    self.apply_action(a)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/roboschool/gym_reacher.py", line 30, in apply_action
    assert( np.isfinite(a).all() )
AssertionError


2019-07-18 03:29:55,267	INFO ray_trial_executor.py:187 -- Destroying actor for trial IMPALA_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - IMPALA_RoboschoolReacher-v1_0:	RUNNING, 3 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-26-42.txt, [12 CPUs, 1 GPUs], [pid=19585], 185 s, 18 iter, 2757000 ts, -147 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 10.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-impala
Number of trials: 1 ({'ERROR': 1})
ERROR trials:
 - IMPALA_RoboschoolReacher-v1_0:	ERROR, 4 failures: /home/amr/kayray_results/parallel/gym-reacher-impala/IMPALA_RoboschoolReacher-v1_0_2019-07-18_03-16-158ac3tial/error_2019-07-18_03-29-55.txt, [12 CPUs, 1 GPUs], [pid=19585], 185 s, 18 iter, 2757000 ts, -147 rew

Traceback (most recent call last):
  File "train.py", line 181, in <module>
    run(args, parser, dot_dict)
  File "train.py", line 170, in run
    resume=args.resume)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/tune.py", line 333, in run_experiments
    raise_on_failed_trial=raise_on_failed_trial)
  File "/home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/tune.py", line 273, in run
    raise TuneError("Trials did not complete", errored_trials)
ray.tune.error.TuneError: ('Trials did not complete', [IMPALA_RoboschoolReacher-v1_0])
[2m[36m(pid=19585)[0m 2019-07-18 03:29:55,260	INFO trainer.py:361 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.
