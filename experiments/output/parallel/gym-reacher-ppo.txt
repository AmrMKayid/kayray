WARNING: Logging before flag parsing goes to stderr.
W0717 13:34:19.097261 140302452221376 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2019-07-17 13:34:19,378	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-17_13-34-19_378093_29831/logs.
2019-07-17 13:34:19,486	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-17 13:34:19,602	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-17 13:34:19,605	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-17 13:34:19,628	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-17_13-34-19_378093_29831/logs.
2019-07-17 13:34:19,629	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-17 13:34:19,788	INFO tune.py:65 -- Did not find checkpoint file in /home/amr/kayray_results/parallel/gym-reacher-ppo.
2019-07-17 13:34:19,788	INFO tune.py:233 -- Starting a new experiment.
2019-07-17 13:34:19,835	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
W0717 13:34:19.837341 140302452221376 deprecation_wrapper.py:119] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/logger.py:136: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.

W0717 13:34:19.837760 140302452221376 deprecation_wrapper.py:119] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/logger.py:141: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

[32m [     0.32508s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.32544s,  INFO] Experiment configs: 
 {
  "gym-reacher-ppo": {
    "env": "RoboschoolReacher-v1",
    "run": "PPO",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 100,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 18,
      "timesteps_total": 5000000
    },
    "config": {
      "gamma": 0.995,
      "kl_coeff": 1.0,
      "num_sgd_iter": 20,
      "lr": 0.0001,
      "sgd_minibatch_size": 1000,
      "train_batch_size": 25000,
      "model": {
        "free_log_std": true
      },
      "num_gpus": 1,
      "num_workers": {
        "grid_search": [
          11,
          7
        ]
      },
      "num_envs_per_worker": {
        "grid_search": [
          16,
          8,
          4
        ]
      },
      "batch_mode": "complete_episodes",
      "observation_filter": "MeanStdFilter"
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 7.1/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING

[2m[36m(pid=29871)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29871)[0m W0717 13:34:21.562691 140019485513152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29871)[0m Instructions for updating:
[2m[36m(pid=29871)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21,827	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.833641: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.837712: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29871)[0m [32m [     0.02820s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.899920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.900214: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e9803c4700 executing computations on platform CUDA. Devices:
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.900231: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.919488: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920287: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e980243110 executing computations on platform Host. Devices:
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920302: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
[2m[36m(pid=29871)[0m name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
[2m[36m(pid=29871)[0m pciBusID: 0000:01:00.0
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920783: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920850: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920911: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.920974: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.921034: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.921095: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.925327: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.925345: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.925359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.925365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:21.925371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[2m[36m(pid=29871)[0m W0717 13:34:21.928981 140019485513152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29871)[0m Instructions for updating:
[2m[36m(pid=29871)[0m Use keras.layers.dense instead.
[2m[36m(pid=29871)[0m W0717 13:34:22.141619 140019485513152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29871)[0m Instructions for updating:
[2m[36m(pid=29871)[0m Use `tf.cast` instead.
[2m[36m(pid=29871)[0m 2019-07-17 13:34:22.191105: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29871)[0m 2019-07-17 13:34:22,205	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29871)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=29871)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=29871)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29871)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29871)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29871)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m W0717 13:34:22.241813 140019485513152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29871)[0m Instructions for updating:
[2m[36m(pid=29871)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29871)[0m 2019-07-17 13:34:22,893	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f56f23efac8>}
[2m[36m(pid=29871)[0m 2019-07-17 13:34:22,893	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f56f23ef9e8>}
[2m[36m(pid=29871)[0m 2019-07-17 13:34:22,894	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=29871)[0m [32m [     1.09208s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09247s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09287s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09325s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09362s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09400s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09444s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09486s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09523s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09560s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09597s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09635s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09673s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09710s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m [32m [     1.09747s,  INFO] TimeLimit:
[2m[36m(pid=29871)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29871)[0m - action_space = Box(2,)
[2m[36m(pid=29871)[0m - observation_space = Box(9,)
[2m[36m(pid=29871)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29871)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29871)[0m - _max_episode_steps = 150
[2m[36m(pid=29871)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29871)[0m 2019-07-17 13:34:22,991	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=29868)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29868)[0m W0717 13:34:25.652436 140439281640896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29868)[0m Instructions for updating:
[2m[36m(pid=29868)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29865)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29865)[0m W0717 13:34:25.654138 139762880554432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29865)[0m Instructions for updating:
[2m[36m(pid=29865)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29873)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29873)[0m W0717 13:34:25.668673 140704490796480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29873)[0m Instructions for updating:
[2m[36m(pid=29873)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29874)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29874)[0m W0717 13:34:25.673538 140235645593024 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29874)[0m Instructions for updating:
[2m[36m(pid=29874)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29875)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29875)[0m W0717 13:34:25.635026 140105879508416 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29875)[0m Instructions for updating:
[2m[36m(pid=29875)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29902)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29902)[0m W0717 13:34:25.669935 139934154466752 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29902)[0m Instructions for updating:
[2m[36m(pid=29902)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29867)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29867)[0m W0717 13:34:25.701154 139739888428480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29867)[0m Instructions for updating:
[2m[36m(pid=29867)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29872)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29872)[0m W0717 13:34:25.770215 140257172370880 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29872)[0m Instructions for updating:
[2m[36m(pid=29872)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29866)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29866)[0m W0717 13:34:25.767446 140379804480960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29866)[0m Instructions for updating:
[2m[36m(pid=29866)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29870)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29870)[0m W0717 13:34:25.766856 140058490410432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29870)[0m Instructions for updating:
[2m[36m(pid=29870)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29904)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29904)[0m W0717 13:34:25.903085 140490998826432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29904)[0m Instructions for updating:
[2m[36m(pid=29904)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26,263	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.285589: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26,284	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29875)[0m [32m [     0.04465s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26,259	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.280017: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.288404: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29865)[0m [32m [     0.03723s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     0.03723s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26,309	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.330562: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29874)[0m [32m [     0.03822s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.294304: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.297778: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.297840: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.297854: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.297970: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.298008: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.298020: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.308847: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.309297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561c0e8fd6c0 executing computations on platform Host. Devices:
[2m[36m(pid=29865)[0m 2019-07-17 13:34:26.309325: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29865)[0m W0717 13:34:26.316787 139762880554432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29865)[0m Instructions for updating:
[2m[36m(pid=29865)[0m Use keras.layers.dense instead.
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.293459: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.293792: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_1 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_1/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_1/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.294016: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_2 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_2/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_2/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.294178: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_3 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_3/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_3/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.294313: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_4 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_4/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_4/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.294461: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_5 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_5/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_5/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.294591: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.304224: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.312684: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.316096: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.316141: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.316153: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.316263: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.316298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.316308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.319000: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.319459: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56307958b9c0 executing computations on platform Host. Devices:
[2m[36m(pid=29873)[0m 2019-07-17 13:34:26.319487: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29873)[0m W0717 13:34:26.326608 140704490796480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29873)[0m Instructions for updating:
[2m[36m(pid=29873)[0m Use keras.layers.dense instead.
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26,293	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.315542: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.323686: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.327305: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.327369: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.327383: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.327518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.327558: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.327571: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.330397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.330872: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ba8fff3360 executing computations on platform Host. Devices:
[2m[36m(pid=29874)[0m 2019-07-17 13:34:26.330897: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29868)[0m [32m [     0.03335s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.291658: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.291716: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.291729: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.291855: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.291889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.291899: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.323452: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.323997: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5565a8204bc0 executing computations on platform Host. Devices:
[2m[36m(pid=29875)[0m 2019-07-17 13:34:26.324026: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29875)[0m W0717 13:34:26.331537 140105879508416 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29875)[0m Instructions for updating:
[2m[36m(pid=29875)[0m Use keras.layers.dense instead.
[2m[36m(pid=29867)[0m [32m [     0.03639s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26,328	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_6 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_6/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_6/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:26.294741: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29871)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29871)[0m See below for details of this colocation group:
[2m[36m(pid=29871)[0m Colocation Debug Info:
[2m[36m(pid=29871)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29871)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29871)[0m Assign: CPU 
[2m[36m(pid=29871)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29871)[0m VariableV2: CPU 
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_7 (VariableV2) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_7/Assign (Assign) /device:GPU:0
[2m[36m(pid=29871)[0m   default_policy_1/tower_1/Variable_7/read (Identity) /device:GPU:0
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.345418: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.370158: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.370221: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.370236: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.370358: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.370396: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.370408: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.378515: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.379063: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557844de81d0 executing computations on platform Host. Devices:
[2m[36m(pid=29868)[0m 2019-07-17 13:34:26.379098: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29868)[0m W0717 13:34:26.385997 140439281640896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29868)[0m Instructions for updating:
[2m[36m(pid=29868)[0m Use keras.layers.dense instead.
[2m[36m(pid=29872)[0m [32m [     0.03323s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m W0717 13:34:26.347852 140235645593024 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29874)[0m Instructions for updating:
[2m[36m(pid=29874)[0m Use keras.layers.dense instead.
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26,380	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.351518: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.359717: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.363427: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.363486: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.363503: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.363627: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.363669: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.363679: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.371576: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.387549: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55695d339f20 executing computations on platform Host. Devices:
[2m[36m(pid=29867)[0m 2019-07-17 13:34:26.387594: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29867)[0m W0717 13:34:26.394376 139739888428480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29867)[0m Instructions for updating:
[2m[36m(pid=29867)[0m Use keras.layers.dense instead.
[2m[36m(pid=29870)[0m [32m [     0.03243s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26,380	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29902)[0m [32m [     0.03541s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26,391	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.404522: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.412892: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.415872: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.415944: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.415958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.416078: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.416121: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.416134: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.431544: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.432025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f608cf2c80 executing computations on platform Host. Devices:
[2m[36m(pid=29872)[0m 2019-07-17 13:34:26.432053: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29872)[0m W0717 13:34:26.438897 140257172370880 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29872)[0m Instructions for updating:
[2m[36m(pid=29872)[0m Use keras.layers.dense instead.
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26,420	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.441171: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.449687: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29866)[0m [32m [     0.03546s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.402837: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.411188: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.426359: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.426413: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.426427: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.426540: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.426575: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.426587: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.439600: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.440064: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55835d915ab0 executing computations on platform Host. Devices:
[2m[36m(pid=29870)[0m 2019-07-17 13:34:26.440097: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29870)[0m W0717 13:34:26.448055 140058490410432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29870)[0m Instructions for updating:
[2m[36m(pid=29870)[0m Use keras.layers.dense instead.
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.412887: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.421142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.424240: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.424295: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.424306: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.424418: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.424448: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.424457: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.427023: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.427429: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ab4deb2a60 executing computations on platform Host. Devices:
[2m[36m(pid=29902)[0m 2019-07-17 13:34:26.427449: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29902)[0m W0717 13:34:26.433834 139934154466752 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29902)[0m Instructions for updating:
[2m[36m(pid=29902)[0m Use keras.layers.dense instead.
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.453072: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.453135: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.453148: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.453274: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.453312: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.453324: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.459550: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.460128: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557185a2b350 executing computations on platform Host. Devices:
[2m[36m(pid=29866)[0m 2019-07-17 13:34:26.460161: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29866)[0m W0717 13:34:26.467772 140379804480960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29866)[0m Instructions for updating:
[2m[36m(pid=29866)[0m Use keras.layers.dense instead.
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26,488	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29904)[0m [32m [     0.03441s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.510733: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.526088: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.529716: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.529779: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.529792: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.529908: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.529945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.529957: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.546017: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.546496: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5588d0a41a50 executing computations on platform Host. Devices:
[2m[36m(pid=29904)[0m 2019-07-17 13:34:26.546520: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29904)[0m W0717 13:34:26.553198 140490998826432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29904)[0m Instructions for updating:
[2m[36m(pid=29904)[0m Use keras.layers.dense instead.
[2m[36m(pid=29865)[0m W0717 13:34:26.906421 139762880554432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29865)[0m Instructions for updating:
[2m[36m(pid=29865)[0m Use `tf.cast` instead.
[2m[36m(pid=29873)[0m W0717 13:34:26.870471 140704490796480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29873)[0m Instructions for updating:
[2m[36m(pid=29873)[0m Use `tf.cast` instead.
[2m[36m(pid=29875)[0m W0717 13:34:26.889806 140105879508416 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29875)[0m Instructions for updating:
[2m[36m(pid=29875)[0m Use `tf.cast` instead.
[2m[36m(pid=29874)[0m W0717 13:34:26.963183 140235645593024 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29874)[0m Instructions for updating:
[2m[36m(pid=29874)[0m Use `tf.cast` instead.
[2m[36m(pid=29868)[0m W0717 13:34:26.972689 140439281640896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29868)[0m Instructions for updating:
[2m[36m(pid=29868)[0m Use `tf.cast` instead.
[2m[36m(pid=29867)[0m W0717 13:34:26.965373 139739888428480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29867)[0m Instructions for updating:
[2m[36m(pid=29867)[0m Use `tf.cast` instead.
[2m[36m(pid=29870)[0m W0717 13:34:26.998961 140058490410432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29870)[0m Instructions for updating:
[2m[36m(pid=29870)[0m Use `tf.cast` instead.
[2m[36m(pid=29902)[0m W0717 13:34:27.014297 139934154466752 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29902)[0m Instructions for updating:
[2m[36m(pid=29902)[0m Use `tf.cast` instead.
[2m[36m(pid=29865)[0m 2019-07-17 13:34:27.051686: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29873)[0m 2019-07-17 13:34:27.021945: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29875)[0m 2019-07-17 13:34:27.026576: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29872)[0m W0717 13:34:27.018146 140257172370880 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29872)[0m Instructions for updating:
[2m[36m(pid=29872)[0m Use `tf.cast` instead.
[2m[36m(pid=29868)[0m 2019-07-17 13:34:27.118713: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29867)[0m 2019-07-17 13:34:27.114003: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29866)[0m W0717 13:34:27.085991 140379804480960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29866)[0m Instructions for updating:
[2m[36m(pid=29866)[0m Use `tf.cast` instead.
[2m[36m(pid=29904)[0m W0717 13:34:27.115664 140490998826432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29904)[0m Instructions for updating:
[2m[36m(pid=29904)[0m Use `tf.cast` instead.
[2m[36m(pid=29874)[0m 2019-07-17 13:34:27.130298: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29875)[0m W0717 13:34:27.151377 140105879508416 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29875)[0m Instructions for updating:
[2m[36m(pid=29875)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29872)[0m 2019-07-17 13:34:27.160412: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29870)[0m 2019-07-17 13:34:27.146737: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29902)[0m 2019-07-17 13:34:27.159810: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29865)[0m W0717 13:34:27.190083 139762880554432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29865)[0m Instructions for updating:
[2m[36m(pid=29865)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29873)[0m W0717 13:34:27.183858 140704490796480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29873)[0m Instructions for updating:
[2m[36m(pid=29873)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29902)[0m 2019-07-17 13:34:27,197	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29902)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29902)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29902)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=29902)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=29902)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29902)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29902)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29902)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29902)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29902)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29902)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29868)[0m W0717 13:34:27.250947 140439281640896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29868)[0m Instructions for updating:
[2m[36m(pid=29868)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29874)[0m W0717 13:34:27.254309 140235645593024 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29874)[0m Instructions for updating:
[2m[36m(pid=29874)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29867)[0m W0717 13:34:27.243393 139739888428480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29867)[0m Instructions for updating:
[2m[36m(pid=29867)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29866)[0m 2019-07-17 13:34:27.241793: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29870)[0m W0717 13:34:27.269019 140058490410432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29870)[0m Instructions for updating:
[2m[36m(pid=29870)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29904)[0m 2019-07-17 13:34:27.265586: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29872)[0m W0717 13:34:27.287948 140257172370880 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29872)[0m Instructions for updating:
[2m[36m(pid=29872)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29902)[0m W0717 13:34:27.287326 139934154466752 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29902)[0m Instructions for updating:
[2m[36m(pid=29902)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29866)[0m W0717 13:34:27.374737 140379804480960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29866)[0m Instructions for updating:
[2m[36m(pid=29866)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29904)[0m W0717 13:34:27.417809 140490998826432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29904)[0m Instructions for updating:
[2m[36m(pid=29904)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29871)[0m W0717 13:34:27.899539 140019485513152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29871)[0m Instructions for updating:
[2m[36m(pid=29871)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29868)[0m [32m [     2.62443s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62510s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62573s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62635s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62697s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62760s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62824s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62898s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.62962s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.63023s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.63085s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.63148s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.63214s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m W0717 13:34:28.910938 140439281640896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29868)[0m Instructions for updating:
[2m[36m(pid=29868)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.63278s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29868)[0m [32m [     2.63341s,  INFO] TimeLimit:
[2m[36m(pid=29868)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29868)[0m - action_space = Box(2,)
[2m[36m(pid=29868)[0m - observation_space = Box(9,)
[2m[36m(pid=29868)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29868)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29868)[0m - _max_episode_steps = 150
[2m[36m(pid=29868)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m W0717 13:34:28.944977 140704490796480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29873)[0m Instructions for updating:
[2m[36m(pid=29873)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29873)[0m [32m [     2.68709s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.68785s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.68854s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.68920s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.68988s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69053s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69117s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69183s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69252s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69319s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69386s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69451s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69518s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69582s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29873)[0m [32m [     2.69647s,  INFO] TimeLimit:
[2m[36m(pid=29873)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29873)[0m - action_space = Box(2,)
[2m[36m(pid=29873)[0m - observation_space = Box(9,)
[2m[36m(pid=29873)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29873)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29873)[0m - _max_episode_steps = 150
[2m[36m(pid=29873)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m W0717 13:34:28.999494 139762880554432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29865)[0m Instructions for updating:
[2m[36m(pid=29865)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29865)[0m [32m [     2.76375s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76431s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76484s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76535s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76590s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76640s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76689s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76754s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76808s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76859s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76915s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.76976s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.77029s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m [32m [     2.66445s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.66524s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.66603s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.66682s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.66763s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.66848s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.66932s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67011s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.72658s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.77078s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29865)[0m [32m [     2.77150s,  INFO] TimeLimit:
[2m[36m(pid=29865)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29865)[0m - action_space = Box(2,)
[2m[36m(pid=29865)[0m - observation_space = Box(9,)
[2m[36m(pid=29865)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29865)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29865)[0m - _max_episode_steps = 150
[2m[36m(pid=29865)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67099s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67188s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.72743s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67287s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.72841s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67385s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.72940s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73036s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73127s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67482s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67569s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73216s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73291s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m [32m [     2.67662s,  INFO] TimeLimit:
[2m[36m(pid=29870)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29870)[0m - action_space = Box(2,)
[2m[36m(pid=29870)[0m - observation_space = Box(9,)
[2m[36m(pid=29870)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29870)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29870)[0m - _max_episode_steps = 150
[2m[36m(pid=29870)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73366s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73439s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29870)[0m W0717 13:34:29.026383 140058490410432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29870)[0m Instructions for updating:
[2m[36m(pid=29870)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29867)[0m [32m [     2.73514s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73586s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73659s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73728s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m [32m [     2.73796s,  INFO] TimeLimit:
[2m[36m(pid=29867)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29867)[0m - action_space = Box(2,)
[2m[36m(pid=29867)[0m - observation_space = Box(9,)
[2m[36m(pid=29867)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29867)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29867)[0m - _max_episode_steps = 150
[2m[36m(pid=29867)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29867)[0m W0717 13:34:29.031772 139739888428480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29867)[0m Instructions for updating:
[2m[36m(pid=29867)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29874)[0m [32m [     2.87150s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87220s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87286s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87350s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87414s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87490s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87553s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87618s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87684s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87749s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87811s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.86891s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.86944s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87005s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87064s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87120s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87172s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87221s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87270s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87319s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87365s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87411s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87458s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87516s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m W0717 13:34:29.091984 140105879508416 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29875)[0m Instructions for updating:
[2m[36m(pid=29875)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29874)[0m [32m [     2.87875s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.87942s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.88008s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87565s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29875)[0m [32m [     2.87610s,  INFO] TimeLimit:
[2m[36m(pid=29875)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29875)[0m - action_space = Box(2,)
[2m[36m(pid=29875)[0m - observation_space = Box(9,)
[2m[36m(pid=29875)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29875)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29875)[0m - _max_episode_steps = 150
[2m[36m(pid=29875)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m [32m [     2.88076s,  INFO] TimeLimit:
[2m[36m(pid=29874)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29874)[0m - action_space = Box(2,)
[2m[36m(pid=29874)[0m - observation_space = Box(9,)
[2m[36m(pid=29874)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29874)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29874)[0m - _max_episode_steps = 150
[2m[36m(pid=29874)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29874)[0m W0717 13:34:29.137662 140235645593024 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29874)[0m Instructions for updating:
[2m[36m(pid=29874)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29872)[0m [32m [     2.84287s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.84371s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.84476s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.84575s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.84674s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.84765s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.84867s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85006s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85135s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85221s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85311s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m W0717 13:34:29.166656 140379804480960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29866)[0m Instructions for updating:
[2m[36m(pid=29866)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29866)[0m [32m [     2.76648s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.76738s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.76822s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.76924s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.76995s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77082s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77180s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77276s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77373s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77478s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77577s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77665s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77756s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m [32m [     2.82409s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.82492s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.82571s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.82647s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.82730s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.82821s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.82915s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83010s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83101s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83183s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83261s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83340s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83416s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m W0717 13:34:29.193884 139934154466752 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29902)[0m Instructions for updating:
[2m[36m(pid=29902)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29872)[0m [32m [     2.85411s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85506s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85596s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m [32m [     2.85685s,  INFO] TimeLimit:
[2m[36m(pid=29872)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29872)[0m - action_space = Box(2,)
[2m[36m(pid=29872)[0m - observation_space = Box(9,)
[2m[36m(pid=29872)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29872)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29872)[0m - _max_episode_steps = 150
[2m[36m(pid=29872)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77846s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29866)[0m [32m [     2.77945s,  INFO] TimeLimit:
[2m[36m(pid=29866)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29866)[0m - action_space = Box(2,)
[2m[36m(pid=29866)[0m - observation_space = Box(9,)
[2m[36m(pid=29866)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29866)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29866)[0m - _max_episode_steps = 150
[2m[36m(pid=29866)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83493s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m [32m [     2.83589s,  INFO] TimeLimit:
[2m[36m(pid=29902)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29902)[0m - action_space = Box(2,)
[2m[36m(pid=29902)[0m - observation_space = Box(9,)
[2m[36m(pid=29902)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29902)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29902)[0m - _max_episode_steps = 150
[2m[36m(pid=29902)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29872)[0m W0717 13:34:29.206854 140257172370880 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29872)[0m Instructions for updating:
[2m[36m(pid=29872)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29904)[0m W0717 13:34:29.231451 140490998826432 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29904)[0m Instructions for updating:
[2m[36m(pid=29904)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29904)[0m [32m [     2.76206s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76308s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76428s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76531s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76645s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76730s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76814s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76905s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.76991s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.77084s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.77176s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.77275s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.77378s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.77480s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29904)[0m [32m [     2.77565s,  INFO] TimeLimit:
[2m[36m(pid=29904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29904)[0m - action_space = Box(2,)
[2m[36m(pid=29904)[0m - observation_space = Box(9,)
[2m[36m(pid=29904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29904)[0m - _max_episode_steps = 150
[2m[36m(pid=29904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,313	INFO rollout_worker.py:428 -- Generating sample batch of size 3200
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,460	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.826, max=0.836, mean=-0.065)},
[2m[36m(pid=29902)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.943, max=0.333, mean=-0.157)},
[2m[36m(pid=29902)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.689, max=0.999, mean=0.023)},
[2m[36m(pid=29902)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.969, max=0.266, mean=-0.094)},
[2m[36m(pid=29902)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.483, max=0.891, mean=0.118)},
[2m[36m(pid=29902)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.131, max=0.997, mean=0.222)},
[2m[36m(pid=29902)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.817, max=0.957, mean=0.084)},
[2m[36m(pid=29902)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.997, max=0.676, mean=-0.044)},
[2m[36m(pid=29902)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.293, max=0.956, mean=0.134)},
[2m[36m(pid=29902)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.91, max=0.415, mean=-0.063)},
[2m[36m(pid=29902)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.304, max=0.933, mean=0.211)},
[2m[36m(pid=29902)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.879, max=1.0, mean=0.022)},
[2m[36m(pid=29902)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.959, max=0.283, mean=-0.059)},
[2m[36m(pid=29902)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.966, max=0.211, mean=-0.184)},
[2m[36m(pid=29902)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.298, max=0.958, mean=0.092)},
[2m[36m(pid=29902)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.983, max=0.825, mean=-0.034)}}
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,460	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=29902)[0m   1: {'agent0': None},
[2m[36m(pid=29902)[0m   2: {'agent0': None},
[2m[36m(pid=29902)[0m   3: {'agent0': None},
[2m[36m(pid=29902)[0m   4: {'agent0': None},
[2m[36m(pid=29902)[0m   5: {'agent0': None},
[2m[36m(pid=29902)[0m   6: {'agent0': None},
[2m[36m(pid=29902)[0m   7: {'agent0': None},
[2m[36m(pid=29902)[0m   8: {'agent0': None},
[2m[36m(pid=29902)[0m   9: {'agent0': None},
[2m[36m(pid=29902)[0m   10: {'agent0': None},
[2m[36m(pid=29902)[0m   11: {'agent0': None},
[2m[36m(pid=29902)[0m   12: {'agent0': None},
[2m[36m(pid=29902)[0m   13: {'agent0': None},
[2m[36m(pid=29902)[0m   14: {'agent0': None},
[2m[36m(pid=29902)[0m   15: {'agent0': None}}
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,461	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.826, max=0.836, mean=-0.065)
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,461	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,470	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 0,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 1,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.079),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 2,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.612, max=1.068, mean=0.309),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 3,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.457, max=1.489, mean=0.153),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 4,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.753, max=1.365, mean=0.257),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 5,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.112, max=1.222, mean=0.376),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 6,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.002, max=1.109, mean=0.18),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 7,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.616, max=0.667, mean=-0.142),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 8,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.551, max=1.433, mean=0.156),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 9,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-2.211, max=1.276, mean=-0.186),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 10,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.969, max=1.215, mean=0.421),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 11,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-2.378, max=2.049, mean=-0.003),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 12,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.437, max=0.873, mean=-0.128),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 13,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.97, max=1.247, mean=-0.343),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 14,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.919, max=1.241, mean=0.258),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29902)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29902)[0m                                   'env_id': 15,
[2m[36m(pid=29902)[0m                                   'info': None,
[2m[36m(pid=29902)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-2.418, max=2.349, mean=-0.032),
[2m[36m(pid=29902)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29902)[0m                                   'rnn_state': []},
[2m[36m(pid=29902)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,471	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=29902)[0m 2019-07-17 13:34:29,574	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-1.016, max=1.773, mean=0.204),
[2m[36m(pid=29902)[0m                       [],
[2m[36m(pid=29902)[0m                       { 'action_prob': np.ndarray((16,), dtype=float32, min=0.03, max=0.159, mean=0.1),
[2m[36m(pid=29902)[0m                         'behaviour_logits': np.ndarray((16, 4), dtype=float32, min=-0.009, max=0.011, mean=0.0),
[2m[36m(pid=29902)[0m                         'vf_preds': np.ndarray((16,), dtype=float32, min=-0.006, max=0.008, mean=-0.0)})}
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m 2019-07-17 13:34:30,605	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.0, max=0.158, mean=0.084),
[2m[36m(pid=29902)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-3.238, max=2.291, mean=-0.015),
[2m[36m(pid=29902)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-16.711, max=8.718, mean=-0.407),
[2m[36m(pid=29902)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.008, max=0.006, mean=0.0),
[2m[36m(pid=29902)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=29902)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1640183449.0, max=1640183449.0, mean=1640183449.0),
[2m[36m(pid=29902)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=29902)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-3.881, max=2.422, mean=-0.229),
[2m[36m(pid=29902)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-3.881, max=2.422, mean=-0.23),
[2m[36m(pid=29902)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-3.238, max=2.291, mean=-0.006),
[2m[36m(pid=29902)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-2.208, max=2.074, mean=-0.113),
[2m[36m(pid=29902)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-2.208, max=2.074, mean=-0.106),
[2m[36m(pid=29902)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=29902)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-16.708, max=8.721, mean=-0.404),
[2m[36m(pid=29902)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.007, max=0.012, mean=0.003)},
[2m[36m(pid=29902)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m 2019-07-17 13:34:32,126	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29902)[0m { 'data': { 'action_prob': np.ndarray((3300,), dtype=float32, min=0.0, max=0.159, mean=0.08),
[2m[36m(pid=29902)[0m             'actions': np.ndarray((3300, 2), dtype=float32, min=-4.288, max=4.084, mean=-0.002),
[2m[36m(pid=29902)[0m             'advantages': np.ndarray((3300,), dtype=float32, min=-36.508, max=31.935, mean=-5.275),
[2m[36m(pid=29902)[0m             'agent_index': np.ndarray((3300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29902)[0m             'behaviour_logits': np.ndarray((3300, 4), dtype=float32, min=-0.016, max=0.012, mean=-0.0),
[2m[36m(pid=29902)[0m             'dones': np.ndarray((3300,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=29902)[0m             'eps_id': np.ndarray((3300,), dtype=int64, min=165687356.0, max=1937589079.0, mean=900355726.545),
[2m[36m(pid=29902)[0m             'infos': np.ndarray((3300,), dtype=object, head={}),
[2m[36m(pid=29902)[0m             'new_obs': np.ndarray((3300, 9), dtype=float32, min=-4.948, max=5.168, mean=0.0),
[2m[36m(pid=29902)[0m             'obs': np.ndarray((3300, 9), dtype=float32, min=-4.948, max=5.168, mean=0.0),
[2m[36m(pid=29902)[0m             'prev_actions': np.ndarray((3300, 2), dtype=float32, min=-4.288, max=4.084, mean=-0.003),
[2m[36m(pid=29902)[0m             'prev_rewards': np.ndarray((3300,), dtype=float32, min=-6.454, max=7.44, mean=-0.086),
[2m[36m(pid=29902)[0m             'rewards': np.ndarray((3300,), dtype=float32, min=-6.454, max=7.44, mean=-0.079),
[2m[36m(pid=29902)[0m             't': np.ndarray((3300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=29902)[0m             'unroll_id': np.ndarray((3300,), dtype=int64, min=0.0, max=1.0, mean=0.273),
[2m[36m(pid=29902)[0m             'value_targets': np.ndarray((3300,), dtype=float32, min=-36.514, max=31.934, mean=-5.275),
[2m[36m(pid=29902)[0m             'vf_preds': np.ndarray((3300,), dtype=float32, min=-0.012, max=0.015, mean=0.0)},
[2m[36m(pid=29902)[0m   'type': 'SampleBatch'}
[2m[36m(pid=29902)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:32,196	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m { 'inputs': [ np.ndarray((36300, 2), dtype=float32, min=-4.288, max=4.084, mean=0.0),
[2m[36m(pid=29871)[0m               np.ndarray((36300,), dtype=float32, min=-27.634, max=25.687, mean=-0.1),
[2m[36m(pid=29871)[0m               np.ndarray((36300, 9), dtype=float32, min=-12.042, max=12.69, mean=-0.001),
[2m[36m(pid=29871)[0m               np.ndarray((36300, 2), dtype=float32, min=-4.288, max=4.084, mean=0.0),
[2m[36m(pid=29871)[0m               np.ndarray((36300,), dtype=float32, min=-4.888, max=3.822, mean=0.0),
[2m[36m(pid=29871)[0m               np.ndarray((36300, 4), dtype=float32, min=-0.016, max=0.015, mean=-0.0),
[2m[36m(pid=29871)[0m               np.ndarray((36300,), dtype=float32, min=-60.581, max=34.619, mean=-7.158),
[2m[36m(pid=29871)[0m               np.ndarray((36300,), dtype=float32, min=-0.017, max=0.018, mean=0.0)],
[2m[36m(pid=29871)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29871)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=29871)[0m   'state_inputs': []}
[2m[36m(pid=29871)[0m 
[2m[36m(pid=29871)[0m 2019-07-17 13:34:32,196	INFO multi_gpu_impl.py:191 -- Divided 36300 rollout sequences, each of length 1, among 1 devices.
[2m[36m(pid=29869)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29869)[0m W0717 13:34:34.001291 140017070982912 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29869)[0m Instructions for updating:
[2m[36m(pid=29869)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29876)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=29876)[0m W0717 13:34:34.006253 140135202518784 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=29876)[0m Instructions for updating:
[2m[36m(pid=29876)[0m non-resource variables are not supported in the long termW0717 13:34:36.115689 140302452221376 deprecation_wrapper.py:119] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/tune/logger.py:119: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.


Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-34-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.022202079560525
  episode_reward_mean: -15.279385070741887
  episode_reward_min: -64.59696350752941
  episodes_this_iter: 242
  episodes_total: 242
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3809.648
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.836812734603882
        kl: 0.00101862836163491
        policy_loss: -0.0025450950488448143
        total_loss: 99.30496215820312
        vf_explained_var: 0.1834060400724411
        vf_loss: 99.30648040771484
    load_time_ms: 43.914
    num_steps_sampled: 36300
    num_steps_trained: 36000
    sample_time_ms: 3189.069
    update_time_ms: 1026.642
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.196772795722981
    mean_inference_ms: 2.0976573525869173
    mean_processing_ms: 3.197946277711731
  time_since_restore: 8.128690242767334
  time_this_iter_s: 8.128690242767334
  time_total_s: 8.128690242767334
  timestamp: 1563363276
  timesteps_since_restore: 36300
  timesteps_this_iter: 36300
  timesteps_total: 36300
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 8 s, 1 iter, 36300 ts, -15.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-34-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.398150419058226
  episode_reward_mean: -13.817572673995606
  episode_reward_min: -50.906440007447344
  episodes_this_iter: 242
  episodes_total: 726
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3584.197
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.825350761413574
        kl: 0.004546476528048515
        policy_loss: -0.00571061298251152
        total_loss: 64.01315307617188
        vf_explained_var: 0.3935016393661499
        vf_loss: 64.01773071289062
    load_time_ms: 15.196
    num_steps_sampled: 108900
    num_steps_trained: 108000
    sample_time_ms: 2389.634
    update_time_ms: 344.834
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.997763252881693
    mean_inference_ms: 1.6791895564048924
    mean_processing_ms: 3.240844381157155
  time_since_restore: 19.10149359703064
  time_this_iter_s: 6.197405576705933
  time_total_s: 19.10149359703064
  timestamp: 1563363287
  timesteps_since_restore: 108900
  timesteps_this_iter: 36300
  timesteps_total: 108900
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 19 s, 3 iter, 108900 ts, -13.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-34-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.38446176523059
  episode_reward_mean: -11.873477349270173
  episode_reward_min: -49.68432350931373
  episodes_this_iter: 242
  episodes_total: 1210
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3588.63
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.809004306793213
        kl: 0.006532576866447926
        policy_loss: -0.007019825279712677
        total_loss: 47.73563766479492
        vf_explained_var: 0.4368301033973694
        vf_loss: 47.74184036254883
    load_time_ms: 9.465
    num_steps_sampled: 181500
    num_steps_trained: 180000
    sample_time_ms: 1999.283
    update_time_ms: 208.578
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.033895525055024
    mean_inference_ms: 1.6105444556484485
    mean_processing_ms: 3.3304379293842508
  time_since_restore: 29.171133279800415
  time_this_iter_s: 5.10282039642334
  time_total_s: 29.171133279800415
  timestamp: 1563363297
  timesteps_since_restore: 181500
  timesteps_this_iter: 36300
  timesteps_total: 181500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 29 s, 5 iter, 181500 ts, -11.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.960841805520197
  episode_reward_mean: -8.649037961874079
  episode_reward_min: -35.36675741877101
  episodes_this_iter: 242
  episodes_total: 1452
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3529.154
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.795051336288452
        kl: 0.006903572008013725
        policy_loss: -0.008109393529593945
        total_loss: 43.70436477661133
        vf_explained_var: 0.4288627505302429
        vf_loss: 43.711612701416016
    load_time_ms: 8.021
    num_steps_sampled: 217800
    num_steps_trained: 216000
    sample_time_ms: 2179.752
    update_time_ms: 174.582
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.1108406229380465
    mean_inference_ms: 1.6146362345244807
    mean_processing_ms: 3.466407268142469
  time_since_restore: 35.509796142578125
  time_this_iter_s: 6.33866286277771
  time_total_s: 35.509796142578125
  timestamp: 1563363303
  timesteps_since_restore: 217800
  timesteps_this_iter: 36300
  timesteps_total: 217800
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 35 s, 6 iter, 217800 ts, -8.65 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.402607340194756
  episode_reward_mean: -9.729817046625993
  episode_reward_min: -52.45366198637454
  episodes_this_iter: 242
  episodes_total: 1694
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3573.361
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.786996841430664
        kl: 0.009439628571271896
        policy_loss: -0.007521012797951698
        total_loss: 32.79283142089844
        vf_explained_var: 0.5204850435256958
        vf_loss: 32.79916763305664
    load_time_ms: 6.998
    num_steps_sampled: 254100
    num_steps_trained: 252000
    sample_time_ms: 2058.991
    update_time_ms: 150.084
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.086308011164795
    mean_inference_ms: 1.590603565102624
    mean_processing_ms: 3.449251835007316
  time_since_restore: 40.70918893814087
  time_this_iter_s: 5.199392795562744
  time_total_s: 40.70918893814087
  timestamp: 1563363308
  timesteps_since_restore: 254100
  timesteps_this_iter: 36300
  timesteps_total: 254100
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 40 s, 7 iter, 254100 ts, -9.73 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.04989689842251
  episode_reward_mean: -5.505162244801734
  episode_reward_min: -38.863617447454274
  episodes_this_iter: 242
  episodes_total: 2178
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3498.625
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7773325443267822
        kl: 0.008296982385218143
        policy_loss: -0.008684597909450531
        total_loss: 25.35541343688965
        vf_explained_var: 0.5686227083206177
        vf_loss: 25.363059997558594
    load_time_ms: 5.639
    num_steps_sampled: 326700
    num_steps_trained: 324000
    sample_time_ms: 2040.752
    update_time_ms: 117.834
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0360221957889335
    mean_inference_ms: 1.5416617695891197
    mean_processing_ms: 3.41009995926349
  time_since_restore: 51.18928647041321
  time_this_iter_s: 5.884249210357666
  time_total_s: 51.18928647041321
  timestamp: 1563363319
  timesteps_since_restore: 326700
  timesteps_this_iter: 36300
  timesteps_total: 326700
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 51 s, 9 iter, 326700 ts, -5.51 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.141941416753383
  episode_reward_mean: -1.507185980558519
  episode_reward_min: -33.0072779214122
  episodes_this_iter: 242
  episodes_total: 2662
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3479.767
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7523305416107178
        kl: 0.009486693888902664
        policy_loss: -0.009125928394496441
        total_loss: 21.30254554748535
        vf_explained_var: 0.602794885635376
        vf_loss: 21.31048583984375
    load_time_ms: 0.849
    num_steps_sampled: 399300
    num_steps_trained: 396000
    sample_time_ms: 1916.377
    update_time_ms: 4.146
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006648125947661
    mean_inference_ms: 1.5123881032053355
    mean_processing_ms: 3.39522824311427
  time_since_restore: 62.3411500453949
  time_this_iter_s: 6.40108847618103
  time_total_s: 62.3411500453949
  timestamp: 1563363330
  timesteps_since_restore: 399300
  timesteps_this_iter: 36300
  timesteps_total: 399300
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 62 s, 11 iter, 399300 ts, -1.51 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.743677206606314
  episode_reward_mean: 0.49254209973481394
  episode_reward_min: -24.50174518737976
  episodes_this_iter: 242
  episodes_total: 2904
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3506.903
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.748415470123291
        kl: 0.009756390005350113
        policy_loss: -0.008564683608710766
        total_loss: 16.836097717285156
        vf_explained_var: 0.6324316263198853
        vf_loss: 16.84344482421875
    load_time_ms: 0.858
    num_steps_sampled: 435600
    num_steps_trained: 432000
    sample_time_ms: 1920.982
    update_time_ms: 4.108
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.004632427060193
    mean_inference_ms: 1.5084314344614191
    mean_processing_ms: 3.4039233713988457
  time_since_restore: 67.43531894683838
  time_this_iter_s: 5.0941689014434814
  time_total_s: 67.43531894683838
  timestamp: 1563363335
  timesteps_since_restore: 435600
  timesteps_this_iter: 36300
  timesteps_total: 435600
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 67 s, 12 iter, 435600 ts, 0.493 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.394687348462526
  episode_reward_mean: 1.6585581090433985
  episode_reward_min: -30.206144665006278
  episodes_this_iter: 242
  episodes_total: 3146
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3533.728
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7325708866119385
        kl: 0.01063709706068039
        policy_loss: -0.010311024263501167
        total_loss: 16.32523536682129
        vf_explained_var: 0.6560526490211487
        vf_loss: 16.33422088623047
    load_time_ms: 0.862
    num_steps_sampled: 471900
    num_steps_trained: 468000
    sample_time_ms: 1801.781
    update_time_ms: 4.177
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.013602894344588
    mean_inference_ms: 1.5077380216662128
    mean_processing_ms: 3.420908823180538
  time_since_restore: 72.71292996406555
  time_this_iter_s: 5.277611017227173
  time_total_s: 72.71292996406555
  timestamp: 1563363340
  timesteps_since_restore: 471900
  timesteps_this_iter: 36300
  timesteps_total: 471900
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 72 s, 13 iter, 471900 ts, 1.66 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.60805272516815
  episode_reward_mean: 3.0344436324094297
  episode_reward_min: -37.95625881923413
  episodes_this_iter: 242
  episodes_total: 3388
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3555.154
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.714337110519409
        kl: 0.009115655906498432
        policy_loss: -0.009421814233064651
        total_loss: 17.30961036682129
        vf_explained_var: 0.6524460315704346
        vf_loss: 17.31789207458496
    load_time_ms: 0.871
    num_steps_sampled: 508200
    num_steps_trained: 504000
    sample_time_ms: 1960.565
    update_time_ms: 4.161
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.048980848556147
    mean_inference_ms: 1.5169293368177956
    mean_processing_ms: 3.452094370917757
  time_since_restore: 79.48296117782593
  time_this_iter_s: 6.770031213760376
  time_total_s: 79.48296117782593
  timestamp: 1563363347
  timesteps_since_restore: 508200
  timesteps_this_iter: 36300
  timesteps_total: 508200
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 79 s, 14 iter, 508200 ts, 3.03 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-35-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.32907742954747
  episode_reward_mean: 5.964429079097069
  episode_reward_min: -24.733571895995723
  episodes_this_iter: 242
  episodes_total: 3872
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3568.511
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.695134162902832
        kl: 0.010277931578457355
        policy_loss: -0.011475647799670696
        total_loss: 12.421996116638184
        vf_explained_var: 0.7359882593154907
        vf_loss: 12.4321870803833
    load_time_ms: 0.886
    num_steps_sampled: 580800
    num_steps_trained: 576000
    sample_time_ms: 1801.272
    update_time_ms: 4.129
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0587773847946345
    mean_inference_ms: 1.5167788556270103
    mean_processing_ms: 3.465487179518869
  time_since_restore: 89.4712586402893
  time_this_iter_s: 5.120906114578247
  time_total_s: 89.4712586402893
  timestamp: 1563363357
  timesteps_since_restore: 580800
  timesteps_this_iter: 36300
  timesteps_total: 580800
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 89 s, 16 iter, 580800 ts, 5.96 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.38301149244546
  episode_reward_mean: 7.924215155118829
  episode_reward_min: -16.045467746604263
  episodes_this_iter: 242
  episodes_total: 4114
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3574.647
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6792914867401123
        kl: 0.007745020091533661
        policy_loss: -0.009594526141881943
        total_loss: 7.537850379943848
        vf_explained_var: 0.8239049315452576
        vf_loss: 7.546477317810059
    load_time_ms: 0.882
    num_steps_sampled: 617100
    num_steps_trained: 612000
    sample_time_ms: 1955.42
    update_time_ms: 4.257
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.068962623806327
    mean_inference_ms: 1.5179881968665199
    mean_processing_ms: 3.477042826010926
  time_since_restore: 96.27039837837219
  time_this_iter_s: 6.799139738082886
  time_total_s: 96.27039837837219
  timestamp: 1563363364
  timesteps_since_restore: 617100
  timesteps_this_iter: 36300
  timesteps_total: 617100
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 96 s, 17 iter, 617100 ts, 7.92 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.58501922772149
  episode_reward_mean: 8.501209477280058
  episode_reward_min: -16.247175316431917
  episodes_this_iter: 242
  episodes_total: 4356
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3634.812
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.666623115539551
        kl: 0.010366026312112808
        policy_loss: -0.011967143043875694
        total_loss: 6.399419784545898
        vf_explained_var: 0.8364595770835876
        vf_loss: 6.410091400146484
    load_time_ms: 0.876
    num_steps_sampled: 653400
    num_steps_trained: 648000
    sample_time_ms: 1975.971
    update_time_ms: 4.149
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.081625277493178
    mean_inference_ms: 1.5240716782812551
    mean_processing_ms: 3.4884228182498873
  time_since_restore: 101.6696240901947
  time_this_iter_s: 5.39922571182251
  time_total_s: 101.6696240901947
  timestamp: 1563363369
  timesteps_since_restore: 653400
  timesteps_this_iter: 36300
  timesteps_total: 653400
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 101 s, 18 iter, 653400 ts, 8.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.29339458255322
  episode_reward_mean: 8.799404650382495
  episode_reward_min: -12.625589258219202
  episodes_this_iter: 242
  episodes_total: 4598
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3705.445
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.64704966545105
        kl: 0.011924090795218945
        policy_loss: -0.012846375815570354
        total_loss: 4.267899990081787
        vf_explained_var: 0.8767520189285278
        vf_loss: 4.2792558670043945
    load_time_ms: 0.866
    num_steps_sampled: 689700
    num_steps_trained: 684000
    sample_time_ms: 2037.527
    update_time_ms: 4.104
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.119742262023951
    mean_inference_ms: 1.5583247530195776
    mean_processing_ms: 3.5178207580144534
  time_since_restore: 108.87780499458313
  time_this_iter_s: 7.208180904388428
  time_total_s: 108.87780499458313
  timestamp: 1563363377
  timesteps_since_restore: 689700
  timesteps_this_iter: 36300
  timesteps_total: 689700
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 108 s, 19 iter, 689700 ts, 8.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.584028382789967
  episode_reward_mean: 9.474944620356805
  episode_reward_min: -17.218121440404246
  episodes_this_iter: 242
  episodes_total: 4840
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3726.614
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6443591117858887
        kl: 0.009866426698863506
        policy_loss: -0.009399420581758022
        total_loss: 4.104635715484619
        vf_explained_var: 0.8948811292648315
        vf_loss: 4.112802505493164
    load_time_ms: 0.856
    num_steps_sampled: 726000
    num_steps_trained: 720000
    sample_time_ms: 2053.423
    update_time_ms: 4.126
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.126696737158629
    mean_inference_ms: 1.5616008362472182
    mean_processing_ms: 3.521527403251386
  time_since_restore: 113.99918413162231
  time_this_iter_s: 5.121379137039185
  time_total_s: 113.99918413162231
  timestamp: 1563363382
  timesteps_since_restore: 726000
  timesteps_this_iter: 36300
  timesteps_total: 726000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 113 s, 20 iter, 726000 ts, 9.47 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.711031228420346
  episode_reward_mean: 9.67450701014283
  episode_reward_min: -15.752953138160747
  episodes_this_iter: 242
  episodes_total: 5082
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3716.64
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.628911256790161
        kl: 0.00972870085388422
        policy_loss: -0.010780645534396172
        total_loss: 3.06671142578125
        vf_explained_var: 0.9103512167930603
        vf_loss: 3.0762758255004883
    load_time_ms: 0.863
    num_steps_sampled: 762300
    num_steps_trained: 756000
    sample_time_ms: 1929.7
    update_time_ms: 4.181
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.125231658990701
    mean_inference_ms: 1.5586434055048688
    mean_processing_ms: 3.5208595154968445
  time_since_restore: 119.06970429420471
  time_this_iter_s: 5.0705201625823975
  time_total_s: 119.06970429420471
  timestamp: 1563363387
  timesteps_since_restore: 762300
  timesteps_this_iter: 36300
  timesteps_total: 762300
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 119 s, 21 iter, 762300 ts, 9.67 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.618409739460525
  episode_reward_mean: 10.881388627896177
  episode_reward_min: -14.94970279446659
  episodes_this_iter: 242
  episodes_total: 5324
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3694.08
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.616370439529419
        kl: 0.00979723408818245
        policy_loss: -0.012030687183141708
        total_loss: 2.611070156097412
        vf_explained_var: 0.9251830577850342
        vf_loss: 2.6218767166137695
    load_time_ms: 0.862
    num_steps_sampled: 798600
    num_steps_trained: 792000
    sample_time_ms: 2089.205
    update_time_ms: 4.356
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.119135916334422
    mean_inference_ms: 1.5584733371022923
    mean_processing_ms: 3.5348038684492065
  time_since_restore: 125.53607106208801
  time_this_iter_s: 6.466366767883301
  time_total_s: 125.53607106208801
  timestamp: 1563363393
  timesteps_since_restore: 798600
  timesteps_this_iter: 36300
  timesteps_total: 798600
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 125 s, 22 iter, 798600 ts, 10.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.67114399738234
  episode_reward_mean: 11.148699636905608
  episode_reward_min: -8.516849998689041
  episodes_this_iter: 242
  episodes_total: 5808
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3634.066
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5833981037139893
        kl: 0.01137932762503624
        policy_loss: -0.010242464952170849
        total_loss: 1.7115659713745117
        vf_explained_var: 0.9488140940666199
        vf_loss: 1.7203859090805054
    load_time_ms: 0.836
    num_steps_sampled: 871200
    num_steps_trained: 864000
    sample_time_ms: 1937.913
    update_time_ms: 4.313
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.118274791791286
    mean_inference_ms: 1.5570972715310458
    mean_processing_ms: 3.535828735998214
  time_since_restore: 135.46436977386475
  time_this_iter_s: 4.9694671630859375
  time_total_s: 135.46436977386475
  timestamp: 1563363403
  timesteps_since_restore: 871200
  timesteps_this_iter: 36300
  timesteps_total: 871200
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 135 s, 24 iter, 871200 ts, 11.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.49014962402868
  episode_reward_mean: 11.737089319342697
  episode_reward_min: -18.630492434922107
  episodes_this_iter: 242
  episodes_total: 6050
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3640.632
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.570142984390259
        kl: 0.009003250859677792
        policy_loss: -0.010729048401117325
        total_loss: 1.514005184173584
        vf_explained_var: 0.9515834450721741
        vf_loss: 1.523608684539795
    load_time_ms: 0.829
    num_steps_sampled: 907500
    num_steps_trained: 900000
    sample_time_ms: 2092.324
    update_time_ms: 4.37
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.1244485405054
    mean_inference_ms: 1.5632767349652366
    mean_processing_ms: 3.546012929535084
  time_since_restore: 141.93715739250183
  time_this_iter_s: 6.472787618637085
  time_total_s: 141.93715739250183
  timestamp: 1563363410
  timesteps_since_restore: 907500
  timesteps_this_iter: 36300
  timesteps_total: 907500
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 141 s, 25 iter, 907500 ts, 11.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-36-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.14775708252243
  episode_reward_mean: 12.407240367828056
  episode_reward_min: -6.6838724704398365
  episodes_this_iter: 242
  episodes_total: 6292
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3654.927
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5564565658569336
        kl: 0.01077957171946764
        policy_loss: -0.010454881004989147
        total_loss: 1.1983211040496826
        vf_explained_var: 0.9612512588500977
        vf_loss: 1.2074284553527832
    load_time_ms: 0.83
    num_steps_sampled: 943800
    num_steps_trained: 936000
    sample_time_ms: 2101.101
    update_time_ms: 4.29
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.1342038370359
    mean_inference_ms: 1.5766402888564097
    mean_processing_ms: 3.5479419736375326
  time_since_restore: 147.284117937088
  time_this_iter_s: 5.346960544586182
  time_total_s: 147.284117937088
  timestamp: 1563363415
  timesteps_since_restore: 943800
  timesteps_this_iter: 36300
  timesteps_total: 943800
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 147 s, 26 iter, 943800 ts, 12.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.55243161747443
  episode_reward_mean: 12.122671127389056
  episode_reward_min: -9.345044313045097
  episodes_this_iter: 242
  episodes_total: 6534
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3625.837
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.537520408630371
        kl: 0.011225408874452114
        policy_loss: -0.011533447541296482
        total_loss: 1.2074954509735107
        vf_explained_var: 0.9626772403717041
        vf_loss: 1.2176257371902466
    load_time_ms: 0.829
    num_steps_sampled: 980100
    num_steps_trained: 972000
    sample_time_ms: 2097.022
    update_time_ms: 4.255
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.130030238739211
    mean_inference_ms: 1.5752882492946974
    mean_processing_ms: 3.545176811605747
  time_since_restore: 153.7536427974701
  time_this_iter_s: 6.46952486038208
  time_total_s: 153.7536427974701
  timestamp: 1563363422
  timesteps_since_restore: 980100
  timesteps_this_iter: 36300
  timesteps_total: 980100
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 153 s, 27 iter, 980100 ts, 12.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.77468689843094
  episode_reward_mean: 13.956714330128195
  episode_reward_min: -11.42669581578388
  episodes_this_iter: 242
  episodes_total: 7018
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3577.669
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.517770767211914
        kl: 0.010594320483505726
        policy_loss: -0.010673005133867264
        total_loss: 0.9832308292388916
        vf_explained_var: 0.9729491472244263
        vf_loss: 0.9925795793533325
    load_time_ms: 0.841
    num_steps_sampled: 1052700
    num_steps_trained: 1044000
    sample_time_ms: 1898.672
    update_time_ms: 4.302
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.1222765485764095
    mean_inference_ms: 1.5730372423100716
    mean_processing_ms: 3.537324938710757
  time_since_restore: 163.89574551582336
  time_this_iter_s: 5.1769750118255615
  time_total_s: 163.89574551582336
  timestamp: 1563363432
  timesteps_since_restore: 1052700
  timesteps_this_iter: 36300
  timesteps_total: 1052700
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 163 s, 29 iter, 1052700 ts, 14 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.51772350167068
  episode_reward_mean: 13.254172855642857
  episode_reward_min: -7.309355664933074
  episodes_this_iter: 242
  episodes_total: 7260
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3584.918
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.508408784866333
        kl: 0.009608623571693897
        policy_loss: -0.010634458623826504
        total_loss: 1.0931379795074463
        vf_explained_var: 0.966528594493866
        vf_loss: 1.1025712490081787
    load_time_ms: 0.849
    num_steps_sampled: 1089000
    num_steps_trained: 1080000
    sample_time_ms: 2019.822
    update_time_ms: 4.28
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.112641844496486
    mean_inference_ms: 1.566863483441559
    mean_processing_ms: 3.532086639705902
  time_since_restore: 170.3068070411682
  time_this_iter_s: 6.411061525344849
  time_total_s: 170.3068070411682
  timestamp: 1563363438
  timesteps_since_restore: 1089000
  timesteps_this_iter: 36300
  timesteps_total: 1089000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 170 s, 30 iter, 1089000 ts, 13.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.60041047270417
  episode_reward_mean: 12.276264560342659
  episode_reward_min: -11.200190292296272
  episodes_this_iter: 242
  episodes_total: 7502
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3585.321
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4912028312683105
        kl: 0.010915009304881096
        policy_loss: -0.009339507669210434
        total_loss: 0.9402841925621033
        vf_explained_var: 0.9683895111083984
        vf_loss: 0.9482592344284058
    load_time_ms: 0.841
    num_steps_sampled: 1125300
    num_steps_trained: 1116000
    sample_time_ms: 2031.292
    update_time_ms: 4.303
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.116754292907556
    mean_inference_ms: 1.569082196670919
    mean_processing_ms: 3.5344196903710805
  time_since_restore: 175.49249291419983
  time_this_iter_s: 5.185685873031616
  time_total_s: 175.49249291419983
  timestamp: 1563363443
  timesteps_since_restore: 1125300
  timesteps_this_iter: 36300
  timesteps_total: 1125300
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 175 s, 31 iter, 1125300 ts, 12.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.47027156336865
  episode_reward_mean: 12.733850977311238
  episode_reward_min: -17.936641602814277
  episodes_this_iter: 242
  episodes_total: 7986
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3580.623
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.460768222808838
        kl: 0.009705800563097
        policy_loss: -0.011951967142522335
        total_loss: 0.9492194056510925
        vf_explained_var: 0.9718336462974548
        vf_loss: 0.9599579572677612
    load_time_ms: 0.821
    num_steps_sampled: 1197900
    num_steps_trained: 1188000
    sample_time_ms: 2016.864
    update_time_ms: 4.184
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.115301381377846
    mean_inference_ms: 1.570311513021016
    mean_processing_ms: 3.5341409107914847
  time_since_restore: 186.72676873207092
  time_this_iter_s: 6.254198312759399
  time_total_s: 186.72676873207092
  timestamp: 1563363455
  timesteps_since_restore: 1197900
  timesteps_this_iter: 36300
  timesteps_total: 1197900
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 186 s, 33 iter, 1197900 ts, 12.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.803340675053214
  episode_reward_mean: 12.25182142894119
  episode_reward_min: -7.19219301265959
  episodes_this_iter: 242
  episodes_total: 8228
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3613.288
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.452988386154175
        kl: 0.010694882832467556
        policy_loss: -0.00958205759525299
        total_loss: 0.886911928653717
        vf_explained_var: 0.969380259513855
        vf_loss: 0.8951570987701416
    load_time_ms: 0.825
    num_steps_sampled: 1234200
    num_steps_trained: 1224000
    sample_time_ms: 2000.81
    update_time_ms: 4.286
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.1119188219768015
    mean_inference_ms: 1.566598476269691
    mean_processing_ms: 3.52961596450848
  time_since_restore: 191.86410570144653
  time_this_iter_s: 5.13733696937561
  time_total_s: 191.86410570144653
  timestamp: 1563363460
  timesteps_since_restore: 1234200
  timesteps_this_iter: 36300
  timesteps_total: 1234200
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 191 s, 34 iter, 1234200 ts, 12.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.34899216048153
  episode_reward_mean: 14.033253724469178
  episode_reward_min: -6.447093008513131
  episodes_this_iter: 242
  episodes_total: 8470
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3599.758
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.439242124557495
        kl: 0.009696871973574162
        policy_loss: -0.010182199068367481
        total_loss: 0.756359338760376
        vf_explained_var: 0.9769026637077332
        vf_loss: 0.7653294205665588
    load_time_ms: 0.821
    num_steps_sampled: 1270500
    num_steps_trained: 1260000
    sample_time_ms: 1975.38
    update_time_ms: 4.243
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.104114445949948
    mean_inference_ms: 1.560756161559635
    mean_processing_ms: 3.5235484672087174
  time_since_restore: 197.94651436805725
  time_this_iter_s: 6.082408666610718
  time_total_s: 197.94651436805725
  timestamp: 1563363466
  timesteps_since_restore: 1270500
  timesteps_this_iter: 36300
  timesteps_total: 1270500
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 197 s, 35 iter, 1270500 ts, 14 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-37-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.65613385395361
  episode_reward_mean: 13.832759021700964
  episode_reward_min: -5.387313936953574
  episodes_this_iter: 242
  episodes_total: 8954
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3567.953
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4115967750549316
        kl: 0.011300959624350071
        policy_loss: -0.009393922053277493
        total_loss: 0.6720633506774902
        vf_explained_var: 0.9787247776985168
        vf_loss: 0.6800447702407837
    load_time_ms: 0.824
    num_steps_sampled: 1343100
    num_steps_trained: 1332000
    sample_time_ms: 1797.484
    update_time_ms: 4.308
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.094535566421901
    mean_inference_ms: 1.5534122166891002
    mean_processing_ms: 3.514875424973257
  time_since_restore: 207.66854619979858
  time_this_iter_s: 5.136217832565308
  time_total_s: 207.66854619979858
  timestamp: 1563363476
  timesteps_since_restore: 1343100
  timesteps_this_iter: 36300
  timesteps_total: 1343100
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 207 s, 37 iter, 1343100 ts, 13.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.17053691428296
  episode_reward_mean: 14.481204285147507
  episode_reward_min: -6.281212812252645
  episodes_this_iter: 242
  episodes_total: 9196
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3540.213
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.396500825881958
        kl: 0.011095261201262474
        policy_loss: -0.013400857336819172
        total_loss: 0.688543438911438
        vf_explained_var: 0.9786398410797119
        vf_loss: 0.7005573511123657
    load_time_ms: 0.822
    num_steps_sampled: 1379400
    num_steps_trained: 1368000
    sample_time_ms: 1929.491
    update_time_ms: 4.296
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.086574748272776
    mean_inference_ms: 1.5481753627410595
    mean_processing_ms: 3.513012570147721
  time_since_restore: 213.6763813495636
  time_this_iter_s: 6.007835149765015
  time_total_s: 213.6763813495636
  timestamp: 1563363482
  timesteps_since_restore: 1379400
  timesteps_this_iter: 36300
  timesteps_total: 1379400
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 213 s, 38 iter, 1379400 ts, 14.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.98285255688362
  episode_reward_mean: 13.813567850851951
  episode_reward_min: -5.823032672081948
  episodes_this_iter: 242
  episodes_total: 9438
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3519.382
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.384549856185913
        kl: 0.011514752171933651
        policy_loss: -0.012308163568377495
        total_loss: 0.7049780488014221
        vf_explained_var: 0.9774085283279419
        vf_loss: 0.7158468961715698
    load_time_ms: 0.814
    num_steps_sampled: 1415700
    num_steps_trained: 1404000
    sample_time_ms: 1931.505
    update_time_ms: 4.252
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.087721973330191
    mean_inference_ms: 1.5469157666740834
    mean_processing_ms: 3.5136785817669907
  time_since_restore: 218.6667504310608
  time_this_iter_s: 4.990369081497192
  time_total_s: 218.6667504310608
  timestamp: 1563363487
  timesteps_since_restore: 1415700
  timesteps_this_iter: 36300
  timesteps_total: 1415700
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 218 s, 39 iter, 1415700 ts, 13.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.936392735431234
  episode_reward_mean: 14.559955969113297
  episode_reward_min: -5.003024848761672
  episodes_this_iter: 242
  episodes_total: 9922
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3471.711
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.350269317626953
        kl: 0.011118356138467789
        policy_loss: -0.01264631561934948
        total_loss: 0.5786842703819275
        vf_explained_var: 0.9820944666862488
        vf_loss: 0.5899407863616943
    load_time_ms: 0.823
    num_steps_sampled: 1488300
    num_steps_trained: 1476000
    sample_time_ms: 1925.373
    update_time_ms: 4.367
  iterations_since_restore: 41
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.084591989183338
    mean_inference_ms: 1.5429431075255726
    mean_processing_ms: 3.5136567734043185
  time_since_restore: 229.72312927246094
  time_this_iter_s: 6.174753427505493
  time_total_s: 229.72312927246094
  timestamp: 1563363498
  timesteps_since_restore: 1488300
  timesteps_this_iter: 36300
  timesteps_total: 1488300
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 229 s, 41 iter, 1488300 ts, 14.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.51785581160591
  episode_reward_mean: 14.140284423258798
  episode_reward_min: -6.888413589335898
  episodes_this_iter: 242
  episodes_total: 10406
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3507.42
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3208279609680176
        kl: 0.012051214464008808
        policy_loss: -0.01217652391642332
        total_loss: 0.571518063545227
        vf_explained_var: 0.9808056354522705
        vf_loss: 0.5821881890296936
    load_time_ms: 0.831
    num_steps_sampled: 1560900
    num_steps_trained: 1548000
    sample_time_ms: 1923.185
    update_time_ms: 4.381
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.083450539997681
    mean_inference_ms: 1.5430889779221004
    mean_processing_ms: 3.512326899358971
  time_since_restore: 241.29051876068115
  time_this_iter_s: 6.761392831802368
  time_total_s: 241.29051876068115
  timestamp: 1563363509
  timesteps_since_restore: 1560900
  timesteps_this_iter: 36300
  timesteps_total: 1560900
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 241 s, 43 iter, 1560900 ts, 14.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.712352141536975
  episode_reward_mean: 13.30359931072349
  episode_reward_min: -5.084653933177629
  episodes_this_iter: 242
  episodes_total: 10648
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3504.505
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.314049482345581
        kl: 0.01209163200110197
        policy_loss: -0.011071461252868176
        total_loss: 0.6000334620475769
        vf_explained_var: 0.979343831539154
        vf_loss: 0.6095935106277466
    load_time_ms: 0.827
    num_steps_sampled: 1597200
    num_steps_trained: 1584000
    sample_time_ms: 1930.908
    update_time_ms: 4.388
  iterations_since_restore: 44
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.084153272511096
    mean_inference_ms: 1.5416288549704578
    mean_processing_ms: 3.5129242824641946
  time_since_restore: 246.47602915763855
  time_this_iter_s: 5.1855103969573975
  time_total_s: 246.47602915763855
  timestamp: 1563363515
  timesteps_since_restore: 1597200
  timesteps_this_iter: 36300
  timesteps_total: 1597200
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 246 s, 44 iter, 1597200 ts, 13.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.823396161982835
  episode_reward_mean: 14.632929231687214
  episode_reward_min: -6.252934166851258
  episodes_this_iter: 242
  episodes_total: 10890
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3552.525
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.299980640411377
        kl: 0.011662445031106472
        policy_loss: -0.011665129102766514
        total_loss: 0.5433660745620728
        vf_explained_var: 0.9827166795730591
        vf_loss: 0.5535733699798584
    load_time_ms: 0.834
    num_steps_sampled: 1633500
    num_steps_trained: 1620000
    sample_time_ms: 1802.066
    update_time_ms: 4.428
  iterations_since_restore: 45
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.084921930895899
    mean_inference_ms: 1.5402923581225263
    mean_processing_ms: 3.5123877016338514
  time_since_restore: 251.753075838089
  time_this_iter_s: 5.2770466804504395
  time_total_s: 251.753075838089
  timestamp: 1563363520
  timesteps_since_restore: 1633500
  timesteps_this_iter: 36300
  timesteps_total: 1633500
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 251 s, 45 iter, 1633500 ts, 14.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.37301789674301
  episode_reward_mean: 14.909820441317361
  episode_reward_min: -6.109494694081661
  episodes_this_iter: 242
  episodes_total: 11132
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3628.08
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.277752161026001
        kl: 0.011890050023794174
        policy_loss: -0.015182018280029297
        total_loss: 0.4997011721134186
        vf_explained_var: 0.9849873781204224
        vf_loss: 0.5133969187736511
    load_time_ms: 0.82
    num_steps_sampled: 1669800
    num_steps_trained: 1656000
    sample_time_ms: 1947.48
    update_time_ms: 4.505
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.085219325962036
    mean_inference_ms: 1.5376543337491668
    mean_processing_ms: 3.5130345277657753
  time_since_restore: 258.54947996139526
  time_this_iter_s: 6.796404123306274
  time_total_s: 258.54947996139526
  timestamp: 1563363527
  timesteps_since_restore: 1669800
  timesteps_this_iter: 36300
  timesteps_total: 1669800
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 258 s, 46 iter, 1669800 ts, 14.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-38-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.91719063160702
  episode_reward_mean: 14.990686518079944
  episode_reward_min: -5.452335633665563
  episodes_this_iter: 242
  episodes_total: 11616
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3643.877
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.254446268081665
        kl: 0.012619984336197376
        policy_loss: -0.012562830932438374
        total_loss: 0.5154868364334106
        vf_explained_var: 0.9838799238204956
        vf_loss: 0.5264722108840942
    load_time_ms: 0.838
    num_steps_sampled: 1742400
    num_steps_trained: 1728000
    sample_time_ms: 1812.762
    update_time_ms: 4.419
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.077437009115351
    mean_inference_ms: 1.5335627068278974
    mean_processing_ms: 3.5063616276824994
  time_since_restore: 268.5040855407715
  time_this_iter_s: 5.1646668910980225
  time_total_s: 268.5040855407715
  timestamp: 1563363537
  timesteps_since_restore: 1742400
  timesteps_this_iter: 36300
  timesteps_total: 1742400
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 268 s, 48 iter, 1742400 ts, 15 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.453678897909576
  episode_reward_mean: 15.78408081207128
  episode_reward_min: -6.288511710851036
  episodes_this_iter: 242
  episodes_total: 11858
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3643.449
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2317087650299072
        kl: 0.011803309433162212
        policy_loss: -0.01368603203445673
        total_loss: 0.45637190341949463
        vf_explained_var: 0.9861754179000854
        vf_loss: 0.468582421541214
    load_time_ms: 0.842
    num_steps_sampled: 1778700
    num_steps_trained: 1764000
    sample_time_ms: 1946.098
    update_time_ms: 4.496
  iterations_since_restore: 49
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0754692793995195
    mean_inference_ms: 1.5315206456845816
    mean_processing_ms: 3.5072384168219295
  time_since_restore: 274.82402086257935
  time_this_iter_s: 6.319935321807861
  time_total_s: 274.82402086257935
  timestamp: 1563363543
  timesteps_since_restore: 1778700
  timesteps_this_iter: 36300
  timesteps_total: 1778700
  training_iteration: 49
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 274 s, 49 iter, 1778700 ts, 15.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.524895278351465
  episode_reward_mean: 16.02192205961949
  episode_reward_min: -5.0181240972382355
  episodes_this_iter: 242
  episodes_total: 12100
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3657.887
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2226803302764893
        kl: 0.013282157480716705
        policy_loss: -0.011595542542636395
        total_loss: 0.44727393984794617
        vf_explained_var: 0.9868403673171997
        vf_loss: 0.4572092294692993
    load_time_ms: 0.837
    num_steps_sampled: 1815000
    num_steps_trained: 1800000
    sample_time_ms: 1944.362
    update_time_ms: 4.433
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.076640694144382
    mean_inference_ms: 1.5306692098034484
    mean_processing_ms: 3.5085726386938894
  time_since_restore: 279.8333070278168
  time_this_iter_s: 5.009286165237427
  time_total_s: 279.8333070278168
  timestamp: 1563363548
  timesteps_since_restore: 1815000
  timesteps_this_iter: 36300
  timesteps_total: 1815000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 279 s, 50 iter, 1815000 ts, 16 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.90841866660987
  episode_reward_mean: 15.085014963916898
  episode_reward_min: -5.616484876997562
  episodes_this_iter: 242
  episodes_total: 12342
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3708.245
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.205958366394043
        kl: 0.012498482130467892
        policy_loss: -0.012229842133820057
        total_loss: 0.4099951982498169
        vf_explained_var: 0.986731767654419
        vf_loss: 0.4206627309322357
    load_time_ms: 0.844
    num_steps_sampled: 1851300
    num_steps_trained: 1836000
    sample_time_ms: 1959.03
    update_time_ms: 4.415
  iterations_since_restore: 51
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.079445942312605
    mean_inference_ms: 1.5289921997876745
    mean_processing_ms: 3.511964661777469
  time_since_restore: 286.65894746780396
  time_this_iter_s: 6.825640439987183
  time_total_s: 286.65894746780396
  timestamp: 1563363555
  timesteps_since_restore: 1851300
  timesteps_this_iter: 36300
  timesteps_total: 1851300
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 286 s, 51 iter, 1851300 ts, 15.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.82931617853065
  episode_reward_mean: 14.387920050447999
  episode_reward_min: -5.2620964576515314
  episodes_this_iter: 242
  episodes_total: 12584
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3758.142
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1985278129577637
        kl: 0.013577098026871681
        policy_loss: -0.012248043902218342
        total_loss: 0.4069652557373047
        vf_explained_var: 0.9857116341590881
        vf_loss: 0.41751614212989807
    load_time_ms: 0.843
    num_steps_sampled: 1887600
    num_steps_trained: 1872000
    sample_time_ms: 1950.502
    update_time_ms: 4.471
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.07946639842746
    mean_inference_ms: 1.5276053725377299
    mean_processing_ms: 3.5128717639771323
  time_since_restore: 291.8849518299103
  time_this_iter_s: 5.226004362106323
  time_total_s: 291.8849518299103
  timestamp: 1563363560
  timesteps_since_restore: 1887600
  timesteps_this_iter: 36300
  timesteps_total: 1887600
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 291 s, 52 iter, 1887600 ts, 14.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.01556973736897
  episode_reward_mean: 16.42440527705693
  episode_reward_min: -5.500399430776035
  episodes_this_iter: 242
  episodes_total: 12826
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3717.702
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.181342601776123
        kl: 0.0123581038787961
        policy_loss: -0.012575213797390461
        total_loss: 0.4076945185661316
        vf_explained_var: 0.9877007603645325
        vf_loss: 0.41872501373291016
    load_time_ms: 0.848
    num_steps_sampled: 1923900
    num_steps_trained: 1908000
    sample_time_ms: 1819.423
    update_time_ms: 4.646
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.079713668694672
    mean_inference_ms: 1.5265468052382973
    mean_processing_ms: 3.5136470138685723
  time_since_restore: 296.9310290813446
  time_this_iter_s: 5.046077251434326
  time_total_s: 296.9310290813446
  timestamp: 1563363565
  timesteps_since_restore: 1923900
  timesteps_this_iter: 36300
  timesteps_total: 1923900
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 296 s, 53 iter, 1923900 ts, 16.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.27821169633495
  episode_reward_mean: 15.166035132111631
  episode_reward_min: -6.075170733956111
  episodes_this_iter: 242
  episodes_total: 13068
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3673.221
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1584129333496094
        kl: 0.011490593664348125
        policy_loss: -0.014045958407223225
        total_loss: 0.4170900583267212
        vf_explained_var: 0.9871229529380798
        vf_loss: 0.42969968914985657
    load_time_ms: 0.858
    num_steps_sampled: 1960200
    num_steps_trained: 1944000
    sample_time_ms: 1967.935
    update_time_ms: 4.465
  iterations_since_restore: 54
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.082629634032876
    mean_inference_ms: 1.5257519902230632
    mean_processing_ms: 3.515903805312077
  time_since_restore: 303.1553313732147
  time_this_iter_s: 6.224302291870117
  time_total_s: 303.1553313732147
  timestamp: 1563363571
  timesteps_since_restore: 1960200
  timesteps_this_iter: 36300
  timesteps_total: 1960200
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 303 s, 54 iter, 1960200 ts, 15.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.123327933696274
  episode_reward_mean: 14.988826832290792
  episode_reward_min: -6.042238250381577
  episodes_this_iter: 242
  episodes_total: 13310
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3663.98
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1502275466918945
        kl: 0.013345376588404179
        policy_loss: -0.012850776314735413
        total_loss: 0.4122897684574127
        vf_explained_var: 0.9857431054115295
        vf_loss: 0.4234723746776581
    load_time_ms: 0.858
    num_steps_sampled: 1996500
    num_steps_trained: 1980000
    sample_time_ms: 1961.866
    update_time_ms: 4.416
  iterations_since_restore: 55
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.079423869469163
    mean_inference_ms: 1.5239027549858688
    mean_processing_ms: 3.5132040173199583
  time_since_restore: 308.2767984867096
  time_this_iter_s: 5.121467113494873
  time_total_s: 308.2767984867096
  timestamp: 1563363576
  timesteps_since_restore: 1996500
  timesteps_this_iter: 36300
  timesteps_total: 1996500
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 308 s, 55 iter, 1996500 ts, 15 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.39549905326336
  episode_reward_mean: 15.83589323746851
  episode_reward_min: -4.190348349363248
  episodes_this_iter: 242
  episodes_total: 13794
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3610.349
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1167612075805664
        kl: 0.012414734810590744
        policy_loss: -0.016231806948781013
        total_loss: 0.3513442277908325
        vf_explained_var: 0.988754391670227
        vf_loss: 0.3660241961479187
    load_time_ms: 0.862
    num_steps_sampled: 2069100
    num_steps_trained: 2052000
    sample_time_ms: 1945.775
    update_time_ms: 4.319
  iterations_since_restore: 57
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.070444706060373
    mean_inference_ms: 1.5181296924876053
    mean_processing_ms: 3.505885127813154
  time_since_restore: 319.16266441345215
  time_this_iter_s: 6.249034643173218
  time_total_s: 319.16266441345215
  timestamp: 1563363587
  timesteps_since_restore: 2069100
  timesteps_this_iter: 36300
  timesteps_total: 2069100
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 319 s, 57 iter, 2069100 ts, 15.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.10265933495206
  episode_reward_mean: 16.717479251919343
  episode_reward_min: -3.3106275402809797
  episodes_this_iter: 242
  episodes_total: 14036
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3606.769
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1020660400390625
        kl: 0.014640238136053085
        policy_loss: -0.013525795191526413
        total_loss: 0.3218771517276764
        vf_explained_var: 0.9898656010627747
        vf_loss: 0.3335729241371155
    load_time_ms: 0.847
    num_steps_sampled: 2105400
    num_steps_trained: 2088000
    sample_time_ms: 1943.757
    update_time_ms: 4.346
  iterations_since_restore: 58
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0677883680097
    mean_inference_ms: 1.5164726954113084
    mean_processing_ms: 3.503887848277899
  time_since_restore: 324.27249574661255
  time_this_iter_s: 5.1098313331604
  time_total_s: 324.27249574661255
  timestamp: 1563363592
  timesteps_since_restore: 2105400
  timesteps_this_iter: 36300
  timesteps_total: 2105400
  training_iteration: 58
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 324 s, 58 iter, 2105400 ts, 16.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-39-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.60809223650101
  episode_reward_mean: 17.024923399764603
  episode_reward_min: -5.297082137432318
  episodes_this_iter: 242
  episodes_total: 14278
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3655.148
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0925090312957764
        kl: 0.012424899265170097
        policy_loss: -0.013502872548997402
        total_loss: 0.30837976932525635
        vf_explained_var: 0.9908047914505005
        vf_loss: 0.32032954692840576
    load_time_ms: 0.848
    num_steps_sampled: 2141700
    num_steps_trained: 2124000
    sample_time_ms: 1929.214
    update_time_ms: 4.249
  iterations_since_restore: 59
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.061969976809077
    mean_inference_ms: 1.5132949367963866
    mean_processing_ms: 3.4996919048875514
  time_since_restore: 330.93096709251404
  time_this_iter_s: 6.658471345901489
  time_total_s: 330.93096709251404
  timestamp: 1563363599
  timesteps_since_restore: 2141700
  timesteps_this_iter: 36300
  timesteps_total: 2141700
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 330 s, 59 iter, 2141700 ts, 17 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.575818146585135
  episode_reward_mean: 15.304809516010586
  episode_reward_min: -9.090413611106767
  episodes_this_iter: 242
  episodes_total: 14762
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3574.966
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0700509548187256
        kl: 0.015304231084883213
        policy_loss: -0.01490996778011322
        total_loss: 0.32509827613830566
        vf_explained_var: 0.9888684153556824
        vf_loss: 0.33809521794319153
    load_time_ms: 0.842
    num_steps_sampled: 2214300
    num_steps_trained: 2196000
    sample_time_ms: 1764.431
    update_time_ms: 4.233
  iterations_since_restore: 61
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0568404884992555
    mean_inference_ms: 1.510604347720912
    mean_processing_ms: 3.4944151610103997
  time_since_restore: 340.31709384918213
  time_this_iter_s: 4.705547571182251
  time_total_s: 340.31709384918213
  timestamp: 1563363609
  timesteps_since_restore: 2214300
  timesteps_this_iter: 36300
  timesteps_total: 2214300
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 340 s, 61 iter, 2214300 ts, 15.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.79512174985306
  episode_reward_mean: 15.236012190752389
  episode_reward_min: -4.690271856605484
  episodes_this_iter: 242
  episodes_total: 15004
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3536.845
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0522193908691406
        kl: 0.012020459398627281
        policy_loss: -0.016047649085521698
        total_loss: 0.27105244994163513
        vf_explained_var: 0.99034583568573
        vf_loss: 0.2855975031852722
    load_time_ms: 0.847
    num_steps_sampled: 2250600
    num_steps_trained: 2232000
    sample_time_ms: 1889.066
    update_time_ms: 4.299
  iterations_since_restore: 62
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.05256610989202
    mean_inference_ms: 1.5077314337786525
    mean_processing_ms: 3.491168465321216
  time_since_restore: 346.4020502567291
  time_this_iter_s: 6.084956407546997
  time_total_s: 346.4020502567291
  timestamp: 1563363615
  timesteps_since_restore: 2250600
  timesteps_this_iter: 36300
  timesteps_total: 2250600
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 346 s, 62 iter, 2250600 ts, 15.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.42796321529232
  episode_reward_mean: 15.537791900324141
  episode_reward_min: -7.4586787664758
  episodes_this_iter: 242
  episodes_total: 15246
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3554.042
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0447399616241455
        kl: 0.013752398081123829
        policy_loss: -0.01442760694772005
        total_loss: 0.29470333456993103
        vf_explained_var: 0.9899441599845886
        vf_loss: 0.3074118196964264
    load_time_ms: 0.848
    num_steps_sampled: 2286900
    num_steps_trained: 2268000
    sample_time_ms: 1880.679
    update_time_ms: 4.113
  iterations_since_restore: 63
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.05017747744242
    mean_inference_ms: 1.5062131890416186
    mean_processing_ms: 3.489105466334502
  time_since_restore: 351.53463196754456
  time_this_iter_s: 5.13258171081543
  time_total_s: 351.53463196754456
  timestamp: 1563363620
  timesteps_since_restore: 2286900
  timesteps_this_iter: 36300
  timesteps_total: 2286900
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 351 s, 63 iter, 2286900 ts, 15.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.4830006418996
  episode_reward_mean: 15.909008837520897
  episode_reward_min: -5.476536133059695
  episodes_this_iter: 242
  episodes_total: 15488
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3597.499
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0186872482299805
        kl: 0.015907786786556244
        policy_loss: -0.014135999605059624
        total_loss: 0.25459957122802734
        vf_explained_var: 0.9913911819458008
        vf_loss: 0.26674702763557434
    load_time_ms: 0.849
    num_steps_sampled: 2323200
    num_steps_trained: 2304000
    sample_time_ms: 1722.161
    update_time_ms: 4.185
  iterations_since_restore: 64
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.047650855993214
    mean_inference_ms: 1.5049186078846883
    mean_processing_ms: 3.486713985893358
  time_since_restore: 356.6078054904938
  time_this_iter_s: 5.073173522949219
  time_total_s: 356.6078054904938
  timestamp: 1563363625
  timesteps_since_restore: 2323200
  timesteps_this_iter: 36300
  timesteps_total: 2323200
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 356 s, 64 iter, 2323200 ts, 15.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.19312169556412
  episode_reward_mean: 16.146373365634656
  episode_reward_min: -6.345909368968665
  episodes_this_iter: 242
  episodes_total: 15730
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3575.378
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9995638132095337
        kl: 0.01199013739824295
        policy_loss: -0.014821846969425678
        total_loss: 0.2661113142967224
        vf_explained_var: 0.9917124509811401
        vf_loss: 0.279434472322464
    load_time_ms: 0.848
    num_steps_sampled: 2359500
    num_steps_trained: 2340000
    sample_time_ms: 1850.442
    update_time_ms: 4.235
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.042975099587757
    mean_inference_ms: 1.502479300508049
    mean_processing_ms: 3.4829060444778968
  time_since_restore: 362.7908239364624
  time_this_iter_s: 6.183018445968628
  time_total_s: 362.7908239364624
  timestamp: 1563363631
  timesteps_since_restore: 2359500
  timesteps_this_iter: 36300
  timesteps_total: 2359500
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 362 s, 65 iter, 2359500 ts, 16.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.027980121986445
  episode_reward_mean: 16.947629330766386
  episode_reward_min: -4.703607275873817
  episodes_this_iter: 242
  episodes_total: 16214
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3598.986
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9678480625152588
        kl: 0.013993795029819012
        policy_loss: -0.013099860399961472
        total_loss: 0.22295880317687988
        vf_explained_var: 0.9934319257736206
        vf_loss: 0.23430940508842468
    load_time_ms: 0.849
    num_steps_sampled: 2432100
    num_steps_trained: 2412000
    sample_time_ms: 1855.207
    update_time_ms: 4.304
  iterations_since_restore: 67
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0383260905917036
    mean_inference_ms: 1.500080740787936
    mean_processing_ms: 3.4796802906594166
  time_since_restore: 373.96049427986145
  time_this_iter_s: 6.4587273597717285
  time_total_s: 373.96049427986145
  timestamp: 1563363642
  timesteps_since_restore: 2432100
  timesteps_this_iter: 36300
  timesteps_total: 2432100
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 373 s, 67 iter, 2432100 ts, 16.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.832526806313034
  episode_reward_mean: 17.180380710038893
  episode_reward_min: -5.1837040038729265
  episodes_this_iter: 242
  episodes_total: 16456
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3615.537
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9562201499938965
        kl: 0.014947475865483284
        policy_loss: -0.015091353096067905
        total_loss: 0.21854227781295776
        vf_explained_var: 0.993250846862793
        vf_loss: 0.23176519572734833
    load_time_ms: 0.852
    num_steps_sampled: 2468400
    num_steps_trained: 2448000
    sample_time_ms: 1875.094
    update_time_ms: 4.276
  iterations_since_restore: 68
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0406678667023055
    mean_inference_ms: 1.5012560131085466
    mean_processing_ms: 3.4810696006272357
  time_since_restore: 379.4335067272186
  time_this_iter_s: 5.473012447357178
  time_total_s: 379.4335067272186
  timestamp: 1563363648
  timesteps_since_restore: 2468400
  timesteps_this_iter: 36300
  timesteps_total: 2468400
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 379 s, 68 iter, 2468400 ts, 17.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-40-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.25813450610726
  episode_reward_mean: 16.004374187846043
  episode_reward_min: -4.736074851107407
  episodes_this_iter: 242
  episodes_total: 16940
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3591.47
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9140572547912598
        kl: 0.014848783612251282
        policy_loss: -0.01757078990340233
        total_loss: 0.20199111104011536
        vf_explained_var: 0.9932498931884766
        vf_loss: 0.21770578622817993
    load_time_ms: 0.852
    num_steps_sampled: 2541000
    num_steps_trained: 2520000
    sample_time_ms: 1902.31
    update_time_ms: 4.274
  iterations_since_restore: 70
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.042810907281131
    mean_inference_ms: 1.500213504669498
    mean_processing_ms: 3.4831478264443216
  time_since_restore: 390.80164885520935
  time_this_iter_s: 6.506486654281616
  time_total_s: 390.80164885520935
  timestamp: 1563363659
  timesteps_since_restore: 2541000
  timesteps_this_iter: 36300
  timesteps_total: 2541000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 390 s, 70 iter, 2541000 ts, 16 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-41-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.31917465285836
  episode_reward_mean: 15.477080911331498
  episode_reward_min: -4.67945932563088
  episodes_this_iter: 242
  episodes_total: 17424
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3587.23
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8860174417495728
        kl: 0.015902835875749588
        policy_loss: -0.0145499212667346
        total_loss: 0.19810667634010315
        vf_explained_var: 0.9929153919219971
        vf_loss: 0.21066874265670776
    load_time_ms: 0.871
    num_steps_sampled: 2613600
    num_steps_trained: 2592000
    sample_time_ms: 1771.888
    update_time_ms: 4.135
  iterations_since_restore: 72
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.039318500358866
    mean_inference_ms: 1.49813202373612
    mean_processing_ms: 3.4795921934873193
  time_since_restore: 400.24193143844604
  time_this_iter_s: 4.827331781387329
  time_total_s: 400.24193143844604
  timestamp: 1563363669
  timesteps_since_restore: 2613600
  timesteps_this_iter: 36300
  timesteps_total: 2613600
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 400 s, 72 iter, 2613600 ts, 15.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-41-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.91776943846215
  episode_reward_mean: 16.638838497731793
  episode_reward_min: -4.092362131219388
  episodes_this_iter: 242
  episodes_total: 17666
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3551.201
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.865235686302185
        kl: 0.015008906833827496
        policy_loss: -0.01810482144355774
        total_loss: 0.18503791093826294
        vf_explained_var: 0.9941810965538025
        vf_loss: 0.2012665867805481
    load_time_ms: 0.871
    num_steps_sampled: 2649900
    num_steps_trained: 2628000
    sample_time_ms: 1901.058
    update_time_ms: 4.157
  iterations_since_restore: 73
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.035921861178638
    mean_inference_ms: 1.4960203041968734
    mean_processing_ms: 3.477027780578604
  time_since_restore: 406.30744671821594
  time_this_iter_s: 6.0655152797698975
  time_total_s: 406.30744671821594
  timestamp: 1563363675
  timesteps_since_restore: 2649900
  timesteps_this_iter: 36300
  timesteps_total: 2649900
  training_iteration: 73
  2019-07-17 13:41:35,535	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-17 13:41:35,542	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 406 s, 73 iter, 2649900 ts, 16.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-41-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.05772592089252
  episode_reward_mean: 15.893741715715786
  episode_reward_min: -3.1777876530168814
  episodes_this_iter: 242
  episodes_total: 18150
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3487.898
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8265268802642822
        kl: 0.014550818130373955
        policy_loss: -0.013959556818008423
        total_loss: 0.16239725053310394
        vf_explained_var: 0.9940839409828186
        vf_loss: 0.17453795671463013
    load_time_ms: 0.873
    num_steps_sampled: 2722500
    num_steps_trained: 2700000
    sample_time_ms: 1901.322
    update_time_ms: 4.14
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.030391238303984
    mean_inference_ms: 1.4930698334246515
    mean_processing_ms: 3.472786952612461
  time_since_restore: 416.93241477012634
  time_this_iter_s: 6.049474716186523
  time_total_s: 416.93241477012634
  timestamp: 1563363685
  timesteps_since_restore: 2722500
  timesteps_this_iter: 36300
  timesteps_total: 2722500
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29871], 416 s, 75 iter, 2722500 ts, 15.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-41-35
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 37.254751554615915
  episode_reward_mean: 18.18267721733828
  episode_reward_min: -4.956865909260587
  episodes_this_iter: 242
  episodes_total: 18634
  experiment_id: 278598d886d14792a8caa62751ec5540
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3463.944
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.795473337173462
        kl: 0.016365163028240204
        policy_loss: -0.016115866601467133
        total_loss: 0.1479983776807785
        vf_explained_var: 0.995629072189331
        vf_loss: 0.16206859052181244
    load_time_ms: 0.877
    num_steps_sampled: 2795100
    num_steps_trained: 2772000
    sample_time_ms: 1770.981
    update_time_ms: 4.153
  iterations_since_restore: 77
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29871
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.028075695972918
    mean_inference_ms: 1.4921540856479296
    mean_processing_ms: 3.4704670868800154
  time_since_restore: 426.56063508987427
  time_this_iter_s: 4.975436687469482
  time_total_s: 426.56063508987427
  timestamp: 1563363695
  timesteps_since_restore: 2795100
  timesteps_this_iter: 36300
  timesteps_total: 2795100
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'PENDING': 5})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

[2m[36m(pid=29869)[0m 2019-07-17 13:41:35,702	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.709086: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.717535: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29869)[0m [32m [     0.05520s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.843701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.844637: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d9f692faa0 executing computations on platform CUDA. Devices:
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.844679: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.871535: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.872285: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d9f70b7d30 executing computations on platform Host. Devices:
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.872328: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.872679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
[2m[36m(pid=29869)[0m name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
[2m[36m(pid=29869)[0m pciBusID: 0000:01:00.0
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873305: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873475: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873560: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873644: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873726: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.873806: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.878559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.878624: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.878661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.878673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:35.878682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[2m[36m(pid=29869)[0m W0717 13:41:35.898703 140017427977664 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29869)[0m Instructions for updating:
[2m[36m(pid=29869)[0m Use keras.layers.dense instead.
[2m[36m(pid=29869)[0m W0717 13:41:36.377362 140017427977664 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29869)[0m Instructions for updating:
[2m[36m(pid=29869)[0m Use `tf.cast` instead.
[2m[36m(pid=29869)[0m 2019-07-17 13:41:36.477199: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29869)[0m 2019-07-17 13:41:36,504	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29869)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=29869)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=29869)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29869)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29869)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29869)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m W0717 13:41:36.546344 140017427977664 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29869)[0m Instructions for updating:
[2m[36m(pid=29869)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29869)[0m 2019-07-17 13:41:37,194	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f5678204710>}
[2m[36m(pid=29869)[0m 2019-07-17 13:41:37,194	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f5678204630>}
[2m[36m(pid=29869)[0m 2019-07-17 13:41:37,195	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=29869)[0m [32m [     1.54325s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m [32m [     1.54368s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m [32m [     1.54404s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m [32m [     1.54441s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m [32m [     1.54478s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m [32m [     1.54514s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29869)[0m [32m [     1.54550s,  INFO] TimeLimit:
[2m[36m(pid=29869)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29869)[0m - action_space = Box(2,)
[2m[36m(pid=29869)[0m - observation_space = Box(9,)
[2m[36m(pid=29869)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29869)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29869)[0m - _max_episode_steps = 150
[2m[36m(pid=29869)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     0.02543s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37,266	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=29869)[0m 2019-07-17 13:41:37,279	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.289431: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.300410: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.304128: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.304176: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.304188: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.304439: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.304503: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.304519: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.331413: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.332146: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558388b728d0 executing computations on platform Host. Devices:
[2m[36m(pid=29876)[0m 2019-07-17 13:41:37.332184: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=29876)[0m W0717 13:41:37.340765 140135559554496 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=29876)[0m Instructions for updating:
[2m[36m(pid=29876)[0m Use keras.layers.dense instead.
[2m[36m(pid=29876)[0m W0717 13:41:38.192757 140135559554496 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29876)[0m Instructions for updating:
[2m[36m(pid=29876)[0m Use `tf.cast` instead.
[2m[36m(pid=29876)[0m 2019-07-17 13:41:38.514681: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29876)[0m 2019-07-17 13:41:38,599	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29876)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29876)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29876)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=29876)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=29876)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29876)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29876)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29876)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29876)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29876)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29876)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m W0717 13:41:38.778703 140135559554496 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=29876)[0m Instructions for updating:
[2m[36m(pid=29876)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29876)[0m [32m [     4.33609s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     4.33708s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     4.33788s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     4.33861s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     4.33985s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     4.34054s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m [32m [     4.34930s,  INFO] TimeLimit:
[2m[36m(pid=29876)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=29876)[0m - action_space = Box(2,)
[2m[36m(pid=29876)[0m - observation_space = Box(9,)
[2m[36m(pid=29876)[0m - reward_range = (-inf, inf)
[2m[36m(pid=29876)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=29876)[0m - _max_episode_steps = 150
[2m[36m(pid=29876)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30730)[0m W0717 13:41:42.887759 140069938611968 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30730)[0m Instructions for updating:
[2m[36m(pid=30730)[0m non-resource variables are not supported in the long term
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.069068: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.069364: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_1 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_1/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_1/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.079930: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_2 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_2/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_2/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.080273: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_3 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_3/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_3/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.080561: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_4 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_4/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_4/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.080795: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_5 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_5/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_5/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.080992: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_6 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_6/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_6/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:43.081198: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=29869)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=29869)[0m See below for details of this colocation group:
[2m[36m(pid=29869)[0m Colocation Debug Info:
[2m[36m(pid=29869)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=29869)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=29869)[0m Assign: CPU 
[2m[36m(pid=29869)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=29869)[0m VariableV2: CPU 
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_7 (VariableV2) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_7/Assign (Assign) /device:GPU:0
[2m[36m(pid=29869)[0m   default_policy_1/tower_1/Variable_7/read (Identity) /device:GPU:0
[2m[36m(pid=29869)[0m 
[2m[36m(pid=30728)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30728)[0m W0717 13:41:43.141041 140210905916864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30728)[0m Instructions for updating:
[2m[36m(pid=30728)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30732)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30732)[0m W0717 13:41:43.236598 140232284628736 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30732)[0m Instructions for updating:
[2m[36m(pid=30732)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30734)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30734)[0m W0717 13:41:43.476371 140651343988480 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30734)[0m Instructions for updating:
[2m[36m(pid=30734)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30727)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30727)[0m W0717 13:41:43.527239 139683813566208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30727)[0m Instructions for updating:
[2m[36m(pid=30727)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30735)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30735)[0m W0717 13:41:43.673950 140307488261888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30735)[0m Instructions for updating:
[2m[36m(pid=30735)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30729)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30729)[0m W0717 13:41:43.948331 140699860158208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30729)[0m Instructions for updating:
[2m[36m(pid=30729)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30733)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30733)[0m W0717 13:41:44.020823 140617633974016 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30733)[0m Instructions for updating:
[2m[36m(pid=30733)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30726)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30726)[0m W0717 13:41:44.132251 140145970542336 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30726)[0m Instructions for updating:
[2m[36m(pid=30726)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44,123	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30728)[0m [32m [     0.09073s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.186290: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.220801: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.239602: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.239671: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.239689: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.239841: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.239889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.239904: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.279441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.279967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e75c15e5c0 executing computations on platform Host. Devices:
[2m[36m(pid=30728)[0m 2019-07-17 13:41:44.280001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30728)[0m W0717 13:41:44.288696 140210905916864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30728)[0m Instructions for updating:
[2m[36m(pid=30728)[0m Use keras.layers.dense instead.
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44,316	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30730)[0m [32m [     0.10515s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.363551: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.400032: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.403896: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.403948: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.403961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.404068: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.404102: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.404112: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.416605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.422305: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d91f706200 executing computations on platform Host. Devices:
[2m[36m(pid=30730)[0m 2019-07-17 13:41:44.422344: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30730)[0m W0717 13:41:44.430799 140070295610816 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30730)[0m Instructions for updating:
[2m[36m(pid=30730)[0m Use keras.layers.dense instead.
[2m[36m(pid=30767)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30767)[0m W0717 13:41:44.436369 139884174644992 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30767)[0m Instructions for updating:
[2m[36m(pid=30767)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44,520	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30732)[0m [32m [     0.06927s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.589778: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.620907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.636478: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.636555: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.636573: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.636707: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.636754: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.636768: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.652125: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.652636: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55aabb3798d0 executing computations on platform Host. Devices:
[2m[36m(pid=30732)[0m 2019-07-17 13:41:44.652669: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30732)[0m W0717 13:41:44.660706 140232641635776 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30732)[0m Instructions for updating:
[2m[36m(pid=30732)[0m Use keras.layers.dense instead.
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44,821	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30735)[0m [32m [     0.03991s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30769)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30769)[0m W0717 13:41:44.790842 140365522749184 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30769)[0m Instructions for updating:
[2m[36m(pid=30769)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.842388: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44,865	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30727)[0m [32m [     0.08999s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.851826: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.855701: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.855753: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.855764: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.855871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.855901: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.855911: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.868424: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.869079: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a8b2898d0 executing computations on platform Host. Devices:
[2m[36m(pid=30735)[0m 2019-07-17 13:41:44.869113: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30735)[0m W0717 13:41:44.877143 140307845277120 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30735)[0m Instructions for updating:
[2m[36m(pid=30735)[0m Use keras.layers.dense instead.
[2m[36m(pid=30734)[0m [32m [     0.11236s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44,896	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.914272: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.934574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30731)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30731)[0m W0717 13:41:44.916060 139953208903424 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30731)[0m Instructions for updating:
[2m[36m(pid=30731)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.947722: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30768)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30768)[0m W0717 13:41:44.915442 139963283576576 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30768)[0m Instructions for updating:
[2m[36m(pid=30768)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.959042: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.959110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.959124: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.959253: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.959388: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.959404: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.966828: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.967515: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fbc25028d0 executing computations on platform Host. Devices:
[2m[36m(pid=30727)[0m 2019-07-17 13:41:44.967568: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30727)[0m W0717 13:41:44.987089 139684170569152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30727)[0m Instructions for updating:
[2m[36m(pid=30727)[0m Use keras.layers.dense instead.
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.974873: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.978887: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.978945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.978958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.979078: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.979118: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.979130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.995130: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.995676: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563f5d9378d0 executing computations on platform Host. Devices:
[2m[36m(pid=30734)[0m 2019-07-17 13:41:44.995709: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30734)[0m W0717 13:41:45.003951 140651700979136 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30734)[0m Instructions for updating:
[2m[36m(pid=30734)[0m Use keras.layers.dense instead.
[2m[36m(pid=30785)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30785)[0m W0717 13:41:44.975859 139804772701952 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30785)[0m Instructions for updating:
[2m[36m(pid=30785)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30781)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30781)[0m W0717 13:41:45.037976 140431277221632 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30781)[0m Instructions for updating:
[2m[36m(pid=30781)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30779)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30779)[0m W0717 13:41:45.079401 140653453330176 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30779)[0m Instructions for updating:
[2m[36m(pid=30779)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45,195	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30733)[0m [32m [     0.07732s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30775)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30775)[0m W0717 13:41:45.174780 140576729179904 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30775)[0m Instructions for updating:
[2m[36m(pid=30775)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30793)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30793)[0m W0717 13:41:45.167957 140215915255552 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30793)[0m Instructions for updating:
[2m[36m(pid=30793)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30796)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30796)[0m W0717 13:41:45.187077 139879953155840 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30796)[0m Instructions for updating:
[2m[36m(pid=30796)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.268087: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30786)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30786)[0m W0717 13:41:45.226179 140652136404736 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30786)[0m Instructions for updating:
[2m[36m(pid=30786)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30772)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30772)[0m W0717 13:41:45.229450 139689466742528 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30772)[0m Instructions for updating:
[2m[36m(pid=30772)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45,296	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30726)[0m [32m [     0.09306s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.301622: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.305185: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.305233: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.305245: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.305348: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.305379: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.305387: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.319572: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.319987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5559636ec750 executing computations on platform Host. Devices:
[2m[36m(pid=30733)[0m 2019-07-17 13:41:45.320011: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45,305	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30729)[0m [32m [     0.08363s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.357593: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30733)[0m W0717 13:41:45.327626 140617990968768 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30733)[0m Instructions for updating:
[2m[36m(pid=30733)[0m Use keras.layers.dense instead.
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.358443: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30788)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=30788)[0m W0717 13:41:45.343353 139726151673600 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=30788)[0m Instructions for updating:
[2m[36m(pid=30788)[0m non-resource variables are not supported in the long term
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.378668: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.392883: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.396870: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.396928: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.396938: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.397046: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.397080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.397090: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.422975: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.423381: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5557cf18b8d0 executing computations on platform Host. Devices:
[2m[36m(pid=30726)[0m 2019-07-17 13:41:45.423411: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30726)[0m W0717 13:41:45.431573 140146327537088 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30726)[0m Instructions for updating:
[2m[36m(pid=30726)[0m Use keras.layers.dense instead.
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.385371: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.385444: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.385461: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.385598: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.385647: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.385662: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.405087: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.405634: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5595ed9d08d0 executing computations on platform Host. Devices:
[2m[36m(pid=30729)[0m 2019-07-17 13:41:45.405663: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30729)[0m W0717 13:41:45.413210 140700217193920 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30729)[0m Instructions for updating:
[2m[36m(pid=30729)[0m Use keras.layers.dense instead.
[2m[36m(pid=30730)[0m W0717 13:41:45.615804 140070295610816 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30730)[0m Instructions for updating:
[2m[36m(pid=30730)[0m Use `tf.cast` instead.
[2m[36m(pid=30728)[0m W0717 13:41:45.669218 140210905916864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30728)[0m Instructions for updating:
[2m[36m(pid=30728)[0m Use `tf.cast` instead.
[2m[36m(pid=30730)[0m 2019-07-17 13:41:45.822085: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30727)[0m W0717 13:41:45.876457 139684170569152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30727)[0m Instructions for updating:
[2m[36m(pid=30727)[0m Use `tf.cast` instead.
[2m[36m(pid=30728)[0m 2019-07-17 13:41:45.975504: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30730)[0m W0717 13:41:45.970434 140070295610816 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30730)[0m Instructions for updating:
[2m[36m(pid=30730)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30732)[0m W0717 13:41:45.966410 140232641635776 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30732)[0m Instructions for updating:
[2m[36m(pid=30732)[0m Use `tf.cast` instead.
[2m[36m(pid=30727)[0m 2019-07-17 13:41:46.171213: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30735)[0m W0717 13:41:46.196825 140307845277120 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30735)[0m Instructions for updating:
[2m[36m(pid=30735)[0m Use `tf.cast` instead.
[2m[36m(pid=30734)[0m W0717 13:41:46.185612 140651700979136 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30734)[0m Instructions for updating:
[2m[36m(pid=30734)[0m Use `tf.cast` instead.
[2m[36m(pid=30728)[0m W0717 13:41:46.242014 140210905916864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30728)[0m Instructions for updating:
[2m[36m(pid=30728)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30731)[0m [32m [     0.04150s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m 2019-07-17 13:41:46.221950: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46,259	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.282545: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.294242: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.298157: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.298214: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.298224: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.298356: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.298393: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.298402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.303619: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.304051: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bd559908d0 executing computations on platform Host. Devices:
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.304083: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30731)[0m W0717 13:41:46.313388 139953565894080 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30731)[0m Instructions for updating:
[2m[36m(pid=30731)[0m Use keras.layers.dense instead.
[2m[36m(pid=30727)[0m W0717 13:41:46.338931 139684170569152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30727)[0m Instructions for updating:
[2m[36m(pid=30727)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30732)[0m W0717 13:41:46.376193 140232641635776 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30732)[0m Instructions for updating:
[2m[36m(pid=30732)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30735)[0m 2019-07-17 13:41:46.337009: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30734)[0m 2019-07-17 13:41:46.337077: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=29869)[0m W0717 13:41:46.407795 140017427977664 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29869)[0m Instructions for updating:
[2m[36m(pid=29869)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30726)[0m W0717 13:41:46.386162 140146327537088 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30726)[0m Instructions for updating:
[2m[36m(pid=30726)[0m Use `tf.cast` instead.
[2m[36m(pid=30733)[0m W0717 13:41:46.425051 140617990968768 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30733)[0m Instructions for updating:
[2m[36m(pid=30733)[0m Use `tf.cast` instead.
[2m[36m(pid=30729)[0m W0717 13:41:46.393655 140700217193920 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30729)[0m Instructions for updating:
[2m[36m(pid=30729)[0m Use `tf.cast` instead.
[2m[36m(pid=29876)[0m W0717 13:41:46.481732 140135559554496 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=29876)[0m Instructions for updating:
[2m[36m(pid=29876)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30735)[0m W0717 13:41:46.466331 140307845277120 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30735)[0m Instructions for updating:
[2m[36m(pid=30735)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30734)[0m W0717 13:41:46.467660 140651700979136 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30734)[0m Instructions for updating:
[2m[36m(pid=30734)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30726)[0m 2019-07-17 13:41:46.535359: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30729)[0m 2019-07-17 13:41:46.540383: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30733)[0m 2019-07-17 13:41:46.573689: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30726)[0m W0717 13:41:46.666246 140146327537088 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30726)[0m Instructions for updating:
[2m[36m(pid=30726)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30733)[0m W0717 13:41:46.710777 140617990968768 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30733)[0m Instructions for updating:
[2m[36m(pid=30733)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30729)[0m W0717 13:41:46.669444 140700217193920 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30729)[0m Instructions for updating:
[2m[36m(pid=30729)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30731)[0m W0717 13:41:46.753936 139953565894080 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30731)[0m Instructions for updating:
[2m[36m(pid=30731)[0m Use `tf.cast` instead.
[2m[36m(pid=30731)[0m 2019-07-17 13:41:46.899812: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30731)[0m W0717 13:41:47.030398 139953565894080 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30731)[0m Instructions for updating:
[2m[36m(pid=30731)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,554	INFO rollout_worker.py:428 -- Generating sample batch of size 1600
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,634	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.192, max=0.974, mean=0.237)},
[2m[36m(pid=29876)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.867, max=0.1, mean=-0.17)},
[2m[36m(pid=29876)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.006, max=0.993, mean=0.152)},
[2m[36m(pid=29876)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.543, max=0.839, mean=-0.035)},
[2m[36m(pid=29876)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.805, max=0.593, mean=-0.011)},
[2m[36m(pid=29876)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.904, max=0.082, mean=-0.183)},
[2m[36m(pid=29876)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.822, max=0.57, mean=-0.053)},
[2m[36m(pid=29876)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.342, max=0.94, mean=0.084)}}
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,635	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=29876)[0m   1: {'agent0': None},
[2m[36m(pid=29876)[0m   2: {'agent0': None},
[2m[36m(pid=29876)[0m   3: {'agent0': None},
[2m[36m(pid=29876)[0m   4: {'agent0': None},
[2m[36m(pid=29876)[0m   5: {'agent0': None},
[2m[36m(pid=29876)[0m   6: {'agent0': None},
[2m[36m(pid=29876)[0m   7: {'agent0': None}}
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,635	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.192, max=0.974, mean=0.237)
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,635	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,642	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 0,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 1,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.393),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 2,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.665, max=1.154, mean=0.302),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 3,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.202, max=1.388, mean=-0.213),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 4,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.259, max=1.153, mean=-0.039),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 5,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.504, max=1.606, mean=-0.316),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 6,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.454, max=1.566, mean=-0.107),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=29876)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=29876)[0m                                   'env_id': 7,
[2m[36m(pid=29876)[0m                                   'info': None,
[2m[36m(pid=29876)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.883, max=1.551, mean=0.183),
[2m[36m(pid=29876)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=29876)[0m                                   'rnn_state': []},
[2m[36m(pid=29876)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,642	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=30730)[0m W0717 13:41:47.633992 140070295610816 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30730)[0m Instructions for updating:
[2m[36m(pid=30730)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30730)[0m [32m [     3.41540s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     3.41650s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     3.41726s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     3.41839s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     3.41910s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     3.42030s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     3.42120s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=29876)[0m 2019-07-17 13:41:47,759	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-1.251, max=1.768, mean=0.29),
[2m[36m(pid=29876)[0m                       [],
[2m[36m(pid=29876)[0m                       { 'action_prob': np.ndarray((8,), dtype=float32, min=0.032, max=0.155, mean=0.09),
[2m[36m(pid=29876)[0m                         'behaviour_logits': np.ndarray((8, 4), dtype=float32, min=-0.004, max=0.002, mean=-0.0),
[2m[36m(pid=29876)[0m                         'vf_preds': np.ndarray((8,), dtype=float32, min=-0.004, max=0.004, mean=0.0)})}
[2m[36m(pid=29876)[0m 
[2m[36m(pid=30728)[0m [32m [     3.82616s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m [32m [     3.82712s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m [32m [     3.82793s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m [32m [     3.82856s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m [32m [     3.82968s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m [32m [     3.83036s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m [32m [     3.83124s,  INFO] TimeLimit:
[2m[36m(pid=30728)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30728)[0m - action_space = Box(2,)
[2m[36m(pid=30728)[0m - observation_space = Box(9,)
[2m[36m(pid=30728)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30728)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30728)[0m - _max_episode_steps = 150
[2m[36m(pid=30728)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30728)[0m W0717 13:41:47.865832 140210905916864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30728)[0m Instructions for updating:
[2m[36m(pid=30728)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30727)[0m W0717 13:41:48.197896 139684170569152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30727)[0m Instructions for updating:
[2m[36m(pid=30727)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30727)[0m [32m [     3.41607s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30727)[0m [32m [     3.41715s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30727)[0m [32m [     3.41779s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30727)[0m [32m [     3.41854s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30727)[0m [32m [     3.41930s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30727)[0m [32m [     3.41989s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30727)[0m [32m [     3.42082s,  INFO] TimeLimit:
[2m[36m(pid=30727)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30727)[0m - action_space = Box(2,)
[2m[36m(pid=30727)[0m - observation_space = Box(9,)
[2m[36m(pid=30727)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30727)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30727)[0m - _max_episode_steps = 150
[2m[36m(pid=30727)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m W0717 13:41:48.255353 140232641635776 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30732)[0m Instructions for updating:
[2m[36m(pid=30732)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30732)[0m [32m [     3.79609s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m [32m [     3.79705s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m [32m [     3.79820s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m [32m [     3.79934s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m [32m [     3.80012s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m [32m [     3.80137s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30732)[0m [32m [     3.80217s,  INFO] TimeLimit:
[2m[36m(pid=30732)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30732)[0m - action_space = Box(2,)
[2m[36m(pid=30732)[0m - observation_space = Box(9,)
[2m[36m(pid=30732)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30732)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30732)[0m - _max_episode_steps = 150
[2m[36m(pid=30732)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m W0717 13:41:48.255715 140307845277120 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30735)[0m Instructions for updating:
[2m[36m(pid=30735)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30735)[0m [32m [     3.46813s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m [32m [     3.46917s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m [32m [     3.46996s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m [32m [     3.47083s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m [32m [     3.47171s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m [32m [     3.47241s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30735)[0m [32m [     3.47314s,  INFO] TimeLimit:
[2m[36m(pid=30735)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30735)[0m - action_space = Box(2,)
[2m[36m(pid=30735)[0m - observation_space = Box(9,)
[2m[36m(pid=30735)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30735)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30735)[0m - _max_episode_steps = 150
[2m[36m(pid=30735)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.54522s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.54614s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.54703s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.54782s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.54893s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.54966s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m [32m [     3.55087s,  INFO] TimeLimit:
[2m[36m(pid=30734)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30734)[0m - action_space = Box(2,)
[2m[36m(pid=30734)[0m - observation_space = Box(9,)
[2m[36m(pid=30734)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30734)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30734)[0m - _max_episode_steps = 150
[2m[36m(pid=30734)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30734)[0m W0717 13:41:48.336564 140651700979136 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30734)[0m Instructions for updating:
[2m[36m(pid=30734)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29876)[0m 2019-07-17 13:41:48,447	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.001, max=0.158, mean=0.085),
[2m[36m(pid=29876)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-2.608, max=2.588, mean=0.017),
[2m[36m(pid=29876)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-10.802, max=19.587, mean=-3.524),
[2m[36m(pid=29876)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.005, max=0.007, mean=0.001),
[2m[36m(pid=29876)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=29876)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=871695908.0, max=871695908.0, mean=871695908.0),
[2m[36m(pid=29876)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=29876)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-2.667, max=2.667, mean=0.179),
[2m[36m(pid=29876)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-2.667, max=2.667, mean=0.18),
[2m[36m(pid=29876)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-2.608, max=2.588, mean=0.023),
[2m[36m(pid=29876)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-1.956, max=4.234, mean=-0.054),
[2m[36m(pid=29876)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-1.956, max=4.234, mean=-0.032),
[2m[36m(pid=29876)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=29876)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-10.806, max=19.579, mean=-3.527),
[2m[36m(pid=29876)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.009, max=0.004, mean=-0.003)},
[2m[36m(pid=29876)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=29876)[0m 
[2m[36m(pid=30726)[0m [32m [     3.29630s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m [32m [     3.29710s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m [32m [     3.29810s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m [32m [     3.29878s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m [32m [     3.29973s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m W0717 13:41:48.484563 140700217193920 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30729)[0m Instructions for updating:
[2m[36m(pid=30729)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30729)[0m [32m [     3.25522s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m [32m [     3.25629s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m [32m [     3.25714s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m [32m [     3.25809s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m [32m [     3.25896s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m [32m [     3.25989s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30729)[0m [32m [     3.26091s,  INFO] TimeLimit:
[2m[36m(pid=30729)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30729)[0m - action_space = Box(2,)
[2m[36m(pid=30729)[0m - observation_space = Box(9,)
[2m[36m(pid=30729)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30729)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30729)[0m - _max_episode_steps = 150
[2m[36m(pid=30729)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m [32m [     3.30047s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m [32m [     3.30152s,  INFO] TimeLimit:
[2m[36m(pid=30726)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30726)[0m - action_space = Box(2,)
[2m[36m(pid=30726)[0m - observation_space = Box(9,)
[2m[36m(pid=30726)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30726)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30726)[0m - _max_episode_steps = 150
[2m[36m(pid=30726)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30726)[0m W0717 13:41:48.507147 140146327537088 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30726)[0m Instructions for updating:
[2m[36m(pid=30726)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30733)[0m W0717 13:41:48.583514 140617990968768 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30733)[0m Instructions for updating:
[2m[36m(pid=30733)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30733)[0m [32m [     3.45882s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m [32m [     3.45981s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m [32m [     3.46057s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m [32m [     3.46124s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m [32m [     3.46207s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m [32m [     3.46310s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30733)[0m [32m [     3.46385s,  INFO] TimeLimit:
[2m[36m(pid=30733)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30733)[0m - action_space = Box(2,)
[2m[36m(pid=30733)[0m - observation_space = Box(9,)
[2m[36m(pid=30733)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30733)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30733)[0m - _max_episode_steps = 150
[2m[36m(pid=30733)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.61913s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.62002s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.62073s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.62184s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.62253s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.62351s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m [32m [     2.62435s,  INFO] TimeLimit:
[2m[36m(pid=30731)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30731)[0m - action_space = Box(2,)
[2m[36m(pid=30731)[0m - observation_space = Box(9,)
[2m[36m(pid=30731)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30731)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30731)[0m - _max_episode_steps = 150
[2m[36m(pid=30731)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30731)[0m W0717 13:41:48.843630 139953565894080 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30731)[0m Instructions for updating:
[2m[36m(pid=30731)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=29876)[0m 2019-07-17 13:41:49,383	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29876)[0m { 'data': { 'action_prob': np.ndarray((1650,), dtype=float32, min=0.0, max=0.159, mean=0.08),
[2m[36m(pid=29876)[0m             'actions': np.ndarray((1650, 2), dtype=float32, min=-3.722, max=3.191, mean=0.004),
[2m[36m(pid=29876)[0m             'advantages': np.ndarray((1650,), dtype=float32, min=-33.86, max=19.587, mean=-7.961),
[2m[36m(pid=29876)[0m             'agent_index': np.ndarray((1650,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=29876)[0m             'behaviour_logits': np.ndarray((1650, 4), dtype=float32, min=-0.008, max=0.007, mean=0.0),
[2m[36m(pid=29876)[0m             'dones': np.ndarray((1650,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=29876)[0m             'eps_id': np.ndarray((1650,), dtype=int64, min=469422797.0, max=1871159734.0, mean=986899657.364),
[2m[36m(pid=29876)[0m             'infos': np.ndarray((1650,), dtype=object, head={}),
[2m[36m(pid=29876)[0m             'new_obs': np.ndarray((1650, 9), dtype=float32, min=-3.865, max=3.875, mean=0.066),
[2m[36m(pid=29876)[0m             'obs': np.ndarray((1650, 9), dtype=float32, min=-3.865, max=3.875, mean=0.066),
[2m[36m(pid=29876)[0m             'prev_actions': np.ndarray((1650, 2), dtype=float32, min=-3.722, max=3.191, mean=0.004),
[2m[36m(pid=29876)[0m             'prev_rewards': np.ndarray((1650,), dtype=float32, min=-4.807, max=4.534, mean=-0.118),
[2m[36m(pid=29876)[0m             'rewards': np.ndarray((1650,), dtype=float32, min=-4.807, max=4.534, mean=-0.115),
[2m[36m(pid=29876)[0m             't': np.ndarray((1650,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=29876)[0m             'unroll_id': np.ndarray((1650,), dtype=int64, min=0.0, max=1.0, mean=0.273),
[2m[36m(pid=29876)[0m             'value_targets': np.ndarray((1650,), dtype=float32, min=-33.854, max=19.579, mean=-7.961),
[2m[36m(pid=29876)[0m             'vf_preds': np.ndarray((1650,), dtype=float32, min=-0.01, max=0.011, mean=-0.001)},
[2m[36m(pid=29876)[0m   'type': 'SampleBatch'}
[2m[36m(pid=29876)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:50,656	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m { 'inputs': [ np.ndarray((26400, 2), dtype=float32, min=-4.316, max=4.143, mean=0.003),
[2m[36m(pid=29869)[0m               np.ndarray((26400,), dtype=float32, min=-7.38, max=5.782, mean=-0.095),
[2m[36m(pid=29869)[0m               np.ndarray((26400, 9), dtype=float32, min=-5.122, max=6.347, mean=0.002),
[2m[36m(pid=29869)[0m               np.ndarray((26400, 2), dtype=float32, min=-4.316, max=4.143, mean=0.003),
[2m[36m(pid=29869)[0m               np.ndarray((26400,), dtype=float32, min=-3.755, max=4.032, mean=-0.0),
[2m[36m(pid=29869)[0m               np.ndarray((26400, 4), dtype=float32, min=-0.009, max=0.009, mean=-0.0),
[2m[36m(pid=29869)[0m               np.ndarray((26400,), dtype=float32, min=-45.172, max=34.427, mean=-6.781),
[2m[36m(pid=29869)[0m               np.ndarray((26400,), dtype=float32, min=-0.011, max=0.011, mean=-0.0)],
[2m[36m(pid=29869)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=29869)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=29869)[0m   'state_inputs': []}
[2m[36m(pid=29869)[0m 
[2m[36m(pid=29869)[0m 2019-07-17 13:41:50,656	INFO multi_gpu_impl.py:191 -- Divided 26400 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-41-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.786817386293677
  episode_reward_mean: -14.531538028160107
  episode_reward_min: -56.694573954200976
  episodes_this_iter: 176
  episodes_total: 176
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2872.193
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.833773136138916
        kl: 0.0008734909351915121
        policy_loss: -0.0033541631419211626
        total_loss: 93.09355163574219
        vf_explained_var: 0.12265805155038834
        vf_loss: 93.09602355957031
    load_time_ms: 41.822
    num_steps_sampled: 26400
    num_steps_trained: 26000
    sample_time_ms: 3180.932
    update_time_ms: 986.781
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.197989471865782
    mean_inference_ms: 1.6458390533606655
    mean_processing_ms: 1.699197331273686
  time_since_restore: 7.141883373260498
  time_this_iter_s: 7.141883373260498
  time_total_s: 7.141883373260498
  timestamp: 1563363713
  timesteps_since_restore: 26400
  timesteps_this_iter: 26400
  timesteps_total: 26400
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 7 s, 1 iter, 26400 ts, -14.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.43064694013188
  episode_reward_mean: -14.696854228065842
  episode_reward_min: -45.46988370940958
  episodes_this_iter: 176
  episodes_total: 528
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2760.867
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.8226683139801025
        kl: 0.005604063160717487
        policy_loss: -0.0068888431414961815
        total_loss: 69.7615966796875
        vf_explained_var: 0.3406696617603302
        vf_loss: 69.76708984375
    load_time_ms: 14.457
    num_steps_sampled: 79200
    num_steps_trained: 78000
    sample_time_ms: 2314.834
    update_time_ms: 332.061
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.096987688389987
    mean_inference_ms: 1.5571406139542576
    mean_processing_ms: 1.70901806057253
  time_since_restore: 16.363167762756348
  time_this_iter_s: 4.428518295288086
  time_total_s: 16.363167762756348
  timestamp: 1563363722
  timesteps_since_restore: 79200
  timesteps_this_iter: 26400
  timesteps_total: 79200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 16 s, 3 iter, 79200 ts, -14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.68713530620131
  episode_reward_mean: -13.151550248913106
  episode_reward_min: -47.47910035899451
  episodes_this_iter: 176
  episodes_total: 704
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2783.264
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.8165688514709473
        kl: 0.005190711934119463
        policy_loss: -0.005907754879444838
        total_loss: 61.682823181152344
        vf_explained_var: 0.35472482442855835
        vf_loss: 61.687442779541016
    load_time_ms: 11.021
    num_steps_sampled: 105600
    num_steps_trained: 104000
    sample_time_ms: 2272.415
    update_time_ms: 250.184
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.0983841676379886
    mean_inference_ms: 1.5059627806499405
    mean_processing_ms: 1.7191450245163702
  time_since_restore: 21.38334894180298
  time_this_iter_s: 5.020181179046631
  time_total_s: 21.38334894180298
  timestamp: 1563363727
  timesteps_since_restore: 105600
  timesteps_this_iter: 26400
  timesteps_total: 105600
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 21 s, 4 iter, 105600 ts, -13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.681046487996756
  episode_reward_mean: -10.364382388424799
  episode_reward_min: -47.33664105277112
  episodes_this_iter: 176
  episodes_total: 1056
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2721.658
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7979931831359863
        kl: 0.006574655417352915
        policy_loss: -0.006938690319657326
        total_loss: 44.534584045410156
        vf_explained_var: 0.39826297760009766
        vf_loss: 44.53987503051758
    load_time_ms: 7.618
    num_steps_sampled: 158400
    num_steps_trained: 156000
    sample_time_ms: 2090.429
    update_time_ms: 168.224
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.0578663634986243
    mean_inference_ms: 1.4307742034509272
    mean_processing_ms: 1.7171922243982858
  time_since_restore: 30.081201553344727
  time_this_iter_s: 4.585334300994873
  time_total_s: 30.081201553344727
  timestamp: 1563363736
  timesteps_since_restore: 158400
  timesteps_this_iter: 26400
  timesteps_total: 158400
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 30 s, 6 iter, 158400 ts, -10.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.958978310218153
  episode_reward_mean: -7.877136661122311
  episode_reward_min: -43.63212602635536
  episodes_this_iter: 176
  episodes_total: 1408
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2676.114
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7885653972625732
        kl: 0.006146890576928854
        policy_loss: -0.006797082256525755
        total_loss: 34.20758819580078
        vf_explained_var: 0.4498692750930786
        vf_loss: 34.212852478027344
    load_time_ms: 5.904
    num_steps_sampled: 211200
    num_steps_trained: 208000
    sample_time_ms: 2050.362
    update_time_ms: 127.408
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.014511356565223
    mean_inference_ms: 1.364086910521536
    mean_processing_ms: 1.694562747392424
  time_since_restore: 39.07238268852234
  time_this_iter_s: 4.411979913711548
  time_total_s: 39.07238268852234
  timestamp: 1563363745
  timesteps_since_restore: 211200
  timesteps_this_iter: 26400
  timesteps_total: 211200
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 39 s, 8 iter, 211200 ts, -7.88 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.292903593625056
  episode_reward_mean: -7.365439565109177
  episode_reward_min: -39.9946633048302
  episodes_this_iter: 176
  episodes_total: 1760
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2630.644
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7765400409698486
        kl: 0.007099971640855074
        policy_loss: -0.008619235828518867
        total_loss: 24.483827590942383
        vf_explained_var: 0.5218492746353149
        vf_loss: 24.490673065185547
    load_time_ms: 4.875
    num_steps_sampled: 264000
    num_steps_trained: 260000
    sample_time_ms: 2003.617
    update_time_ms: 102.879
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9949037446722173
    mean_inference_ms: 1.3283441757629282
    mean_processing_ms: 1.6844720669982507
  time_since_restore: 47.64946722984314
  time_this_iter_s: 4.173013925552368
  time_total_s: 47.64946722984314
  timestamp: 1563363754
  timesteps_since_restore: 264000
  timesteps_this_iter: 26400
  timesteps_total: 264000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 47 s, 10 iter, 264000 ts, -7.37 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.312081877980297
  episode_reward_mean: -3.103405107647549
  episode_reward_min: -28.149940392758666
  episodes_this_iter: 176
  episodes_total: 2112
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2617.404
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7542052268981934
        kl: 0.006987427826970816
        policy_loss: -0.007927182130515575
        total_loss: 18.113893508911133
        vf_explained_var: 0.6041717529296875
        vf_loss: 18.120073318481445
    load_time_ms: 0.768
    num_steps_sampled: 316800
    num_steps_trained: 312000
    sample_time_ms: 1834.222
    update_time_ms: 4.622
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9484602918097138
    mean_inference_ms: 1.2879937837642372
    mean_processing_ms: 1.6484113488387526
  time_since_restore: 56.691564083099365
  time_this_iter_s: 4.763908624649048
  time_total_s: 56.691564083099365
  timestamp: 1563363763
  timesteps_since_restore: 316800
  timesteps_this_iter: 26400
  timesteps_total: 316800
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 56 s, 12 iter, 316800 ts, -3.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-42-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.210667736914843
  episode_reward_mean: -0.4496292873863059
  episode_reward_min: -30.524282956886143
  episodes_this_iter: 176
  episodes_total: 2464
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2588.245
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7421507835388184
        kl: 0.006448027677834034
        policy_loss: -0.010766603983938694
        total_loss: 15.735795974731445
        vf_explained_var: 0.654492974281311
        vf_loss: 15.744951248168945
    load_time_ms: 0.773
    num_steps_sampled: 369600
    num_steps_trained: 364000
    sample_time_ms: 1816.678
    update_time_ms: 4.53
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.920269188119255
    mean_inference_ms: 1.2631031485251514
    mean_processing_ms: 1.626037914382768
  time_since_restore: 65.66978240013123
  time_this_iter_s: 4.144938707351685
  time_total_s: 65.66978240013123
  timestamp: 1563363772
  timesteps_since_restore: 369600
  timesteps_this_iter: 26400
  timesteps_total: 369600
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 65 s, 14 iter, 369600 ts, -0.45 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.669948223036357
  episode_reward_mean: 0.5672048832713484
  episode_reward_min: -39.44800286074451
  episodes_this_iter: 176
  episodes_total: 2816
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2560.089
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7207980155944824
        kl: 0.007905123755335808
        policy_loss: -0.010955402627587318
        total_loss: 13.170357704162598
        vf_explained_var: 0.6871376037597656
        vf_loss: 13.179337501525879
    load_time_ms: 0.761
    num_steps_sampled: 422400
    num_steps_trained: 416000
    sample_time_ms: 1842.885
    update_time_ms: 4.537
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9258198202444492
    mean_inference_ms: 1.262021166759553
    mean_processing_ms: 1.634129503298452
  time_since_restore: 74.3432126045227
  time_this_iter_s: 4.315996170043945
  time_total_s: 74.3432126045227
  timestamp: 1563363781
  timesteps_since_restore: 422400
  timesteps_this_iter: 26400
  timesteps_total: 422400
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 74 s, 16 iter, 422400 ts, 0.567 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.4746659905708
  episode_reward_mean: 4.456235804077867
  episode_reward_min: -26.144840942541265
  episodes_this_iter: 176
  episodes_total: 3168
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2551.997
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.692253589630127
        kl: 0.007661297917366028
        policy_loss: -0.010613158345222473
        total_loss: 10.551850318908691
        vf_explained_var: 0.7697502374649048
        vf_loss: 10.560547828674316
    load_time_ms: 0.756
    num_steps_sampled: 475200
    num_steps_trained: 468000
    sample_time_ms: 1818.634
    update_time_ms: 4.35
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9028724582907115
    mean_inference_ms: 1.2407638596502386
    mean_processing_ms: 1.6133268856210625
  time_since_restore: 83.0022304058075
  time_this_iter_s: 4.463952541351318
  time_total_s: 83.0022304058075
  timestamp: 1563363789
  timesteps_since_restore: 475200
  timesteps_this_iter: 26400
  timesteps_total: 475200
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 83 s, 18 iter, 475200 ts, 4.46 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.60698284987252
  episode_reward_mean: 6.6081579183374
  episode_reward_min: -20.223619666284264
  episodes_this_iter: 176
  episodes_total: 3520
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2590.665
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.67400860786438
        kl: 0.008246010169386864
        policy_loss: -0.011241416446864605
        total_loss: 8.17555046081543
        vf_explained_var: 0.810594916343689
        vf_loss: 8.18472957611084
    load_time_ms: 0.755
    num_steps_sampled: 528000
    num_steps_trained: 520000
    sample_time_ms: 1812.414
    update_time_ms: 4.232
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8964112497836063
    mean_inference_ms: 1.235048734978001
    mean_processing_ms: 1.612146425589698
  time_since_restore: 91.90398907661438
  time_this_iter_s: 4.228620290756226
  time_total_s: 91.90398907661438
  timestamp: 1563363798
  timesteps_since_restore: 528000
  timesteps_this_iter: 26400
  timesteps_total: 528000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 91 s, 20 iter, 528000 ts, 6.61 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.055101485700103
  episode_reward_mean: 7.184438933187189
  episode_reward_min: -16.754822722995527
  episodes_this_iter: 176
  episodes_total: 3872
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2636.122
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.653168201446533
        kl: 0.007964104413986206
        policy_loss: -0.010762169025838375
        total_loss: 5.339274883270264
        vf_explained_var: 0.8444017767906189
        vf_loss: 5.348045825958252
    load_time_ms: 0.737
    num_steps_sampled: 580800
    num_steps_trained: 572000
    sample_time_ms: 1801.502
    update_time_ms: 4.335
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8908200488690268
    mean_inference_ms: 1.2318866424190886
    mean_processing_ms: 1.6113547628952318
  time_since_restore: 101.29462194442749
  time_this_iter_s: 4.6940412521362305
  time_total_s: 101.29462194442749
  timestamp: 1563363808
  timesteps_since_restore: 580800
  timesteps_this_iter: 26400
  timesteps_total: 580800
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 101 s, 22 iter, 580800 ts, 7.18 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.80787034698019
  episode_reward_mean: 7.713273606937498
  episode_reward_min: -17.43882933791493
  episodes_this_iter: 176
  episodes_total: 4224
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2651.278
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.6386609077453613
        kl: 0.007484416477382183
        policy_loss: -0.008939172141253948
        total_loss: 3.32082462310791
        vf_explained_var: 0.893275260925293
        vf_loss: 3.327892780303955
    load_time_ms: 0.735
    num_steps_sampled: 633600
    num_steps_trained: 624000
    sample_time_ms: 1829.024
    update_time_ms: 4.359
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.891559093165201
    mean_inference_ms: 1.2300796645835745
    mean_processing_ms: 1.6113523153311429
  time_since_restore: 110.69944977760315
  time_this_iter_s: 4.727293252944946
  time_total_s: 110.69944977760315
  timestamp: 1563363817
  timesteps_since_restore: 633600
  timesteps_this_iter: 26400
  timesteps_total: 633600
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 110 s, 24 iter, 633600 ts, 7.71 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.263797167279996
  episode_reward_mean: 9.003073851472061
  episode_reward_min: -10.394138828396207
  episodes_this_iter: 176
  episodes_total: 4576
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2676.699
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.615783214569092
        kl: 0.007966624572873116
        policy_loss: -0.010057821869850159
        total_loss: 2.644357442855835
        vf_explained_var: 0.9182567000389099
        vf_loss: 2.652423620223999
    load_time_ms: 0.749
    num_steps_sampled: 686400
    num_steps_trained: 676000
    sample_time_ms: 1831.111
    update_time_ms: 4.373
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9045075910457483
    mean_inference_ms: 1.2371346057829242
    mean_processing_ms: 1.6205888481172146
  time_since_restore: 119.65341663360596
  time_this_iter_s: 4.301804542541504
  time_total_s: 119.65341663360596
  timestamp: 1563363826
  timesteps_since_restore: 686400
  timesteps_this_iter: 26400
  timesteps_total: 686400
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 119 s, 26 iter, 686400 ts, 9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-43-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.481228684742103
  episode_reward_mean: 9.490591097298857
  episode_reward_min: -16.15657790394981
  episodes_this_iter: 176
  episodes_total: 4928
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2738.833
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.6036088466644287
        kl: 0.007900125347077847
        policy_loss: -0.009209135547280312
        total_loss: 2.286533832550049
        vf_explained_var: 0.9277420043945312
        vf_loss: 2.2937681674957275
    load_time_ms: 0.752
    num_steps_sampled: 739200
    num_steps_trained: 728000
    sample_time_ms: 1863.436
    update_time_ms: 4.596
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.911281791633102
    mean_inference_ms: 1.241488236463759
    mean_processing_ms: 1.6276977895166214
  time_since_restore: 129.2694399356842
  time_this_iter_s: 4.918910980224609
  time_total_s: 129.2694399356842
  timestamp: 1563363836
  timesteps_since_restore: 739200
  timesteps_this_iter: 26400
  timesteps_total: 739200
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 129 s, 28 iter, 739200 ts, 9.49 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.84927623674364
  episode_reward_mean: 10.291536111681374
  episode_reward_min: -13.280361994167968
  episodes_this_iter: 176
  episodes_total: 5280
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2750.965
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.577230215072632
        kl: 0.009149863384664059
        policy_loss: -0.012604196555912495
        total_loss: 1.733410120010376
        vf_explained_var: 0.9465237855911255
        vf_loss: 1.7437267303466797
    load_time_ms: 0.745
    num_steps_sampled: 792000
    num_steps_trained: 780000
    sample_time_ms: 1877.066
    update_time_ms: 4.815
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9036321777387892
    mean_inference_ms: 1.2317243888104614
    mean_processing_ms: 1.6253732328723784
  time_since_restore: 138.4317557811737
  time_this_iter_s: 4.704105615615845
  time_total_s: 138.4317557811737
  timestamp: 1563363845
  timesteps_since_restore: 792000
  timesteps_this_iter: 26400
  timesteps_total: 792000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 138 s, 30 iter, 792000 ts, 10.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.35033362991975
  episode_reward_mean: 10.79249198344064
  episode_reward_min: -8.68854723566097
  episodes_this_iter: 176
  episodes_total: 5632
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2650.555
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5536863803863525
        kl: 0.007974444888532162
        policy_loss: -0.010939786210656166
        total_loss: 1.5889065265655518
        vf_explained_var: 0.9470537900924683
        vf_loss: 1.5978528261184692
    load_time_ms: 0.754
    num_steps_sampled: 844800
    num_steps_trained: 832000
    sample_time_ms: 1871.194
    update_time_ms: 4.938
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9005476501559364
    mean_inference_ms: 1.2293876526063445
    mean_processing_ms: 1.620705168414302
  time_since_restore: 146.7656168937683
  time_this_iter_s: 4.150641679763794
  time_total_s: 146.7656168937683
  timestamp: 1563363853
  timesteps_since_restore: 844800
  timesteps_this_iter: 26400
  timesteps_total: 844800
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 146 s, 32 iter, 844800 ts, 10.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.81189932325971
  episode_reward_mean: 11.75492342261292
  episode_reward_min: -10.781256449456706
  episodes_this_iter: 176
  episodes_total: 5984
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2613.113
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5353939533233643
        kl: 0.007792064920067787
        policy_loss: -0.01256785448640585
        total_loss: 1.384613275527954
        vf_explained_var: 0.9566723108291626
        vf_loss: 1.3952332735061646
    load_time_ms: 0.765
    num_steps_sampled: 897600
    num_steps_trained: 884000
    sample_time_ms: 1859.038
    update_time_ms: 4.997
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8971325022803633
    mean_inference_ms: 1.2247633134992015
    mean_processing_ms: 1.6160119875844416
  time_since_restore: 155.6745960712433
  time_this_iter_s: 4.588803768157959
  time_total_s: 155.6745960712433
  timestamp: 1563363862
  timesteps_since_restore: 897600
  timesteps_this_iter: 26400
  timesteps_total: 897600
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 155 s, 34 iter, 897600 ts, 11.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.00822419126149
  episode_reward_mean: 12.346555821907925
  episode_reward_min: -8.850087058088054
  episodes_this_iter: 176
  episodes_total: 6336
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2619.058
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5156185626983643
        kl: 0.008931375108659267
        policy_loss: -0.013075517490506172
        total_loss: 1.1089293956756592
        vf_explained_var: 0.9666117429733276
        vf_loss: 1.1197720766067505
    load_time_ms: 0.756
    num_steps_sampled: 950400
    num_steps_trained: 936000
    sample_time_ms: 1853.222
    update_time_ms: 5.037
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8948337301897888
    mean_inference_ms: 1.224927367098844
    mean_processing_ms: 1.614719892738406
  time_since_restore: 164.62891793251038
  time_this_iter_s: 4.53506064414978
  time_total_s: 164.62891793251038
  timestamp: 1563363871
  timesteps_since_restore: 950400
  timesteps_this_iter: 26400
  timesteps_total: 950400
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 164 s, 36 iter, 950400 ts, 12.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.778790772814254
  episode_reward_mean: 12.71982356643166
  episode_reward_min: -9.6274364672675
  episodes_this_iter: 176
  episodes_total: 6688
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2621.574
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.4931132793426514
        kl: 0.00844477117061615
        policy_loss: -0.012776828370988369
        total_loss: 0.9124810099601746
        vf_explained_var: 0.9724248051643372
        vf_loss: 0.923146665096283
    load_time_ms: 0.755
    num_steps_sampled: 1003200
    num_steps_trained: 988000
    sample_time_ms: 1809.12
    update_time_ms: 5.02
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9032623563705755
    mean_inference_ms: 1.2289511837694564
    mean_processing_ms: 1.623855850831282
  time_since_restore: 173.8254919052124
  time_this_iter_s: 4.595435380935669
  time_total_s: 173.8254919052124
  timestamp: 1563363880
  timesteps_since_restore: 1003200
  timesteps_this_iter: 26400
  timesteps_total: 1003200
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 173 s, 38 iter, 1003200 ts, 12.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.486419348145848
  episode_reward_mean: 13.207351046966359
  episode_reward_min: -9.984081854005517
  episodes_this_iter: 176
  episodes_total: 7040
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2642.512
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.4738922119140625
        kl: 0.009405621327459812
        policy_loss: -0.012537389993667603
        total_loss: 0.9162566661834717
        vf_explained_var: 0.9730126857757568
        vf_loss: 0.926442563533783
    load_time_ms: 0.762
    num_steps_sampled: 1056000
    num_steps_trained: 1040000
    sample_time_ms: 1838.395
    update_time_ms: 4.884
  iterations_since_restore: 40
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9068667602459684
    mean_inference_ms: 1.230148693860903
    mean_processing_ms: 1.629081663050856
  time_since_restore: 183.48759174346924
  time_this_iter_s: 5.046621322631836
  time_total_s: 183.48759174346924
  timestamp: 1563363890
  timesteps_since_restore: 1056000
  timesteps_this_iter: 26400
  timesteps_total: 1056000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 183 s, 40 iter, 1056000 ts, 13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-44-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.974522128272866
  episode_reward_mean: 13.97063689139373
  episode_reward_min: -5.691446303501028
  episodes_this_iter: 176
  episodes_total: 7392
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2707.255
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.451035976409912
        kl: 0.008894030004739761
        policy_loss: -0.01214219443500042
        total_loss: 0.7865457534790039
        vf_explained_var: 0.9765018820762634
        vf_loss: 0.7964645624160767
    load_time_ms: 0.766
    num_steps_sampled: 1108800
    num_steps_trained: 1092000
    sample_time_ms: 1851.515
    update_time_ms: 4.741
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9074248769119877
    mean_inference_ms: 1.2299738397841382
    mean_processing_ms: 1.6284417188779101
  time_since_restore: 192.5936713218689
  time_this_iter_s: 4.310011386871338
  time_total_s: 192.5936713218689
  timestamp: 1563363899
  timesteps_since_restore: 1108800
  timesteps_this_iter: 26400
  timesteps_total: 1108800
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 192 s, 42 iter, 1108800 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.49686481228252
  episode_reward_mean: 12.681540490761813
  episode_reward_min: -8.595881633438132
  episodes_this_iter: 176
  episodes_total: 7744
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2687.597
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.424983501434326
        kl: 0.01007751002907753
        policy_loss: -0.012494360096752644
        total_loss: 0.8321974277496338
        vf_explained_var: 0.9756529331207275
        vf_loss: 0.842172384262085
    load_time_ms: 0.761
    num_steps_sampled: 1161600
    num_steps_trained: 1144000
    sample_time_ms: 1840.787
    update_time_ms: 4.656
  iterations_since_restore: 44
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9053996242142412
    mean_inference_ms: 1.2297389334912447
    mean_processing_ms: 1.6290932350194878
  time_since_restore: 201.1965982913971
  time_this_iter_s: 4.213191747665405
  time_total_s: 201.1965982913971
  timestamp: 1563363908
  timesteps_since_restore: 1161600
  timesteps_this_iter: 26400
  timesteps_total: 1161600
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 201 s, 44 iter, 1161600 ts, 12.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.456343294944205
  episode_reward_mean: 12.997388068846373
  episode_reward_min: -6.806361678690885
  episodes_this_iter: 176
  episodes_total: 8096
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2706.147
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.4021527767181396
        kl: 0.009986062534153461
        policy_loss: -0.013169980607926846
        total_loss: 0.7889784574508667
        vf_explained_var: 0.9748216867446899
        vf_loss: 0.7996518015861511
    load_time_ms: 0.759
    num_steps_sampled: 1214400
    num_steps_trained: 1196000
    sample_time_ms: 1849.453
    update_time_ms: 4.717
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8944245924755116
    mean_inference_ms: 1.220064899743039
    mean_processing_ms: 1.6180137475622083
  time_since_restore: 210.4283311367035
  time_this_iter_s: 4.6648125648498535
  time_total_s: 210.4283311367035
  timestamp: 1563363917
  timesteps_since_restore: 1214400
  timesteps_this_iter: 26400
  timesteps_total: 1214400
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 210 s, 46 iter, 1214400 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.11318769124339
  episode_reward_mean: 13.892824339478357
  episode_reward_min: -8.702578957109328
  episodes_this_iter: 176
  episodes_total: 8448
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2687.482
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.3795907497406006
        kl: 0.00909392535686493
        policy_loss: -0.013124106451869011
        total_loss: 0.6728780269622803
        vf_explained_var: 0.9776630401611328
        vf_loss: 0.683728814125061
    load_time_ms: 0.775
    num_steps_sampled: 1267200
    num_steps_trained: 1248000
    sample_time_ms: 1853.611
    update_time_ms: 4.615
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9009133543384744
    mean_inference_ms: 1.2243126114925387
    mean_processing_ms: 1.6247967876717966
  time_since_restore: 219.47801756858826
  time_this_iter_s: 4.444441318511963
  time_total_s: 219.47801756858826
  timestamp: 1563363926
  timesteps_since_restore: 1267200
  timesteps_this_iter: 26400
  timesteps_total: 1267200
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 219 s, 48 iter, 1267200 ts, 13.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.508726390815426
  episode_reward_mean: 13.296621044403624
  episode_reward_min: -7.651483744244397
  episodes_this_iter: 176
  episodes_total: 8800
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2681.935
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.351032018661499
        kl: 0.0104209678247571
        policy_loss: -0.013756122440099716
        total_loss: 0.6428555846214294
        vf_explained_var: 0.9787283539772034
        vf_loss: 0.6540065407752991
    load_time_ms: 0.801
    num_steps_sampled: 1320000
    num_steps_trained: 1300000
    sample_time_ms: 1825.987
    update_time_ms: 4.669
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9003442147001743
    mean_inference_ms: 1.2245133379716497
    mean_processing_ms: 1.6248590637782798
  time_since_restore: 228.81439232826233
  time_this_iter_s: 4.6206488609313965
  time_total_s: 228.81439232826233
  timestamp: 1563363935
  timesteps_since_restore: 1320000
  timesteps_this_iter: 26400
  timesteps_total: 1320000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 228 s, 50 iter, 1320000 ts, 13.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.44602701513849
  episode_reward_mean: 15.407533179586126
  episode_reward_min: -6.5765279460163395
  episodes_this_iter: 176
  episodes_total: 9152
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2710.839
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.3317041397094727
        kl: 0.009474840946495533
        policy_loss: -0.012732376344501972
        total_loss: 0.6151686310768127
        vf_explained_var: 0.9836381673812866
        vf_loss: 0.6255323886871338
    load_time_ms: 0.805
    num_steps_sampled: 1372800
    num_steps_trained: 1352000
    sample_time_ms: 1804.334
    update_time_ms: 4.594
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8975883124434438
    mean_inference_ms: 1.2185336669507172
    mean_processing_ms: 1.621783133692712
  time_since_restore: 237.99536395072937
  time_this_iter_s: 4.39382529258728
  time_total_s: 237.99536395072937
  timestamp: 1563363945
  timesteps_since_restore: 1372800
  timesteps_this_iter: 26400
  timesteps_total: 1372800
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 237 s, 52 iter, 1372800 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.39598318719477
  episode_reward_mean: 13.756615807935828
  episode_reward_min: -8.914743822965194
  episodes_this_iter: 176
  episodes_total: 9328
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2744.237
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.318181037902832
        kl: 0.007980044931173325
        policy_loss: -0.011665777303278446
        total_loss: 0.5892825722694397
        vf_explained_var: 0.9814198613166809
        vf_loss: 0.598953366279602
    load_time_ms: 0.806
    num_steps_sampled: 1399200
    num_steps_trained: 1378000
    sample_time_ms: 1837.837
    update_time_ms: 4.741
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9039069135954336
    mean_inference_ms: 1.2272526215451613
    mean_processing_ms: 1.6270204044246375
  time_since_restore: 243.05752849578857
  time_this_iter_s: 5.062164545059204
  time_total_s: 243.05752849578857
  timestamp: 1563363950
  timesteps_since_restore: 1399200
  timesteps_this_iter: 26400
  timesteps_total: 1399200
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 243 s, 53 iter, 1399200 ts, 13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-45-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.62039644972323
  episode_reward_mean: 13.493636670440491
  episode_reward_min: -6.380357574710409
  episodes_this_iter: 176
  episodes_total: 9680
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2810.573
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.297194480895996
        kl: 0.009794137440621853
        policy_loss: -0.016875112429261208
        total_loss: 0.5262596607208252
        vf_explained_var: 0.9841862916946411
        vf_loss: 0.5406861901283264
    load_time_ms: 0.788
    num_steps_sampled: 1452000
    num_steps_trained: 1430000
    sample_time_ms: 1857.976
    update_time_ms: 4.8
  iterations_since_restore: 55
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.907829573264211
    mean_inference_ms: 1.2252870970892873
    mean_processing_ms: 1.6300928207232497
  time_since_restore: 252.70449018478394
  time_this_iter_s: 5.01317286491394
  time_total_s: 252.70449018478394
  timestamp: 1563363959
  timesteps_since_restore: 1452000
  timesteps_this_iter: 26400
  timesteps_total: 1452000
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 252 s, 55 iter, 1452000 ts, 13.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-46-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.78696977595816
  episode_reward_mean: 15.349752782772395
  episode_reward_min: -6.792349246128289
  episodes_this_iter: 176
  episodes_total: 10032
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2721.182
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2810773849487305
        kl: 0.01118686143308878
        policy_loss: -0.014095454476773739
        total_loss: 0.5190936326980591
        vf_explained_var: 0.9852797389030457
        vf_loss: 0.5303922891616821
    load_time_ms: 0.773
    num_steps_sampled: 1504800
    num_steps_trained: 1482000
    sample_time_ms: 1826.276
    update_time_ms: 4.66
  iterations_since_restore: 57
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.905989618201066
    mean_inference_ms: 1.227049747810945
    mean_processing_ms: 1.629273046241934
  time_since_restore: 260.7583408355713
  time_this_iter_s: 3.7531988620758057
  time_total_s: 260.7583408355713
  timestamp: 1563363967
  timesteps_since_restore: 1504800
  timesteps_this_iter: 26400
  timesteps_total: 1504800
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 260 s, 57 iter, 1504800 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-46-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.09719798663022
  episode_reward_mean: 13.950542527868265
  episode_reward_min: -6.990695583332603
  episodes_this_iter: 176
  episodes_total: 10384
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2672.544
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2580597400665283
        kl: 0.011262327432632446
        policy_loss: -0.014218151569366455
        total_loss: 0.4957358241081238
        vf_explained_var: 0.9830480217933655
        vf_loss: 0.5071383714675903
    load_time_ms: 0.766
    num_steps_sampled: 1557600
    num_steps_trained: 1534000
    sample_time_ms: 1834.682
    update_time_ms: 4.569
  iterations_since_restore: 59
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.898307535878198
    mean_inference_ms: 1.2219961859894406
    mean_processing_ms: 1.6232329848011824
  time_since_restore: 269.50677394866943
  time_this_iter_s: 3.9517335891723633
  time_total_s: 269.50677394866943
  timestamp: 1563363976
  timesteps_since_restore: 1557600
  timesteps_this_iter: 26400
  timesteps_total: 1557600
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 269 s, 59 iter, 1557600 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-46-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.28099997314884
  episode_reward_mean: 15.561646875765456
  episode_reward_min: -4.094379858178197
  episodes_this_iter: 176
  episodes_total: 10736
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2644.515
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2329249382019043
        kl: 0.011129035614430904
        policy_loss: -0.016510169953107834
        total_loss: 0.48087650537490845
        vf_explained_var: 0.98658287525177
        vf_loss: 0.4946044087409973
    load_time_ms: 0.733
    num_steps_sampled: 1610400
    num_steps_trained: 1586000
    sample_time_ms: 1810.398
    update_time_ms: 4.605
  iterations_since_restore: 61
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9009379281810512
    mean_inference_ms: 1.2249180267244952
    mean_processing_ms: 1.625814788292307
  time_since_restore: 278.3879132270813
  time_this_iter_s: 4.25821590423584
  time_total_s: 278.3879132270813
  timestamp: 1563363985
  timesteps_since_restore: 1610400
  timesteps_this_iter: 26400
  timesteps_total: 1610400
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 278 s, 61 iter, 1610400 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-46-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.610205752057176
  episode_reward_mean: 15.983825857042337
  episode_reward_min: -3.228923449373931
  episodes_this_iter: 176
  episodes_total: 11088
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2588.621
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2097456455230713
        kl: 0.010189269669353962
        policy_loss: -0.014134383760392666
        total_loss: 0.42712581157684326
        vf_explained_var: 0.9881641864776611
        vf_loss: 0.4387129247188568
    load_time_ms: 0.739
    num_steps_sampled: 1663200
    num_steps_trained: 1638000
    sample_time_ms: 1819.959
    update_time_ms: 4.271
  iterations_since_restore: 63
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.899227589635628
    mean_inference_ms: 1.2227834253157868
    mean_processing_ms: 1.624149064192693
  time_since_restore: 287.37039971351624
  time_this_iter_s: 4.600926876068115
  time_total_s: 287.37039971351624
  timestamp: 1563363994
  timesteps_since_restore: 1663200
  timesteps_this_iter: 26400
  timesteps_total: 1663200
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 287 s, 63 iter, 1663200 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-46-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.46332226513575
  episode_reward_mean: 13.760679360824016
  episode_reward_min: -5.79193589868051
  episodes_this_iter: 176
  episodes_total: 11440
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2558.027
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.185153007507324
        kl: 0.01031493116170168
        policy_loss: -0.013811706565320492
        total_loss: 0.44143912196159363
        vf_explained_var: 0.9858849048614502
        vf_loss: 0.4526720345020294
    load_time_ms: 0.754
    num_steps_sampled: 1716000
    num_steps_trained: 1690000
    sample_time_ms: 1800.362
    update_time_ms: 4.247
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9004307575601316
    mean_inference_ms: 1.220846675124096
    mean_processing_ms: 1.6245983384540756
  time_since_restore: 296.5134947299957
  time_this_iter_s: 4.82038950920105
  time_total_s: 296.5134947299957
  timestamp: 1563364003
  timesteps_since_restore: 1716000
  timesteps_this_iter: 26400
  timesteps_total: 1716000
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 296 s, 65 iter, 1716000 ts, 13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-46-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.39710135070088
  episode_reward_mean: 14.33562006920261
  episode_reward_min: -5.0789065985022654
  episodes_this_iter: 176
  episodes_total: 11792
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2640.78
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.160507917404175
        kl: 0.010595177300274372
        policy_loss: -0.014582177624106407
        total_loss: 0.3937641680240631
        vf_explained_var: 0.9876817464828491
        vf_loss: 0.4056974947452545
    load_time_ms: 0.758
    num_steps_sampled: 1768800
    num_steps_trained: 1742000
    sample_time_ms: 1849.183
    update_time_ms: 4.198
  iterations_since_restore: 67
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8967515533008963
    mean_inference_ms: 1.2210593708294093
    mean_processing_ms: 1.6242360448808209
  time_since_restore: 305.8871145248413
  time_this_iter_s: 4.652331352233887
  time_total_s: 305.8871145248413
  timestamp: 1563364013
  timesteps_since_restore: 1768800
  timesteps_this_iter: 26400
  timesteps_total: 1768800
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 305 s, 67 iter, 1768800 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.59374383575052
  episode_reward_mean: 14.83954257903004
  episode_reward_min: -4.1493877948547775
  episodes_this_iter: 176
  episodes_total: 12144
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2704.883
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.1363322734832764
        kl: 0.010015719570219517
        policy_loss: -0.013590633869171143
        total_loss: 0.3697698712348938
        vf_explained_var: 0.9879714250564575
        vf_loss: 0.38085657358169556
    load_time_ms: 0.749
    num_steps_sampled: 1821600
    num_steps_trained: 1794000
    sample_time_ms: 1846.055
    update_time_ms: 4.29
  iterations_since_restore: 69
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8956323300913385
    mean_inference_ms: 1.2185760105779477
    mean_processing_ms: 1.622684295394931
  time_since_restore: 315.25214886665344
  time_this_iter_s: 4.747801303863525
  time_total_s: 315.25214886665344
  timestamp: 1563364022
  timesteps_since_restore: 1821600
  timesteps_this_iter: 26400
  timesteps_total: 1821600
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 315 s, 69 iter, 1821600 ts, 14.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.67020243593647
  episode_reward_mean: 15.375129730334123
  episode_reward_min: -4.8672681110631455
  episodes_this_iter: 176
  episodes_total: 12496
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2661.396
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.105485200881958
        kl: 0.010074938647449017
        policy_loss: -0.016553815454244614
        total_loss: 0.35100072622299194
        vf_explained_var: 0.9901151061058044
        vf_loss: 0.36503586173057556
    load_time_ms: 0.754
    num_steps_sampled: 1874400
    num_steps_trained: 1846000
    sample_time_ms: 1846.964
    update_time_ms: 4.156
  iterations_since_restore: 71
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8960124491508537
    mean_inference_ms: 1.2215473180406549
    mean_processing_ms: 1.6199312450007601
  time_since_restore: 323.70411133766174
  time_this_iter_s: 4.19634485244751
  time_total_s: 323.70411133766174
  timestamp: 1563364031
  timesteps_since_restore: 1874400
  timesteps_this_iter: 26400
  timesteps_total: 1874400
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 323 s, 71 iter, 1874400 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.3076087429344
  episode_reward_mean: 14.782657312707437
  episode_reward_min: -4.268149581360761
  episodes_this_iter: 176
  episodes_total: 12848
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2645.161
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.075864315032959
        kl: 0.01030975766479969
        policy_loss: -0.01778383180499077
        total_loss: 0.30956780910491943
        vf_explained_var: 0.989816427230835
        vf_loss: 0.32477423548698425
    load_time_ms: 0.751
    num_steps_sampled: 1927200
    num_steps_trained: 1898000
    sample_time_ms: 1865.742
    update_time_ms: 4.232
  iterations_since_restore: 73
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8962822256935907
    mean_inference_ms: 1.2192799793617524
    mean_processing_ms: 1.6207284891717444
  time_since_restore: 332.71706795692444
  time_this_iter_s: 4.680639028549194
  time_total_s: 332.71706795692444
  timestamp: 1563364040
  timesteps_since_restore: 1927200
  timesteps_this_iter: 26400
  timesteps_total: 1927200
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 332 s, 73 iter, 1927200 ts, 14.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.56504839475496
  episode_reward_mean: 16.09898311844325
  episode_reward_min: -2.9359321143595327
  episodes_this_iter: 176
  episodes_total: 13200
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2633.174
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.051192283630371
        kl: 0.0102141248062253
        policy_loss: -0.014671438373625278
        total_loss: 0.30648571252822876
        vf_explained_var: 0.9919074773788452
        vf_loss: 0.31860363483428955
    load_time_ms: 0.746
    num_steps_sampled: 1980000
    num_steps_trained: 1950000
    sample_time_ms: 1858.425
    update_time_ms: 4.249
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9007945812838116
    mean_inference_ms: 1.2250244164229422
    mean_processing_ms: 1.6242280450460442
  time_since_restore: 341.66336703300476
  time_this_iter_s: 4.850573301315308
  time_total_s: 341.66336703300476
  timestamp: 1563364049
  timesteps_since_restore: 1980000
  timesteps_this_iter: 26400
  timesteps_total: 1980000
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 341 s, 75 iter, 1980000 ts, 16.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.55358819439944
  episode_reward_mean: 16.34810626236869
  episode_reward_min: -4.41327093470849
  episodes_this_iter: 176
  episodes_total: 13552
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2602.935
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.034822702407837
        kl: 0.010954229161143303
        policy_loss: -0.01471614371985197
        total_loss: 0.2882850468158722
        vf_explained_var: 0.9912446141242981
        vf_loss: 0.3002626597881317
    load_time_ms: 0.752
    num_steps_sampled: 2032800
    num_steps_trained: 2002000
    sample_time_ms: 1840.374
    update_time_ms: 4.116
  iterations_since_restore: 77
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9032963404587242
    mean_inference_ms: 1.225111882242382
    mean_processing_ms: 1.6248237394074736
  time_since_restore: 350.54895663261414
  time_this_iter_s: 4.47834849357605
  time_total_s: 350.54895663261414
  timestamp: 1563364057
  timesteps_since_restore: 2032800
  timesteps_this_iter: 26400
  timesteps_total: 2032800
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 350 s, 77 iter, 2032800 ts, 16.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.244881406438836
  episode_reward_mean: 16.103029040745376
  episode_reward_min: -2.83950207918377
  episodes_this_iter: 176
  episodes_total: 13904
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2554.054
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.0129404067993164
        kl: 0.012237269431352615
        policy_loss: -0.01662461645901203
        total_loss: 0.23704053461551666
        vf_explained_var: 0.9927082657814026
        vf_loss: 0.2506057918071747
    load_time_ms: 0.753
    num_steps_sampled: 2085600
    num_steps_trained: 2054000
    sample_time_ms: 1831.906
    update_time_ms: 4.048
  iterations_since_restore: 79
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.895433758864644
    mean_inference_ms: 1.2200803158215452
    mean_processing_ms: 1.6211805548982143
  time_since_restore: 359.33872270584106
  time_this_iter_s: 4.499854564666748
  time_total_s: 359.33872270584106
  timestamp: 1563364066
  timesteps_since_restore: 2085600
  timesteps_this_iter: 26400
  timesteps_total: 2085600
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 359 s, 79 iter, 2085600 ts, 16.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-47-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.87361811933584
  episode_reward_mean: 16.71292092421927
  episode_reward_min: -4.0070084802826536
  episodes_this_iter: 176
  episodes_total: 14256
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2611.612
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.985062837600708
        kl: 0.012668322771787643
        policy_loss: -0.017946194857358932
        total_loss: 0.2433868944644928
        vf_explained_var: 0.9931771159172058
        vf_loss: 0.25816601514816284
    load_time_ms: 0.748
    num_steps_sampled: 2138400
    num_steps_trained: 2106000
    sample_time_ms: 1854.599
    update_time_ms: 4.205
  iterations_since_restore: 81
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8983665420010893
    mean_inference_ms: 1.222087889692295
    mean_processing_ms: 1.621811695303104
  time_since_restore: 368.59850788116455
  time_this_iter_s: 4.927452325820923
  time_total_s: 368.59850788116455
  timestamp: 1563364076
  timesteps_since_restore: 2138400
  timesteps_this_iter: 26400
  timesteps_total: 2138400
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 368 s, 81 iter, 2138400 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.32454382191208
  episode_reward_mean: 16.084367134074142
  episode_reward_min: -4.590420284284794
  episodes_this_iter: 176
  episodes_total: 14608
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2591.142
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.9615528583526611
        kl: 0.010552611202001572
        policy_loss: -0.016889821738004684
        total_loss: 0.19725605845451355
        vf_explained_var: 0.9938443899154663
        vf_loss: 0.21150770783424377
    load_time_ms: 0.755
    num_steps_sampled: 2191200
    num_steps_trained: 2158000
    sample_time_ms: 1838.485
    update_time_ms: 4.247
  iterations_since_restore: 83
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8962415717339127
    mean_inference_ms: 1.2219693980446054
    mean_processing_ms: 1.6212387839326325
  time_since_restore: 377.2422559261322
  time_this_iter_s: 4.363426923751831
  time_total_s: 377.2422559261322
  timestamp: 1563364084
  timesteps_since_restore: 2191200
  timesteps_this_iter: 26400
  timesteps_total: 2191200
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 377 s, 83 iter, 2191200 ts, 16.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.51792056818694
  episode_reward_mean: 17.542542931148645
  episode_reward_min: -2.725391518785799
  episodes_this_iter: 176
  episodes_total: 14960
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2572.773
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.94464111328125
        kl: 0.010849584825336933
        policy_loss: -0.017872406169772148
        total_loss: 0.2028539925813675
        vf_explained_var: 0.9943350553512573
        vf_loss: 0.21801398694515228
    load_time_ms: 0.76
    num_steps_sampled: 2244000
    num_steps_trained: 2210000
    sample_time_ms: 1813.756
    update_time_ms: 4.551
  iterations_since_restore: 85
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.895235700658369
    mean_inference_ms: 1.2213946408822802
    mean_processing_ms: 1.6214778121725824
  time_since_restore: 385.7619585990906
  time_this_iter_s: 4.464242935180664
  time_total_s: 385.7619585990906
  timestamp: 1563364093
  timesteps_since_restore: 2244000
  timesteps_this_iter: 26400
  timesteps_total: 2244000
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 385 s, 85 iter, 2244000 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 390 s, 86 iter, 2270400 ts, 17.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.053602818909624
  episode_reward_mean: 16.60359950182901
  episode_reward_min: -3.263159203518715
  episodes_this_iter: 176
  episodes_total: 15312
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2610.527
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.9137094020843506
        kl: 0.011886541731655598
        policy_loss: -0.017949901521205902
        total_loss: 0.2009175717830658
        vf_explained_var: 0.9938309192657471
        vf_loss: 0.21589583158493042
    load_time_ms: 0.761
    num_steps_sampled: 2296800
    num_steps_trained: 2262000
    sample_time_ms: 1802.532
    update_time_ms: 4.746
  iterations_since_restore: 87
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.893936908494715
    mean_inference_ms: 1.2194417308325123
    mean_processing_ms: 1.619309683159281
  time_since_restore: 394.9187602996826
  time_this_iter_s: 4.168724298477173
  time_total_s: 394.9187602996826
  timestamp: 1563364102
  timesteps_since_restore: 2296800
  timesteps_this_iter: 26400
  timesteps_total: 2296800
  training_iteration: 87
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 399 s, 88 iter, 2323200 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.77959278695495
  episode_reward_mean: 16.122231107078903
  episode_reward_min: -3.906094529486
  episodes_this_iter: 176
  episodes_total: 15664
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2645.237
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.8908727169036865
        kl: 0.012195482850074768
        policy_loss: -0.013360717333853245
        total_loss: 0.17385455965995789
        vf_explained_var: 0.9940335154533386
        vf_loss: 0.18416635692119598
    load_time_ms: 0.776
    num_steps_sampled: 2349600
    num_steps_trained: 2314000
    sample_time_ms: 1802.087
    update_time_ms: 4.85
  iterations_since_restore: 89
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8956501754972062
    mean_inference_ms: 1.2205898209752675
    mean_processing_ms: 1.6218226918370275
  time_since_restore: 404.052059173584
  time_this_iter_s: 4.381338834762573
  time_total_s: 404.052059173584
  timestamp: 1563364111
  timesteps_since_restore: 2349600
  timesteps_this_iter: 26400
  timesteps_total: 2349600
  training_iteration: 89
  2019-07-17 13:48:58,632	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-17 13:48:58,642	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 408 s, 90 iter, 2376000 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.904282588443245
  episode_reward_mean: 16.328971056686797
  episode_reward_min: -2.206050504733063
  episodes_this_iter: 176
  episodes_total: 16016
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2600.168
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.860202670097351
        kl: 0.012789309956133366
        policy_loss: -0.017169980332255363
        total_loss: 0.15382617712020874
        vf_explained_var: 0.9947242736816406
        vf_loss: 0.16779883205890656
    load_time_ms: 0.785
    num_steps_sampled: 2402400
    num_steps_trained: 2366000
    sample_time_ms: 1784.71
    update_time_ms: 4.726
  iterations_since_restore: 91
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8916999603158693
    mean_inference_ms: 1.219330910981938
    mean_processing_ms: 1.6182961656121249
  time_since_restore: 412.6830003261566
  time_this_iter_s: 4.309982776641846
  time_total_s: 412.6830003261566
  timestamp: 1563364120
  timesteps_since_restore: 2402400
  timesteps_this_iter: 26400
  timesteps_total: 2402400
  training_iteration: 91
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 417 s, 92 iter, 2428800 ts, 16.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.57583644898168
  episode_reward_mean: 16.843421855086607
  episode_reward_min: -3.5714211091635506
  episodes_this_iter: 176
  episodes_total: 16368
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2643.824
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.8312503099441528
        kl: 0.012426270171999931
        policy_loss: -0.018827170133590698
        total_loss: 0.1532023698091507
        vf_explained_var: 0.994995653629303
        vf_loss: 0.16892296075820923
    load_time_ms: 0.774
    num_steps_sampled: 2455200
    num_steps_trained: 2418000
    sample_time_ms: 1805.392
    update_time_ms: 4.751
  iterations_since_restore: 93
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8957556733493768
    mean_inference_ms: 1.2201592501892544
    mean_processing_ms: 1.6220522140915163
  time_since_restore: 421.9738233089447
  time_this_iter_s: 4.745332479476929
  time_total_s: 421.9738233089447
  timestamp: 1563364129
  timesteps_since_restore: 2455200
  timesteps_this_iter: 26400
  timesteps_total: 2455200
  training_iteration: 93
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=29869], 426 s, 94 iter, 2481600 ts, 16.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-48-58
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 36.867003861844914
  episode_reward_mean: 18.054040107941095
  episode_reward_min: -4.149128417261208
  episodes_this_iter: 176
  episodes_total: 16720
  experiment_id: eef3331b7ea240cabc0c9b6e9c7e1971
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2638.106
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.8073737621307373
        kl: 0.012267367914319038
        policy_loss: -0.015620950609445572
        total_loss: 0.1418607532978058
        vf_explained_var: 0.9962297677993774
        vf_loss: 0.15441487729549408
    load_time_ms: 0.777
    num_steps_sampled: 2508000
    num_steps_trained: 2470000
    sample_time_ms: 1857.13
    update_time_ms: 4.551
  iterations_since_restore: 95
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 29869
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8991885549161134
    mean_inference_ms: 1.2198547543613254
    mean_processing_ms: 1.626064856233173
  time_since_restore: 430.9525029659271
  time_this_iter_s: 4.630241870880127
  time_total_s: 430.9525029659271
  timestamp: 1563364138
  timesteps_since_restore: 2508000
  timesteps_this_iter: 26400
  timesteps_total: 2508000
  training_iteration: 95
  
[2m[36m(pid=30796)[0m [32m [     0.10273s,  INFO] TimeLimit:
[2m[36m(pid=30796)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30796)[0m - action_space = Box(2,)
[2m[36m(pid=30796)[0m - observation_space = Box(9,)
[2m[36m(pid=30796)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30796)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30796)[0m - _max_episode_steps = 150
[2m[36m(pid=30796)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30796)[0m 2019-07-17 13:48:58,878	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=30796)[0m 2019-07-17 13:48:58.906591: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30796)[0m 2019-07-17 13:48:58.927283: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.164466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.168367: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e4f747eaa0 executing computations on platform CUDA. Devices:
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.168448: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.207443: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.208027: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e4f7c05190 executing computations on platform Host. Devices:
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.208062: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.208360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.208687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
[2m[36m(pid=30796)[0m name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
[2m[36m(pid=30796)[0m pciBusID: 0000:01:00.0
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.208995: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.209192: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.209300: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.209412: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.209515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.209621: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.214374: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.214437: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.214469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.214481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.214490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[2m[36m(pid=30796)[0m W0717 13:48:59.233889 139880310220224 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30796)[0m Instructions for updating:
[2m[36m(pid=30796)[0m Use keras.layers.dense instead.
[2m[36m(pid=30796)[0m W0717 13:48:59.669472 139880310220224 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30796)[0m Instructions for updating:
[2m[36m(pid=30796)[0m Use `tf.cast` instead.
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59.720464: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30796)[0m 2019-07-17 13:48:59,734	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30796)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30796)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=30796)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30796)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30796)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30796)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m W0717 13:48:59.770043 139880310220224 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30796)[0m Instructions for updating:
[2m[36m(pid=30796)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30796)[0m [32m [     1.60991s,  INFO] TimeLimit:
[2m[36m(pid=30796)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30796)[0m - action_space = Box(2,)
[2m[36m(pid=30796)[0m - observation_space = Box(9,)
[2m[36m(pid=30796)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30796)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30796)[0m - _max_episode_steps = 150
[2m[36m(pid=30796)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30796)[0m [32m [     1.61029s,  INFO] TimeLimit:
[2m[36m(pid=30796)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30796)[0m - action_space = Box(2,)
[2m[36m(pid=30796)[0m - observation_space = Box(9,)
[2m[36m(pid=30796)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30796)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30796)[0m - _max_episode_steps = 150
[2m[36m(pid=30796)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30796)[0m [32m [     1.61066s,  INFO] TimeLimit:
[2m[36m(pid=30796)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30796)[0m - action_space = Box(2,)
[2m[36m(pid=30796)[0m - observation_space = Box(9,)
[2m[36m(pid=30796)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30796)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30796)[0m - _max_episode_steps = 150
[2m[36m(pid=30796)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30796)[0m 2019-07-17 13:49:00,411	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f368ac59828>}
[2m[36m(pid=30796)[0m 2019-07-17 13:49:00,411	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f368ac59748>}
[2m[36m(pid=30796)[0m 2019-07-17 13:49:00,411	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00,492	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.505375: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30779)[0m [32m [     0.02316s,  INFO] TimeLimit:
[2m[36m(pid=30779)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30779)[0m - action_space = Box(2,)
[2m[36m(pid=30779)[0m - observation_space = Box(9,)
[2m[36m(pid=30779)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30779)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30779)[0m - _max_episode_steps = 150
[2m[36m(pid=30779)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30781)[0m [32m [     0.02120s,  INFO] TimeLimit:
[2m[36m(pid=30781)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30781)[0m - action_space = Box(2,)
[2m[36m(pid=30781)[0m - observation_space = Box(9,)
[2m[36m(pid=30781)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30781)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30781)[0m - _max_episode_steps = 150
[2m[36m(pid=30781)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00,491	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30788)[0m [32m [     0.03146s,  INFO] TimeLimit:
[2m[36m(pid=30788)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30788)[0m - action_space = Box(2,)
[2m[36m(pid=30788)[0m - observation_space = Box(9,)
[2m[36m(pid=30788)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30788)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30788)[0m - _max_episode_steps = 150
[2m[36m(pid=30788)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00,475	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.492692: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.498787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.501421: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.501466: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.501474: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.501559: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.501580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.501586: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00,510	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30793)[0m [32m [     0.02779s,  INFO] TimeLimit:
[2m[36m(pid=30793)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30793)[0m - action_space = Box(2,)
[2m[36m(pid=30793)[0m - observation_space = Box(9,)
[2m[36m(pid=30793)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30793)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30793)[0m - _max_episode_steps = 150
[2m[36m(pid=30793)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.512042: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.511902: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30786)[0m [32m [     0.03514s,  INFO] TimeLimit:
[2m[36m(pid=30786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30786)[0m - action_space = Box(2,)
[2m[36m(pid=30786)[0m - observation_space = Box(9,)
[2m[36m(pid=30786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30786)[0m - _max_episode_steps = 150
[2m[36m(pid=30786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00,554	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30768)[0m [32m [     0.03028s,  INFO] TimeLimit:
[2m[36m(pid=30768)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30768)[0m - action_space = Box(2,)
[2m[36m(pid=30768)[0m - observation_space = Box(9,)
[2m[36m(pid=30768)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30768)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30768)[0m - _max_episode_steps = 150
[2m[36m(pid=30768)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00,556	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00,547	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30775)[0m [32m [     0.03442s,  INFO] TimeLimit:
[2m[36m(pid=30775)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30775)[0m - action_space = Box(2,)
[2m[36m(pid=30775)[0m - observation_space = Box(9,)
[2m[36m(pid=30775)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30775)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30775)[0m - _max_episode_steps = 150
[2m[36m(pid=30775)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.515396: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.515454: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.515468: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.515585: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.515623: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.515637: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.525866: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.526303: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561e6c15c8d0 executing computations on platform Host. Devices:
[2m[36m(pid=30779)[0m 2019-07-17 13:49:00.526331: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30779)[0m W0717 13:49:00.533560 140653810357696 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30779)[0m Instructions for updating:
[2m[36m(pid=30779)[0m Use keras.layers.dense instead.
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.530149: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.539480: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.542915: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.542975: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.542990: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.543120: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.543158: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.543169: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.563564: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.563978: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5604780258d0 executing computations on platform Host. Devices:
[2m[36m(pid=30793)[0m 2019-07-17 13:49:00.564006: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.520537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.525547: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.525616: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.525630: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.525743: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.525777: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.525786: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.529556: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.529979: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c33432d8d0 executing computations on platform Host. Devices:
[2m[36m(pid=30788)[0m 2019-07-17 13:49:00.530004: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30788)[0m W0717 13:49:00.537462 139726508664256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30788)[0m Instructions for updating:
[2m[36m(pid=30788)[0m Use keras.layers.dense instead.
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.523474: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.524200: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ac84d70cf0 executing computations on platform Host. Devices:
[2m[36m(pid=30781)[0m 2019-07-17 13:49:00.524246: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30781)[0m W0717 13:49:00.531563 140431634224576 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30781)[0m Instructions for updating:
[2m[36m(pid=30781)[0m Use keras.layers.dense instead.
[2m[36m(pid=30796)[0m 2019-07-17 13:49:00,520	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.568003: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30767)[0m [32m [     0.03617s,  INFO] TimeLimit:
[2m[36m(pid=30767)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30767)[0m - action_space = Box(2,)
[2m[36m(pid=30767)[0m - observation_space = Box(9,)
[2m[36m(pid=30767)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30767)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30767)[0m - _max_episode_steps = 150
[2m[36m(pid=30767)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00,617	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00,585	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.600866: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.610413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.614253: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.614298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.614313: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.614432: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.614470: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.614483: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.620228: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.620737: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565080c658d0 executing computations on platform Host. Devices:
[2m[36m(pid=30769)[0m 2019-07-17 13:49:00.620762: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.576099: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.584593: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.587841: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.587897: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.587910: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.588024: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.588058: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.588069: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.593747: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.594438: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555db14018d0 executing computations on platform Host. Devices:
[2m[36m(pid=30786)[0m 2019-07-17 13:49:00.594465: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30786)[0m W0717 13:49:00.602141 140652493403584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30786)[0m Instructions for updating:
[2m[36m(pid=30786)[0m Use keras.layers.dense instead.
[2m[36m(pid=30772)[0m [32m [     0.03579s,  INFO] TimeLimit:
[2m[36m(pid=30772)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30772)[0m - action_space = Box(2,)
[2m[36m(pid=30772)[0m - observation_space = Box(9,)
[2m[36m(pid=30772)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30772)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30772)[0m - _max_episode_steps = 150
[2m[36m(pid=30772)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00,592	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.615942: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30769)[0m [32m [     0.03999s,  INFO] TimeLimit:
[2m[36m(pid=30769)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30769)[0m - action_space = Box(2,)
[2m[36m(pid=30769)[0m - observation_space = Box(9,)
[2m[36m(pid=30769)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30769)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30769)[0m - _max_episode_steps = 150
[2m[36m(pid=30769)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.577559: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.594812: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.598545: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.598613: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.598628: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.598758: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.598800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.598814: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.614081: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.614592: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5642e56518d0 executing computations on platform Host. Devices:
[2m[36m(pid=30768)[0m 2019-07-17 13:49:00.614622: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.577211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.580575: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.580621: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.580629: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.580722: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.580746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.580753: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.582465: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.582805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5576747198d0 executing computations on platform Host. Devices:
[2m[36m(pid=30775)[0m 2019-07-17 13:49:00.582829: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30775)[0m W0717 13:49:00.598807 140577086166464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30775)[0m Instructions for updating:
[2m[36m(pid=30775)[0m Use keras.layers.dense instead.
[2m[36m(pid=30785)[0m [32m [     0.03551s,  INFO] TimeLimit:
[2m[36m(pid=30785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30785)[0m - action_space = Box(2,)
[2m[36m(pid=30785)[0m - observation_space = Box(9,)
[2m[36m(pid=30785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30785)[0m - _max_episode_steps = 150
[2m[36m(pid=30785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00,600	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.621368: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30793)[0m W0717 13:49:00.574657 140216272246208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30793)[0m Instructions for updating:
[2m[36m(pid=30793)[0m Use keras.layers.dense instead.
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.623721: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.626938: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.626990: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.627004: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.627114: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.627153: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.627165: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.629083: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.629778: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.630197: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561f9ba298d0 executing computations on platform Host. Devices:
[2m[36m(pid=30772)[0m 2019-07-17 13:49:00.630219: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30768)[0m W0717 13:49:00.629305 139963640579520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30768)[0m Instructions for updating:
[2m[36m(pid=30768)[0m Use keras.layers.dense instead.
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.632312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.632365: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.632378: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.632488: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.632522: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.632533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.638343: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.647779: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.651174: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.651237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.651252: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.651421: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.651460: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.651472: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.656645: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.658068: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5652340ee8d0 executing computations on platform Host. Devices:
[2m[36m(pid=30767)[0m 2019-07-17 13:49:00.658104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30767)[0m W0717 13:49:00.670114 139884531652032 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30767)[0m Instructions for updating:
[2m[36m(pid=30767)[0m Use keras.layers.dense instead.
[2m[36m(pid=30769)[0m W0717 13:49:00.639547 140365879743936 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30769)[0m Instructions for updating:
[2m[36m(pid=30769)[0m Use keras.layers.dense instead.
[2m[36m(pid=30772)[0m W0717 13:49:00.639764 139689823782336 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30772)[0m Instructions for updating:
[2m[36m(pid=30772)[0m Use keras.layers.dense instead.
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.637570: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.639698: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5574decb88d0 executing computations on platform Host. Devices:
[2m[36m(pid=30785)[0m 2019-07-17 13:49:00.639727: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=30785)[0m W0717 13:49:00.665068 139805129692608 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=30785)[0m Instructions for updating:
[2m[36m(pid=30785)[0m Use keras.layers.dense instead.
[2m[36m(pid=30779)[0m W0717 13:49:01.068188 140653810357696 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30779)[0m Instructions for updating:
[2m[36m(pid=30779)[0m Use `tf.cast` instead.
[2m[36m(pid=30788)[0m W0717 13:49:01.074750 139726508664256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30788)[0m Instructions for updating:
[2m[36m(pid=30788)[0m Use `tf.cast` instead.
[2m[36m(pid=30781)[0m W0717 13:49:01.069525 140431634224576 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30781)[0m Instructions for updating:
[2m[36m(pid=30781)[0m Use `tf.cast` instead.
[2m[36m(pid=30793)[0m W0717 13:49:01.121283 140216272246208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30793)[0m Instructions for updating:
[2m[36m(pid=30793)[0m Use `tf.cast` instead.
[2m[36m(pid=30786)[0m W0717 13:49:01.158322 140652493403584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30786)[0m Instructions for updating:
[2m[36m(pid=30786)[0m Use `tf.cast` instead.
[2m[36m(pid=30772)[0m W0717 13:49:01.192502 139689823782336 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30772)[0m Instructions for updating:
[2m[36m(pid=30772)[0m Use `tf.cast` instead.
[2m[36m(pid=30775)[0m W0717 13:49:01.171881 140577086166464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30775)[0m Instructions for updating:
[2m[36m(pid=30775)[0m Use `tf.cast` instead.
[2m[36m(pid=30785)[0m W0717 13:49:01.207633 139805129692608 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30785)[0m Instructions for updating:
[2m[36m(pid=30785)[0m Use `tf.cast` instead.
[2m[36m(pid=30779)[0m 2019-07-17 13:49:01.206823: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30781)[0m 2019-07-17 13:49:01.206970: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30769)[0m W0717 13:49:01.231690 140365879743936 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30769)[0m Instructions for updating:
[2m[36m(pid=30769)[0m Use `tf.cast` instead.
[2m[36m(pid=30768)[0m W0717 13:49:01.213600 139963640579520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30768)[0m Instructions for updating:
[2m[36m(pid=30768)[0m Use `tf.cast` instead.
[2m[36m(pid=30793)[0m 2019-07-17 13:49:01.260294: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30788)[0m 2019-07-17 13:49:01.213825: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30781)[0m 2019-07-17 13:49:01,246	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30781)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30781)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30781)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30781)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=30781)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30781)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30781)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30781)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30781)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30781)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30781)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30767)[0m W0717 13:49:01.264861 139884531652032 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30767)[0m Instructions for updating:
[2m[36m(pid=30767)[0m Use `tf.cast` instead.
[2m[36m(pid=30786)[0m 2019-07-17 13:49:01.303154: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30772)[0m 2019-07-17 13:49:01.341071: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30768)[0m 2019-07-17 13:49:01.356598: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30775)[0m 2019-07-17 13:49:01.320990: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30785)[0m 2019-07-17 13:49:01.353724: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30779)[0m W0717 13:49:01.336715 140653810357696 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30779)[0m Instructions for updating:
[2m[36m(pid=30779)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30788)[0m W0717 13:49:01.342804 139726508664256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30788)[0m Instructions for updating:
[2m[36m(pid=30788)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30781)[0m W0717 13:49:01.339906 140431634224576 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30781)[0m Instructions for updating:
[2m[36m(pid=30781)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30767)[0m 2019-07-17 13:49:01.409586: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30769)[0m 2019-07-17 13:49:01.390517: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=30793)[0m W0717 13:49:01.381893 140216272246208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30793)[0m Instructions for updating:
[2m[36m(pid=30793)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30786)[0m W0717 13:49:01.429476 140652493403584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30786)[0m Instructions for updating:
[2m[36m(pid=30786)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30772)[0m W0717 13:49:01.462260 139689823782336 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30772)[0m Instructions for updating:
[2m[36m(pid=30772)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30775)[0m W0717 13:49:01.476646 140577086166464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30775)[0m Instructions for updating:
[2m[36m(pid=30775)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30785)[0m W0717 13:49:01.476378 139805129692608 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30785)[0m Instructions for updating:
[2m[36m(pid=30785)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30769)[0m W0717 13:49:01.520672 140365879743936 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30769)[0m Instructions for updating:
[2m[36m(pid=30769)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30768)[0m W0717 13:49:01.480844 139963640579520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30768)[0m Instructions for updating:
[2m[36m(pid=30768)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30767)[0m W0717 13:49:01.535516 139884531652032 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=30767)[0m Instructions for updating:
[2m[36m(pid=30767)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=30788)[0m [32m [     2.55988s,  INFO] TimeLimit:
[2m[36m(pid=30788)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30788)[0m - action_space = Box(2,)
[2m[36m(pid=30788)[0m - observation_space = Box(9,)
[2m[36m(pid=30788)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30788)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30788)[0m - _max_episode_steps = 150
[2m[36m(pid=30788)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30788)[0m [32m [     2.56066s,  INFO] TimeLimit:
[2m[36m(pid=30788)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30788)[0m - action_space = Box(2,)
[2m[36m(pid=30788)[0m - observation_space = Box(9,)
[2m[36m(pid=30788)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30788)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30788)[0m - _max_episode_steps = 150
[2m[36m(pid=30788)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30788)[0m [32m [     2.56137s,  INFO] TimeLimit:
[2m[36m(pid=30788)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30788)[0m - action_space = Box(2,)
[2m[36m(pid=30788)[0m - observation_space = Box(9,)
[2m[36m(pid=30788)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30788)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30788)[0m - _max_episode_steps = 150
[2m[36m(pid=30788)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30779)[0m [32m [     2.56607s,  INFO] TimeLimit:
[2m[36m(pid=30779)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30779)[0m - action_space = Box(2,)
[2m[36m(pid=30779)[0m - observation_space = Box(9,)
[2m[36m(pid=30779)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30779)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30779)[0m - _max_episode_steps = 150
[2m[36m(pid=30779)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30779)[0m [32m [     2.56699s,  INFO] TimeLimit:
[2m[36m(pid=30779)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30779)[0m - action_space = Box(2,)
[2m[36m(pid=30779)[0m - observation_space = Box(9,)
[2m[36m(pid=30779)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30779)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30779)[0m - _max_episode_steps = 150
[2m[36m(pid=30779)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30779)[0m [32m [     2.56772s,  INFO] TimeLimit:
[2m[36m(pid=30779)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30779)[0m - action_space = Box(2,)
[2m[36m(pid=30779)[0m - observation_space = Box(9,)
[2m[36m(pid=30779)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30779)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30779)[0m - _max_episode_steps = 150
[2m[36m(pid=30779)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30793)[0m [32m [     2.60194s,  INFO] TimeLimit:
[2m[36m(pid=30793)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30793)[0m - action_space = Box(2,)
[2m[36m(pid=30793)[0m - observation_space = Box(9,)
[2m[36m(pid=30793)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30793)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30793)[0m - _max_episode_steps = 150
[2m[36m(pid=30793)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30793)[0m [32m [     2.60271s,  INFO] TimeLimit:
[2m[36m(pid=30793)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30793)[0m - action_space = Box(2,)
[2m[36m(pid=30793)[0m - observation_space = Box(9,)
[2m[36m(pid=30793)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30793)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30793)[0m - _max_episode_steps = 150
[2m[36m(pid=30793)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30793)[0m [32m [     2.60345s,  INFO] TimeLimit:
[2m[36m(pid=30793)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30793)[0m - action_space = Box(2,)
[2m[36m(pid=30793)[0m - observation_space = Box(9,)
[2m[36m(pid=30793)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30793)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30793)[0m - _max_episode_steps = 150
[2m[36m(pid=30793)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30772)[0m [32m [     2.62226s,  INFO] TimeLimit:
[2m[36m(pid=30772)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30772)[0m - action_space = Box(2,)
[2m[36m(pid=30772)[0m - observation_space = Box(9,)
[2m[36m(pid=30772)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30772)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30772)[0m - _max_episode_steps = 150
[2m[36m(pid=30772)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30772)[0m [32m [     2.62298s,  INFO] TimeLimit:
[2m[36m(pid=30772)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30772)[0m - action_space = Box(2,)
[2m[36m(pid=30772)[0m - observation_space = Box(9,)
[2m[36m(pid=30772)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30772)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30772)[0m - _max_episode_steps = 150
[2m[36m(pid=30772)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30772)[0m [32m [     2.62375s,  INFO] TimeLimit:
[2m[36m(pid=30772)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30772)[0m - action_space = Box(2,)
[2m[36m(pid=30772)[0m - observation_space = Box(9,)
[2m[36m(pid=30772)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30772)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30772)[0m - _max_episode_steps = 150
[2m[36m(pid=30772)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30785)[0m [32m [     2.59757s,  INFO] TimeLimit:
[2m[36m(pid=30785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30785)[0m - action_space = Box(2,)
[2m[36m(pid=30785)[0m - observation_space = Box(9,)
[2m[36m(pid=30785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30785)[0m - _max_episode_steps = 150
[2m[36m(pid=30785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30785)[0m [32m [     2.59809s,  INFO] TimeLimit:
[2m[36m(pid=30785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30785)[0m - action_space = Box(2,)
[2m[36m(pid=30785)[0m - observation_space = Box(9,)
[2m[36m(pid=30785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30785)[0m - _max_episode_steps = 150
[2m[36m(pid=30785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30785)[0m [32m [     2.59859s,  INFO] TimeLimit:
[2m[36m(pid=30785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30785)[0m - action_space = Box(2,)
[2m[36m(pid=30785)[0m - observation_space = Box(9,)
[2m[36m(pid=30785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30785)[0m - _max_episode_steps = 150
[2m[36m(pid=30785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30781)[0m [32m [     2.70191s,  INFO] TimeLimit:
[2m[36m(pid=30781)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30781)[0m - action_space = Box(2,)
[2m[36m(pid=30781)[0m - observation_space = Box(9,)
[2m[36m(pid=30781)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30781)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30781)[0m - _max_episode_steps = 150
[2m[36m(pid=30781)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30781)[0m [32m [     2.70273s,  INFO] TimeLimit:
[2m[36m(pid=30781)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30781)[0m - action_space = Box(2,)
[2m[36m(pid=30781)[0m - observation_space = Box(9,)
[2m[36m(pid=30781)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30781)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30781)[0m - _max_episode_steps = 150
[2m[36m(pid=30781)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30781)[0m [32m [     2.70342s,  INFO] TimeLimit:
[2m[36m(pid=30781)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30781)[0m - action_space = Box(2,)
[2m[36m(pid=30781)[0m - observation_space = Box(9,)
[2m[36m(pid=30781)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30781)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30781)[0m - _max_episode_steps = 150
[2m[36m(pid=30781)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30767)[0m [32m [     2.62097s,  INFO] TimeLimit:
[2m[36m(pid=30767)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30767)[0m - action_space = Box(2,)
[2m[36m(pid=30767)[0m - observation_space = Box(9,)
[2m[36m(pid=30767)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30767)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30767)[0m - _max_episode_steps = 150
[2m[36m(pid=30767)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30767)[0m [32m [     2.62168s,  INFO] TimeLimit:
[2m[36m(pid=30767)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30767)[0m - action_space = Box(2,)
[2m[36m(pid=30767)[0m - observation_space = Box(9,)
[2m[36m(pid=30767)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30767)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30767)[0m - _max_episode_steps = 150
[2m[36m(pid=30767)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30767)[0m [32m [     2.62227s,  INFO] TimeLimit:
[2m[36m(pid=30767)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30767)[0m - action_space = Box(2,)
[2m[36m(pid=30767)[0m - observation_space = Box(9,)
[2m[36m(pid=30767)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30767)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30767)[0m - _max_episode_steps = 150
[2m[36m(pid=30767)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30786)[0m [32m [     2.68581s,  INFO] TimeLimit:
[2m[36m(pid=30786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30786)[0m - action_space = Box(2,)
[2m[36m(pid=30786)[0m - observation_space = Box(9,)
[2m[36m(pid=30786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30786)[0m - _max_episode_steps = 150
[2m[36m(pid=30786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30786)[0m [32m [     2.68628s,  INFO] TimeLimit:
[2m[36m(pid=30786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30786)[0m - action_space = Box(2,)
[2m[36m(pid=30786)[0m - observation_space = Box(9,)
[2m[36m(pid=30786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30786)[0m - _max_episode_steps = 150
[2m[36m(pid=30786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30786)[0m [32m [     2.68672s,  INFO] TimeLimit:
[2m[36m(pid=30786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30786)[0m - action_space = Box(2,)
[2m[36m(pid=30786)[0m - observation_space = Box(9,)
[2m[36m(pid=30786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30786)[0m - _max_episode_steps = 150
[2m[36m(pid=30786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30768)[0m [32m [     2.70449s,  INFO] TimeLimit:
[2m[36m(pid=30768)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30768)[0m - action_space = Box(2,)
[2m[36m(pid=30768)[0m - observation_space = Box(9,)
[2m[36m(pid=30768)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30768)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30768)[0m - _max_episode_steps = 150
[2m[36m(pid=30768)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30768)[0m [32m [     2.70490s,  INFO] TimeLimit:
[2m[36m(pid=30768)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30768)[0m - action_space = Box(2,)
[2m[36m(pid=30768)[0m - observation_space = Box(9,)
[2m[36m(pid=30768)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30768)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30768)[0m - _max_episode_steps = 150
[2m[36m(pid=30768)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30768)[0m [32m [     2.70528s,  INFO] TimeLimit:
[2m[36m(pid=30768)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30768)[0m - action_space = Box(2,)
[2m[36m(pid=30768)[0m - observation_space = Box(9,)
[2m[36m(pid=30768)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30768)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30768)[0m - _max_episode_steps = 150
[2m[36m(pid=30768)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30769)[0m [32m [     2.68033s,  INFO] TimeLimit:
[2m[36m(pid=30769)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30769)[0m - action_space = Box(2,)
[2m[36m(pid=30769)[0m - observation_space = Box(9,)
[2m[36m(pid=30769)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30769)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30769)[0m - _max_episode_steps = 150
[2m[36m(pid=30769)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30769)[0m [32m [     2.68074s,  INFO] TimeLimit:
[2m[36m(pid=30769)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30769)[0m - action_space = Box(2,)
[2m[36m(pid=30769)[0m - observation_space = Box(9,)
[2m[36m(pid=30769)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30769)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30769)[0m - _max_episode_steps = 150
[2m[36m(pid=30769)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30769)[0m [32m [     2.68113s,  INFO] TimeLimit:
[2m[36m(pid=30769)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30769)[0m - action_space = Box(2,)
[2m[36m(pid=30769)[0m - observation_space = Box(9,)
[2m[36m(pid=30769)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30769)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30769)[0m - _max_episode_steps = 150
[2m[36m(pid=30769)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30775)[0m [32m [     2.68831s,  INFO] TimeLimit:
[2m[36m(pid=30775)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30775)[0m - action_space = Box(2,)
[2m[36m(pid=30775)[0m - observation_space = Box(9,)
[2m[36m(pid=30775)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30775)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30775)[0m - _max_episode_steps = 150
[2m[36m(pid=30775)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30775)[0m [32m [     2.68894s,  INFO] TimeLimit:
[2m[36m(pid=30775)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30775)[0m - action_space = Box(2,)
[2m[36m(pid=30775)[0m - observation_space = Box(9,)
[2m[36m(pid=30775)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30775)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30775)[0m - _max_episode_steps = 150
[2m[36m(pid=30775)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30775)[0m [32m [     2.68951s,  INFO] TimeLimit:
[2m[36m(pid=30775)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30775)[0m - action_space = Box(2,)
[2m[36m(pid=30775)[0m - observation_space = Box(9,)
[2m[36m(pid=30775)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30775)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30775)[0m - _max_episode_steps = 150
[2m[36m(pid=30775)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397332: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397424: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_1 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_1/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_1/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397489: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_2 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_2/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_2/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397544: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_3 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_3/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_3/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397600: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_4 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_4/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_4/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397662: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_5 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_5/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_5/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397718: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_6 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_6/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_6/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:03.397777: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=30796)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=30796)[0m See below for details of this colocation group:
[2m[36m(pid=30796)[0m Colocation Debug Info:
[2m[36m(pid=30796)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=30796)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=30796)[0m Assign: CPU 
[2m[36m(pid=30796)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=30796)[0m VariableV2: CPU 
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_7 (VariableV2) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_7/Assign (Assign) /device:GPU:0
[2m[36m(pid=30796)[0m   default_policy_1/tower_1/Variable_7/read (Identity) /device:GPU:0
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m W0717 13:49:04.004059 139880310220224 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30796)[0m Instructions for updating:
[2m[36m(pid=30796)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30767)[0m W0717 13:49:04.052144 139884531652032 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30767)[0m Instructions for updating:
[2m[36m(pid=30767)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30769)[0m W0717 13:49:04.038276 140365879743936 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30769)[0m Instructions for updating:
[2m[36m(pid=30769)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30786)[0m W0717 13:49:04.038067 140652493403584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30786)[0m Instructions for updating:
[2m[36m(pid=30786)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30772)[0m W0717 13:49:04.038291 139689823782336 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30772)[0m Instructions for updating:
[2m[36m(pid=30772)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30768)[0m W0717 13:49:04.038186 139963640579520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30768)[0m Instructions for updating:
[2m[36m(pid=30768)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30775)[0m W0717 13:49:04.038009 140577086166464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30775)[0m Instructions for updating:
[2m[36m(pid=30775)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30785)[0m W0717 13:49:04.038623 139805129692608 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30785)[0m Instructions for updating:
[2m[36m(pid=30785)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30779)[0m W0717 13:49:04.037684 140653810357696 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30779)[0m Instructions for updating:
[2m[36m(pid=30779)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30793)[0m W0717 13:49:04.037910 140216272246208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30793)[0m Instructions for updating:
[2m[36m(pid=30793)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30788)[0m W0717 13:49:04.037776 139726508664256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30788)[0m Instructions for updating:
[2m[36m(pid=30788)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30781)[0m W0717 13:49:04.037640 140431634224576 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=30781)[0m Instructions for updating:
[2m[36m(pid=30781)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,567	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,613	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.531, max=0.848, mean=0.09)},
[2m[36m(pid=30781)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.725, max=0.991, mean=0.109)},
[2m[36m(pid=30781)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.977, max=0.517, mean=-0.079)},
[2m[36m(pid=30781)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.953, max=0.168, mean=-0.233)}}
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,613	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=30781)[0m   1: {'agent0': None},
[2m[36m(pid=30781)[0m   2: {'agent0': None},
[2m[36m(pid=30781)[0m   3: {'agent0': None}}
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,613	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.531, max=0.848, mean=0.09)
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,613	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,616	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30781)[0m                                   'env_id': 0,
[2m[36m(pid=30781)[0m                                   'info': None,
[2m[36m(pid=30781)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30781)[0m                                   'rnn_state': []},
[2m[36m(pid=30781)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30781)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30781)[0m                                   'env_id': 1,
[2m[36m(pid=30781)[0m                                   'info': None,
[2m[36m(pid=30781)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=0.079),
[2m[36m(pid=30781)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30781)[0m                                   'rnn_state': []},
[2m[36m(pid=30781)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30781)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30781)[0m                                   'env_id': 2,
[2m[36m(pid=30781)[0m                                   'info': None,
[2m[36m(pid=30781)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.121, max=0.889, mean=-0.384),
[2m[36m(pid=30781)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30781)[0m                                   'rnn_state': []},
[2m[36m(pid=30781)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30781)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30781)[0m                                   'env_id': 3,
[2m[36m(pid=30781)[0m                                   'info': None,
[2m[36m(pid=30781)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.482, max=1.355, mean=-0.308),
[2m[36m(pid=30781)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30781)[0m                                   'rnn_state': []},
[2m[36m(pid=30781)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,616	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=30781)[0m 2019-07-17 13:49:04,725	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-0.778, max=1.112, mean=0.042),
[2m[36m(pid=30781)[0m                       [],
[2m[36m(pid=30781)[0m                       { 'action_prob': np.ndarray((4,), dtype=float32, min=0.084, max=0.128, mean=0.105),
[2m[36m(pid=30781)[0m                         'behaviour_logits': np.ndarray((4, 4), dtype=float32, min=-0.005, max=0.007, mean=0.0),
[2m[36m(pid=30781)[0m                         'vf_preds': np.ndarray((4,), dtype=float32, min=-0.003, max=0.006, mean=0.002)})}
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m 2019-07-17 13:49:05,165	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.001, max=0.157, mean=0.081),
[2m[36m(pid=30781)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-3.035, max=2.749, mean=0.044),
[2m[36m(pid=30781)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-17.765, max=6.833, mean=-1.421),
[2m[36m(pid=30781)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.009, max=0.01, mean=0.001),
[2m[36m(pid=30781)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=30781)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=59071671.0, max=59071671.0, mean=59071671.0),
[2m[36m(pid=30781)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=30781)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-3.175, max=3.322, mean=0.094),
[2m[36m(pid=30781)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-3.175, max=3.322, mean=0.095),
[2m[36m(pid=30781)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-3.035, max=2.749, mean=0.04),
[2m[36m(pid=30781)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-4.88, max=2.336, mean=-0.124),
[2m[36m(pid=30781)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-4.88, max=2.336, mean=-0.125),
[2m[36m(pid=30781)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=30781)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-17.767, max=6.827, mean=-1.425),
[2m[36m(pid=30781)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.009, max=0.003, mean=-0.004)},
[2m[36m(pid=30781)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m 2019-07-17 13:49:05,697	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30781)[0m { 'data': { 'action_prob': np.ndarray((900,), dtype=float32, min=0.0, max=0.159, mean=0.08),
[2m[36m(pid=30781)[0m             'actions': np.ndarray((900, 2), dtype=float32, min=-3.301, max=3.978, mean=-0.001),
[2m[36m(pid=30781)[0m             'advantages': np.ndarray((900,), dtype=float32, min=-35.473, max=17.936, mean=-4.326),
[2m[36m(pid=30781)[0m             'agent_index': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30781)[0m             'behaviour_logits': np.ndarray((900, 4), dtype=float32, min=-0.011, max=0.01, mean=-0.0),
[2m[36m(pid=30781)[0m             'dones': np.ndarray((900,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=30781)[0m             'eps_id': np.ndarray((900,), dtype=int64, min=59071671.0, max=1931680677.0, mean=924332003.667),
[2m[36m(pid=30781)[0m             'infos': np.ndarray((900,), dtype=object, head={}),
[2m[36m(pid=30781)[0m             'new_obs': np.ndarray((900, 9), dtype=float32, min=-4.65, max=3.762, mean=-0.001),
[2m[36m(pid=30781)[0m             'obs': np.ndarray((900, 9), dtype=float32, min=-4.65, max=3.762, mean=-0.001),
[2m[36m(pid=30781)[0m             'prev_actions': np.ndarray((900, 2), dtype=float32, min=-3.301, max=3.978, mean=-0.002),
[2m[36m(pid=30781)[0m             'prev_rewards': np.ndarray((900,), dtype=float32, min=-4.88, max=5.498, mean=-0.084),
[2m[36m(pid=30781)[0m             'rewards': np.ndarray((900,), dtype=float32, min=-4.88, max=5.498, mean=-0.083),
[2m[36m(pid=30781)[0m             't': np.ndarray((900,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=30781)[0m             'unroll_id': np.ndarray((900,), dtype=int64, min=0.0, max=1.0, mean=0.333),
[2m[36m(pid=30781)[0m             'value_targets': np.ndarray((900,), dtype=float32, min=-35.47, max=17.939, mean=-4.327),
[2m[36m(pid=30781)[0m             'vf_preds': np.ndarray((900,), dtype=float32, min=-0.009, max=0.008, mean=-0.001)},
[2m[36m(pid=30781)[0m   'type': 'SampleBatch'}
[2m[36m(pid=30781)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:06,932	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m { 'inputs': [ np.ndarray((26100, 2), dtype=float32, min=-4.103, max=4.251, mean=0.01),
[2m[36m(pid=30796)[0m               np.ndarray((26100,), dtype=float32, min=-8.771, max=6.305, mean=-0.099),
[2m[36m(pid=30796)[0m               np.ndarray((26100, 9), dtype=float32, min=-11.74, max=5.291, mean=-0.018),
[2m[36m(pid=30796)[0m               np.ndarray((26100, 2), dtype=float32, min=-4.103, max=4.251, mean=0.01),
[2m[36m(pid=30796)[0m               np.ndarray((26100,), dtype=float32, min=-3.509, max=3.365, mean=-0.0),
[2m[36m(pid=30796)[0m               np.ndarray((26100, 4), dtype=float32, min=-0.012, max=0.013, mean=0.0),
[2m[36m(pid=30796)[0m               np.ndarray((26100,), dtype=float32, min=-41.859, max=27.074, mean=-6.671),
[2m[36m(pid=30796)[0m               np.ndarray((26100,), dtype=float32, min=-0.011, max=0.013, mean=-0.0)],
[2m[36m(pid=30796)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30796)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=30796)[0m   'state_inputs': []}
[2m[36m(pid=30796)[0m 
[2m[36m(pid=30796)[0m 2019-07-17 13:49:06,932	INFO multi_gpu_impl.py:191 -- Divided 26100 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.86872008055539
  episode_reward_mean: -14.718279483580151
  episode_reward_min: -54.337840875534894
  episodes_this_iter: 174
  episodes_total: 174
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2850.874
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.833808660507202
        kl: 0.0011480951216071844
        policy_loss: -0.0033334041945636272
        total_loss: 89.38822174072266
        vf_explained_var: 0.12141762673854828
        vf_loss: 89.39041137695312
    load_time_ms: 41.762
    num_steps_sampled: 26100
    num_steps_trained: 26000
    sample_time_ms: 2438.018
    update_time_ms: 450.607
  iterations_since_restore: 1
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.060246063741817
    mean_inference_ms: 1.3254527061911312
    mean_processing_ms: 0.8531481041375113
  time_since_restore: 5.832701921463013
  time_this_iter_s: 5.832701921463013
  time_total_s: 5.832701921463013
  timestamp: 1563364149
  timesteps_since_restore: 26100
  timesteps_this_iter: 26100
  timesteps_total: 26100
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 5 s, 1 iter, 26100 ts, -14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.394454104363067
  episode_reward_mean: -15.05706979682046
  episode_reward_min: -55.400592876390405
  episodes_this_iter: 174
  episodes_total: 522
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2644.763
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.829148530960083
        kl: 0.005120725370943546
        policy_loss: -0.006705967243760824
        total_loss: 68.41022491455078
        vf_explained_var: 0.3713184595108032
        vf_loss: 68.41565704345703
    load_time_ms: 14.426
    num_steps_sampled: 78300
    num_steps_trained: 78000
    sample_time_ms: 2264.937
    update_time_ms: 152.952
  iterations_since_restore: 3
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0307850700512453
    mean_inference_ms: 1.292480678664886
    mean_processing_ms: 0.8657125416779223
  time_since_restore: 15.316078424453735
  time_this_iter_s: 4.847907543182373
  time_total_s: 15.316078424453735
  timestamp: 1563364159
  timesteps_since_restore: 78300
  timesteps_this_iter: 26100
  timesteps_total: 78300
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 15 s, 3 iter, 78300 ts, -15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.591599597930667
  episode_reward_mean: -11.171037713444187
  episode_reward_min: -39.22701061181474
  episodes_this_iter: 174
  episodes_total: 870
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2614.486
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.814758062362671
        kl: 0.005108923185616732
        policy_loss: -0.006358879152685404
        total_loss: 45.183677673339844
        vf_explained_var: 0.4055241048336029
        vf_loss: 45.188758850097656
    load_time_ms: 8.943
    num_steps_sampled: 130500
    num_steps_trained: 130000
    sample_time_ms: 2223.635
    update_time_ms: 93.599
  iterations_since_restore: 5
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0241638559104167
    mean_inference_ms: 1.2485624585815487
    mean_processing_ms: 0.867913712204488
  time_since_restore: 24.826483011245728
  time_this_iter_s: 4.707077503204346
  time_total_s: 24.826483011245728
  timestamp: 1563364168
  timesteps_since_restore: 130500
  timesteps_this_iter: 26100
  timesteps_total: 130500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 24 s, 5 iter, 130500 ts, -11.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.05920920790091
  episode_reward_mean: -6.837957728322251
  episode_reward_min: -41.10346051480742
  episodes_this_iter: 174
  episodes_total: 1044
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2632.704
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.8105671405792236
        kl: 0.0055461423471570015
        policy_loss: -0.006520721595734358
        total_loss: 38.62800979614258
        vf_explained_var: 0.3419669270515442
        vf_loss: 38.63314437866211
    load_time_ms: 7.568
    num_steps_sampled: 156600
    num_steps_trained: 156000
    sample_time_ms: 2237.97
    update_time_ms: 78.639
  iterations_since_restore: 6
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0310249132749074
    mean_inference_ms: 1.2582485494362152
    mean_processing_ms: 0.8812704631568952
  time_since_restore: 29.880218505859375
  time_this_iter_s: 5.0537354946136475
  time_total_s: 29.880218505859375
  timestamp: 1563364173
  timesteps_since_restore: 156600
  timesteps_this_iter: 26100
  timesteps_total: 156600
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 29 s, 6 iter, 156600 ts, -6.84 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.358786638714452
  episode_reward_mean: -6.950790953954282
  episode_reward_min: -35.23647401467821
  episodes_this_iter: 174
  episodes_total: 1392
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2619.415
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.795696258544922
        kl: 0.0065489537082612514
        policy_loss: -0.007957309484481812
        total_loss: 29.170223236083984
        vf_explained_var: 0.4136199653148651
        vf_loss: 29.17654037475586
    load_time_ms: 5.869
    num_steps_sampled: 208800
    num_steps_trained: 208000
    sample_time_ms: 2250.697
    update_time_ms: 59.983
  iterations_since_restore: 8
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.038311344682399
    mean_inference_ms: 1.2509815788052827
    mean_processing_ms: 0.8922146427870234
  time_since_restore: 39.659499168395996
  time_this_iter_s: 4.9167799949646
  time_total_s: 39.659499168395996
  timestamp: 1563364183
  timesteps_since_restore: 208800
  timesteps_this_iter: 26100
  timesteps_total: 208800
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 39 s, 8 iter, 208800 ts, -6.95 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.937985170567742
  episode_reward_mean: -2.4592841554537572
  episode_reward_min: -34.92284831179417
  episodes_this_iter: 174
  episodes_total: 1740
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2615.903
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.775423288345337
        kl: 0.006405059713870287
        policy_loss: -0.007957752794027328
        total_loss: 23.454572677612305
        vf_explained_var: 0.5112282037734985
        vf_loss: 23.460927963256836
    load_time_ms: 4.837
    num_steps_sampled: 261000
    num_steps_trained: 260000
    sample_time_ms: 2253.356
    update_time_ms: 48.85
  iterations_since_restore: 10
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0458477827178168
    mean_inference_ms: 1.2511658408014343
    mean_processing_ms: 0.9009115511429885
  time_since_restore: 49.43664360046387
  time_this_iter_s: 4.8215415477752686
  time_total_s: 49.43664360046387
  timestamp: 1563364193
  timesteps_since_restore: 261000
  timesteps_this_iter: 26100
  timesteps_total: 261000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 49 s, 10 iter, 261000 ts, -2.46 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-49-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.510878871583202
  episode_reward_mean: -1.4358534521963517
  episode_reward_min: -27.40594958706957
  episodes_this_iter: 174
  episodes_total: 1914
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2599.502
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7689318656921387
        kl: 0.0055820695124566555
        policy_loss: -0.00791919231414795
        total_loss: 18.10051918029785
        vf_explained_var: 0.641930341720581
        vf_loss: 18.10704231262207
    load_time_ms: 0.737
    num_steps_sampled: 287100
    num_steps_trained: 286000
    sample_time_ms: 2238.82
    update_time_ms: 4.269
  iterations_since_restore: 11
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490816937795175
    mean_inference_ms: 1.2506601475146628
    mean_processing_ms: 0.9052891651766759
  time_since_restore: 54.44135904312134
  time_this_iter_s: 5.004715442657471
  time_total_s: 54.44135904312134
  timestamp: 1563364198
  timesteps_since_restore: 287100
  timesteps_this_iter: 26100
  timesteps_total: 287100
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 54 s, 11 iter, 287100 ts, -1.44 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.260885907139127
  episode_reward_mean: -0.5162735184932948
  episode_reward_min: -34.061345837633965
  episodes_this_iter: 174
  episodes_total: 2262
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2596.299
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7502024173736572
        kl: 0.006092623341828585
        policy_loss: -0.008955002762377262
        total_loss: 17.300209045410156
        vf_explained_var: 0.6058140397071838
        vf_loss: 17.307640075683594
    load_time_ms: 0.728
    num_steps_sampled: 339300
    num_steps_trained: 338000
    sample_time_ms: 2236.54
    update_time_ms: 4.398
  iterations_since_restore: 13
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0449235251205302
    mean_inference_ms: 1.2452989792505476
    mean_processing_ms: 0.9021995739217289
  time_since_restore: 63.86821508407593
  time_this_iter_s: 4.4968812465667725
  time_total_s: 63.86821508407593
  timestamp: 1563364208
  timesteps_since_restore: 339300
  timesteps_this_iter: 26100
  timesteps_total: 339300
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 63 s, 13 iter, 339300 ts, -0.516 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.296919942652902
  episode_reward_mean: 0.8259720266150578
  episode_reward_min: -39.80769748940214
  episodes_this_iter: 174
  episodes_total: 2610
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2564.945
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7341482639312744
        kl: 0.00782005488872528
        policy_loss: -0.010731581598520279
        total_loss: 16.815242767333984
        vf_explained_var: 0.6863707304000854
        vf_loss: 16.824018478393555
    load_time_ms: 0.732
    num_steps_sampled: 391500
    num_steps_trained: 390000
    sample_time_ms: 2236.58
    update_time_ms: 4.261
  iterations_since_restore: 15
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0391047092608792
    mean_inference_ms: 1.2371548293120502
    mean_processing_ms: 0.8976392539501371
  time_since_restore: 73.05794763565063
  time_this_iter_s: 4.532285451889038
  time_total_s: 73.05794763565063
  timestamp: 1563364217
  timesteps_since_restore: 391500
  timesteps_this_iter: 26100
  timesteps_total: 391500
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 73 s, 15 iter, 391500 ts, 0.826 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.114735889178938
  episode_reward_mean: 4.715697806375707
  episode_reward_min: -17.605685328519773
  episodes_this_iter: 174
  episodes_total: 2958
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2551.875
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.723477840423584
        kl: 0.006988226901739836
        policy_loss: -0.009011806920170784
        total_loss: 10.24459171295166
        vf_explained_var: 0.7696595788002014
        vf_loss: 10.251856803894043
    load_time_ms: 0.744
    num_steps_sampled: 443700
    num_steps_trained: 442000
    sample_time_ms: 2206.348
    update_time_ms: 4.271
  iterations_since_restore: 17
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.033513216676084
    mean_inference_ms: 1.230889408296934
    mean_processing_ms: 0.8954570838522392
  time_since_restore: 82.54229736328125
  time_this_iter_s: 4.634375095367432
  time_total_s: 82.54229736328125
  timestamp: 1563364226
  timesteps_since_restore: 443700
  timesteps_this_iter: 26100
  timesteps_total: 443700
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 82 s, 17 iter, 443700 ts, 4.72 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.693816630754675
  episode_reward_mean: 5.881708961747666
  episode_reward_min: -29.913289752132894
  episodes_this_iter: 174
  episodes_total: 3132
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2569.712
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7138609886169434
        kl: 0.007508258335292339
        policy_loss: -0.010779572650790215
        total_loss: 10.370786666870117
        vf_explained_var: 0.7668028473854065
        vf_loss: 10.379690170288086
    load_time_ms: 0.73
    num_steps_sampled: 469800
    num_steps_trained: 468000
    sample_time_ms: 2202.754
    update_time_ms: 4.308
  iterations_since_restore: 18
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0330939422253045
    mean_inference_ms: 1.2303822929213961
    mean_processing_ms: 0.8951028269145861
  time_since_restore: 87.60124897956848
  time_this_iter_s: 5.0589516162872314
  time_total_s: 87.60124897956848
  timestamp: 1563364231
  timesteps_since_restore: 469800
  timesteps_this_iter: 26100
  timesteps_total: 469800
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 87 s, 18 iter, 469800 ts, 5.88 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.33316131901869
  episode_reward_mean: 6.4773499829128856
  episode_reward_min: -22.558782338271087
  episodes_this_iter: 174
  episodes_total: 3306
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2571.752
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.7016894817352295
        kl: 0.007479807361960411
        policy_loss: -0.010485549457371235
        total_loss: 9.00343132019043
        vf_explained_var: 0.7912009358406067
        vf_loss: 9.012046813964844
    load_time_ms: 0.733
    num_steps_sampled: 495900
    num_steps_trained: 494000
    sample_time_ms: 2205.757
    update_time_ms: 4.364
  iterations_since_restore: 19
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0359744251348526
    mean_inference_ms: 1.2317660438652185
    mean_processing_ms: 0.8974466284968454
  time_since_restore: 92.605717420578
  time_this_iter_s: 5.0044684410095215
  time_total_s: 92.605717420578
  timestamp: 1563364236
  timesteps_since_restore: 495900
  timesteps_this_iter: 26100
  timesteps_total: 495900
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 92 s, 19 iter, 495900 ts, 6.48 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.138787092183865
  episode_reward_mean: 7.809928146202843
  episode_reward_min: -15.1884388927312
  episodes_this_iter: 174
  episodes_total: 3480
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2597.329
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.694816827774048
        kl: 0.007728229742497206
        policy_loss: -0.01128496415913105
        total_loss: 7.411453723907471
        vf_explained_var: 0.8290706276893616
        vf_loss: 7.420806407928467
    load_time_ms: 0.734
    num_steps_sampled: 522000
    num_steps_trained: 520000
    sample_time_ms: 2198.328
    update_time_ms: 4.45
  iterations_since_restore: 20
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0369155916359554
    mean_inference_ms: 1.2318378697110617
    mean_processing_ms: 0.9001774104203268
  time_since_restore: 97.60820269584656
  time_this_iter_s: 5.002485275268555
  time_total_s: 97.60820269584656
  timestamp: 1563364241
  timesteps_since_restore: 522000
  timesteps_this_iter: 26100
  timesteps_total: 522000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 97 s, 20 iter, 522000 ts, 7.81 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.93185140477762
  episode_reward_mean: 7.020554575811006
  episode_reward_min: -17.340856854391237
  episodes_this_iter: 174
  episodes_total: 3654
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2613.548
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.6804873943328857
        kl: 0.00786907784640789
        policy_loss: -0.012361699715256691
        total_loss: 4.80671501159668
        vf_explained_var: 0.862074613571167
        vf_loss: 4.817109107971191
    load_time_ms: 0.733
    num_steps_sampled: 548100
    num_steps_trained: 546000
    sample_time_ms: 2206.307
    update_time_ms: 4.385
  iterations_since_restore: 21
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0388815536715243
    mean_inference_ms: 1.2312061996118395
    mean_processing_ms: 0.9010035019628351
  time_since_restore: 102.85135865211487
  time_this_iter_s: 5.2431559562683105
  time_total_s: 102.85135865211487
  timestamp: 1563364247
  timesteps_since_restore: 548100
  timesteps_this_iter: 26100
  timesteps_total: 548100
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 102 s, 21 iter, 548100 ts, 7.02 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-50-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.34512105364311
  episode_reward_mean: 7.098478799547751
  episode_reward_min: -11.488092223083859
  episodes_this_iter: 174
  episodes_total: 3828
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2624.62
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.6639404296875
        kl: 0.008683893829584122
        policy_loss: -0.011545018292963505
        total_loss: 4.899655818939209
        vf_explained_var: 0.8594068884849548
        vf_loss: 4.909029960632324
    load_time_ms: 0.734
    num_steps_sampled: 574200
    num_steps_trained: 572000
    sample_time_ms: 2203.69
    update_time_ms: 4.276
  iterations_since_restore: 22
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0394920873136153
    mean_inference_ms: 1.230568463180754
    mean_processing_ms: 0.9020501641979483
  time_since_restore: 107.86731624603271
  time_this_iter_s: 5.015957593917847
  time_total_s: 107.86731624603271
  timestamp: 1563364252
  timesteps_since_restore: 574200
  timesteps_this_iter: 26100
  timesteps_total: 574200
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 107 s, 22 iter, 574200 ts, 7.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.366508624915163
  episode_reward_mean: 9.14247286618896
  episode_reward_min: -15.107488158854535
  episodes_this_iter: 174
  episodes_total: 4176
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2688.238
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.638291358947754
        kl: 0.00851464830338955
        policy_loss: -0.012255774810910225
        total_loss: 3.3415746688842773
        vf_explained_var: 0.9052215218544006
        vf_loss: 3.351701259613037
    load_time_ms: 0.737
    num_steps_sampled: 626400
    num_steps_trained: 624000
    sample_time_ms: 2243.038
    update_time_ms: 4.505
  iterations_since_restore: 24
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0441289453059786
    mean_inference_ms: 1.2355139822062269
    mean_processing_ms: 0.9057925649577188
  time_since_restore: 118.06322979927063
  time_this_iter_s: 5.26874303817749
  time_total_s: 118.06322979927063
  timestamp: 1563364262
  timesteps_since_restore: 626400
  timesteps_this_iter: 26100
  timesteps_total: 626400
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 118 s, 24 iter, 626400 ts, 9.14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.115710708256824
  episode_reward_mean: 10.078097241201437
  episode_reward_min: -11.996482301728948
  episodes_this_iter: 174
  episodes_total: 4524
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2706.548
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.6113057136535645
        kl: 0.006918476894497871
        policy_loss: -0.01100563071668148
        total_loss: 2.385890007019043
        vf_explained_var: 0.9270538091659546
        vf_loss: 2.3951659202575684
    load_time_ms: 0.733
    num_steps_sampled: 678600
    num_steps_trained: 676000
    sample_time_ms: 2243.87
    update_time_ms: 4.676
  iterations_since_restore: 26
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0404745711564323
    mean_inference_ms: 1.2313631175337862
    mean_processing_ms: 0.9034890804905379
  time_since_restore: 127.64119863510132
  time_this_iter_s: 4.679590702056885
  time_total_s: 127.64119863510132
  timestamp: 1563364271
  timesteps_since_restore: 678600
  timesteps_this_iter: 26100
  timesteps_total: 678600
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 127 s, 26 iter, 678600 ts, 10.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.91038586627308
  episode_reward_mean: 10.47099535967604
  episode_reward_min: -11.577938970388198
  episodes_this_iter: 174
  episodes_total: 4872
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2658.43
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5878102779388428
        kl: 0.008097883313894272
        policy_loss: -0.011631221510469913
        total_loss: 2.2150752544403076
        vf_explained_var: 0.9297804236412048
        vf_loss: 2.224681854248047
    load_time_ms: 0.738
    num_steps_sampled: 730800
    num_steps_trained: 728000
    sample_time_ms: 2228.113
    update_time_ms: 4.708
  iterations_since_restore: 28
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0387820304821374
    mean_inference_ms: 1.2277687026807036
    mean_processing_ms: 0.9022225818955865
  time_since_restore: 136.6965069770813
  time_this_iter_s: 4.60487961769104
  time_total_s: 136.6965069770813
  timestamp: 1563364281
  timesteps_since_restore: 730800
  timesteps_this_iter: 26100
  timesteps_total: 730800
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 136 s, 28 iter, 730800 ts, 10.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.221302660078642
  episode_reward_mean: 12.799141012955268
  episode_reward_min: -8.131926765493425
  episodes_this_iter: 174
  episodes_total: 5220
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2636.757
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5561022758483887
        kl: 0.007327900268137455
        policy_loss: -0.012148738838732243
        total_loss: 1.5083318948745728
        vf_explained_var: 0.9601948857307434
        vf_loss: 1.518648624420166
    load_time_ms: 0.739
    num_steps_sampled: 783000
    num_steps_trained: 780000
    sample_time_ms: 2206.189
    update_time_ms: 4.613
  iterations_since_restore: 30
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0349091739294836
    mean_inference_ms: 1.2231317591065085
    mean_processing_ms: 0.8995940978444795
  time_since_restore: 146.27095890045166
  time_this_iter_s: 4.74010443687439
  time_total_s: 146.27095890045166
  timestamp: 1563364290
  timesteps_since_restore: 783000
  timesteps_this_iter: 26100
  timesteps_total: 783000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 146 s, 30 iter, 783000 ts, 12.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.00648723104871
  episode_reward_mean: 11.53231755008095
  episode_reward_min: -15.46363123765891
  episodes_this_iter: 174
  episodes_total: 5394
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2620.151
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5470967292785645
        kl: 0.007307642139494419
        policy_loss: -0.011079193092882633
        total_loss: 1.5059950351715088
        vf_explained_var: 0.9563922882080078
        vf_loss: 1.5152473449707031
    load_time_ms: 0.736
    num_steps_sampled: 809100
    num_steps_trained: 806000
    sample_time_ms: 2205.099
    update_time_ms: 4.665
  iterations_since_restore: 31
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0406744608103735
    mean_inference_ms: 1.2297044648665791
    mean_processing_ms: 0.9034662245839592
  time_since_restore: 151.3401746749878
  time_this_iter_s: 5.069215774536133
  time_total_s: 151.3401746749878
  timestamp: 1563364295
  timesteps_since_restore: 809100
  timesteps_this_iter: 26100
  timesteps_total: 809100
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 151 s, 31 iter, 809100 ts, 11.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.96211770840106
  episode_reward_mean: 12.18787566408838
  episode_reward_min: -8.390985982156595
  episodes_this_iter: 174
  episodes_total: 5742
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2617.428
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5212197303771973
        kl: 0.007874120958149433
        policy_loss: -0.011051440611481667
        total_loss: 1.0646783113479614
        vf_explained_var: 0.9678903222084045
        vf_loss: 1.0737611055374146
    load_time_ms: 0.736
    num_steps_sampled: 861300
    num_steps_trained: 858000
    sample_time_ms: 2219.49
    update_time_ms: 4.512
  iterations_since_restore: 33
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.041459200094841
    mean_inference_ms: 1.2314430542379178
    mean_processing_ms: 0.9039496276016105
  time_since_restore: 161.3930733203888
  time_this_iter_s: 5.1402366161346436
  time_total_s: 161.3930733203888
  timestamp: 1563364305
  timesteps_since_restore: 861300
  timesteps_this_iter: 26100
  timesteps_total: 861300
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 161 s, 33 iter, 861300 ts, 12.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-51-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.12926741069497
  episode_reward_mean: 11.943120259855725
  episode_reward_min: -8.619124901072864
  episodes_this_iter: 174
  episodes_total: 6090
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2562.673
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.5015199184417725
        kl: 0.0083027807995677
        policy_loss: -0.013665364123880863
        total_loss: 1.0489418506622314
        vf_explained_var: 0.9674152135848999
        vf_loss: 1.0605313777923584
    load_time_ms: 0.749
    num_steps_sampled: 913500
    num_steps_trained: 910000
    sample_time_ms: 2226.055
    update_time_ms: 4.29
  iterations_since_restore: 35
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0431330680319093
    mean_inference_ms: 1.2307373952881995
    mean_processing_ms: 0.9067068436781134
  time_since_restore: 171.07890510559082
  time_this_iter_s: 4.9186084270477295
  time_total_s: 171.07890510559082
  timestamp: 1563364315
  timesteps_since_restore: 913500
  timesteps_this_iter: 26100
  timesteps_total: 913500
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 171 s, 35 iter, 913500 ts, 11.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.53846170565061
  episode_reward_mean: 12.499864655083075
  episode_reward_min: -7.488033262521934
  episodes_this_iter: 174
  episodes_total: 6264
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2565.38
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.4900095462799072
        kl: 0.008259912021458149
        policy_loss: -0.01118413731455803
        total_loss: 0.9509981870651245
        vf_explained_var: 0.9716344475746155
        vf_loss: 0.9601173400878906
    load_time_ms: 0.762
    num_steps_sampled: 939600
    num_steps_trained: 936000
    sample_time_ms: 2255.775
    update_time_ms: 4.419
  iterations_since_restore: 36
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0458862723983255
    mean_inference_ms: 1.2352369224518762
    mean_processing_ms: 0.907749773784985
  time_since_restore: 176.08475708961487
  time_this_iter_s: 5.005851984024048
  time_total_s: 176.08475708961487
  timestamp: 1563364320
  timesteps_since_restore: 939600
  timesteps_this_iter: 26100
  timesteps_total: 939600
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 176 s, 36 iter, 939600 ts, 12.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.61981256830393
  episode_reward_mean: 13.537804693775552
  episode_reward_min: -11.448803504883765
  episodes_this_iter: 174
  episodes_total: 6612
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2620.344
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.4649505615234375
        kl: 0.008120623417198658
        policy_loss: -0.011234879493713379
        total_loss: 2.19919490814209
        vf_explained_var: 0.9419224858283997
        vf_loss: 2.208399534225464
    load_time_ms: 0.766
    num_steps_sampled: 991800
    num_steps_trained: 988000
    sample_time_ms: 2255.313
    update_time_ms: 4.385
  iterations_since_restore: 38
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0438568299110111
    mean_inference_ms: 1.233058551464782
    mean_processing_ms: 0.9072695303817949
  time_since_restore: 185.68649244308472
  time_this_iter_s: 4.760885715484619
  time_total_s: 185.68649244308472
  timestamp: 1563364330
  timesteps_since_restore: 991800
  timesteps_this_iter: 26100
  timesteps_total: 991800
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 185 s, 38 iter, 991800 ts, 13.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.180719547072
  episode_reward_mean: 12.933341972128824
  episode_reward_min: -6.915342275552295
  episodes_this_iter: 174
  episodes_total: 6960
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2575.479
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.431532859802246
        kl: 0.00888018123805523
        policy_loss: -0.014362056739628315
        total_loss: 0.8017969131469727
        vf_explained_var: 0.9744212627410889
        vf_loss: 0.8139388561248779
    load_time_ms: 0.76
    num_steps_sampled: 1044000
    num_steps_trained: 1040000
    sample_time_ms: 2255.878
    update_time_ms: 4.311
  iterations_since_restore: 40
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0412982282671215
    mean_inference_ms: 1.229222255757513
    mean_processing_ms: 0.9039661173360074
  time_since_restore: 194.81408309936523
  time_this_iter_s: 4.522250413894653
  time_total_s: 194.81408309936523
  timestamp: 1563364339
  timesteps_since_restore: 1044000
  timesteps_this_iter: 26100
  timesteps_total: 1044000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 194 s, 40 iter, 1044000 ts, 12.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.90181536391451
  episode_reward_mean: 12.12846921456957
  episode_reward_min: -6.318565206962505
  episodes_this_iter: 174
  episodes_total: 7308
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2618.226
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.416107416152954
        kl: 0.008231077343225479
        policy_loss: -0.013260943815112114
        total_loss: 0.8954187035560608
        vf_explained_var: 0.9710354208946228
        vf_loss: 0.9066218137741089
    load_time_ms: 0.773
    num_steps_sampled: 1096200
    num_steps_trained: 1092000
    sample_time_ms: 2218.413
    update_time_ms: 4.258
  iterations_since_restore: 42
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0411535633647782
    mean_inference_ms: 1.2285954322482828
    mean_processing_ms: 0.9053667881609698
  time_since_restore: 204.84828662872314
  time_this_iter_s: 5.231092691421509
  time_total_s: 204.84828662872314
  timestamp: 1563364349
  timesteps_since_restore: 1096200
  timesteps_this_iter: 26100
  timesteps_total: 1096200
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 204 s, 42 iter, 1096200 ts, 12.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.33331855423607
  episode_reward_mean: 13.46634732876413
  episode_reward_min: -8.37926239808082
  episodes_this_iter: 174
  episodes_total: 7482
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2610.378
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.4055237770080566
        kl: 0.009495267644524574
        policy_loss: -0.014265754260122776
        total_loss: 0.7406265735626221
        vf_explained_var: 0.9783637523651123
        vf_loss: 0.7525185346603394
    load_time_ms: 0.776
    num_steps_sampled: 1122300
    num_steps_trained: 1118000
    sample_time_ms: 2232.473
    update_time_ms: 4.29
  iterations_since_restore: 43
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.04182533174146
    mean_inference_ms: 1.2310431061549603
    mean_processing_ms: 0.90546692099331
  time_since_restore: 210.05133938789368
  time_this_iter_s: 5.203052759170532
  time_total_s: 210.05133938789368
  timestamp: 1563364354
  timesteps_since_restore: 1122300
  timesteps_this_iter: 26100
  timesteps_total: 1122300
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 210 s, 43 iter, 1122300 ts, 13.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.8469763682632
  episode_reward_mean: 13.350702878998902
  episode_reward_min: -10.373639932435687
  episodes_this_iter: 174
  episodes_total: 7656
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2647.043
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.401212215423584
        kl: 0.007812316995114088
        policy_loss: -0.010718239471316338
        total_loss: 1.0993043184280396
        vf_explained_var: 0.9707035422325134
        vf_loss: 1.1080694198608398
    load_time_ms: 0.764
    num_steps_sampled: 1148400
    num_steps_trained: 1144000
    sample_time_ms: 2254.61
    update_time_ms: 4.392
  iterations_since_restore: 44
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0445817971905051
    mean_inference_ms: 1.2342087401356443
    mean_processing_ms: 0.9090788670652292
  time_since_restore: 215.4072461128235
  time_this_iter_s: 5.35590672492981
  time_total_s: 215.4072461128235
  timestamp: 1563364359
  timesteps_since_restore: 1148400
  timesteps_this_iter: 26100
  timesteps_total: 1148400
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 215 s, 44 iter, 1148400 ts, 13.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.91807508830715
  episode_reward_mean: 14.28956913163197
  episode_reward_min: -6.02547481194722
  episodes_this_iter: 174
  episodes_total: 7830
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2672.689
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.3907737731933594
        kl: 0.008795560337603092
        policy_loss: -0.013181607238948345
        total_loss: 0.7953230738639832
        vf_explained_var: 0.9763839244842529
        vf_loss: 0.8063058257102966
    load_time_ms: 0.763
    num_steps_sampled: 1174500
    num_steps_trained: 1170000
    sample_time_ms: 2284.806
    update_time_ms: 4.381
  iterations_since_restore: 45
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047603543327887
    mean_inference_ms: 1.2396287653279534
    mean_processing_ms: 0.9116885756402758
  time_since_restore: 220.87728071212769
  time_this_iter_s: 5.470034599304199
  time_total_s: 220.87728071212769
  timestamp: 1563364365
  timesteps_since_restore: 1174500
  timesteps_this_iter: 26100
  timesteps_total: 1174500
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 220 s, 45 iter, 1174500 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-52-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.49475190317651
  episode_reward_mean: 13.70327061947808
  episode_reward_min: -6.915713589110396
  episodes_this_iter: 174
  episodes_total: 8004
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2707.899
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.3738293647766113
        kl: 0.008565958589315414
        policy_loss: -0.013211862184107304
        total_loss: 0.6749414205551147
        vf_explained_var: 0.9786961674690247
        vf_loss: 0.6860118508338928
    load_time_ms: 0.756
    num_steps_sampled: 1200600
    num_steps_trained: 1196000
    sample_time_ms: 2258.744
    update_time_ms: 4.19
  iterations_since_restore: 46
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0469747726658678
    mean_inference_ms: 1.2370733493456836
    mean_processing_ms: 0.9106163870428786
  time_since_restore: 225.97309279441833
  time_this_iter_s: 5.095812082290649
  time_total_s: 225.97309279441833
  timestamp: 1563364370
  timesteps_since_restore: 1200600
  timesteps_this_iter: 26100
  timesteps_total: 1200600
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 225 s, 46 iter, 1200600 ts, 13.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.262003071222544
  episode_reward_mean: 13.522032359605612
  episode_reward_min: -6.2529794304062145
  episodes_this_iter: 174
  episodes_total: 8352
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2681.201
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.3351895809173584
        kl: 0.009411311708390713
        policy_loss: -0.013216282241046429
        total_loss: 0.6223924160003662
        vf_explained_var: 0.9803008437156677
        vf_loss: 0.6332558989524841
    load_time_ms: 0.759
    num_steps_sampled: 1252800
    num_steps_trained: 1248000
    sample_time_ms: 2284.31
    update_time_ms: 4.323
  iterations_since_restore: 48
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0464437913068019
    mean_inference_ms: 1.2374532770583422
    mean_processing_ms: 0.9117408806924554
  time_since_restore: 235.56288266181946
  time_this_iter_s: 4.7257606983184814
  time_total_s: 235.56288266181946
  timestamp: 1563364380
  timesteps_since_restore: 1252800
  timesteps_this_iter: 26100
  timesteps_total: 1252800
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 235 s, 48 iter, 1252800 ts, 13.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.70709548879555
  episode_reward_mean: 13.698350923419921
  episode_reward_min: -7.4809263674114925
  episodes_this_iter: 174
  episodes_total: 8700
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2691.996
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.308566093444824
        kl: 0.008403456769883633
        policy_loss: -0.012667587958276272
        total_loss: 0.6529089212417603
        vf_explained_var: 0.9802844524383545
        vf_loss: 0.6634756922721863
    load_time_ms: 0.771
    num_steps_sampled: 1305000
    num_steps_trained: 1300000
    sample_time_ms: 2315.975
    update_time_ms: 4.467
  iterations_since_restore: 50
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488697475612885
    mean_inference_ms: 1.2420357817949959
    mean_processing_ms: 0.9131280963487982
  time_since_restore: 245.11477661132812
  time_this_iter_s: 4.832959175109863
  time_total_s: 245.11477661132812
  timestamp: 1563364389
  timesteps_since_restore: 1305000
  timesteps_this_iter: 26100
  timesteps_total: 1305000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 245 s, 50 iter, 1305000 ts, 13.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.13726330900396
  episode_reward_mean: 13.513724320700915
  episode_reward_min: -6.9421773371185544
  episodes_this_iter: 174
  episodes_total: 9048
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2643.252
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2835042476654053
        kl: 0.009015033952891827
        policy_loss: -0.01441224105656147
        total_loss: 0.5606105327606201
        vf_explained_var: 0.9801997542381287
        vf_loss: 0.5727689266204834
    load_time_ms: 0.757
    num_steps_sampled: 1357200
    num_steps_trained: 1352000
    sample_time_ms: 2309.795
    update_time_ms: 4.44
  iterations_since_restore: 52
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.046573604266785
    mean_inference_ms: 1.2389554431264826
    mean_processing_ms: 0.9108258954529238
  time_since_restore: 254.59592247009277
  time_this_iter_s: 4.846882343292236
  time_total_s: 254.59592247009277
  timestamp: 1563364399
  timesteps_since_restore: 1357200
  timesteps_this_iter: 26100
  timesteps_total: 1357200
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 254 s, 52 iter, 1357200 ts, 13.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.07854829396481
  episode_reward_mean: 14.154672623103375
  episode_reward_min: -5.775028327571448
  episodes_this_iter: 174
  episodes_total: 9396
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2602.034
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2537052631378174
        kl: 0.008430471643805504
        policy_loss: -0.012017001397907734
        total_loss: 0.49770960211753845
        vf_explained_var: 0.9822733998298645
        vf_loss: 0.5076190233230591
    load_time_ms: 0.763
    num_steps_sampled: 1409400
    num_steps_trained: 1404000
    sample_time_ms: 2248.782
    update_time_ms: 4.346
  iterations_since_restore: 54
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0444441732500795
    mean_inference_ms: 1.2367988235846115
    mean_processing_ms: 0.9094683491625815
  time_since_restore: 264.1300175189972
  time_this_iter_s: 4.7528462409973145
  time_total_s: 264.1300175189972
  timestamp: 1563364408
  timesteps_since_restore: 1409400
  timesteps_this_iter: 26100
  timesteps_total: 1409400
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 264 s, 54 iter, 1409400 ts, 14.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.02279068769285
  episode_reward_mean: 15.421139929493066
  episode_reward_min: -5.75216727859219
  episodes_this_iter: 174
  episodes_total: 9744
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2533.429
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.2226197719573975
        kl: 0.009256081655621529
        policy_loss: -0.013764534145593643
        total_loss: 0.4328579306602478
        vf_explained_var: 0.9868441224098206
        vf_loss: 0.44430842995643616
    load_time_ms: 0.76
    num_steps_sampled: 1461600
    num_steps_trained: 1456000
    sample_time_ms: 2201.632
    update_time_ms: 4.494
  iterations_since_restore: 56
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0435815916106985
    mean_inference_ms: 1.2348191687976746
    mean_processing_ms: 0.9085399148315693
  time_since_restore: 273.5393531322479
  time_this_iter_s: 4.768298625946045
  time_total_s: 273.5393531322479
  timestamp: 1563364418
  timesteps_since_restore: 1461600
  timesteps_this_iter: 26100
  timesteps_total: 1461600
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 273 s, 56 iter, 1461600 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.433486985130266
  episode_reward_mean: 14.30435628033197
  episode_reward_min: -5.747807515373849
  episodes_this_iter: 174
  episodes_total: 10092
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2539.448
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.1991219520568848
        kl: 0.010055681690573692
        policy_loss: -0.014492420479655266
        total_loss: 0.44921618700027466
        vf_explained_var: 0.9856525659561157
        vf_loss: 0.4611946940422058
    load_time_ms: 0.751
    num_steps_sampled: 1513800
    num_steps_trained: 1508000
    sample_time_ms: 2201.901
    update_time_ms: 4.402
  iterations_since_restore: 58
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.046168284316217
    mean_inference_ms: 1.2363864123420403
    mean_processing_ms: 0.9110306706507972
  time_since_restore: 283.1928126811981
  time_this_iter_s: 4.754420042037964
  time_total_s: 283.1928126811981
  timestamp: 1563364427
  timesteps_since_restore: 1513800
  timesteps_this_iter: 26100
  timesteps_total: 1513800
  training_iteration: 58
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 283 s, 58 iter, 1513800 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-53-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.70776011384971
  episode_reward_mean: 14.60242339568165
  episode_reward_min: -5.715158385677273
  episodes_this_iter: 174
  episodes_total: 10266
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2564.917
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.185551166534424
        kl: 0.010005803778767586
        policy_loss: -0.013582221232354641
        total_loss: 0.4222719371318817
        vf_explained_var: 0.9852161407470703
        vf_loss: 0.4333527684211731
    load_time_ms: 0.742
    num_steps_sampled: 1539900
    num_steps_trained: 1534000
    sample_time_ms: 2206.018
    update_time_ms: 4.346
  iterations_since_restore: 59
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0463316167881729
    mean_inference_ms: 1.236852349428111
    mean_processing_ms: 0.911038085053965
  time_since_restore: 288.2077901363373
  time_this_iter_s: 5.01497745513916
  time_total_s: 288.2077901363373
  timestamp: 1563364432
  timesteps_since_restore: 1539900
  timesteps_this_iter: 26100
  timesteps_total: 1539900
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 288 s, 59 iter, 1539900 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.31551786435807
  episode_reward_mean: 14.919085441540101
  episode_reward_min: -5.890074133891624
  episodes_this_iter: 174
  episodes_total: 10614
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2604.437
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.160104990005493
        kl: 0.010082503780722618
        policy_loss: -0.01678255759179592
        total_loss: 0.39514172077178955
        vf_explained_var: 0.986534059047699
        vf_loss: 0.40940365195274353
    load_time_ms: 0.754
    num_steps_sampled: 1592100
    num_steps_trained: 1586000
    sample_time_ms: 2213.445
    update_time_ms: 4.272
  iterations_since_restore: 61
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0457917411328155
    mean_inference_ms: 1.2388225678961724
    mean_processing_ms: 0.9106523563971147
  time_since_restore: 298.14915132522583
  time_this_iter_s: 5.153958559036255
  time_total_s: 298.14915132522583
  timestamp: 1563364442
  timesteps_since_restore: 1592100
  timesteps_this_iter: 26100
  timesteps_total: 1592100
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 298 s, 61 iter, 1592100 ts, 14.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.734685426180604
  episode_reward_mean: 16.15885209041083
  episode_reward_min: -5.023798541265834
  episodes_this_iter: 174
  episodes_total: 10962
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2634.455
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.1311166286468506
        kl: 0.011641902849078178
        policy_loss: -0.01607532612979412
        total_loss: 0.3768240511417389
        vf_explained_var: 0.9883833527565002
        vf_loss: 0.38998889923095703
    load_time_ms: 0.755
    num_steps_sampled: 1644300
    num_steps_trained: 1638000
    sample_time_ms: 2233.408
    update_time_ms: 4.271
  iterations_since_restore: 63
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0463699648072442
    mean_inference_ms: 1.2388183117976488
    mean_processing_ms: 0.9116446864719148
  time_since_restore: 308.28003096580505
  time_this_iter_s: 5.2715163230896
  time_total_s: 308.28003096580505
  timestamp: 1563364453
  timesteps_since_restore: 1644300
  timesteps_this_iter: 26100
  timesteps_total: 1644300
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 308 s, 63 iter, 1644300 ts, 16.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.97914526702423
  episode_reward_mean: 13.71819529057315
  episode_reward_min: -5.095908599300533
  episodes_this_iter: 174
  episodes_total: 11136
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2652.978
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.1170828342437744
        kl: 0.010125876404345036
        policy_loss: -0.015584278851747513
        total_loss: 0.35821056365966797
        vf_explained_var: 0.9863789081573486
        vf_loss: 0.37126338481903076
    load_time_ms: 0.748
    num_steps_sampled: 1670400
    num_steps_trained: 1664000
    sample_time_ms: 2250.738
    update_time_ms: 4.306
  iterations_since_restore: 64
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.045024873133283
    mean_inference_ms: 1.238908601938621
    mean_processing_ms: 0.9098873440025526
  time_since_restore: 313.39281940460205
  time_this_iter_s: 5.112788438796997
  time_total_s: 313.39281940460205
  timestamp: 1563364458
  timesteps_since_restore: 1670400
  timesteps_this_iter: 26100
  timesteps_total: 1670400
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 313 s, 64 iter, 1670400 ts, 13.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.82029128153436
  episode_reward_mean: 14.353421049013123
  episode_reward_min: -5.051491714853542
  episodes_this_iter: 174
  episodes_total: 11310
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2667.124
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.097752571105957
        kl: 0.010685009881854057
        policy_loss: -0.017381850630044937
        total_loss: 0.3582311272621155
        vf_explained_var: 0.9871412515640259
        vf_loss: 0.3729417622089386
    load_time_ms: 0.745
    num_steps_sampled: 1696500
    num_steps_trained: 1690000
    sample_time_ms: 2272.566
    update_time_ms: 4.157
  iterations_since_restore: 65
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0466779450637618
    mean_inference_ms: 1.2425732173638462
    mean_processing_ms: 0.9126025751684416
  time_since_restore: 318.39446783065796
  time_this_iter_s: 5.001648426055908
  time_total_s: 318.39446783065796
  timestamp: 1563364463
  timesteps_since_restore: 1696500
  timesteps_this_iter: 26100
  timesteps_total: 1696500
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 318 s, 65 iter, 1696500 ts, 14.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.180598168056434
  episode_reward_mean: 16.69667821190093
  episode_reward_min: -4.605229928921016
  episodes_this_iter: 174
  episodes_total: 11658
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2650.229
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.0594067573547363
        kl: 0.009976529516279697
        policy_loss: -0.01635323092341423
        total_loss: 0.31965553760528564
        vf_explained_var: 0.9902238249778748
        vf_loss: 0.33351466059684753
    load_time_ms: 0.75
    num_steps_sampled: 1748700
    num_steps_trained: 1742000
    sample_time_ms: 2262.919
    update_time_ms: 4.195
  iterations_since_restore: 67
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0471362816791852
    mean_inference_ms: 1.2437094716951222
    mean_processing_ms: 0.9127687331765624
  time_since_restore: 327.7928020954132
  time_this_iter_s: 4.649433612823486
  time_total_s: 327.7928020954132
  timestamp: 1563364472
  timesteps_since_restore: 1748700
  timesteps_this_iter: 26100
  timesteps_total: 1748700
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 327 s, 67 iter, 1748700 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.0321191027165
  episode_reward_mean: 15.596899077650882
  episode_reward_min: -11.65716708188342
  episodes_this_iter: 174
  episodes_total: 11832
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2676.666
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.0479233264923096
        kl: 0.011012210510671139
        policy_loss: -0.01627451926469803
        total_loss: 0.2902010679244995
        vf_explained_var: 0.9900516271591187
        vf_loss: 0.3037225306034088
    load_time_ms: 0.745
    num_steps_sampled: 1774800
    num_steps_trained: 1768000
    sample_time_ms: 2268.427
    update_time_ms: 4.187
  iterations_since_restore: 68
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0452604137011285
    mean_inference_ms: 1.2411532682015394
    mean_processing_ms: 0.9116248870186182
  time_since_restore: 332.86893248558044
  time_this_iter_s: 5.076130390167236
  time_total_s: 332.86893248558044
  timestamp: 1563364477
  timesteps_since_restore: 1774800
  timesteps_this_iter: 26100
  timesteps_total: 1774800
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 332 s, 68 iter, 1774800 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.74586816585939
  episode_reward_mean: 15.993785492255876
  episode_reward_min: -3.1647449308289604
  episodes_this_iter: 174
  episodes_total: 12180
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2656.88
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.0180845260620117
        kl: 0.010976039804518223
        policy_loss: -0.01602310501039028
        total_loss: 0.2553524076938629
        vf_explained_var: 0.991344153881073
        vf_loss: 0.2686314880847931
    load_time_ms: 0.763
    num_steps_sampled: 1827000
    num_steps_trained: 1820000
    sample_time_ms: 2258.743
    update_time_ms: 4.238
  iterations_since_restore: 70
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0435355647170588
    mean_inference_ms: 1.2382378549877051
    mean_processing_ms: 0.9098122609352588
  time_since_restore: 342.3723318576813
  time_this_iter_s: 4.781903505325317
  time_total_s: 342.3723318576813
  timestamp: 1563364487
  timesteps_since_restore: 1827000
  timesteps_this_iter: 26100
  timesteps_total: 1827000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 342 s, 70 iter, 1827000 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-54-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.476926872962764
  episode_reward_mean: 16.00701424995857
  episode_reward_min: -1.7290274966457686
  episodes_this_iter: 174
  episodes_total: 12528
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2634.075
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.9842495918273926
        kl: 0.012598053552210331
        policy_loss: -0.017316464334726334
        total_loss: 0.2645983099937439
        vf_explained_var: 0.9912160634994507
        vf_loss: 0.2787652611732483
    load_time_ms: 0.767
    num_steps_sampled: 1879200
    num_steps_trained: 1872000
    sample_time_ms: 2241.523
    update_time_ms: 4.344
  iterations_since_restore: 72
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0448416099954567
    mean_inference_ms: 1.2420497822881043
    mean_processing_ms: 0.911754618486747
  time_since_restore: 351.98535227775574
  time_this_iter_s: 4.870609998703003
  time_total_s: 351.98535227775574
  timestamp: 1563364496
  timesteps_since_restore: 1879200
  timesteps_this_iter: 26100
  timesteps_total: 1879200
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 351 s, 72 iter, 1879200 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.33409426352455
  episode_reward_mean: 15.742477398215991
  episode_reward_min: -2.917461358907801
  episodes_this_iter: 174
  episodes_total: 12702
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2632.907
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.9725781679153442
        kl: 0.012035917490720749
        policy_loss: -0.016686268150806427
        total_loss: 0.23510411381721497
        vf_explained_var: 0.991814374923706
        vf_loss: 0.24878138303756714
    load_time_ms: 0.767
    num_steps_sampled: 1905300
    num_steps_trained: 1898000
    sample_time_ms: 2239.747
    update_time_ms: 4.379
  iterations_since_restore: 73
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0454946026114018
    mean_inference_ms: 1.2425012302809444
    mean_processing_ms: 0.9124277979388505
  time_since_restore: 357.2299244403839
  time_this_iter_s: 5.244572162628174
  time_total_s: 357.2299244403839
  timestamp: 1563364502
  timesteps_since_restore: 1905300
  timesteps_this_iter: 26100
  timesteps_total: 1905300
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 357 s, 73 iter, 1905300 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.06394496050973
  episode_reward_mean: 16.957607892007598
  episode_reward_min: -8.3865315063763
  episodes_this_iter: 174
  episodes_total: 12876
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2657.389
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.9521814584732056
        kl: 0.011845463886857033
        policy_loss: -0.014941984787583351
        total_loss: 0.2133692055940628
        vf_explained_var: 0.993113100528717
        vf_loss: 0.22534982860088348
    load_time_ms: 0.773
    num_steps_sampled: 1931400
    num_steps_trained: 1924000
    sample_time_ms: 2230.726
    update_time_ms: 4.419
  iterations_since_restore: 74
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0437688680298047
    mean_inference_ms: 1.239591440769525
    mean_processing_ms: 0.9106812524146404
  time_since_restore: 362.5014798641205
  time_this_iter_s: 5.271555423736572
  time_total_s: 362.5014798641205
  timestamp: 1563364507
  timesteps_since_restore: 1931400
  timesteps_this_iter: 26100
  timesteps_total: 1931400
  training_iteration: 74
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 362 s, 74 iter, 1931400 ts, 17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.30280911147001
  episode_reward_mean: 15.340498214706415
  episode_reward_min: -3.2871401397648854
  episodes_this_iter: 174
  episodes_total: 13224
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2651.168
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.9216364622116089
        kl: 0.010536056011915207
        policy_loss: -0.015007728710770607
        total_loss: 0.21679972112178802
        vf_explained_var: 0.9922071695327759
        vf_loss: 0.2291734367609024
    load_time_ms: 0.77
    num_steps_sampled: 1983600
    num_steps_trained: 1976000
    sample_time_ms: 2198.436
    update_time_ms: 4.479
  iterations_since_restore: 76
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0433071513894732
    mean_inference_ms: 1.2392793006921135
    mean_processing_ms: 0.910123379301914
  time_since_restore: 371.8671429157257
  time_this_iter_s: 4.550026178359985
  time_total_s: 371.8671429157257
  timestamp: 1563364516
  timesteps_since_restore: 1983600
  timesteps_this_iter: 26100
  timesteps_total: 1983600
  training_iteration: 76
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 371 s, 76 iter, 1983600 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.80915712872456
  episode_reward_mean: 16.449255039678178
  episode_reward_min: -2.6822607152209472
  episodes_this_iter: 174
  episodes_total: 13572
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2667.006
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.8865431547164917
        kl: 0.011965366080403328
        policy_loss: -0.01823323220014572
        total_loss: 0.19936028122901917
        vf_explained_var: 0.9932658076286316
        vf_loss: 0.21460217237472534
    load_time_ms: 0.755
    num_steps_sampled: 2035800
    num_steps_trained: 2028000
    sample_time_ms: 2192.474
    update_time_ms: 4.575
  iterations_since_restore: 78
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0424758140416939
    mean_inference_ms: 1.237985142978509
    mean_processing_ms: 0.9094034757246703
  time_since_restore: 381.69742226600647
  time_this_iter_s: 4.905653238296509
  time_total_s: 381.69742226600647
  timestamp: 1563364526
  timesteps_since_restore: 2035800
  timesteps_this_iter: 26100
  timesteps_total: 2035800
  training_iteration: 78
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 381 s, 78 iter, 2035800 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.75222386294062
  episode_reward_mean: 16.186830869600257
  episode_reward_min: -0.7180372882234365
  episodes_this_iter: 174
  episodes_total: 13746
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2670.438
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.8726075887680054
        kl: 0.009709687903523445
        policy_loss: -0.01625584438443184
        total_loss: 0.1951914131641388
        vf_explained_var: 0.9932279586791992
        vf_loss: 0.20901983976364136
    load_time_ms: 0.756
    num_steps_sampled: 2061900
    num_steps_trained: 2054000
    sample_time_ms: 2223.437
    update_time_ms: 4.649
  iterations_since_restore: 79
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.044485357126074
    mean_inference_ms: 1.2404901896509386
    mean_processing_ms: 0.9120130443163966
  time_since_restore: 386.7637460231781
  time_this_iter_s: 5.066323757171631
  time_total_s: 386.7637460231781
  timestamp: 1563364531
  timesteps_since_restore: 2061900
  timesteps_this_iter: 26100
  timesteps_total: 2061900
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 386 s, 79 iter, 2061900 ts, 16.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.74935991144582
  episode_reward_mean: 17.509127538219385
  episode_reward_min: -3.414043327376327
  episodes_this_iter: 174
  episodes_total: 14094
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2614.181
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.838478922843933
        kl: 0.013369383290410042
        policy_loss: -0.017784973606467247
        total_loss: 0.17527106404304504
        vf_explained_var: 0.9948320388793945
        vf_loss: 0.18971370160579681
    load_time_ms: 0.744
    num_steps_sampled: 2114100
    num_steps_trained: 2106000
    sample_time_ms: 2228.519
    update_time_ms: 4.552
  iterations_since_restore: 81
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0429189682316748
    mean_inference_ms: 1.2375155151500703
    mean_processing_ms: 0.9098059388701875
  time_since_restore: 395.7746834754944
  time_this_iter_s: 4.466107130050659
  time_total_s: 395.7746834754944
  timestamp: 1563364540
  timesteps_since_restore: 2114100
  timesteps_this_iter: 26100
  timesteps_total: 2114100
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 395 s, 81 iter, 2114100 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-55-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.784513120211926
  episode_reward_mean: 17.234970637630465
  episode_reward_min: -3.181309101774806
  episodes_this_iter: 174
  episodes_total: 14442
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2614.445
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.8108985424041748
        kl: 0.011729510501027107
        policy_loss: -0.01833624392747879
        total_loss: 0.13950440287590027
        vf_explained_var: 0.9954515099525452
        vf_loss: 0.15490826964378357
    load_time_ms: 0.735
    num_steps_sampled: 2166300
    num_steps_trained: 2158000
    sample_time_ms: 2197.158
    update_time_ms: 4.616
  iterations_since_restore: 83
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0434265674649796
    mean_inference_ms: 1.2375806458472192
    mean_processing_ms: 0.9090898026809335
  time_since_restore: 405.5793263912201
  time_this_iter_s: 4.821264982223511
  time_total_s: 405.5793263912201
  timestamp: 1563364550
  timesteps_since_restore: 2166300
  timesteps_this_iter: 26100
  timesteps_total: 2166300
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 405 s, 83 iter, 2166300 ts, 17.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-56-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.003984059758864
  episode_reward_mean: 15.07584508259214
  episode_reward_min: -2.875183921582069
  episodes_this_iter: 174
  episodes_total: 14790
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2556.459
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.7770012617111206
        kl: 0.010401939041912556
        policy_loss: -0.016368979588150978
        total_loss: 0.1475488841533661
        vf_explained_var: 0.9944369196891785
        vf_loss: 0.1613173633813858
    load_time_ms: 0.747
    num_steps_sampled: 2218500
    num_steps_trained: 2210000
    sample_time_ms: 2216.39
    update_time_ms: 4.561
  iterations_since_restore: 85
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0418808356019316
    mean_inference_ms: 1.2376579616310304
    mean_processing_ms: 0.9081356768279132
  time_since_restore: 415.2731685638428
  time_this_iter_s: 4.853384017944336
  time_total_s: 415.2731685638428
  timestamp: 1563364560
  timesteps_since_restore: 2218500
  timesteps_this_iter: 26100
  timesteps_total: 2218500
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 415 s, 85 iter, 2218500 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-56-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.34423709566279
  episode_reward_mean: 17.53733609438807
  episode_reward_min: -3.8608008203973583
  episodes_this_iter: 174
  episodes_total: 14964
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2599.882
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.7716052532196045
        kl: 0.010998116806149483
        policy_loss: -0.018145574256777763
        total_loss: 0.13577468693256378
        vf_explained_var: 0.9957514405250549
        vf_loss: 0.1511707305908203
    load_time_ms: 0.752
    num_steps_sampled: 2244600
    num_steps_trained: 2236000
    sample_time_ms: 2235.428
    update_time_ms: 4.547
  iterations_since_restore: 86
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0445209871047358
    mean_inference_ms: 1.2398186581163182
    mean_processing_ms: 0.9105182911553619
  time_since_restore: 420.4511477947235
  time_this_iter_s: 5.177979230880737
  time_total_s: 420.4511477947235
  timestamp: 1563364565
  timesteps_since_restore: 2244600
  timesteps_this_iter: 26100
  timesteps_total: 2244600
  training_iteration: 86
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 420 s, 86 iter, 2244600 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-56-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.166484004587765
  episode_reward_mean: 17.865768848685207
  episode_reward_min: -1.8767957459799687
  episodes_this_iter: 174
  episodes_total: 15312
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2633.953
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.7399324178695679
        kl: 0.011664428748190403
        policy_loss: -0.018223626539111137
        total_loss: 0.11365117132663727
        vf_explained_var: 0.9960759878158569
        vf_loss: 0.12895868718624115
    load_time_ms: 0.757
    num_steps_sampled: 2296800
    num_steps_trained: 2288000
    sample_time_ms: 2267.621
    update_time_ms: 4.559
  iterations_since_restore: 88
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0442787074758288
    mean_inference_ms: 1.2409902018111028
    mean_processing_ms: 0.9105005366111197
  time_since_restore: 430.9412009716034
  time_this_iter_s: 5.50359320640564
  time_total_s: 430.9412009716034
  timestamp: 1563364576
  timesteps_since_restore: 2296800
  timesteps_this_iter: 26100
  timesteps_total: 2296800
  training_iteration: 88
  2019-07-17 13:56:26,340	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-17 13:56:26,382	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 430 s, 88 iter, 2296800 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-56-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.71251090715353
  episode_reward_mean: 17.52793893424109
  episode_reward_min: -2.951726648021693
  episodes_this_iter: 174
  episodes_total: 15486
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2646.129
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.7288336753845215
        kl: 0.01317999605089426
        policy_loss: -0.017011137679219246
        total_loss: 0.10631366819143295
        vf_explained_var: 0.9964586496353149
        vf_loss: 0.12002979964017868
    load_time_ms: 0.761
    num_steps_sampled: 2322900
    num_steps_trained: 2314000
    sample_time_ms: 2250.551
    update_time_ms: 4.623
  iterations_since_restore: 89
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0450541298999845
    mean_inference_ms: 1.2424174731730029
    mean_processing_ms: 0.9107876164641894
  time_since_restore: 435.9633090496063
  time_this_iter_s: 5.02210807800293
  time_total_s: 435.9633090496063
  timestamp: 1563364581
  timesteps_since_restore: 2322900
  timesteps_this_iter: 26100
  timesteps_total: 2322900
  training_iteration: 89
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=30796], 435 s, 89 iter, 2322900 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-17_13-56-26
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 37.202379807835825
  episode_reward_mean: 18.149118592491746
  episode_reward_min: -0.9898516095469372
  episodes_this_iter: 174
  episodes_total: 15660
  experiment_id: c8a7a476d81344bd8ab83cc1501e2f4a
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2700.744
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 1.7183680534362793
        kl: 0.011980959214270115
        policy_loss: -0.019556643441319466
        total_loss: 0.10949284583330154
        vf_explained_var: 0.9964748620986938
        vf_loss: 0.12605425715446472
    load_time_ms: 0.761
    num_steps_sampled: 2349000
    num_steps_trained: 2340000
    sample_time_ms: 2260.073
    update_time_ms: 4.678
  iterations_since_restore: 90
  node_ip: 10.16.128.38
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30796
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0444241049722356
    mean_inference_ms: 1.2407782610437428
    mean_processing_ms: 0.9107195632019838
  time_since_restore: 441.1526288986206
  time_this_iter_s: 5.189319849014282
  time_total_s: 441.1526288986206
  timestamp: 1563364586
  timesteps_since_restore: 2349000
  timesteps_this_iter: 26100
  timesteps_total: 2349000
  training_iteration: 90
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 10.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'PENDING': 3})
PENDING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

[2m[36m(pid=32075)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32075)[0m W0717 13:56:28.761818 140258483836672 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32075)[0m Instructions for updating:
[2m[36m(pid=32075)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32075)[0m [32m [     0.02619s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29,067	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=32084)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32084)[0m W0717 13:56:29.022732 139862117598976 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32084)[0m Instructions for updating:
[2m[36m(pid=32084)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.070683: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.075416: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.142575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.142892: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad039b9aa0 executing computations on platform CUDA. Devices:
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.142912: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.163525: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164278: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad041402d0 executing computations on platform Host. Devices:
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164299: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
[2m[36m(pid=32075)[0m name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
[2m[36m(pid=32075)[0m pciBusID: 0000:01:00.0
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164839: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.164957: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.165012: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.165066: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.165121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.167687: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.167709: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.167724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.167731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.167736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[2m[36m(pid=32075)[0m W0717 13:56:29.171593 140258840835520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32075)[0m Instructions for updating:
[2m[36m(pid=32075)[0m Use keras.layers.dense instead.
[2m[36m(pid=32075)[0m W0717 13:56:29.404258 140258840835520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32075)[0m Instructions for updating:
[2m[36m(pid=32075)[0m Use `tf.cast` instead.
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29.456899: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32075)[0m 2019-07-17 13:56:29,471	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32075)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32075)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=32075)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32075)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32075)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32075)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m W0717 13:56:29.507532 140258840835520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32075)[0m Instructions for updating:
[2m[36m(pid=32075)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32075)[0m [32m [     1.14847s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.14886s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m 2019-07-17 13:56:30,191	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f8ead74d7b8>}
[2m[36m(pid=32075)[0m 2019-07-17 13:56:30,191	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f8ead74d6d8>}
[2m[36m(pid=32075)[0m 2019-07-17 13:56:30,191	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=32075)[0m [32m [     1.14923s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.14961s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.14997s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15035s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15072s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15108s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15150s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15187s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15225s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15276s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15314s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15354s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m [32m [     1.15397s,  INFO] TimeLimit:
[2m[36m(pid=32075)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32075)[0m - action_space = Box(2,)
[2m[36m(pid=32075)[0m - observation_space = Box(9,)
[2m[36m(pid=32075)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32075)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32075)[0m - _max_episode_steps = 150
[2m[36m(pid=32075)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m 2019-07-17 13:56:30,251	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30,272	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.286704: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.293630: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.296137: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.296181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.296188: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.296279: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.296302: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.296308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32084)[0m [32m [     0.02643s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.315438: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.316024: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56247b6aa8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.316052: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32084)[0m W0717 13:56:30.320584 139862474589632 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32084)[0m Instructions for updating:
[2m[36m(pid=32084)[0m Use keras.layers.dense instead.
[2m[36m(pid=32084)[0m W0717 13:56:30.729825 139862474589632 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32084)[0m Instructions for updating:
[2m[36m(pid=32084)[0m Use `tf.cast` instead.
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30.882140: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32084)[0m 2019-07-17 13:56:30,921	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32084)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32084)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32084)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32084)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=32084)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32084)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32084)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32084)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32084)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32084)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32084)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m W0717 13:56:31.085452 139862474589632 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32084)[0m Instructions for updating:
[2m[36m(pid=32084)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32084)[0m [32m [     3.50428s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.50512s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.50873s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.51604s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.51861s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.51946s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.52026s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.52110s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.52725s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.52813s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.52892s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.53204s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.53289s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.53385s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m [32m [     3.53451s,  INFO] TimeLimit:
[2m[36m(pid=32084)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32084)[0m - action_space = Box(2,)
[2m[36m(pid=32084)[0m - observation_space = Box(9,)
[2m[36m(pid=32084)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32084)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32084)[0m - _max_episode_steps = 150
[2m[36m(pid=32084)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.294080: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.294384: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_1 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_1/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_1/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.294563: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_2 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_2/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_2/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.294717: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_3 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_3/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_3/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.294854: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_4 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_4/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_4/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.295011: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_5 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_5/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_5/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.295201: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_6 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_6/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_6/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:35.295418: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32075)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32075)[0m See below for details of this colocation group:
[2m[36m(pid=32075)[0m Colocation Debug Info:
[2m[36m(pid=32075)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32075)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32075)[0m Assign: CPU 
[2m[36m(pid=32075)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32075)[0m VariableV2: CPU 
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_7 (VariableV2) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_7/Assign (Assign) /device:GPU:0
[2m[36m(pid=32075)[0m   default_policy_1/tower_1/Variable_7/read (Identity) /device:GPU:0
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32136)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32136)[0m W0717 13:56:35.732029 140656677152512 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32136)[0m Instructions for updating:
[2m[36m(pid=32136)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32137)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32137)[0m W0717 13:56:35.769223 140430486071040 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32137)[0m Instructions for updating:
[2m[36m(pid=32137)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32138)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32138)[0m W0717 13:56:35.833266 140643521902336 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32138)[0m Instructions for updating:
[2m[36m(pid=32138)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32175)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32175)[0m W0717 13:56:36.066733 139922559932160 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32175)[0m Instructions for updating:
[2m[36m(pid=32175)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32173)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32173)[0m W0717 13:56:36.079153 139621808633600 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32173)[0m Instructions for updating:
[2m[36m(pid=32173)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32140)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32140)[0m W0717 13:56:36.124370 140526060705536 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32140)[0m Instructions for updating:
[2m[36m(pid=32140)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32135)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32135)[0m W0717 13:56:36.139416 139631543248640 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32135)[0m Instructions for updating:
[2m[36m(pid=32135)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32172)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32172)[0m W0717 13:56:36.315777 140047578777344 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32172)[0m Instructions for updating:
[2m[36m(pid=32172)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32187)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32187)[0m W0717 13:56:36.498856 140257101575936 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32187)[0m Instructions for updating:
[2m[36m(pid=32187)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36,521	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.543343: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.553001: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.557126: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.557194: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.557208: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.557335: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.557379: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.557391: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32138)[0m [32m [     0.03844s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32189)[0m W0717 13:56:36.523484 140657010730752 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32189)[0m Instructions for updating:
[2m[36m(pid=32189)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32139)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32139)[0m W0717 13:56:36.628426 139963691595520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32139)[0m Instructions for updating:
[2m[36m(pid=32139)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.603480: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.604125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e4a1c268d0 executing computations on platform Host. Devices:
[2m[36m(pid=32138)[0m 2019-07-17 13:56:36.604159: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32138)[0m W0717 13:56:36.612959 140643878901184 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32138)[0m Instructions for updating:
[2m[36m(pid=32138)[0m Use keras.layers.dense instead.
[2m[36m(pid=32174)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32174)[0m W0717 13:56:36.589420 139907108497152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32174)[0m Instructions for updating:
[2m[36m(pid=32174)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36,762	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32136)[0m [32m [     0.04472s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.800953: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.810769: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.821614: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.821741: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.821757: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.821888: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.821932: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.821947: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.833692: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.834252: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fe97ce18d0 executing computations on platform Host. Devices:
[2m[36m(pid=32136)[0m 2019-07-17 13:56:36.834288: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32136)[0m W0717 13:56:36.842112 140657034143168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32136)[0m Instructions for updating:
[2m[36m(pid=32136)[0m Use keras.layers.dense instead.
[2m[36m(pid=32176)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32176)[0m W0717 13:56:36.832156 140351920318208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32176)[0m Instructions for updating:
[2m[36m(pid=32176)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32171)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32171)[0m W0717 13:56:36.953355 139871490782976 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32171)[0m Instructions for updating:
[2m[36m(pid=32171)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32137)[0m [32m [     0.06903s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m 2019-07-17 13:56:36,979	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32181)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32181)[0m W0717 13:56:36.974557 140380986558208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32181)[0m Instructions for updating:
[2m[36m(pid=32181)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.033535: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.053433: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.062018: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.062089: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.062106: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.062244: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.062286: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.062299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32177)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32177)[0m W0717 13:56:37.028087 139624287782656 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32177)[0m Instructions for updating:
[2m[36m(pid=32177)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.074994: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.075473: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c5c0b4f8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.075505: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32137)[0m W0717 13:56:37.099991 140430843069888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32137)[0m Instructions for updating:
[2m[36m(pid=32137)[0m Use keras.layers.dense instead.
[2m[36m(pid=32140)[0m [32m [     0.06828s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37,343	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32170)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32170)[0m W0717 13:56:37.356523 139981846779648 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32170)[0m Instructions for updating:
[2m[36m(pid=32170)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.365971: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.375354: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.385068: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.385128: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.385141: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.385245: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.385281: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.385292: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.403486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.404018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d072241db0 executing computations on platform Host. Devices:
[2m[36m(pid=32140)[0m 2019-07-17 13:56:37.404050: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32135)[0m [32m [     0.03844s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37,360	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.383289: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.396901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.401116: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.401177: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.401192: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.401323: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.401367: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.401381: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32182)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=32182)[0m W0717 13:56:37.400292 139846595069696 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=32182)[0m Instructions for updating:
[2m[36m(pid=32182)[0m non-resource variables are not supported in the long term
[2m[36m(pid=32140)[0m W0717 13:56:37.415990 140526417700288 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32140)[0m Instructions for updating:
[2m[36m(pid=32140)[0m Use keras.layers.dense instead.
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.420261: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.420719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5641f93c78d0 executing computations on platform Host. Devices:
[2m[36m(pid=32135)[0m 2019-07-17 13:56:37.420751: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32135)[0m W0717 13:56:37.430117 139631900251584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32135)[0m Instructions for updating:
[2m[36m(pid=32135)[0m Use keras.layers.dense instead.
[2m[36m(pid=32138)[0m W0717 13:56:37.572127 140643878901184 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32138)[0m Instructions for updating:
[2m[36m(pid=32138)[0m Use `tf.cast` instead.
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37,589	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.609768: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.618447: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.621984: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.622049: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.622063: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.622194: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.622236: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.622249: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32139)[0m [32m [     0.03833s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.636846: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.637627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b3616eb8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32139)[0m 2019-07-17 13:56:37.637673: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32139)[0m W0717 13:56:37.658568 139964048598464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32139)[0m Instructions for updating:
[2m[36m(pid=32139)[0m Use keras.layers.dense instead.
[2m[36m(pid=32136)[0m W0717 13:56:37.633422 140657034143168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32136)[0m Instructions for updating:
[2m[36m(pid=32136)[0m Use `tf.cast` instead.
[2m[36m(pid=32138)[0m 2019-07-17 13:56:37.706741: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32137)[0m W0717 13:56:37.793780 140430843069888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32137)[0m Instructions for updating:
[2m[36m(pid=32137)[0m Use `tf.cast` instead.
[2m[36m(pid=32136)[0m 2019-07-17 13:56:37.770813: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32138)[0m W0717 13:56:37.830188 140643878901184 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32138)[0m Instructions for updating:
[2m[36m(pid=32138)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32075)[0m W0717 13:56:37.889509 140258840835520 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32075)[0m Instructions for updating:
[2m[36m(pid=32075)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32137)[0m 2019-07-17 13:56:37.901876: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32136)[0m W0717 13:56:37.874661 140657034143168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32136)[0m Instructions for updating:
[2m[36m(pid=32136)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32084)[0m W0717 13:56:37.955128 139862474589632 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32084)[0m Instructions for updating:
[2m[36m(pid=32084)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32135)[0m W0717 13:56:37.941873 139631900251584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32135)[0m Instructions for updating:
[2m[36m(pid=32135)[0m Use `tf.cast` instead.
[2m[36m(pid=32140)[0m W0717 13:56:37.978786 140526417700288 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32140)[0m Instructions for updating:
[2m[36m(pid=32140)[0m Use `tf.cast` instead.
[2m[36m(pid=32137)[0m W0717 13:56:37.983894 140430843069888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32137)[0m Instructions for updating:
[2m[36m(pid=32137)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32135)[0m 2019-07-17 13:56:38.033655: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32140)[0m 2019-07-17 13:56:38.079077: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32135)[0m W0717 13:56:38.112459 139631900251584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32135)[0m Instructions for updating:
[2m[36m(pid=32135)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32140)[0m W0717 13:56:38.166813 140526417700288 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32140)[0m Instructions for updating:
[2m[36m(pid=32140)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32139)[0m W0717 13:56:38.198438 139964048598464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32139)[0m Instructions for updating:
[2m[36m(pid=32139)[0m Use `tf.cast` instead.
[2m[36m(pid=32139)[0m 2019-07-17 13:56:38.324659: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32139)[0m W0717 13:56:38.430217 139964048598464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32139)[0m Instructions for updating:
[2m[36m(pid=32139)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,741	INFO rollout_worker.py:428 -- Generating sample batch of size 3200
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,848	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.951, max=0.174, mean=-0.155)},
[2m[36m(pid=32084)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.468, max=0.883, mean=0.036)},
[2m[36m(pid=32084)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.177, max=0.998, mean=0.195)},
[2m[36m(pid=32084)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.991, max=0.964, mean=0.027)},
[2m[36m(pid=32084)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.942, max=0.175, mean=-0.183)},
[2m[36m(pid=32084)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.818, max=0.993, mean=0.041)},
[2m[36m(pid=32084)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.787, max=0.895, mean=0.088)},
[2m[36m(pid=32084)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.805, max=0.541, mean=-0.113)},
[2m[36m(pid=32084)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.923, max=0.384, mean=-0.045)},
[2m[36m(pid=32084)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.247, max=0.753, mean=0.245)},
[2m[36m(pid=32084)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.039, max=0.186, mean=-0.235)},
[2m[36m(pid=32084)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.605, max=0.796, mean=-0.032)},
[2m[36m(pid=32084)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.923, max=0.645, mean=0.023)},
[2m[36m(pid=32084)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.875, max=0.0, mean=-0.201)},
[2m[36m(pid=32084)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.847, max=0.884, mean=-0.054)},
[2m[36m(pid=32084)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.332, max=0.985, mean=0.168)}}
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,848	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=32084)[0m   1: {'agent0': None},
[2m[36m(pid=32084)[0m   2: {'agent0': None},
[2m[36m(pid=32084)[0m   3: {'agent0': None},
[2m[36m(pid=32084)[0m   4: {'agent0': None},
[2m[36m(pid=32084)[0m   5: {'agent0': None},
[2m[36m(pid=32084)[0m   6: {'agent0': None},
[2m[36m(pid=32084)[0m   7: {'agent0': None},
[2m[36m(pid=32084)[0m   8: {'agent0': None},
[2m[36m(pid=32084)[0m   9: {'agent0': None},
[2m[36m(pid=32084)[0m   10: {'agent0': None},
[2m[36m(pid=32084)[0m   11: {'agent0': None},
[2m[36m(pid=32084)[0m   12: {'agent0': None},
[2m[36m(pid=32084)[0m   13: {'agent0': None},
[2m[36m(pid=32084)[0m   14: {'agent0': None},
[2m[36m(pid=32084)[0m   15: {'agent0': None}}
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,848	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.951, max=0.174, mean=-0.155)
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,849	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,855	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 0,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 1,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.079),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 2,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.12, max=1.151, mean=0.407),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 3,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.385, max=1.407, mean=-0.061),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 4,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.54, max=1.429, mean=-0.213),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 5,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.025, max=1.267, mean=0.065),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 6,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.145, max=1.459, mean=0.222),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 7,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.013, max=0.828, mean=-0.221),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 8,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.017, max=0.599, mean=-0.104),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 9,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.267, max=1.46, mean=0.302),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 10,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.505, max=1.492, mean=-0.274),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 11,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.071, max=0.958, mean=-0.043),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 12,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.106, max=1.376, mean=0.107),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 13,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.963, max=0.0, mean=-0.371),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 14,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.052, max=1.238, mean=-0.038),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32084)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32084)[0m                                   'env_id': 15,
[2m[36m(pid=32084)[0m                                   'info': None,
[2m[36m(pid=32084)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.704, max=1.593, mean=0.308),
[2m[36m(pid=32084)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32084)[0m                                   'rnn_state': []},
[2m[36m(pid=32084)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,855	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=32084)[0m 2019-07-17 13:56:38,936	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-1.2, max=1.963, mean=0.406),
[2m[36m(pid=32084)[0m                       [],
[2m[36m(pid=32084)[0m                       { 'action_prob': np.ndarray((16,), dtype=float32, min=0.013, max=0.149, mean=0.082),
[2m[36m(pid=32084)[0m                         'behaviour_logits': np.ndarray((16, 4), dtype=float32, min=-0.014, max=0.008, mean=-0.001),
[2m[36m(pid=32084)[0m                         'vf_preds': np.ndarray((16,), dtype=float32, min=-0.006, max=0.004, mean=-0.001)})}
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32136)[0m [32m [     2.50794s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.50878s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.50957s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51032s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m W0717 13:56:39.206935 140643878901184 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32138)[0m Instructions for updating:
[2m[36m(pid=32138)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32138)[0m [32m [     2.71353s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71422s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71484s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71548s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71613s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71689s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71776s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71845s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71923s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.71991s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.72052s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.72124s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.72195s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m [32m [     2.51111s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51216s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.72255s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32138)[0m [32m [     2.72317s,  INFO] TimeLimit:
[2m[36m(pid=32138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32138)[0m - action_space = Box(2,)
[2m[36m(pid=32138)[0m - observation_space = Box(9,)
[2m[36m(pid=32138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32138)[0m - _max_episode_steps = 150
[2m[36m(pid=32138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51285s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51368s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51438s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51493s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51565s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51630s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51691s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51750s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m [32m [     2.51808s,  INFO] TimeLimit:
[2m[36m(pid=32136)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32136)[0m - action_space = Box(2,)
[2m[36m(pid=32136)[0m - observation_space = Box(9,)
[2m[36m(pid=32136)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32136)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32136)[0m - _max_episode_steps = 150
[2m[36m(pid=32136)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32136)[0m W0717 13:56:39.236655 140657034143168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32136)[0m Instructions for updating:
[2m[36m(pid=32136)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32137)[0m [32m [     2.47517s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47567s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47611s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47655s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47714s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47780s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47825s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47867s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47924s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.47982s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.48025s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.48071s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.48121s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m W0717 13:56:39.393712 140430843069888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32137)[0m Instructions for updating:
[2m[36m(pid=32137)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.48169s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32137)[0m [32m [     2.48214s,  INFO] TimeLimit:
[2m[36m(pid=32137)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32137)[0m - action_space = Box(2,)
[2m[36m(pid=32137)[0m - observation_space = Box(9,)
[2m[36m(pid=32137)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32137)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32137)[0m - _max_episode_steps = 150
[2m[36m(pid=32137)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.09759s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.09833s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.09906s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.09979s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10049s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10117s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10205s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10289s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10362s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10423s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10494s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10573s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10641s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m W0717 13:56:39.430602 139631900251584 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32135)[0m Instructions for updating:
[2m[36m(pid=32135)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10715s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32135)[0m [32m [     2.10784s,  INFO] TimeLimit:
[2m[36m(pid=32135)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32135)[0m - action_space = Box(2,)
[2m[36m(pid=32135)[0m - observation_space = Box(9,)
[2m[36m(pid=32135)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32135)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32135)[0m - _max_episode_steps = 150
[2m[36m(pid=32135)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.18707s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.18804s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.18879s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.18950s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19023s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19104s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19174s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19252s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19324s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19391s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19497s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19558s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19637s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m W0717 13:56:39.474547 140526417700288 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32140)[0m Instructions for updating:
[2m[36m(pid=32140)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19719s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32140)[0m [32m [     2.19785s,  INFO] TimeLimit:
[2m[36m(pid=32140)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32140)[0m - action_space = Box(2,)
[2m[36m(pid=32140)[0m - observation_space = Box(9,)
[2m[36m(pid=32140)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32140)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32140)[0m - _max_episode_steps = 150
[2m[36m(pid=32140)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m W0717 13:56:39.639721 139964048598464 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32139)[0m Instructions for updating:
[2m[36m(pid=32139)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32139)[0m [32m [     2.08033s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08105s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08157s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08209s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08262s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08313s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08368s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08435s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08502s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08557s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08609s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08666s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08719s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08769s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32139)[0m [32m [     2.08814s,  INFO] TimeLimit:
[2m[36m(pid=32139)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32139)[0m - action_space = Box(2,)
[2m[36m(pid=32139)[0m - observation_space = Box(9,)
[2m[36m(pid=32139)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32139)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32139)[0m - _max_episode_steps = 150
[2m[36m(pid=32139)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32084)[0m 2019-07-17 13:56:39,729	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.002, max=0.155, mean=0.079),
[2m[36m(pid=32084)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-2.477, max=2.704, mean=0.062),
[2m[36m(pid=32084)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-35.169, max=-2.697, mean=-16.612),
[2m[36m(pid=32084)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.006, max=0.013, mean=0.001),
[2m[36m(pid=32084)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=32084)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=354661126.0, max=354661126.0, mean=354661126.0),
[2m[36m(pid=32084)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=32084)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-3.881, max=3.881, mean=0.09),
[2m[36m(pid=32084)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-3.881, max=3.881, mean=0.092),
[2m[36m(pid=32084)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-2.477, max=2.704, mean=0.061),
[2m[36m(pid=32084)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-3.214, max=1.792, mean=-0.118),
[2m[36m(pid=32084)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-3.214, max=1.792, mean=-0.136),
[2m[36m(pid=32084)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32084)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-35.171, max=-2.695, mean=-16.609),
[2m[36m(pid=32084)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.003, max=0.006, mean=0.002)},
[2m[36m(pid=32084)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m 2019-07-17 13:56:40,964	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32084)[0m { 'data': { 'action_prob': np.ndarray((3300,), dtype=float32, min=0.0, max=0.159, mean=0.079),
[2m[36m(pid=32084)[0m             'actions': np.ndarray((3300, 2), dtype=float32, min=-3.707, max=3.652, mean=-0.007),
[2m[36m(pid=32084)[0m             'advantages': np.ndarray((3300,), dtype=float32, min=-41.646, max=20.293, mean=-9.301),
[2m[36m(pid=32084)[0m             'agent_index': np.ndarray((3300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32084)[0m             'behaviour_logits': np.ndarray((3300, 4), dtype=float32, min=-0.015, max=0.014, mean=0.0),
[2m[36m(pid=32084)[0m             'dones': np.ndarray((3300,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=32084)[0m             'eps_id': np.ndarray((3300,), dtype=int64, min=81121958.0, max=1995531990.0, mean=1074154161.636),
[2m[36m(pid=32084)[0m             'infos': np.ndarray((3300,), dtype=object, head={}),
[2m[36m(pid=32084)[0m             'new_obs': np.ndarray((3300, 9), dtype=float32, min=-5.059, max=5.387, mean=-0.007),
[2m[36m(pid=32084)[0m             'obs': np.ndarray((3300, 9), dtype=float32, min=-5.059, max=5.387, mean=-0.008),
[2m[36m(pid=32084)[0m             'prev_actions': np.ndarray((3300, 2), dtype=float32, min=-3.707, max=3.652, mean=-0.008),
[2m[36m(pid=32084)[0m             'prev_rewards': np.ndarray((3300,), dtype=float32, min=-6.785, max=7.618, mean=-0.12),
[2m[36m(pid=32084)[0m             'rewards': np.ndarray((3300,), dtype=float32, min=-6.785, max=7.618, mean=-0.121),
[2m[36m(pid=32084)[0m             't': np.ndarray((3300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32084)[0m             'unroll_id': np.ndarray((3300,), dtype=int64, min=0.0, max=1.0, mean=0.273),
[2m[36m(pid=32084)[0m             'value_targets': np.ndarray((3300,), dtype=float32, min=-41.645, max=20.288, mean=-9.301),
[2m[36m(pid=32084)[0m             'vf_preds': np.ndarray((3300,), dtype=float32, min=-0.008, max=0.01, mean=0.001)},
[2m[36m(pid=32084)[0m   'type': 'SampleBatch'}
[2m[36m(pid=32084)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:41,945	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m { 'inputs': [ np.ndarray((26400, 2), dtype=float32, min=-4.111, max=4.402, mean=-0.005),
[2m[36m(pid=32075)[0m               np.ndarray((26400,), dtype=float32, min=-8.252, max=9.035, mean=-0.098),
[2m[36m(pid=32075)[0m               np.ndarray((26400, 9), dtype=float32, min=-6.432, max=5.579, mean=-0.005),
[2m[36m(pid=32075)[0m               np.ndarray((26400, 2), dtype=float32, min=-4.111, max=4.402, mean=-0.004),
[2m[36m(pid=32075)[0m               np.ndarray((26400,), dtype=float32, min=-3.609, max=4.065, mean=0.0),
[2m[36m(pid=32075)[0m               np.ndarray((26400, 4), dtype=float32, min=-0.017, max=0.016, mean=0.0),
[2m[36m(pid=32075)[0m               np.ndarray((26400,), dtype=float32, min=-44.745, max=35.587, mean=-6.96),
[2m[36m(pid=32075)[0m               np.ndarray((26400,), dtype=float32, min=-0.01, max=0.01, mean=0.0)],
[2m[36m(pid=32075)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32075)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=32075)[0m   'state_inputs': []}
[2m[36m(pid=32075)[0m 
[2m[36m(pid=32075)[0m 2019-07-17 13:56:41,945	INFO multi_gpu_impl.py:191 -- Divided 26400 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-56-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.496102391128147
  episode_reward_mean: -14.853756038847813
  episode_reward_min: -55.52208305094269
  episodes_this_iter: 176
  episodes_total: 176
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3017.774
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.838405132293701
        kl: 0.0010858959285542369
        policy_loss: -0.004661117680370808
        total_loss: 94.45654296875
        vf_explained_var: 0.14765506982803345
        vf_loss: 94.46012878417969
    load_time_ms: 42.694
    num_steps_sampled: 26400
    num_steps_trained: 26000
    sample_time_ms: 3243.534
    update_time_ms: 741.237
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.402774527330537
    mean_inference_ms: 1.5145057572887206
    mean_processing_ms: 2.5148911413031634
  time_since_restore: 7.100729942321777
  time_this_iter_s: 7.100729942321777
  time_total_s: 7.100729942321777
  timestamp: 1563364605
  timesteps_since_restore: 26400
  timesteps_this_iter: 26400
  timesteps_total: 26400
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 7 s, 1 iter, 26400 ts, -14.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-56-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.631011974295124
  episode_reward_mean: -14.600968896116957
  episode_reward_min: -54.22248171797919
  episodes_this_iter: 176
  episodes_total: 528
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2780.467
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.834473133087158
        kl: 0.004557757172733545
        policy_loss: -0.004381801467388868
        total_loss: 67.67920684814453
        vf_explained_var: 0.35882753133773804
        vf_loss: 67.68244934082031
    load_time_ms: 14.764
    num_steps_sampled: 79200
    num_steps_trained: 78000
    sample_time_ms: 2481.754
    update_time_ms: 249.941
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.1381279971429152
    mean_inference_ms: 1.1782669548874638
    mean_processing_ms: 2.4543219512292707
  time_since_restore: 16.667847394943237
  time_this_iter_s: 4.651875734329224
  time_total_s: 16.667847394943237
  timestamp: 1563364614
  timesteps_since_restore: 79200
  timesteps_this_iter: 26400
  timesteps_total: 79200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 16 s, 3 iter, 79200 ts, -14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.87408912771985
  episode_reward_mean: -11.90417679769135
  episode_reward_min: -44.69566947719077
  episodes_this_iter: 176
  episodes_total: 880
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2779.488
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.815643310546875
        kl: 0.007845820859074593
        policy_loss: -0.007511905860155821
        total_loss: 53.777156829833984
        vf_explained_var: 0.38876914978027344
        vf_loss: 53.783687591552734
    load_time_ms: 9.161
    num_steps_sampled: 132000
    num_steps_trained: 130000
    sample_time_ms: 2270.027
    update_time_ms: 151.935
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.076713747340803
    mean_inference_ms: 1.1111314502370007
    mean_processing_ms: 2.463597073397528
  time_since_restore: 26.175150394439697
  time_this_iter_s: 5.0178046226501465
  time_total_s: 26.175150394439697
  timestamp: 1563364624
  timesteps_since_restore: 132000
  timesteps_this_iter: 26400
  timesteps_total: 132000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 26 s, 5 iter, 132000 ts, -11.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.951435616946426
  episode_reward_mean: -10.180052383323657
  episode_reward_min: -40.92070393519475
  episodes_this_iter: 176
  episodes_total: 1056
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2782.304
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.806056499481201
        kl: 0.007334904745221138
        policy_loss: -0.008022459223866463
        total_loss: 50.05126190185547
        vf_explained_var: 0.4152289032936096
        vf_loss: 50.05836486816406
    load_time_ms: 7.769
    num_steps_sampled: 158400
    num_steps_trained: 156000
    sample_time_ms: 2268.947
    update_time_ms: 127.436
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9923859725486652
    mean_inference_ms: 1.0668942442577871
    mean_processing_ms: 2.3979010372764926
  time_since_restore: 31.25505256652832
  time_this_iter_s: 5.079902172088623
  time_total_s: 31.25505256652832
  timestamp: 1563364629
  timesteps_since_restore: 158400
  timesteps_this_iter: 26400
  timesteps_total: 158400
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 31 s, 6 iter, 158400 ts, -10.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.54053315038014
  episode_reward_mean: -7.388284226628136
  episode_reward_min: -36.38017377243565
  episodes_this_iter: 176
  episodes_total: 1408
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2734.103
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.792963743209839
        kl: 0.007421220187097788
        policy_loss: -0.00744861364364624
        total_loss: 39.44935607910156
        vf_explained_var: 0.4528474509716034
        vf_loss: 39.455875396728516
    load_time_ms: 6.035
    num_steps_sampled: 211200
    num_steps_trained: 208000
    sample_time_ms: 2211.596
    update_time_ms: 96.585
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.013021478576145
    mean_inference_ms: 1.0563267339277873
    mean_processing_ms: 2.4417036778402714
  time_since_restore: 40.553189277648926
  time_this_iter_s: 5.231726408004761
  time_total_s: 40.553189277648926
  timestamp: 1563364638
  timesteps_since_restore: 211200
  timesteps_this_iter: 26400
  timesteps_total: 211200
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 40 s, 8 iter, 211200 ts, -7.39 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.827727433152008
  episode_reward_mean: -6.384212740722841
  episode_reward_min: -31.44829326930833
  episodes_this_iter: 176
  episodes_total: 1760
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2728.812
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.777942657470703
        kl: 0.007406384684145451
        policy_loss: -0.00742337154224515
        total_loss: 26.807647705078125
        vf_explained_var: 0.5079652667045593
        vf_loss: 26.814146041870117
    load_time_ms: 5.006
    num_steps_sampled: 264000
    num_steps_trained: 260000
    sample_time_ms: 2221.601
    update_time_ms: 78.034
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.934279081153842
    mean_inference_ms: 1.0196155664673106
    mean_processing_ms: 2.378401787259715
  time_since_restore: 50.535893201828
  time_this_iter_s: 5.1057586669921875
  time_total_s: 50.535893201828
  timestamp: 1563364648
  timesteps_since_restore: 264000
  timesteps_this_iter: 26400
  timesteps_total: 264000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 50 s, 10 iter, 264000 ts, -6.38 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.511307263880816
  episode_reward_mean: -4.45731763229851
  episode_reward_min: -41.93307073304048
  episodes_this_iter: 176
  episodes_total: 2112
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2682.852
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.759369134902954
        kl: 0.010291069746017456
        policy_loss: -0.00952464435249567
        total_loss: 24.727365493774414
        vf_explained_var: 0.5666095018386841
        vf_loss: 24.73560333251953
    load_time_ms: 0.8
    num_steps_sampled: 316800
    num_steps_trained: 312000
    sample_time_ms: 2043.113
    update_time_ms: 4.642
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.967219167141694
    mean_inference_ms: 1.0334321611921855
    mean_processing_ms: 2.4347193099254913
  time_since_restore: 59.48560929298401
  time_this_iter_s: 4.570284843444824
  time_total_s: 59.48560929298401
  timestamp: 1563364657
  timesteps_since_restore: 316800
  timesteps_this_iter: 26400
  timesteps_total: 316800
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 59 s, 12 iter, 316800 ts, -4.46 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.927624308559913
  episode_reward_mean: -1.9973889781975507
  episode_reward_min: -38.899787256667864
  episodes_this_iter: 176
  episodes_total: 2464
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2666.003
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.736074209213257
        kl: 0.008710161782801151
        policy_loss: -0.008921591565012932
        total_loss: 19.770782470703125
        vf_explained_var: 0.6575224995613098
        vf_loss: 19.778614044189453
    load_time_ms: 0.797
    num_steps_sampled: 369600
    num_steps_trained: 364000
    sample_time_ms: 2100.734
    update_time_ms: 4.191
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9380374797899367
    mean_inference_ms: 1.0160778319314547
    mean_processing_ms: 2.4052072777226794
  time_since_restore: 69.02587342262268
  time_this_iter_s: 4.9108006954193115
  time_total_s: 69.02587342262268
  timestamp: 1563364667
  timesteps_since_restore: 369600
  timesteps_this_iter: 26400
  timesteps_total: 369600
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 69 s, 14 iter, 369600 ts, -2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.696399679389206
  episode_reward_mean: 0.6729793096802015
  episode_reward_min: -25.958766306427574
  episodes_this_iter: 176
  episodes_total: 2640
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2679.697
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7213947772979736
        kl: 0.010356143116950989
        policy_loss: -0.010263131000101566
        total_loss: 17.09588050842285
        vf_explained_var: 0.6406644582748413
        vf_loss: 17.10485076904297
    load_time_ms: 0.796
    num_steps_sampled: 396000
    num_steps_trained: 390000
    sample_time_ms: 2091.01
    update_time_ms: 4.361
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9414605568279018
    mean_inference_ms: 1.0165395864422153
    mean_processing_ms: 2.4110160874070172
  time_since_restore: 74.08355832099915
  time_this_iter_s: 5.057684898376465
  time_total_s: 74.08355832099915
  timestamp: 1563364672
  timesteps_since_restore: 396000
  timesteps_this_iter: 26400
  timesteps_total: 396000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 74 s, 15 iter, 396000 ts, 0.673 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-57-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.48898000725095
  episode_reward_mean: 1.4164403694244467
  episode_reward_min: -23.592101769312183
  episodes_this_iter: 176
  episodes_total: 2816
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2660.476
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7078440189361572
        kl: 0.009115785360336304
        policy_loss: -0.010072092525660992
        total_loss: 16.110374450683594
        vf_explained_var: 0.6871687769889832
        vf_loss: 16.119306564331055
    load_time_ms: 0.792
    num_steps_sampled: 422400
    num_steps_trained: 416000
    sample_time_ms: 2125.443
    update_time_ms: 4.213
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9338913007391896
    mean_inference_ms: 1.0134253968757585
    mean_processing_ms: 2.401339792711918
  time_since_restore: 79.3137276172638
  time_this_iter_s: 5.230169296264648
  time_total_s: 79.3137276172638
  timestamp: 1563364677
  timesteps_since_restore: 422400
  timesteps_this_iter: 26400
  timesteps_total: 422400
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 79 s, 16 iter, 422400 ts, 1.42 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.50586226844636
  episode_reward_mean: 2.078869376134026
  episode_reward_min: -26.760019828065932
  episodes_this_iter: 176
  episodes_total: 2992
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2686.101
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7031707763671875
        kl: 0.009782827459275723
        policy_loss: -0.010867549106478691
        total_loss: 14.704463005065918
        vf_explained_var: 0.6647622585296631
        vf_loss: 14.714106559753418
    load_time_ms: 0.781
    num_steps_sampled: 448800
    num_steps_trained: 442000
    sample_time_ms: 2229.54
    update_time_ms: 4.144
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.961057916662297
    mean_inference_ms: 1.0230775596636648
    mean_processing_ms: 2.4391330364616923
  time_since_restore: 84.67628049850464
  time_this_iter_s: 5.362552881240845
  time_total_s: 84.67628049850464
  timestamp: 1563364682
  timesteps_since_restore: 448800
  timesteps_this_iter: 26400
  timesteps_total: 448800
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 84 s, 17 iter, 448800 ts, 2.08 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.7346249072573
  episode_reward_mean: 3.6031869865208224
  episode_reward_min: -30.459836753311837
  episodes_this_iter: 176
  episodes_total: 3344
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2706.477
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.678692102432251
        kl: 0.008816503919661045
        policy_loss: -0.011676562950015068
        total_loss: 12.939268112182617
        vf_explained_var: 0.7163204550743103
        vf_loss: 12.949843406677246
    load_time_ms: 0.796
    num_steps_sampled: 501600
    num_steps_trained: 494000
    sample_time_ms: 2186.168
    update_time_ms: 4.189
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9497137919491405
    mean_inference_ms: 1.0199070319594548
    mean_processing_ms: 2.4212871169326267
  time_since_restore: 94.55609774589539
  time_this_iter_s: 5.26970362663269
  time_total_s: 94.55609774589539
  timestamp: 1563364692
  timesteps_since_restore: 501600
  timesteps_this_iter: 26400
  timesteps_total: 501600
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 94 s, 19 iter, 501600 ts, 3.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.5256567705048
  episode_reward_mean: 6.103910198681214
  episode_reward_min: -20.466276887082657
  episodes_this_iter: 176
  episodes_total: 3696
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2802.905
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.653614044189453
        kl: 0.008846951648592949
        policy_loss: -0.011328338645398617
        total_loss: 8.180814743041992
        vf_explained_var: 0.8229624629020691
        vf_loss: 8.19103717803955
    load_time_ms: 0.783
    num_steps_sampled: 554400
    num_steps_trained: 546000
    sample_time_ms: 2209.69
    update_time_ms: 3.868
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.968562365811341
    mean_inference_ms: 1.0236079667175717
    mean_processing_ms: 2.435604619465638
  time_since_restore: 105.23425149917603
  time_this_iter_s: 5.941868305206299
  time_total_s: 105.23425149917603
  timestamp: 1563364703
  timesteps_since_restore: 554400
  timesteps_this_iter: 26400
  timesteps_total: 554400
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 105 s, 21 iter, 554400 ts, 6.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.419794721172718
  episode_reward_mean: 8.026354799123878
  episode_reward_min: -12.8431056860688
  episodes_this_iter: 176
  episodes_total: 4048
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2864.407
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6245815753936768
        kl: 0.009097808040678501
        policy_loss: -0.011873850598931313
        total_loss: 5.963079452514648
        vf_explained_var: 0.8652672171592712
        vf_loss: 5.973816394805908
    load_time_ms: 0.789
    num_steps_sampled: 607200
    num_steps_trained: 598000
    sample_time_ms: 2279.928
    update_time_ms: 4.002
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9890078327931944
    mean_inference_ms: 1.0340938773333923
    mean_processing_ms: 2.446957755330371
  time_since_restore: 115.75982236862183
  time_this_iter_s: 5.643950462341309
  time_total_s: 115.75982236862183
  timestamp: 1563364713
  timesteps_since_restore: 607200
  timesteps_this_iter: 26400
  timesteps_total: 607200
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 115 s, 23 iter, 607200 ts, 8.03 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.505396934323137
  episode_reward_mean: 8.810429783626521
  episode_reward_min: -30.02292911017499
  episodes_this_iter: 176
  episodes_total: 4224
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2870.601
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6190590858459473
        kl: 0.009931419976055622
        policy_loss: -0.012417109683156013
        total_loss: 5.630666732788086
        vf_explained_var: 0.8720723390579224
        vf_loss: 5.641842365264893
    load_time_ms: 0.793
    num_steps_sampled: 633600
    num_steps_trained: 624000
    sample_time_ms: 2287.981
    update_time_ms: 4.229
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.981543073399793
    mean_inference_ms: 1.0311764357548572
    mean_processing_ms: 2.452874354866944
  time_since_restore: 120.81606125831604
  time_this_iter_s: 5.056238889694214
  time_total_s: 120.81606125831604
  timestamp: 1563364719
  timesteps_since_restore: 633600
  timesteps_this_iter: 26400
  timesteps_total: 633600
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 120 s, 24 iter, 633600 ts, 8.81 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.669901354106102
  episode_reward_mean: 8.169632589012075
  episode_reward_min: -13.716132388788548
  episodes_this_iter: 176
  episodes_total: 4400
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2867.621
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6078145503997803
        kl: 0.010419610887765884
        policy_loss: -0.012660782784223557
        total_loss: 4.324886322021484
        vf_explained_var: 0.8806256651878357
        vf_loss: 4.336245059967041
    load_time_ms: 0.798
    num_steps_sampled: 660000
    num_steps_trained: 650000
    sample_time_ms: 2285.816
    update_time_ms: 3.987
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.975280516423001
    mean_inference_ms: 1.0285926406210566
    mean_processing_ms: 2.4413140038885452
  time_since_restore: 125.81898355484009
  time_this_iter_s: 5.002922296524048
  time_total_s: 125.81898355484009
  timestamp: 1563364724
  timesteps_since_restore: 660000
  timesteps_this_iter: 26400
  timesteps_total: 660000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 125 s, 25 iter, 660000 ts, 8.17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.17475468401927
  episode_reward_mean: 10.547132225088548
  episode_reward_min: -14.778690003823556
  episodes_this_iter: 176
  episodes_total: 4576
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2904.062
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6007184982299805
        kl: 0.0109539944678545
        policy_loss: -0.009618381038308144
        total_loss: 3.463663339614868
        vf_explained_var: 0.9147487282752991
        vf_loss: 3.471912384033203
    load_time_ms: 0.802
    num_steps_sampled: 686400
    num_steps_trained: 676000
    sample_time_ms: 2272.6
    update_time_ms: 4.019
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.969724978042174
    mean_inference_ms: 1.025383245470215
    mean_processing_ms: 2.4454127489292503
  time_since_restore: 131.28479552268982
  time_this_iter_s: 5.4658119678497314
  time_total_s: 131.28479552268982
  timestamp: 1563364729
  timesteps_since_restore: 686400
  timesteps_this_iter: 26400
  timesteps_total: 686400
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 131 s, 26 iter, 686400 ts, 10.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-58-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.879094513156048
  episode_reward_mean: 9.913694373933241
  episode_reward_min: -24.07991133462499
  episodes_this_iter: 176
  episodes_total: 4928
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2916.495
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.57852840423584
        kl: 0.009763790294528008
        policy_loss: -0.011963652446866035
        total_loss: 3.4159798622131348
        vf_explained_var: 0.9070589542388916
        vf_loss: 3.426723003387451
    load_time_ms: 0.789
    num_steps_sampled: 739200
    num_steps_trained: 728000
    sample_time_ms: 2293.777
    update_time_ms: 4.062
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9904902639077235
    mean_inference_ms: 1.0304104720512923
    mean_processing_ms: 2.4639970812782934
  time_since_restore: 141.5923261642456
  time_this_iter_s: 5.807654142379761
  time_total_s: 141.5923261642456
  timestamp: 1563364739
  timesteps_since_restore: 739200
  timesteps_this_iter: 26400
  timesteps_total: 739200
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 141 s, 28 iter, 739200 ts, 9.91 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.70776843798038
  episode_reward_mean: 11.277416754065909
  episode_reward_min: -10.295402782780936
  episodes_this_iter: 176
  episodes_total: 5104
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2920.554
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5732247829437256
        kl: 0.009176503866910934
        policy_loss: -0.01095345988869667
        total_loss: 2.65395188331604
        vf_explained_var: 0.9297869205474854
        vf_loss: 2.6637582778930664
    load_time_ms: 0.785
    num_steps_sampled: 765600
    num_steps_trained: 754000
    sample_time_ms: 2279.088
    update_time_ms: 4.11
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9810007657790654
    mean_inference_ms: 1.0272207115651648
    mean_processing_ms: 2.4580795715020143
  time_since_restore: 146.75726795196533
  time_this_iter_s: 5.164941787719727
  time_total_s: 146.75726795196533
  timestamp: 1563364745
  timesteps_since_restore: 765600
  timesteps_this_iter: 26400
  timesteps_total: 765600
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 146 s, 29 iter, 765600 ts, 11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.721054636159902
  episode_reward_mean: 11.777695364483499
  episode_reward_min: -14.854026925067503
  episodes_this_iter: 176
  episodes_total: 5280
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2899.192
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.556434154510498
        kl: 0.009252997115254402
        policy_loss: -0.01098809763789177
        total_loss: 1.9244264364242554
        vf_explained_var: 0.9523588418960571
        vf_loss: 1.934257984161377
    load_time_ms: 0.773
    num_steps_sampled: 792000
    num_steps_trained: 780000
    sample_time_ms: 2332.186
    update_time_ms: 4.082
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9755285353631407
    mean_inference_ms: 1.0252412260295165
    mean_processing_ms: 2.4527688001851904
  time_since_restore: 151.8104124069214
  time_this_iter_s: 5.053144454956055
  time_total_s: 151.8104124069214
  timestamp: 1563364750
  timesteps_since_restore: 792000
  timesteps_this_iter: 26400
  timesteps_total: 792000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 151 s, 30 iter, 792000 ts, 11.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.028728022228606
  episode_reward_mean: 13.440351829680246
  episode_reward_min: -7.842750422411408
  episodes_this_iter: 176
  episodes_total: 5632
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2814.379
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5270402431488037
        kl: 0.010325537994503975
        policy_loss: -0.011756649240851402
        total_loss: 1.4525294303894043
        vf_explained_var: 0.9629453420639038
        vf_loss: 1.4629955291748047
    load_time_ms: 0.774
    num_steps_sampled: 844800
    num_steps_trained: 832000
    sample_time_ms: 2322.448
    update_time_ms: 4.303
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.981058550642487
    mean_inference_ms: 1.0252201790541222
    mean_processing_ms: 2.465389025925781
  time_since_restore: 161.69311833381653
  time_this_iter_s: 5.362127304077148
  time_total_s: 161.69311833381653
  timestamp: 1563364759
  timesteps_since_restore: 844800
  timesteps_this_iter: 26400
  timesteps_total: 844800
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 161 s, 32 iter, 844800 ts, 13.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.81959257901848
  episode_reward_mean: 12.51815863735956
  episode_reward_min: -9.058630848587878
  episodes_this_iter: 176
  episodes_total: 5808
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2818.01
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.519820213317871
        kl: 0.01066409982740879
        policy_loss: -0.01208764873445034
        total_loss: 1.4373730421066284
        vf_explained_var: 0.9637020826339722
        vf_loss: 1.4481277465820312
    load_time_ms: 0.774
    num_steps_sampled: 871200
    num_steps_trained: 858000
    sample_time_ms: 2284.635
    update_time_ms: 4.271
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.995357740527077
    mean_inference_ms: 1.0315321050664878
    mean_processing_ms: 2.473417599955568
  time_since_restore: 166.99916911125183
  time_this_iter_s: 5.306050777435303
  time_total_s: 166.99916911125183
  timestamp: 1563364765
  timesteps_since_restore: 871200
  timesteps_this_iter: 26400
  timesteps_total: 871200
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 166 s, 33 iter, 871200 ts, 12.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.071524879414945
  episode_reward_mean: 12.66333650320305
  episode_reward_min: -8.372852345746331
  episodes_this_iter: 176
  episodes_total: 5984
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2827.099
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5047695636749268
        kl: 0.01026509515941143
        policy_loss: -0.01117927860468626
        total_loss: 1.211367130279541
        vf_explained_var: 0.9683388471603394
        vf_loss: 1.2212632894515991
    load_time_ms: 0.775
    num_steps_sampled: 897600
    num_steps_trained: 884000
    sample_time_ms: 2296.014
    update_time_ms: 4.288
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.972570129749577
    mean_inference_ms: 1.0223726736639103
    mean_processing_ms: 2.452883209938362
  time_since_restore: 172.26228165626526
  time_this_iter_s: 5.263112545013428
  time_total_s: 172.26228165626526
  timestamp: 1563364770
  timesteps_since_restore: 897600
  timesteps_this_iter: 26400
  timesteps_total: 897600
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 172 s, 34 iter, 897600 ts, 12.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.48972479373898
  episode_reward_mean: 11.282159583987102
  episode_reward_min: -8.541998905301284
  episodes_this_iter: 176
  episodes_total: 6160
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2852.078
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.489957094192505
        kl: 0.0102738868445158
        policy_loss: -0.012135803699493408
        total_loss: 1.0873610973358154
        vf_explained_var: 0.9676546454429626
        vf_loss: 1.098212480545044
    load_time_ms: 0.772
    num_steps_sampled: 924000
    num_steps_trained: 910000
    sample_time_ms: 2329.863
    update_time_ms: 4.349
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9722800972786647
    mean_inference_ms: 1.0205144829942017
    mean_processing_ms: 2.451294181230081
  time_since_restore: 177.855073928833
  time_this_iter_s: 5.592792272567749
  time_total_s: 177.855073928833
  timestamp: 1563364776
  timesteps_since_restore: 924000
  timesteps_this_iter: 26400
  timesteps_total: 924000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 177 s, 35 iter, 924000 ts, 11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.94511136015883
  episode_reward_mean: 13.336168294936995
  episode_reward_min: -6.741276449323446
  episodes_this_iter: 176
  episodes_total: 6336
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2841.582
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4784936904907227
        kl: 0.010590707883238792
        policy_loss: -0.012675419449806213
        total_loss: 1.0612263679504395
        vf_explained_var: 0.9747509360313416
        vf_loss: 1.072577953338623
    load_time_ms: 0.764
    num_steps_sampled: 950400
    num_steps_trained: 936000
    sample_time_ms: 2327.664
    update_time_ms: 4.342
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9917245067096
    mean_inference_ms: 1.0276411171896398
    mean_processing_ms: 2.471826597718675
  time_since_restore: 183.19274973869324
  time_this_iter_s: 5.3376758098602295
  time_total_s: 183.19274973869324
  timestamp: 1563364781
  timesteps_since_restore: 950400
  timesteps_this_iter: 26400
  timesteps_total: 950400
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 183 s, 36 iter, 950400 ts, 13.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.2534548120225
  episode_reward_mean: 12.941800092939895
  episode_reward_min: -10.10155825811018
  episodes_this_iter: 176
  episodes_total: 6688
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2825.412
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.458038568496704
        kl: 0.011025278829038143
        policy_loss: -0.01560346782207489
        total_loss: 0.9862320423126221
        vf_explained_var: 0.9745574593544006
        vf_loss: 1.0004574060440063
    load_time_ms: 0.758
    num_steps_sampled: 1003200
    num_steps_trained: 988000
    sample_time_ms: 2260.314
    update_time_ms: 4.529
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.97998844067866
    mean_inference_ms: 1.0237055194274256
    mean_processing_ms: 2.464212574081425
  time_since_restore: 192.67138528823853
  time_this_iter_s: 4.532760858535767
  time_total_s: 192.67138528823853
  timestamp: 1563364791
  timesteps_since_restore: 1003200
  timesteps_this_iter: 26400
  timesteps_total: 1003200
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 192 s, 38 iter, 1003200 ts, 12.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_13-59-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.26038715714259
  episode_reward_mean: 12.526818495755794
  episode_reward_min: -7.426997062731791
  episodes_this_iter: 176
  episodes_total: 6864
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2830.95
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.442774534225464
        kl: 0.011031225323677063
        policy_loss: -0.012721478007733822
        total_loss: 0.9464641809463501
        vf_explained_var: 0.9736150503158569
        vf_loss: 0.9578067064285278
    load_time_ms: 0.76
    num_steps_sampled: 1029600
    num_steps_trained: 1014000
    sample_time_ms: 2269.414
    update_time_ms: 4.732
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9774703292577853
    mean_inference_ms: 1.0221367289862788
    mean_processing_ms: 2.4637048115020987
  time_since_restore: 197.9840476512909
  time_this_iter_s: 5.312662363052368
  time_total_s: 197.9840476512909
  timestamp: 1563364796
  timesteps_since_restore: 1029600
  timesteps_this_iter: 26400
  timesteps_total: 1029600
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 197 s, 39 iter, 1029600 ts, 12.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.56354808465388
  episode_reward_mean: 13.793340916062592
  episode_reward_min: -6.114506476021708
  episodes_this_iter: 176
  episodes_total: 7040
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2836.375
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.433551788330078
        kl: 0.010364477522671223
        policy_loss: -0.01277339830994606
        total_loss: 1.0452771186828613
        vf_explained_var: 0.9721613526344299
        vf_loss: 1.0567548274993896
    load_time_ms: 0.763
    num_steps_sampled: 1056000
    num_steps_trained: 1040000
    sample_time_ms: 2266.365
    update_time_ms: 4.734
  iterations_since_restore: 40
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9875065075708367
    mean_inference_ms: 1.0253020524906378
    mean_processing_ms: 2.4718798331319576
  time_since_restore: 203.0615017414093
  time_this_iter_s: 5.077454090118408
  time_total_s: 203.0615017414093
  timestamp: 1563364801
  timesteps_since_restore: 1056000
  timesteps_this_iter: 26400
  timesteps_total: 1056000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 203 s, 40 iter, 1056000 ts, 13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.426532264482745
  episode_reward_mean: 13.909479393361634
  episode_reward_min: -5.641573404450848
  episodes_this_iter: 176
  episodes_total: 7392
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2826.893
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.400026798248291
        kl: 0.01236609648913145
        policy_loss: -0.013979803770780563
        total_loss: 0.7709319591522217
        vf_explained_var: 0.9802704453468323
        vf_loss: 0.7833660840988159
    load_time_ms: 0.759
    num_steps_sampled: 1108800
    num_steps_trained: 1092000
    sample_time_ms: 2264.686
    update_time_ms: 4.611
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.985671005948451
    mean_inference_ms: 1.0258280608611252
    mean_processing_ms: 2.4670820077082394
  time_since_restore: 212.8271768093109
  time_this_iter_s: 5.148452043533325
  time_total_s: 212.8271768093109
  timestamp: 1563364811
  timesteps_since_restore: 1108800
  timesteps_this_iter: 26400
  timesteps_total: 1108800
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 212 s, 42 iter, 1108800 ts, 13.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.072140524877575
  episode_reward_mean: 12.700052096374352
  episode_reward_min: -7.961387830921993
  episodes_this_iter: 176
  episodes_total: 7568
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2814.393
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3906664848327637
        kl: 0.011101569049060345
        policy_loss: -0.012812312692403793
        total_loss: 0.8368081450462341
        vf_explained_var: 0.9784063100814819
        vf_loss: 0.8482328057289124
    load_time_ms: 0.756
    num_steps_sampled: 1135200
    num_steps_trained: 1118000
    sample_time_ms: 2262.524
    update_time_ms: 4.642
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9702143923251483
    mean_inference_ms: 1.0179906111864478
    mean_processing_ms: 2.457520324103646
  time_since_restore: 217.98003840446472
  time_this_iter_s: 5.152861595153809
  time_total_s: 217.98003840446472
  timestamp: 1563364816
  timesteps_since_restore: 1135200
  timesteps_this_iter: 26400
  timesteps_total: 1135200
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 217 s, 43 iter, 1135200 ts, 12.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.71778234397767
  episode_reward_mean: 12.861847068796662
  episode_reward_min: -7.655027029733504
  episodes_this_iter: 176
  episodes_total: 7744
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2812.7
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.376241445541382
        kl: 0.010906470939517021
        policy_loss: -0.012450922280550003
        total_loss: 0.8199682831764221
        vf_explained_var: 0.9773880839347839
        vf_loss: 0.8310558795928955
    load_time_ms: 0.756
    num_steps_sampled: 1161600
    num_steps_trained: 1144000
    sample_time_ms: 2262.866
    update_time_ms: 4.504
  iterations_since_restore: 44
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9784537839518577
    mean_inference_ms: 1.0207441633596148
    mean_processing_ms: 2.4647131203820187
  time_since_restore: 223.22784090042114
  time_this_iter_s: 5.247802495956421
  time_total_s: 223.22784090042114
  timestamp: 1563364821
  timesteps_since_restore: 1161600
  timesteps_this_iter: 26400
  timesteps_total: 1161600
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 223 s, 44 iter, 1161600 ts, 12.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.099395369237634
  episode_reward_mean: 14.805253442465213
  episode_reward_min: -10.587509340270493
  episodes_this_iter: 176
  episodes_total: 8096
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2755.015
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.342085123062134
        kl: 0.011454137042164803
        policy_loss: -0.01602642610669136
        total_loss: 0.6813918352127075
        vf_explained_var: 0.9824378490447998
        vf_loss: 0.6959865093231201
    load_time_ms: 0.772
    num_steps_sampled: 1214400
    num_steps_trained: 1196000
    sample_time_ms: 2220.283
    update_time_ms: 4.601
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9641878531486325
    mean_inference_ms: 1.0158846885519859
    mean_processing_ms: 2.4518494000545115
  time_since_restore: 233.15677571296692
  time_this_iter_s: 4.999145746231079
  time_total_s: 233.15677571296692
  timestamp: 1563364831
  timesteps_since_restore: 1214400
  timesteps_this_iter: 26400
  timesteps_total: 1214400
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 233 s, 46 iter, 1214400 ts, 14.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.96477232513373
  episode_reward_mean: 15.098783704228572
  episode_reward_min: -15.158904372228761
  episodes_this_iter: 176
  episodes_total: 8448
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2772.257
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.326024055480957
        kl: 0.01188291609287262
        policy_loss: -0.014047290198504925
        total_loss: 0.7345778346061707
        vf_explained_var: 0.9823792576789856
        vf_loss: 0.7471396923065186
    load_time_ms: 0.771
    num_steps_sampled: 1267200
    num_steps_trained: 1248000
    sample_time_ms: 2243.18
    update_time_ms: 4.693
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9592763750224678
    mean_inference_ms: 1.013544681811205
    mean_processing_ms: 2.447138704752455
  time_since_restore: 243.03262281417847
  time_this_iter_s: 5.279756307601929
  time_total_s: 243.03262281417847
  timestamp: 1563364841
  timesteps_since_restore: 1267200
  timesteps_this_iter: 26400
  timesteps_total: 1267200
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 243 s, 48 iter, 1267200 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.01920877413238
  episode_reward_mean: 13.907552251245649
  episode_reward_min: -5.746427161950081
  episodes_this_iter: 176
  episodes_total: 8624
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2775.694
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3085267543792725
        kl: 0.011614418588578701
        policy_loss: -0.012610397301614285
        total_loss: 0.5754422545433044
        vf_explained_var: 0.9829068779945374
        vf_loss: 0.5866008400917053
    load_time_ms: 0.783
    num_steps_sampled: 1293600
    num_steps_trained: 1274000
    sample_time_ms: 2234.069
    update_time_ms: 4.381
  iterations_since_restore: 49
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9614647232667157
    mean_inference_ms: 1.0163306095338265
    mean_processing_ms: 2.446590677252658
  time_since_restore: 248.28608989715576
  time_this_iter_s: 5.253467082977295
  time_total_s: 248.28608989715576
  timestamp: 1563364846
  timesteps_since_restore: 1293600
  timesteps_this_iter: 26400
  timesteps_total: 1293600
  training_iteration: 49
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 248 s, 49 iter, 1293600 ts, 13.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.23180912714662
  episode_reward_mean: 13.968120349535035
  episode_reward_min: -5.995887488226884
  episodes_this_iter: 176
  episodes_total: 8800
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2805.845
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.297438383102417
        kl: 0.011526230722665787
        policy_loss: -0.012187400832772255
        total_loss: 0.5636121034622192
        vf_explained_var: 0.9840116500854492
        vf_loss: 0.5743587017059326
    load_time_ms: 0.783
    num_steps_sampled: 1320000
    num_steps_trained: 1300000
    sample_time_ms: 2237.872
    update_time_ms: 4.374
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9492188677844537
    mean_inference_ms: 1.0098982627005795
    mean_processing_ms: 2.439689340657706
  time_since_restore: 253.71138453483582
  time_this_iter_s: 5.425294637680054
  time_total_s: 253.71138453483582
  timestamp: 1563364852
  timesteps_since_restore: 1320000
  timesteps_this_iter: 26400
  timesteps_total: 1320000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 253 s, 50 iter, 1320000 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-00-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.336031421105766
  episode_reward_mean: 12.872760629936193
  episode_reward_min: -6.6871202635686515
  episodes_this_iter: 176
  episodes_total: 8976
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2839.644
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2847819328308105
        kl: 0.012829671613872051
        policy_loss: -0.014447527006268501
        total_loss: 0.5509909391403198
        vf_explained_var: 0.9823035597801208
        vf_loss: 0.5638347268104553
    load_time_ms: 0.789
    num_steps_sampled: 1346400
    num_steps_trained: 1326000
    sample_time_ms: 2259.332
    update_time_ms: 4.571
  iterations_since_restore: 51
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.952691871075841
    mean_inference_ms: 1.0105065735950556
    mean_processing_ms: 2.4393110760485204
  time_since_restore: 258.88681387901306
  time_this_iter_s: 5.175429344177246
  time_total_s: 258.88681387901306
  timestamp: 1563364857
  timesteps_since_restore: 1346400
  timesteps_this_iter: 26400
  timesteps_total: 1346400
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 258 s, 51 iter, 1346400 ts, 12.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.589738455005424
  episode_reward_mean: 14.705006652667846
  episode_reward_min: -6.301382657079453
  episodes_this_iter: 176
  episodes_total: 9152
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2899.332
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2697255611419678
        kl: 0.011516398750245571
        policy_loss: -0.013608141802251339
        total_loss: 0.49741777777671814
        vf_explained_var: 0.9865845441818237
        vf_loss: 0.5095863342285156
    load_time_ms: 0.785
    num_steps_sampled: 1372800
    num_steps_trained: 1352000
    sample_time_ms: 2296.63
    update_time_ms: 4.679
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.972294296499926
    mean_inference_ms: 1.0189772167142126
    mean_processing_ms: 2.4606463751645866
  time_since_restore: 265.00555634498596
  time_this_iter_s: 6.1187424659729
  time_total_s: 265.00555634498596
  timestamp: 1563364863
  timesteps_since_restore: 1372800
  timesteps_this_iter: 26400
  timesteps_total: 1372800
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 265 s, 52 iter, 1372800 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.932068084101836
  episode_reward_mean: 15.828473539658045
  episode_reward_min: -5.180013778978446
  episodes_this_iter: 176
  episodes_total: 9328
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2939.661
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2535147666931152
        kl: 0.013733863830566406
        policy_loss: -0.013110017403960228
        total_loss: 0.4797534942626953
        vf_explained_var: 0.9870489835739136
        vf_loss: 0.4911467730998993
    load_time_ms: 0.784
    num_steps_sampled: 1399200
    num_steps_trained: 1378000
    sample_time_ms: 2281.132
    update_time_ms: 4.632
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.974747816002531
    mean_inference_ms: 1.020737053911295
    mean_processing_ms: 2.461945165119448
  time_since_restore: 270.40748858451843
  time_this_iter_s: 5.401932239532471
  time_total_s: 270.40748858451843
  timestamp: 1563364868
  timesteps_since_restore: 1399200
  timesteps_this_iter: 26400
  timesteps_total: 1399200
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 270 s, 53 iter, 1399200 ts, 15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.77868668208687
  episode_reward_mean: 13.71717065928533
  episode_reward_min: -9.9839493575768
  episodes_this_iter: 176
  episodes_total: 9504
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2970.747
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.246762990951538
        kl: 0.01218267809599638
        policy_loss: -0.013042973354458809
        total_loss: 0.4926103353500366
        vf_explained_var: 0.9847206473350525
        vf_loss: 0.504130482673645
    load_time_ms: 0.792
    num_steps_sampled: 1425600
    num_steps_trained: 1404000
    sample_time_ms: 2289.632
    update_time_ms: 4.952
  iterations_since_restore: 54
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9948429040454143
    mean_inference_ms: 1.027983518159365
    mean_processing_ms: 2.4783130564837514
  time_since_restore: 276.05629444122314
  time_this_iter_s: 5.648805856704712
  time_total_s: 276.05629444122314
  timestamp: 1563364874
  timesteps_since_restore: 1425600
  timesteps_this_iter: 26400
  timesteps_total: 1425600
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 276 s, 54 iter, 1425600 ts, 13.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.11948415997399
  episode_reward_mean: 14.652699508669258
  episode_reward_min: -5.3081680729514815
  episodes_this_iter: 176
  episodes_total: 9680
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2999.092
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2297072410583496
        kl: 0.010888928547501564
        policy_loss: -0.014055252075195312
        total_loss: 0.4523462653160095
        vf_explained_var: 0.9878173470497131
        vf_loss: 0.4650403559207916
    load_time_ms: 0.801
    num_steps_sampled: 1452000
    num_steps_trained: 1430000
    sample_time_ms: 2341.897
    update_time_ms: 5.087
  iterations_since_restore: 55
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9846057119089546
    mean_inference_ms: 1.0258603677320397
    mean_processing_ms: 2.4714438370636227
  time_since_restore: 281.797425031662
  time_this_iter_s: 5.741130590438843
  time_total_s: 281.797425031662
  timestamp: 1563364880
  timesteps_since_restore: 1452000
  timesteps_this_iter: 26400
  timesteps_total: 1452000
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 281 s, 55 iter, 1452000 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.97565445083428
  episode_reward_mean: 14.05469468504541
  episode_reward_min: -3.96608963156734
  episodes_this_iter: 176
  episodes_total: 9856
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3041.404
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2162578105926514
        kl: 0.014203227125108242
        policy_loss: -0.01578061282634735
        total_loss: 0.3864975571632385
        vf_explained_var: 0.9877431392669678
        vf_loss: 0.4005028009414673
    load_time_ms: 0.79
    num_steps_sampled: 1478400
    num_steps_trained: 1456000
    sample_time_ms: 2350.293
    update_time_ms: 5.061
  iterations_since_restore: 56
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9837927385826193
    mean_inference_ms: 1.0260083132188882
    mean_processing_ms: 2.4724635422817722
  time_since_restore: 287.3031783103943
  time_this_iter_s: 5.5057532787323
  time_total_s: 287.3031783103943
  timestamp: 1563364885
  timesteps_since_restore: 1478400
  timesteps_this_iter: 26400
  timesteps_total: 1478400
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 287 s, 56 iter, 1478400 ts, 14.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.501992786059496
  episode_reward_mean: 15.389384044497952
  episode_reward_min: -6.5536061998287245
  episodes_this_iter: 176
  episodes_total: 10032
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3059.855
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.201338052749634
        kl: 0.012978240847587585
        policy_loss: -0.01422820519655943
        total_loss: 0.4152168929576874
        vf_explained_var: 0.9886046648025513
        vf_loss: 0.4278227686882019
    load_time_ms: 0.816
    num_steps_sampled: 1504800
    num_steps_trained: 1482000
    sample_time_ms: 2470.684
    update_time_ms: 4.832
  iterations_since_restore: 57
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0002221147731647
    mean_inference_ms: 1.0321927411775271
    mean_processing_ms: 2.485685451699147
  time_since_restore: 293.28959798812866
  time_this_iter_s: 5.986419677734375
  time_total_s: 293.28959798812866
  timestamp: 1563364891
  timesteps_since_restore: 1504800
  timesteps_this_iter: 26400
  timesteps_total: 1504800
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 293 s, 57 iter, 1504800 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.09973293771993
  episode_reward_mean: 15.914387997892966
  episode_reward_min: -5.1161307408942465
  episodes_this_iter: 176
  episodes_total: 10384
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3088.354
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1771726608276367
        kl: 0.011901474557816982
        policy_loss: -0.01340353675186634
        total_loss: 0.4251217544078827
        vf_explained_var: 0.9889599680900574
        vf_loss: 0.4370376467704773
    load_time_ms: 0.804
    num_steps_sampled: 1557600
    num_steps_trained: 1534000
    sample_time_ms: 2459.25
    update_time_ms: 4.761
  iterations_since_restore: 59
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0049622402966736
    mean_inference_ms: 1.034834195662665
    mean_processing_ms: 2.4913138499796705
  time_since_restore: 303.99320125579834
  time_this_iter_s: 5.95398211479187
  time_total_s: 303.99320125579834
  timestamp: 1563364902
  timesteps_since_restore: 1557600
  timesteps_this_iter: 26400
  timesteps_total: 1557600
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 303 s, 59 iter, 1557600 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.250102592624984
  episode_reward_mean: 15.835805301760047
  episode_reward_min: -3.6082369737047753
  episodes_this_iter: 176
  episodes_total: 10560
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3105.78
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1679773330688477
        kl: 0.011516251601278782
        policy_loss: -0.013218775391578674
        total_loss: 0.4043857157230377
        vf_explained_var: 0.9886819124221802
        vf_loss: 0.41616493463516235
    load_time_ms: 0.8
    num_steps_sampled: 1584000
    num_steps_trained: 1560000
    sample_time_ms: 2400.679
    update_time_ms: 4.83
  iterations_since_restore: 60
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9991237185341078
    mean_inference_ms: 1.0334717254959978
    mean_processing_ms: 2.485260053896223
  time_since_restore: 309.001455783844
  time_this_iter_s: 5.008254528045654
  time_total_s: 309.001455783844
  timestamp: 1563364907
  timesteps_since_restore: 1584000
  timesteps_this_iter: 26400
  timesteps_total: 1584000
  training_iteration: 60
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 309 s, 60 iter, 1584000 ts, 15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.11511259414816
  episode_reward_mean: 16.015504723350038
  episode_reward_min: -8.342778833267374
  episodes_this_iter: 176
  episodes_total: 10736
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3091.905
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1540892124176025
        kl: 0.01333360280841589
        policy_loss: -0.015358028933405876
        total_loss: 0.34652453660964966
        vf_explained_var: 0.9906892776489258
        vf_loss: 0.3602158725261688
    load_time_ms: 0.796
    num_steps_sampled: 1610400
    num_steps_trained: 1586000
    sample_time_ms: 2433.049
    update_time_ms: 4.63
  iterations_since_restore: 61
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.9974798154604616
    mean_inference_ms: 1.0322479065952228
    mean_processing_ms: 2.4825608523513236
  time_since_restore: 314.35712027549744
  time_this_iter_s: 5.355664491653442
  time_total_s: 314.35712027549744
  timestamp: 1563364913
  timesteps_since_restore: 1610400
  timesteps_this_iter: 26400
  timesteps_total: 1610400
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 314 s, 61 iter, 1610400 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-01-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.31094493948784
  episode_reward_mean: 15.053626579091883
  episode_reward_min: -10.320570819064196
  episodes_this_iter: 176
  episodes_total: 10912
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3071.941
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1483123302459717
        kl: 0.010958979837596416
        policy_loss: -0.016369061544537544
        total_loss: 0.34310489892959595
        vf_explained_var: 0.9903380274772644
        vf_loss: 0.35810407996177673
    load_time_ms: 0.802
    num_steps_sampled: 1636800
    num_steps_trained: 1612000
    sample_time_ms: 2420.905
    update_time_ms: 4.515
  iterations_since_restore: 62
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.00079569183943
    mean_inference_ms: 1.0340193046259658
    mean_processing_ms: 2.4857057691461164
  time_since_restore: 320.1590623855591
  time_this_iter_s: 5.8019421100616455
  time_total_s: 320.1590623855591
  timestamp: 1563364918
  timesteps_since_restore: 1636800
  timesteps_this_iter: 26400
  timesteps_total: 1636800
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 320 s, 62 iter, 1636800 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.865011285149635
  episode_reward_mean: 15.426158928794623
  episode_reward_min: -8.47261204227253
  episodes_this_iter: 176
  episodes_total: 11264
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3059.507
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1202239990234375
        kl: 0.01380298100411892
        policy_loss: -0.017715832218527794
        total_loss: 0.3240354359149933
        vf_explained_var: 0.9904130697250366
        vf_loss: 0.3400258719921112
    load_time_ms: 0.818
    num_steps_sampled: 1689600
    num_steps_trained: 1664000
    sample_time_ms: 2402.529
    update_time_ms: 4.435
  iterations_since_restore: 64
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0152838049344424
    mean_inference_ms: 1.041124643344055
    mean_processing_ms: 2.5004668349725283
  time_since_restore: 330.9042112827301
  time_this_iter_s: 5.89952278137207
  time_total_s: 330.9042112827301
  timestamp: 1563364929
  timesteps_since_restore: 1689600
  timesteps_this_iter: 26400
  timesteps_total: 1689600
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 330 s, 64 iter, 1689600 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.07895937658668
  episode_reward_mean: 15.235621797041292
  episode_reward_min: -7.5956891547623675
  episodes_this_iter: 176
  episodes_total: 11440
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3043.268
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.103266716003418
        kl: 0.013103441335260868
        policy_loss: -0.014790857210755348
        total_loss: 0.3626902997493744
        vf_explained_var: 0.9896031022071838
        vf_loss: 0.37584322690963745
    load_time_ms: 0.815
    num_steps_sampled: 1716000
    num_steps_trained: 1690000
    sample_time_ms: 2423.283
    update_time_ms: 4.286
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0217422401851977
    mean_inference_ms: 1.0436372156702203
    mean_processing_ms: 2.505017163353951
  time_since_restore: 336.6849226951599
  time_this_iter_s: 5.78071141242981
  time_total_s: 336.6849226951599
  timestamp: 1563364935
  timesteps_since_restore: 1716000
  timesteps_this_iter: 26400
  timesteps_total: 1716000
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 336 s, 65 iter, 1716000 ts, 15.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.049334650218505
  episode_reward_mean: 15.586158621437097
  episode_reward_min: -8.221635750956212
  episodes_this_iter: 176
  episodes_total: 11616
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3020.498
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.088102102279663
        kl: 0.014053504914045334
        policy_loss: -0.014334695413708687
        total_loss: 0.37649276852607727
        vf_explained_var: 0.9893983602523804
        vf_loss: 0.3890707790851593
    load_time_ms: 0.821
    num_steps_sampled: 1742400
    num_steps_trained: 1716000
    sample_time_ms: 2423.896
    update_time_ms: 4.245
  iterations_since_restore: 66
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.025307163748286
    mean_inference_ms: 1.04522188360487
    mean_processing_ms: 2.5083030071804457
  time_since_restore: 341.96757221221924
  time_this_iter_s: 5.282649517059326
  time_total_s: 341.96757221221924
  timestamp: 1563364940
  timesteps_since_restore: 1742400
  timesteps_this_iter: 26400
  timesteps_total: 1742400
  training_iteration: 66
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 341 s, 66 iter, 1742400 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.94964025038006
  episode_reward_mean: 15.166415118646187
  episode_reward_min: -7.330358124837439
  episodes_this_iter: 176
  episodes_total: 11968
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2973.782
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0609512329101562
        kl: 0.01285262405872345
        policy_loss: -0.017232241109013557
        total_loss: 0.2929608225822449
        vf_explained_var: 0.9905007481575012
        vf_loss: 0.3085864782333374
    load_time_ms: 0.803
    num_steps_sampled: 1795200
    num_steps_trained: 1768000
    sample_time_ms: 2414.102
    update_time_ms: 4.211
  iterations_since_restore: 68
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0084016772720052
    mean_inference_ms: 1.0386528344347286
    mean_processing_ms: 2.494073899258744
  time_since_restore: 352.13710284233093
  time_this_iter_s: 5.25564980506897
  time_total_s: 352.13710284233093
  timestamp: 1563364950
  timesteps_since_restore: 1795200
  timesteps_this_iter: 26400
  timesteps_total: 1795200
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 352 s, 68 iter, 1795200 ts, 15.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.654028586035956
  episode_reward_mean: 14.043618054160298
  episode_reward_min: -9.721799829731058
  episodes_this_iter: 176
  episodes_total: 12144
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2965.702
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0476534366607666
        kl: 0.014865168370306492
        policy_loss: -0.01647460274398327
        total_loss: 0.28154313564300537
        vf_explained_var: 0.9904785752296448
        vf_loss: 0.29615962505340576
    load_time_ms: 0.794
    num_steps_sampled: 1821600
    num_steps_trained: 1794000
    sample_time_ms: 2369.583
    update_time_ms: 4.502
  iterations_since_restore: 69
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0054033363255117
    mean_inference_ms: 1.0374339399597572
    mean_processing_ms: 2.4922710979857468
  time_since_restore: 357.5671937465668
  time_this_iter_s: 5.43009090423584
  time_total_s: 357.5671937465668
  timestamp: 1563364956
  timesteps_since_restore: 1821600
  timesteps_this_iter: 26400
  timesteps_total: 1821600
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 357 s, 69 iter, 1821600 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.739057662662496
  episode_reward_mean: 16.401766427664235
  episode_reward_min: -4.363732436792155
  episodes_this_iter: 176
  episodes_total: 12320
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2934.054
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0340356826782227
        kl: 0.014297385700047016
        policy_loss: -0.016021834686398506
        total_loss: 0.2632577121257782
        vf_explained_var: 0.992792010307312
        vf_loss: 0.27749237418174744
    load_time_ms: 0.794
    num_steps_sampled: 1848000
    num_steps_trained: 1820000
    sample_time_ms: 2400.643
    update_time_ms: 4.394
  iterations_since_restore: 70
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.020164442415983
    mean_inference_ms: 1.043267017925799
    mean_processing_ms: 2.505642174401403
  time_since_restore: 362.5648190975189
  time_this_iter_s: 4.997625350952148
  time_total_s: 362.5648190975189
  timestamp: 1563364961
  timesteps_since_restore: 1848000
  timesteps_this_iter: 26400
  timesteps_total: 1848000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 362 s, 70 iter, 1848000 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.25335577331069
  episode_reward_mean: 14.73539155147584
  episode_reward_min: -11.738299004345254
  episodes_this_iter: 176
  episodes_total: 12496
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2927.898
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0293710231781006
        kl: 0.013444481417536736
        policy_loss: -0.013606281951069832
        total_loss: 0.2636769413948059
        vf_explained_var: 0.9915506839752197
        vf_loss: 0.27560269832611084
    load_time_ms: 0.795
    num_steps_sampled: 1874400
    num_steps_trained: 1846000
    sample_time_ms: 2399.009
    update_time_ms: 4.368
  iterations_since_restore: 71
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0213975027336843
    mean_inference_ms: 1.0430584201074051
    mean_processing_ms: 2.5075042482692322
  time_since_restore: 367.84161162376404
  time_this_iter_s: 5.276792526245117
  time_total_s: 367.84161162376404
  timestamp: 1563364966
  timesteps_since_restore: 1874400
  timesteps_this_iter: 26400
  timesteps_total: 1874400
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 367 s, 71 iter, 1874400 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-02-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.32447226047532
  episode_reward_mean: 15.887963611542581
  episode_reward_min: -3.122197977717335
  episodes_this_iter: 176
  episodes_total: 12672
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2904.744
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.010836124420166
        kl: 0.013594912365078926
        policy_loss: -0.01576169952750206
        total_loss: 0.28385573625564575
        vf_explained_var: 0.991484522819519
        vf_loss: 0.29791805148124695
    load_time_ms: 0.8
    num_steps_sampled: 1900800
    num_steps_trained: 1872000
    sample_time_ms: 2352.231
    update_time_ms: 4.585
  iterations_since_restore: 72
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0097808924131755
    mean_inference_ms: 1.0373280418402955
    mean_processing_ms: 2.4942888184650793
  time_since_restore: 372.9395823478699
  time_this_iter_s: 5.097970724105835
  time_total_s: 372.9395823478699
  timestamp: 1563364971
  timesteps_since_restore: 1900800
  timesteps_this_iter: 26400
  timesteps_total: 1900800
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 372 s, 72 iter, 1900800 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.54478549894873
  episode_reward_mean: 16.319628696420963
  episode_reward_min: -4.236487357943565
  episodes_this_iter: 176
  episodes_total: 13024
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2858.672
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9871418476104736
        kl: 0.013276792131364346
        policy_loss: -0.017104359343647957
        total_loss: 0.23693135380744934
        vf_explained_var: 0.9929406046867371
        vf_loss: 0.25237610936164856
    load_time_ms: 0.766
    num_steps_sampled: 1953600
    num_steps_trained: 1924000
    sample_time_ms: 2278.272
    update_time_ms: 4.353
  iterations_since_restore: 74
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.028075879447005
    mean_inference_ms: 1.0452411015338754
    mean_processing_ms: 2.5076036515532794
  time_since_restore: 382.47641229629517
  time_this_iter_s: 4.643364667892456
  time_total_s: 382.47641229629517
  timestamp: 1563364981
  timesteps_since_restore: 1953600
  timesteps_this_iter: 26400
  timesteps_total: 1953600
  training_iteration: 74
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 382 s, 74 iter, 1953600 ts, 16.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.546918418754
  episode_reward_mean: 15.208533967963335
  episode_reward_min: -4.358915566500607
  episodes_this_iter: 176
  episodes_total: 13200
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2857.332
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9793752431869507
        kl: 0.011667534708976746
        policy_loss: -0.014069984667003155
        total_loss: 0.24194499850273132
        vf_explained_var: 0.992409348487854
        vf_loss: 0.2545565366744995
    load_time_ms: 0.777
    num_steps_sampled: 1980000
    num_steps_trained: 1950000
    sample_time_ms: 2238.015
    update_time_ms: 4.298
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.017873085738667
    mean_inference_ms: 1.0401537938902221
    mean_processing_ms: 2.503979211074764
  time_since_restore: 387.8411536216736
  time_this_iter_s: 5.364741325378418
  time_total_s: 387.8411536216736
  timestamp: 1563364986
  timesteps_since_restore: 1980000
  timesteps_this_iter: 26400
  timesteps_total: 1980000
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 387 s, 75 iter, 1980000 ts, 15.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.49023281964921
  episode_reward_mean: 17.819514672416826
  episode_reward_min: -1.7297286617919863
  episodes_this_iter: 176
  episodes_total: 13552
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2896.739
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9415589570999146
        kl: 0.015005663968622684
        policy_loss: -0.017602436244487762
        total_loss: 0.2572043538093567
        vf_explained_var: 0.9929406642913818
        vf_loss: 0.2729310691356659
    load_time_ms: 0.772
    num_steps_sampled: 2032800
    num_steps_trained: 2002000
    sample_time_ms: 2198.508
    update_time_ms: 4.467
  iterations_since_restore: 77
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.014952984022489
    mean_inference_ms: 1.0387083480755548
    mean_processing_ms: 2.50075684800411
  time_since_restore: 398.0426971912384
  time_this_iter_s: 5.430144786834717
  time_total_s: 398.0426971912384
  timestamp: 1563364996
  timesteps_since_restore: 2032800
  timesteps_this_iter: 26400
  timesteps_total: 2032800
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 398 s, 77 iter, 2032800 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.598048384556684
  episode_reward_mean: 16.57899898692633
  episode_reward_min: -3.52885908559576
  episodes_this_iter: 176
  episodes_total: 13904
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2855.24
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9087474346160889
        kl: 0.013737386092543602
        policy_loss: -0.016001513227820396
        total_loss: 0.18976132571697235
        vf_explained_var: 0.994348406791687
        vf_loss: 0.20404566824436188
    load_time_ms: 0.771
    num_steps_sampled: 2085600
    num_steps_trained: 2054000
    sample_time_ms: 2102.257
    update_time_ms: 4.245
  iterations_since_restore: 79
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.022520656361601
    mean_inference_ms: 1.0396705789901224
    mean_processing_ms: 2.5054116099420836
  time_since_restore: 407.3458261489868
  time_this_iter_s: 4.914835453033447
  time_total_s: 407.3458261489868
  timestamp: 1563365006
  timesteps_since_restore: 2085600
  timesteps_this_iter: 26400
  timesteps_total: 2085600
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 407 s, 79 iter, 2085600 ts, 16.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.21686715237966
  episode_reward_mean: 15.721974528485765
  episode_reward_min: -3.6554879164626666
  episodes_this_iter: 176
  episodes_total: 14256
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2870.061
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8784120082855225
        kl: 0.01380598358809948
        policy_loss: -0.016172414645552635
        total_loss: 0.2024683654308319
        vf_explained_var: 0.9937394857406616
        vf_loss: 0.21691499650478363
    load_time_ms: 0.79
    num_steps_sampled: 2138400
    num_steps_trained: 2106000
    sample_time_ms: 2057.487
    update_time_ms: 4.372
  iterations_since_restore: 81
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0086584471051476
    mean_inference_ms: 1.0352803546169083
    mean_processing_ms: 2.4935179185071523
  time_since_restore: 417.3251197338104
  time_this_iter_s: 5.059811592102051
  time_total_s: 417.3251197338104
  timestamp: 1563365016
  timesteps_since_restore: 2138400
  timesteps_this_iter: 26400
  timesteps_total: 2138400
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 417 s, 81 iter, 2138400 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.21329898533869
  episode_reward_mean: 16.39558325590734
  episode_reward_min: -2.4507800729678184
  episodes_this_iter: 176
  episodes_total: 14608
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2859.06
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8576579093933105
        kl: 0.014511017128825188
        policy_loss: -0.018919354304671288
        total_loss: 0.16745725274085999
        vf_explained_var: 0.9953591227531433
        vf_loss: 0.18456269800662994
    load_time_ms: 0.778
    num_steps_sampled: 2191200
    num_steps_trained: 2158000
    sample_time_ms: 2029.204
    update_time_ms: 4.106
  iterations_since_restore: 83
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0032421445061246
    mean_inference_ms: 1.0339990695689845
    mean_processing_ms: 2.4931481791151877
  time_since_restore: 426.91619777679443
  time_this_iter_s: 5.162243604660034
  time_total_s: 426.91619777679443
  timestamp: 1563365025
  timesteps_since_restore: 2191200
  timesteps_this_iter: 26400
  timesteps_total: 2191200
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32075], 426 s, 83 iter, 2191200 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
2019-07-17 14:03:55,491	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-17 14:03:55,506	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

Result for PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-03-55
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 36.65989380417932
  episode_reward_mean: 18.40568025036439
  episode_reward_min: -5.051108108873594
  episodes_this_iter: 176
  episodes_total: 14960
  experiment_id: 4759e7d0028e47bfa9b0fcd246f236a2
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2833.788
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8343626260757446
        kl: 0.013909462839365005
        policy_loss: -0.017269717529416084
        total_loss: 0.14838893711566925
        vf_explained_var: 0.996093213558197
        vf_loss: 0.16391995549201965
    load_time_ms: 0.775
    num_steps_sampled: 2244000
    num_steps_trained: 2210000
    sample_time_ms: 2014.916
    update_time_ms: 3.945
  iterations_since_restore: 85
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32075
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0084547342500456
    mean_inference_ms: 1.0362847112035936
    mean_processing_ms: 2.4904614201291664
  time_since_restore: 436.5264558792114
  time_this_iter_s: 4.733258008956909
  time_total_s: 436.5264558792114
  timestamp: 1563365035
  timesteps_since_restore: 2244000
  timesteps_this_iter: 26400
  timesteps_total: 2244000
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

[2m[36m(pid=32187)[0m [32m [     0.05585s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55,667	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.673284: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.682774: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.787171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.787655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5602b2481aa0 executing computations on platform CUDA. Devices:
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.787682: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.808095: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.808639: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5602b2c0a0b0 executing computations on platform Host. Devices:
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.808674: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.808969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.809324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
[2m[36m(pid=32187)[0m name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
[2m[36m(pid=32187)[0m pciBusID: 0000:01:00.0
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.809574: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.809712: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.809839: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.809933: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.810020: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.810108: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.815010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.815071: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.815101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.815112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:55.815120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[2m[36m(pid=32187)[0m W0717 14:03:55.822163 140257458591168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32187)[0m Instructions for updating:
[2m[36m(pid=32187)[0m Use keras.layers.dense instead.
[2m[36m(pid=32187)[0m W0717 14:03:56.241237 140257458591168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32187)[0m Instructions for updating:
[2m[36m(pid=32187)[0m Use `tf.cast` instead.
[2m[36m(pid=32187)[0m 2019-07-17 14:03:56.294411: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32187)[0m 2019-07-17 14:03:56,309	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32187)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32187)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=32187)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32187)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32187)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32187)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m W0717 14:03:56.346215 140257458591168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32187)[0m Instructions for updating:
[2m[36m(pid=32187)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32187)[0m [32m [     1.39539s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m [32m [     1.39579s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m [32m [     1.39617s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m [32m [     1.39658s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m [32m [     1.39696s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m [32m [     1.39734s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m [32m [     1.39771s,  INFO] TimeLimit:
[2m[36m(pid=32187)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32187)[0m - action_space = Box(2,)
[2m[36m(pid=32187)[0m - observation_space = Box(9,)
[2m[36m(pid=32187)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32187)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32187)[0m - _max_episode_steps = 150
[2m[36m(pid=32187)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m 2019-07-17 14:03:57,010	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f8e5b115828>}
[2m[36m(pid=32187)[0m 2019-07-17 14:03:57,010	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f8e5b115748>}
[2m[36m(pid=32187)[0m 2019-07-17 14:03:57,010	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57,103	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32176)[0m [32m [     0.02485s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     0.02438s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57,088	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.102841: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32187)[0m 2019-07-17 14:03:57,063	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=32182)[0m [32m [     0.03762s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.109916: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57,108	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32170)[0m [32m [     0.03264s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57,159	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57,127	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.143074: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.152054: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.155665: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.155699: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.155706: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.155851: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.155901: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.155918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.159504: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.160248: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562ae15ee8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.160287: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57,114	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.137361: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.146301: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.149951: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.150029: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.150049: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.150197: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.150249: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.150266: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.155132: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.155645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615229408d0 executing computations on platform Host. Devices:
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.155676: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32174)[0m W0717 14:03:57.161702 139907465496000 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32174)[0m Instructions for updating:
[2m[36m(pid=32174)[0m Use keras.layers.dense instead.
[2m[36m(pid=32174)[0m [32m [     0.02749s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     0.02959s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57,125	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.141063: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.150510: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.154779: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.154851: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.154862: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.154968: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.155001: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.155011: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.159509: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.160199: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d4c3e1f8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.160243: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32177)[0m [32m [     0.03452s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.122877: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.131561: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.134965: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.135008: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.135018: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.135105: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.135131: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.135140: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.137984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.138793: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a1b49918d0 executing computations on platform Host. Devices:
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.138847: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32176)[0m W0717 14:03:57.146149 140352277312960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32176)[0m Instructions for updating:
[2m[36m(pid=32176)[0m Use keras.layers.dense instead.
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.113702: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.113778: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.113793: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.113915: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.113959: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.113973: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.143390: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.143859: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c13e0398d0 executing computations on platform Host. Devices:
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.143896: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32189)[0m W0717 14:03:57.151621 140657367725504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32189)[0m Instructions for updating:
[2m[36m(pid=32189)[0m Use keras.layers.dense instead.
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.130422: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.138992: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.141742: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.141803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.141822: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.141956: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.142003: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.142019: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.144807: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.145268: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a5f0ef28d0 executing computations on platform Host. Devices:
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.145289: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32182)[0m W0717 14:03:57.151127 139846952109504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32182)[0m Instructions for updating:
[2m[36m(pid=32182)[0m Use keras.layers.dense instead.
[2m[36m(pid=32173)[0m W0717 14:03:57.167505 139622165624256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32173)[0m Instructions for updating:
[2m[36m(pid=32173)[0m Use keras.layers.dense instead.
[2m[36m(pid=32177)[0m W0717 14:03:57.167252 139624644797888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32177)[0m Instructions for updating:
[2m[36m(pid=32177)[0m Use keras.layers.dense instead.
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.179030: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.188118: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.191989: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.192038: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.192049: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.192141: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.192172: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.192182: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.195012: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.195519: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561b8148d780 executing computations on platform Host. Devices:
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.195547: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32170)[0m W0717 14:03:57.202359 139982203766208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32170)[0m Instructions for updating:
[2m[36m(pid=32170)[0m Use keras.layers.dense instead.
[2m[36m(pid=32176)[0m W0717 14:03:57.534637 140352277312960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32176)[0m Instructions for updating:
[2m[36m(pid=32176)[0m Use `tf.cast` instead.
[2m[36m(pid=32170)[0m W0717 14:03:57.606631 139982203766208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32170)[0m Instructions for updating:
[2m[36m(pid=32170)[0m Use `tf.cast` instead.
[2m[36m(pid=32173)[0m W0717 14:03:57.642627 139622165624256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32173)[0m Instructions for updating:
[2m[36m(pid=32173)[0m Use `tf.cast` instead.
[2m[36m(pid=32189)[0m W0717 14:03:57.611004 140657367725504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32189)[0m Instructions for updating:
[2m[36m(pid=32189)[0m Use `tf.cast` instead.
[2m[36m(pid=32182)[0m W0717 14:03:57.618470 139846952109504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32182)[0m Instructions for updating:
[2m[36m(pid=32182)[0m Use `tf.cast` instead.
[2m[36m(pid=32177)[0m W0717 14:03:57.670751 139624644797888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32177)[0m Instructions for updating:
[2m[36m(pid=32177)[0m Use `tf.cast` instead.
[2m[36m(pid=32176)[0m 2019-07-17 14:03:57.666932: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32170)[0m 2019-07-17 14:03:57.717172: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32174)[0m W0717 14:03:57.736108 139907465496000 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32174)[0m Instructions for updating:
[2m[36m(pid=32174)[0m Use `tf.cast` instead.
[2m[36m(pid=32189)[0m 2019-07-17 14:03:57.751699: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57.727884: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32173)[0m 2019-07-17 14:03:57.761059: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32177)[0m 2019-07-17 14:03:57.804216: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32176)[0m W0717 14:03:57.772487 140352277312960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32176)[0m Instructions for updating:
[2m[36m(pid=32176)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32182)[0m 2019-07-17 14:03:57,756	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32182)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32182)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32182)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32182)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=32182)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32182)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32182)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32182)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32182)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32182)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32182)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32170)[0m W0717 14:03:57.821089 139982203766208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32170)[0m Instructions for updating:
[2m[36m(pid=32170)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32174)[0m 2019-07-17 14:03:57.852431: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32189)[0m W0717 14:03:57.861180 140657367725504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32189)[0m Instructions for updating:
[2m[36m(pid=32189)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32182)[0m W0717 14:03:57.832475 139846952109504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32182)[0m Instructions for updating:
[2m[36m(pid=32182)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32173)[0m W0717 14:03:57.872525 139622165624256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32173)[0m Instructions for updating:
[2m[36m(pid=32173)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32177)[0m W0717 14:03:57.896608 139624644797888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32177)[0m Instructions for updating:
[2m[36m(pid=32177)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32174)[0m W0717 14:03:57.974897 139907465496000 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32174)[0m Instructions for updating:
[2m[36m(pid=32174)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32176)[0m [32m [     2.23793s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m [32m [     2.23904s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m [32m [     2.23998s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m [32m [     2.24081s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m [32m [     2.24171s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m [32m [     2.24257s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32176)[0m [32m [     2.24340s,  INFO] TimeLimit:
[2m[36m(pid=32176)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32176)[0m - action_space = Box(2,)
[2m[36m(pid=32176)[0m - observation_space = Box(9,)
[2m[36m(pid=32176)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32176)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32176)[0m - _max_episode_steps = 150
[2m[36m(pid=32176)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.22697s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.22777s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.22852s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.22925s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.22999s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.23077s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32170)[0m [32m [     2.23152s,  INFO] TimeLimit:
[2m[36m(pid=32170)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32170)[0m - action_space = Box(2,)
[2m[36m(pid=32170)[0m - observation_space = Box(9,)
[2m[36m(pid=32170)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32170)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32170)[0m - _max_episode_steps = 150
[2m[36m(pid=32170)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34317s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34361s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34403s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34444s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34489s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34534s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32177)[0m [32m [     2.34577s,  INFO] TimeLimit:
[2m[36m(pid=32177)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32177)[0m - action_space = Box(2,)
[2m[36m(pid=32177)[0m - observation_space = Box(9,)
[2m[36m(pid=32177)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32177)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32177)[0m - _max_episode_steps = 150
[2m[36m(pid=32177)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34587s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34636s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34683s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34728s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34773s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34817s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32189)[0m [32m [     2.34873s,  INFO] TimeLimit:
[2m[36m(pid=32189)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32189)[0m - action_space = Box(2,)
[2m[36m(pid=32189)[0m - observation_space = Box(9,)
[2m[36m(pid=32189)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32189)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32189)[0m - _max_episode_steps = 150
[2m[36m(pid=32189)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.35993s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.36047s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.36089s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.36129s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.36169s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.36212s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32173)[0m [32m [     2.36270s,  INFO] TimeLimit:
[2m[36m(pid=32173)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32173)[0m - action_space = Box(2,)
[2m[36m(pid=32173)[0m - observation_space = Box(9,)
[2m[36m(pid=32173)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32173)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32173)[0m - _max_episode_steps = 150
[2m[36m(pid=32173)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39314s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39357s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39397s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39435s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39476s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39519s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32182)[0m [32m [     2.39561s,  INFO] TimeLimit:
[2m[36m(pid=32182)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32182)[0m - action_space = Box(2,)
[2m[36m(pid=32182)[0m - observation_space = Box(9,)
[2m[36m(pid=32182)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32182)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32182)[0m - _max_episode_steps = 150
[2m[36m(pid=32182)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41686s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41730s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41769s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41807s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41850s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41890s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32174)[0m [32m [     2.41931s,  INFO] TimeLimit:
[2m[36m(pid=32174)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32174)[0m - action_space = Box(2,)
[2m[36m(pid=32174)[0m - observation_space = Box(9,)
[2m[36m(pid=32174)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32174)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32174)[0m - _max_episode_steps = 150
[2m[36m(pid=32174)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789616: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789710: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_1 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_1/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_1/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789771: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_2 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_2/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_2/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789827: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_3 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_3/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_3/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789885: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_4 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_4/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_4/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789942: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_5 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_5/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_5/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.789997: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_6 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_6/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_6/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:03:59.790054: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32187)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32187)[0m See below for details of this colocation group:
[2m[36m(pid=32187)[0m Colocation Debug Info:
[2m[36m(pid=32187)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32187)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32187)[0m Assign: CPU 
[2m[36m(pid=32187)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32187)[0m VariableV2: CPU 
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_7 (VariableV2) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_7/Assign (Assign) /device:GPU:0
[2m[36m(pid=32187)[0m   default_policy_1/tower_1/Variable_7/read (Identity) /device:GPU:0
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m W0717 14:04:00.419926 140257458591168 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32187)[0m Instructions for updating:
[2m[36m(pid=32187)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32170)[0m W0717 14:04:00.455885 139982203766208 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32170)[0m Instructions for updating:
[2m[36m(pid=32170)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32173)[0m W0717 14:04:00.455963 139622165624256 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32173)[0m Instructions for updating:
[2m[36m(pid=32173)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32174)[0m W0717 14:04:00.455754 139907465496000 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32174)[0m Instructions for updating:
[2m[36m(pid=32174)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32177)[0m W0717 14:04:00.455794 139624644797888 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32177)[0m Instructions for updating:
[2m[36m(pid=32177)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32176)[0m W0717 14:04:00.455671 140352277312960 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32176)[0m Instructions for updating:
[2m[36m(pid=32176)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32189)[0m W0717 14:04:00.455677 140657367725504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32189)[0m Instructions for updating:
[2m[36m(pid=32189)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32182)[0m W0717 14:04:00.455554 139846952109504 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32182)[0m Instructions for updating:
[2m[36m(pid=32182)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32182)[0m 2019-07-17 14:04:00,988	INFO rollout_worker.py:428 -- Generating sample batch of size 1600
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,052	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.751, max=0.136, mean=-0.244)},
[2m[36m(pid=32182)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.966, max=0.174, mean=-0.178)},
[2m[36m(pid=32182)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.201, max=0.999, mean=0.201)},
[2m[36m(pid=32182)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.822, max=0.261, mean=-0.25)},
[2m[36m(pid=32182)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.153, max=0.753, mean=0.218)},
[2m[36m(pid=32182)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.642, max=0.767, mean=-0.0)},
[2m[36m(pid=32182)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.0, max=0.385, mean=-0.072)},
[2m[36m(pid=32182)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.74, max=0.887, mean=-0.058)}}
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,053	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=32182)[0m   1: {'agent0': None},
[2m[36m(pid=32182)[0m   2: {'agent0': None},
[2m[36m(pid=32182)[0m   3: {'agent0': None},
[2m[36m(pid=32182)[0m   4: {'agent0': None},
[2m[36m(pid=32182)[0m   5: {'agent0': None},
[2m[36m(pid=32182)[0m   6: {'agent0': None},
[2m[36m(pid=32182)[0m   7: {'agent0': None}}
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,053	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.751, max=0.136, mean=-0.244)
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,053	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,058	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 0,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 1,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.079),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 2,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.515, max=1.137, mean=0.568),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 3,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.381, max=1.334, mean=-0.209),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 4,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.703, max=1.472, mean=0.466),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 5,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.833, max=1.059, mean=0.001),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 6,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.099, max=0.932, mean=-0.034),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32182)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32182)[0m                                   'env_id': 7,
[2m[36m(pid=32182)[0m                                   'info': None,
[2m[36m(pid=32182)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.021, max=1.385, mean=0.003),
[2m[36m(pid=32182)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32182)[0m                                   'rnn_state': []},
[2m[36m(pid=32182)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,058	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,157	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-1.284, max=2.116, mean=0.132),
[2m[36m(pid=32182)[0m                       [],
[2m[36m(pid=32182)[0m                       { 'action_prob': np.ndarray((8,), dtype=float32, min=0.017, max=0.118, mean=0.066),
[2m[36m(pid=32182)[0m                         'behaviour_logits': np.ndarray((8, 4), dtype=float32, min=-0.005, max=0.005, mean=0.001),
[2m[36m(pid=32182)[0m                         'vf_preds': np.ndarray((8,), dtype=float32, min=-0.003, max=0.004, mean=0.001)})}
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m 2019-07-17 14:04:01,693	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.001, max=0.159, mean=0.081),
[2m[36m(pid=32182)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-2.848, max=3.255, mean=-0.016),
[2m[36m(pid=32182)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-28.274, max=22.219, mean=-0.279),
[2m[36m(pid=32182)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.016, max=0.008, mean=-0.002),
[2m[36m(pid=32182)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=32182)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1115342556.0, max=1115342556.0, mean=1115342556.0),
[2m[36m(pid=32182)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=32182)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-2.825, max=3.601, mean=-0.017),
[2m[36m(pid=32182)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-2.825, max=3.601, mean=-0.015),
[2m[36m(pid=32182)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-2.848, max=3.255, mean=-0.017),
[2m[36m(pid=32182)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-2.236, max=1.597, mean=-0.104),
[2m[36m(pid=32182)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-2.236, max=1.597, mean=-0.1),
[2m[36m(pid=32182)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32182)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-28.273, max=22.219, mean=-0.28),
[2m[36m(pid=32182)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.008, max=0.006, mean=-0.001)},
[2m[36m(pid=32182)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m 2019-07-17 14:04:02,393	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32182)[0m { 'data': { 'action_prob': np.ndarray((1650,), dtype=float32, min=0.0, max=0.159, mean=0.079),
[2m[36m(pid=32182)[0m             'actions': np.ndarray((1650, 2), dtype=float32, min=-4.243, max=3.255, mean=-0.007),
[2m[36m(pid=32182)[0m             'advantages': np.ndarray((1650,), dtype=float32, min=-31.505, max=25.034, mean=-5.697),
[2m[36m(pid=32182)[0m             'agent_index': np.ndarray((1650,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32182)[0m             'behaviour_logits': np.ndarray((1650, 4), dtype=float32, min=-0.017, max=0.017, mean=-0.0),
[2m[36m(pid=32182)[0m             'dones': np.ndarray((1650,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=32182)[0m             'eps_id': np.ndarray((1650,), dtype=int64, min=228285683.0, max=1828026345.0, mean=859788901.727),
[2m[36m(pid=32182)[0m             'infos': np.ndarray((1650,), dtype=object, head={}),
[2m[36m(pid=32182)[0m             'new_obs': np.ndarray((1650, 9), dtype=float32, min=-3.982, max=4.471, mean=0.017),
[2m[36m(pid=32182)[0m             'obs': np.ndarray((1650, 9), dtype=float32, min=-3.982, max=4.471, mean=0.017),
[2m[36m(pid=32182)[0m             'prev_actions': np.ndarray((1650, 2), dtype=float32, min=-4.243, max=3.255, mean=-0.009),
[2m[36m(pid=32182)[0m             'prev_rewards': np.ndarray((1650,), dtype=float32, min=-5.717, max=5.444, mean=-0.094),
[2m[36m(pid=32182)[0m             'rewards': np.ndarray((1650,), dtype=float32, min=-5.717, max=5.444, mean=-0.09),
[2m[36m(pid=32182)[0m             't': np.ndarray((1650,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32182)[0m             'unroll_id': np.ndarray((1650,), dtype=int64, min=0.0, max=1.0, mean=0.273),
[2m[36m(pid=32182)[0m             'value_targets': np.ndarray((1650,), dtype=float32, min=-31.508, max=25.045, mean=-5.697),
[2m[36m(pid=32182)[0m             'vf_preds': np.ndarray((1650,), dtype=float32, min=-0.01, max=0.012, mean=0.0)},
[2m[36m(pid=32182)[0m   'type': 'SampleBatch'}
[2m[36m(pid=32182)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:04:03,741	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m { 'inputs': [ np.ndarray((26400, 2), dtype=float32, min=-4.387, max=4.403, mean=0.006),
[2m[36m(pid=32187)[0m               np.ndarray((26400,), dtype=float32, min=-6.375, max=6.795, mean=-0.097),
[2m[36m(pid=32187)[0m               np.ndarray((26400, 9), dtype=float32, min=-4.654, max=4.818, mean=0.011),
[2m[36m(pid=32187)[0m               np.ndarray((26400, 2), dtype=float32, min=-4.387, max=4.403, mean=0.006),
[2m[36m(pid=32187)[0m               np.ndarray((26400,), dtype=float32, min=-3.309, max=4.057, mean=-0.0),
[2m[36m(pid=32187)[0m               np.ndarray((26400, 4), dtype=float32, min=-0.02, max=0.022, mean=0.0),
[2m[36m(pid=32187)[0m               np.ndarray((26400,), dtype=float32, min=-39.856, max=34.522, mean=-6.449),
[2m[36m(pid=32187)[0m               np.ndarray((26400,), dtype=float32, min=-0.014, max=0.014, mean=0.0)],
[2m[36m(pid=32187)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32187)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=32187)[0m   'state_inputs': []}
[2m[36m(pid=32187)[0m 
[2m[36m(pid=32187)[0m 2019-07-17 14:04:03,741	INFO multi_gpu_impl.py:191 -- Divided 26400 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 13.737443456541602
  episode_reward_mean: -14.607741535823981
  episode_reward_min: -43.12627837348832
  episodes_this_iter: 176
  episodes_total: 176
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3171.097
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.8308801651000977
        kl: 0.0011019601952284575
        policy_loss: -0.0025721220299601555
        total_loss: 91.04576110839844
        vf_explained_var: 0.12367396056652069
        vf_loss: 91.04722595214844
    load_time_ms: 42.322
    num_steps_sampled: 26400
    num_steps_trained: 26000
    sample_time_ms: 2817.165
    update_time_ms: 462.968
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.7065167916991661
    mean_inference_ms: 1.1305531834259128
    mean_processing_ms: 1.3160284959825859
  time_since_restore: 6.559561014175415
  time_this_iter_s: 6.559561014175415
  time_total_s: 6.559561014175415
  timestamp: 1563365047
  timesteps_since_restore: 26400
  timesteps_this_iter: 26400
  timesteps_total: 26400
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 6 s, 1 iter, 26400 ts, -14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.875337777746164
  episode_reward_mean: -15.737725662763461
  episode_reward_min: -45.83252622352988
  episodes_this_iter: 176
  episodes_total: 352
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3019.486
    learner:
      default_policy:
        cur_kl_coeff: 0.5
        cur_lr: 9.999999747378752e-05
        entropy: 2.8216919898986816
        kl: 0.0023670108057558537
        policy_loss: -0.00408078683540225
        total_loss: 70.79063415527344
        vf_explained_var: 0.31617051362991333
        vf_loss: 70.79352569580078
    load_time_ms: 21.554
    num_steps_sampled: 52800
    num_steps_trained: 52000
    sample_time_ms: 2585.14
    update_time_ms: 233.234
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6825655248030846
    mean_inference_ms: 1.0960755177020285
    mean_processing_ms: 1.3792622841057598
  time_since_restore: 11.801072120666504
  time_this_iter_s: 5.241511106491089
  time_total_s: 11.801072120666504
  timestamp: 1563365052
  timesteps_since_restore: 52800
  timesteps_this_iter: 26400
  timesteps_total: 52800
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 11 s, 2 iter, 52800 ts, -15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.788979760996213
  episode_reward_mean: -13.348739530048475
  episode_reward_min: -55.832370275052035
  episodes_this_iter: 176
  episodes_total: 528
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2993.263
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.8179566860198975
        kl: 0.004998465068638325
        policy_loss: -0.00432800967246294
        total_loss: 69.50336456298828
        vf_explained_var: 0.3408297300338745
        vf_loss: 69.5064468383789
    load_time_ms: 14.632
    num_steps_sampled: 79200
    num_steps_trained: 78000
    sample_time_ms: 2478.611
    update_time_ms: 156.951
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.670509460326734
    mean_inference_ms: 1.0637919450109408
    mean_processing_ms: 1.3815945175097746
  time_since_restore: 17.028366327285767
  time_this_iter_s: 5.227294206619263
  time_total_s: 17.028366327285767
  timestamp: 1563365057
  timesteps_since_restore: 79200
  timesteps_this_iter: 26400
  timesteps_total: 79200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 17 s, 3 iter, 79200 ts, -13.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.72877574962794
  episode_reward_mean: -12.611181602132842
  episode_reward_min: -45.07994725212798
  episodes_this_iter: 176
  episodes_total: 704
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3004.578
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.811734676361084
        kl: 0.008716468699276447
        policy_loss: -0.007354169152677059
        total_loss: 60.343894958496094
        vf_explained_var: 0.39140647649765015
        vf_loss: 60.35015869140625
    load_time_ms: 11.165
    num_steps_sampled: 105600
    num_steps_trained: 104000
    sample_time_ms: 2396.131
    update_time_ms: 118.656
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.671167381637057
    mean_inference_ms: 1.0489803129536464
    mean_processing_ms: 1.3876436545992403
  time_since_restore: 22.234452724456787
  time_this_iter_s: 5.2060863971710205
  time_total_s: 22.234452724456787
  timestamp: 1563365062
  timesteps_since_restore: 105600
  timesteps_this_iter: 26400
  timesteps_total: 105600
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 22 s, 4 iter, 105600 ts, -12.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.648222629514283
  episode_reward_mean: -11.279302699857988
  episode_reward_min: -50.374165446445176
  episodes_this_iter: 176
  episodes_total: 880
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3000.727
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8064019680023193
        kl: 0.00782238319516182
        policy_loss: -0.007517210207879543
        total_loss: 60.41427230834961
        vf_explained_var: 0.3752506971359253
        vf_loss: 60.42081069946289
    load_time_ms: 9.088
    num_steps_sampled: 132000
    num_steps_trained: 130000
    sample_time_ms: 2393.885
    update_time_ms: 95.694
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6668460243245697
    mean_inference_ms: 1.041123461814624
    mean_processing_ms: 1.3918401234879552
  time_since_restore: 27.622443914413452
  time_this_iter_s: 5.387991189956665
  time_total_s: 27.622443914413452
  timestamp: 1563365068
  timesteps_since_restore: 132000
  timesteps_this_iter: 26400
  timesteps_total: 132000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 27 s, 5 iter, 132000 ts, -11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.44447876944822
  episode_reward_mean: -10.717388851529149
  episode_reward_min: -44.97542253072186
  episodes_this_iter: 176
  episodes_total: 1056
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3000.475
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.795264959335327
        kl: 0.007572957314550877
        policy_loss: -0.006571406964212656
        total_loss: 45.52417755126953
        vf_explained_var: 0.40202951431274414
        vf_loss: 45.529808044433594
    load_time_ms: 7.7
    num_steps_sampled: 158400
    num_steps_trained: 156000
    sample_time_ms: 2381.345
    update_time_ms: 80.29
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6560690616012017
    mean_inference_ms: 1.044827432879365
    mean_processing_ms: 1.3792896648720399
  time_since_restore: 32.95933532714844
  time_this_iter_s: 5.336891412734985
  time_total_s: 32.95933532714844
  timestamp: 1563365073
  timesteps_since_restore: 158400
  timesteps_this_iter: 26400
  timesteps_total: 158400
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 32 s, 6 iter, 158400 ts, -10.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.65868392837119
  episode_reward_mean: -8.879151932930107
  episode_reward_min: -35.69410297946075
  episodes_this_iter: 176
  episodes_total: 1408
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2919.729
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.78121280670166
        kl: 0.006551159080117941
        policy_loss: -0.006239470560103655
        total_loss: 24.965091705322266
        vf_explained_var: 0.49429163336753845
        vf_loss: 24.97051239013672
    load_time_ms: 5.957
    num_steps_sampled: 211200
    num_steps_trained: 208000
    sample_time_ms: 2307.155
    update_time_ms: 61.218
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6329403983597963
    mean_inference_ms: 1.0207556527299644
    mean_processing_ms: 1.3638990600085243
  time_since_restore: 42.522594690322876
  time_this_iter_s: 4.804835319519043
  time_total_s: 42.522594690322876
  timestamp: 1563365083
  timesteps_since_restore: 211200
  timesteps_this_iter: 26400
  timesteps_total: 211200
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 42 s, 8 iter, 211200 ts, -8.88 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.84177042111995
  episode_reward_mean: -6.354792821424739
  episode_reward_min: -35.17565649227381
  episodes_this_iter: 176
  episodes_total: 1584
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2918.302
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7716686725616455
        kl: 0.00729671074077487
        policy_loss: -0.007447859272360802
        total_loss: 27.351261138916016
        vf_explained_var: 0.45929810404777527
        vf_loss: 27.357797622680664
    load_time_ms: 5.383
    num_steps_sampled: 237600
    num_steps_trained: 234000
    sample_time_ms: 2306.985
    update_time_ms: 54.845
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6323676489923509
    mean_inference_ms: 1.0152689916661037
    mean_processing_ms: 1.3655321899783437
  time_since_restore: 47.75431561470032
  time_this_iter_s: 5.231720924377441
  time_total_s: 47.75431561470032
  timestamp: 1563365088
  timesteps_since_restore: 237600
  timesteps_this_iter: 26400
  timesteps_total: 237600
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 47 s, 9 iter, 237600 ts, -6.35 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.969595251051217
  episode_reward_mean: -5.544989284095646
  episode_reward_min: -34.697882396139036
  episodes_this_iter: 176
  episodes_total: 1760
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2919.094
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7657761573791504
        kl: 0.007612281013280153
        policy_loss: -0.009122484363615513
        total_loss: 18.713783264160156
        vf_explained_var: 0.5753878951072693
        vf_loss: 18.721954345703125
    load_time_ms: 4.928
    num_steps_sampled: 264000
    num_steps_trained: 260000
    sample_time_ms: 2305.521
    update_time_ms: 49.723
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6409558121750474
    mean_inference_ms: 1.0182958404701132
    mean_processing_ms: 1.3729682640512035
  time_since_restore: 52.993735551834106
  time_this_iter_s: 5.239419937133789
  time_total_s: 52.993735551834106
  timestamp: 1563365093
  timesteps_since_restore: 264000
  timesteps_this_iter: 26400
  timesteps_total: 264000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 52 s, 10 iter, 264000 ts, -5.54 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-04-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.200478446354158
  episode_reward_mean: -4.797643522078374
  episode_reward_min: -30.038711999412797
  episodes_this_iter: 176
  episodes_total: 1936
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2909.008
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7545204162597656
        kl: 0.009104529395699501
        policy_loss: -0.008696439675986767
        total_loss: 14.89035701751709
        vf_explained_var: 0.633723795413971
        vf_loss: 14.897916793823242
    load_time_ms: 0.767
    num_steps_sampled: 290400
    num_steps_trained: 286000
    sample_time_ms: 2253.608
    update_time_ms: 3.943
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.638890916537658
    mean_inference_ms: 1.014909926857868
    mean_processing_ms: 1.3698587554641048
  time_since_restore: 58.383177518844604
  time_this_iter_s: 5.389441967010498
  time_total_s: 58.383177518844604
  timestamp: 1563365098
  timesteps_since_restore: 290400
  timesteps_this_iter: 26400
  timesteps_total: 290400
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 58 s, 11 iter, 290400 ts, -4.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.216764665894516
  episode_reward_mean: -4.383621973450889
  episode_reward_min: -32.68224644869158
  episodes_this_iter: 176
  episodes_total: 2112
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2920.623
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.747394323348999
        kl: 0.007983640767633915
        policy_loss: -0.008392670191824436
        total_loss: 12.880316734313965
        vf_explained_var: 0.7058136463165283
        vf_loss: 12.887710571289062
    load_time_ms: 0.77
    num_steps_sampled: 316800
    num_steps_trained: 312000
    sample_time_ms: 2237.101
    update_time_ms: 4.419
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6403508808717033
    mean_inference_ms: 1.0146640707807206
    mean_processing_ms: 1.3722802906126268
  time_since_restore: 63.57750988006592
  time_this_iter_s: 5.1943323612213135
  time_total_s: 63.57750988006592
  timestamp: 1563365104
  timesteps_since_restore: 316800
  timesteps_this_iter: 26400
  timesteps_total: 316800
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 63 s, 12 iter, 316800 ts, -4.38 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.10353511741486
  episode_reward_mean: -2.054387123218593
  episode_reward_min: -33.6895305264441
  episodes_this_iter: 176
  episodes_total: 2288
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2897.68
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7338476181030273
        kl: 0.007241145242005587
        policy_loss: -0.009146916680037975
        total_loss: 14.606598854064941
        vf_explained_var: 0.6616798043251038
        vf_loss: 14.61484146118164
    load_time_ms: 0.771
    num_steps_sampled: 343200
    num_steps_trained: 338000
    sample_time_ms: 2261.827
    update_time_ms: 4.417
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6564177480974287
    mean_inference_ms: 1.0248434000386673
    mean_processing_ms: 1.384920468273345
  time_since_restore: 68.82299566268921
  time_this_iter_s: 5.245485782623291
  time_total_s: 68.82299566268921
  timestamp: 1563365109
  timesteps_since_restore: 343200
  timesteps_this_iter: 26400
  timesteps_total: 343200
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 68 s, 13 iter, 343200 ts, -2.05 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.600067413780547
  episode_reward_mean: -1.9234985543223444
  episode_reward_min: -31.03406697682839
  episodes_this_iter: 176
  episodes_total: 2464
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2891.109
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.72363543510437
        kl: 0.008683552965521812
        policy_loss: -0.00913134403526783
        total_loss: 15.394245147705078
        vf_explained_var: 0.6879276633262634
        vf_loss: 15.402292251586914
    load_time_ms: 0.768
    num_steps_sampled: 369600
    num_steps_trained: 364000
    sample_time_ms: 2261.987
    update_time_ms: 4.464
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6433851550270502
    mean_inference_ms: 1.015082617192431
    mean_processing_ms: 1.3744148719240665
  time_since_restore: 73.9643201828003
  time_this_iter_s: 5.141324520111084
  time_total_s: 73.9643201828003
  timestamp: 1563365114
  timesteps_since_restore: 369600
  timesteps_this_iter: 26400
  timesteps_total: 369600
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 73 s, 14 iter, 369600 ts, -1.92 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.374196936994984
  episode_reward_mean: 3.100049336590055
  episode_reward_min: -30.218766833644693
  episodes_this_iter: 176
  episodes_total: 2816
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2818.466
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.703873634338379
        kl: 0.009271442890167236
        policy_loss: -0.009799545630812645
        total_loss: 13.781230926513672
        vf_explained_var: 0.6929894685745239
        vf_loss: 13.789870262145996
    load_time_ms: 0.763
    num_steps_sampled: 422400
    num_steps_trained: 416000
    sample_time_ms: 2240.829
    update_time_ms: 4.475
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.640926274623085
    mean_inference_ms: 1.0088039084681635
    mean_processing_ms: 1.380427824797181
  time_since_restore: 83.75206565856934
  time_this_iter_s: 5.164539098739624
  time_total_s: 83.75206565856934
  timestamp: 1563365124
  timesteps_since_restore: 422400
  timesteps_this_iter: 26400
  timesteps_total: 422400
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 83 s, 16 iter, 422400 ts, 3.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.450077108865116
  episode_reward_mean: 3.173814731261973
  episode_reward_min: -27.94545646523247
  episodes_this_iter: 176
  episodes_total: 2992
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2843.233
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6879665851593018
        kl: 0.009266296401619911
        policy_loss: -0.012064595706760883
        total_loss: 14.147887229919434
        vf_explained_var: 0.6852356791496277
        vf_loss: 14.158793449401855
    load_time_ms: 0.767
    num_steps_sampled: 448800
    num_steps_trained: 442000
    sample_time_ms: 2251.626
    update_time_ms: 4.467
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6275301732463738
    mean_inference_ms: 0.9998412252008884
    mean_processing_ms: 1.3715771392205693
  time_since_restore: 88.86799383163452
  time_this_iter_s: 5.1159281730651855
  time_total_s: 88.86799383163452
  timestamp: 1563365129
  timesteps_since_restore: 448800
  timesteps_this_iter: 26400
  timesteps_total: 448800
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 88 s, 17 iter, 448800 ts, 3.17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.141880995421406
  episode_reward_mean: 5.524870656232863
  episode_reward_min: -21.788806111017955
  episodes_this_iter: 176
  episodes_total: 3344
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2769.304
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.669119119644165
        kl: 0.008812211453914642
        policy_loss: -0.010131260380148888
        total_loss: 9.620705604553223
        vf_explained_var: 0.7760788798332214
        vf_loss: 9.62973403930664
    load_time_ms: 0.762
    num_steps_sampled: 501600
    num_steps_trained: 494000
    sample_time_ms: 2243.721
    update_time_ms: 4.431
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6308150401323345
    mean_inference_ms: 1.0026480651079854
    mean_processing_ms: 1.3736280194513213
  time_since_restore: 98.08377194404602
  time_this_iter_s: 4.586460113525391
  time_total_s: 98.08377194404602
  timestamp: 1563365138
  timesteps_since_restore: 501600
  timesteps_this_iter: 26400
  timesteps_total: 501600
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 98 s, 19 iter, 501600 ts, 5.52 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.792826770233948
  episode_reward_mean: 7.566839690114597
  episode_reward_min: -20.128287534114513
  episodes_this_iter: 176
  episodes_total: 3696
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2713.768
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6480026245117188
        kl: 0.009536118246614933
        policy_loss: -0.011303151957690716
        total_loss: 6.3601484298706055
        vf_explained_var: 0.8499736785888672
        vf_loss: 6.370259761810303
    load_time_ms: 0.764
    num_steps_sampled: 554400
    num_steps_trained: 546000
    sample_time_ms: 2233.451
    update_time_ms: 4.358
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6219864359383607
    mean_inference_ms: 0.9960200769479409
    mean_processing_ms: 1.3639501951183652
  time_since_restore: 108.05465006828308
  time_this_iter_s: 5.084362983703613
  time_total_s: 108.05465006828308
  timestamp: 1563365148
  timesteps_since_restore: 554400
  timesteps_this_iter: 26400
  timesteps_total: 554400
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 108 s, 21 iter, 554400 ts, 7.57 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-05-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.055882767893756
  episode_reward_mean: 9.854521064775016
  episode_reward_min: -12.764634947339363
  episodes_this_iter: 176
  episodes_total: 4048
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2691.322
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6330413818359375
        kl: 0.01009434275329113
        policy_loss: -0.010817347094416618
        total_loss: 4.243012428283691
        vf_explained_var: 0.8987410664558411
        vf_loss: 4.252567768096924
    load_time_ms: 0.75
    num_steps_sampled: 607200
    num_steps_trained: 598000
    sample_time_ms: 2214.806
    update_time_ms: 4.024
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6229533339166828
    mean_inference_ms: 0.9947142132787252
    mean_processing_ms: 1.3644895376307946
  time_since_restore: 118.08274841308594
  time_this_iter_s: 5.058390855789185
  time_total_s: 118.08274841308594
  timestamp: 1563365158
  timesteps_since_restore: 607200
  timesteps_this_iter: 26400
  timesteps_total: 607200
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 118 s, 23 iter, 607200 ts, 9.85 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.77530588088506
  episode_reward_mean: 9.785293987983822
  episode_reward_min: -13.39833142778333
  episodes_this_iter: 176
  episodes_total: 4400
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2695.401
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6152992248535156
        kl: 0.009076924063265324
        policy_loss: -0.010709833353757858
        total_loss: 2.5228543281555176
        vf_explained_var: 0.9324249029159546
        vf_loss: 2.5324296951293945
    load_time_ms: 0.745
    num_steps_sampled: 660000
    num_steps_trained: 650000
    sample_time_ms: 2224.818
    update_time_ms: 4.096
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6179455549616415
    mean_inference_ms: 0.9916311095416858
    mean_processing_ms: 1.359476946482492
  time_since_restore: 127.99077224731445
  time_this_iter_s: 5.0704569816589355
  time_total_s: 127.99077224731445
  timestamp: 1563365168
  timesteps_since_restore: 660000
  timesteps_this_iter: 26400
  timesteps_total: 660000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 127 s, 25 iter, 660000 ts, 9.79 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.522189362047406
  episode_reward_mean: 11.539967824710825
  episode_reward_min: -10.259126432014925
  episodes_this_iter: 176
  episodes_total: 4576
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2691.301
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6047232151031494
        kl: 0.009888776578009129
        policy_loss: -0.012813521549105644
        total_loss: 2.4465928077697754
        vf_explained_var: 0.9397615790367126
        vf_loss: 2.4581704139709473
    load_time_ms: 0.746
    num_steps_sampled: 686400
    num_steps_trained: 676000
    sample_time_ms: 2212.756
    update_time_ms: 4.297
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6223069970024946
    mean_inference_ms: 0.9932425429172224
    mean_processing_ms: 1.3639406703923802
  time_since_restore: 132.9954080581665
  time_this_iter_s: 5.004635810852051
  time_total_s: 132.9954080581665
  timestamp: 1563365173
  timesteps_since_restore: 686400
  timesteps_this_iter: 26400
  timesteps_total: 686400
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 132 s, 26 iter, 686400 ts, 11.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.04450931498365
  episode_reward_mean: 12.064057346140403
  episode_reward_min: -7.969609549661621
  episodes_this_iter: 176
  episodes_total: 4928
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2682.71
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5742738246917725
        kl: 0.011193197220563889
        policy_loss: -0.010309439152479172
        total_loss: 1.721732258796692
        vf_explained_var: 0.9490348100662231
        vf_loss: 1.7306424379348755
    load_time_ms: 0.74
    num_steps_sampled: 739200
    num_steps_trained: 728000
    sample_time_ms: 2194.813
    update_time_ms: 4.475
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6097504238309008
    mean_inference_ms: 0.9866975640355041
    mean_processing_ms: 1.3514218197778676
  time_since_restore: 142.47796511650085
  time_this_iter_s: 4.7397072315216064
  time_total_s: 142.47796511650085
  timestamp: 1563365183
  timesteps_since_restore: 739200
  timesteps_this_iter: 26400
  timesteps_total: 739200
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 142 s, 28 iter, 739200 ts, 12.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.183488321034066
  episode_reward_mean: 11.600504792189078
  episode_reward_min: -8.44347118093637
  episodes_this_iter: 176
  episodes_total: 5280
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2670.261
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5582406520843506
        kl: 0.00938350334763527
        policy_loss: -0.011802857741713524
        total_loss: 1.5534089803695679
        vf_explained_var: 0.9557554721832275
        vf_loss: 1.5640387535095215
    load_time_ms: 0.748
    num_steps_sampled: 792000
    num_steps_trained: 780000
    sample_time_ms: 2198.359
    update_time_ms: 4.436
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6127369632466069
    mean_inference_ms: 0.9882090106837687
    mean_processing_ms: 1.3543938370015298
  time_since_restore: 151.86161470413208
  time_this_iter_s: 4.788041353225708
  time_total_s: 151.86161470413208
  timestamp: 1563365192
  timesteps_since_restore: 792000
  timesteps_this_iter: 26400
  timesteps_total: 792000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 151 s, 30 iter, 792000 ts, 11.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.13422505212895
  episode_reward_mean: 11.388065520646299
  episode_reward_min: -9.240211071750968
  episodes_this_iter: 176
  episodes_total: 5632
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2633.481
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5293593406677246
        kl: 0.00936820451170206
        policy_loss: -0.012300428003072739
        total_loss: 1.360546588897705
        vf_explained_var: 0.9588454961776733
        vf_loss: 1.371675729751587
    load_time_ms: 0.739
    num_steps_sampled: 844800
    num_steps_trained: 832000
    sample_time_ms: 2157.836
    update_time_ms: 4.237
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6053120046946168
    mean_inference_ms: 0.98304293529645
    mean_processing_ms: 1.3488635356498355
  time_since_restore: 161.13537883758545
  time_this_iter_s: 4.638400316238403
  time_total_s: 161.13537883758545
  timestamp: 1563365201
  timesteps_since_restore: 844800
  timesteps_this_iter: 26400
  timesteps_total: 844800
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 161 s, 32 iter, 844800 ts, 11.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.336607765297714
  episode_reward_mean: 12.958681466723249
  episode_reward_min: -11.157245895173183
  episodes_this_iter: 176
  episodes_total: 5984
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2610.004
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5002872943878174
        kl: 0.009705527685582638
        policy_loss: -0.009410114027559757
        total_loss: 1.0203276872634888
        vf_explained_var: 0.9744583964347839
        vf_loss: 1.02852463722229
    load_time_ms: 0.735
    num_steps_sampled: 897600
    num_steps_trained: 884000
    sample_time_ms: 2130.32
    update_time_ms: 4.168
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6021892695450242
    mean_inference_ms: 0.9814569655896982
    mean_processing_ms: 1.3478429142621442
  time_since_restore: 170.51664280891418
  time_this_iter_s: 4.603044748306274
  time_total_s: 170.51664280891418
  timestamp: 1563365211
  timesteps_since_restore: 897600
  timesteps_this_iter: 26400
  timesteps_total: 897600
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 170 s, 34 iter, 897600 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-06-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.096394959776426
  episode_reward_mean: 11.639069416367914
  episode_reward_min: -8.827760603631067
  episodes_this_iter: 176
  episodes_total: 6160
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2626.783
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.484414577484131
        kl: 0.010180797427892685
        policy_loss: -0.011072134599089622
        total_loss: 0.971297562122345
        vf_explained_var: 0.9720784425735474
        vf_loss: 0.9810971021652222
    load_time_ms: 0.737
    num_steps_sampled: 924000
    num_steps_trained: 910000
    sample_time_ms: 2136.471
    update_time_ms: 4.175
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5998966526206482
    mean_inference_ms: 0.97890118789607
    mean_processing_ms: 1.3448789900224973
  time_since_restore: 175.81355094909668
  time_this_iter_s: 5.296908140182495
  time_total_s: 175.81355094909668
  timestamp: 1563365216
  timesteps_since_restore: 924000
  timesteps_this_iter: 26400
  timesteps_total: 924000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 175 s, 35 iter, 924000 ts, 11.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.17408767412494
  episode_reward_mean: 12.337224707949371
  episode_reward_min: -6.578167658573188
  episodes_this_iter: 176
  episodes_total: 6336
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2634.28
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4761805534362793
        kl: 0.010643483139574528
        policy_loss: -0.010872388258576393
        total_loss: 1.0461337566375732
        vf_explained_var: 0.9704347252845764
        vf_loss: 1.055675745010376
    load_time_ms: 0.769
    num_steps_sampled: 950400
    num_steps_trained: 936000
    sample_time_ms: 2129.767
    update_time_ms: 3.93
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5999445492390294
    mean_inference_ms: 0.9786700303678582
    mean_processing_ms: 1.346019007270948
  time_since_restore: 180.82903575897217
  time_this_iter_s: 5.015484809875488
  time_total_s: 180.82903575897217
  timestamp: 1563365221
  timesteps_since_restore: 950400
  timesteps_this_iter: 26400
  timesteps_total: 950400
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 180 s, 36 iter, 950400 ts, 12.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.00627068394756
  episode_reward_mean: 11.990902865272199
  episode_reward_min: -7.2572903972704665
  episodes_this_iter: 176
  episodes_total: 6512
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2660.097
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.456329822540283
        kl: 0.011347209103405476
        policy_loss: -0.014059617184102535
        total_loss: 0.8324744701385498
        vf_explained_var: 0.9736265540122986
        vf_loss: 0.845115602016449
    load_time_ms: 0.775
    num_steps_sampled: 976800
    num_steps_trained: 962000
    sample_time_ms: 2150.178
    update_time_ms: 4.022
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6008358963237719
    mean_inference_ms: 0.9795247384143395
    mean_processing_ms: 1.3466330389363028
  time_since_restore: 186.03643798828125
  time_this_iter_s: 5.207402229309082
  time_total_s: 186.03643798828125
  timestamp: 1563365226
  timesteps_since_restore: 976800
  timesteps_this_iter: 26400
  timesteps_total: 976800
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 186 s, 37 iter, 976800 ts, 12 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.14008202960463
  episode_reward_mean: 13.048402176796474
  episode_reward_min: -11.642005121613243
  episodes_this_iter: 176
  episodes_total: 6864
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2684.671
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.429689884185791
        kl: 0.011387511156499386
        policy_loss: -0.014788041822612286
        total_loss: 0.8723418116569519
        vf_explained_var: 0.9763641357421875
        vf_loss: 0.88570636510849
    load_time_ms: 0.767
    num_steps_sampled: 1029600
    num_steps_trained: 1014000
    sample_time_ms: 2161.979
    update_time_ms: 3.905
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.599640085519662
    mean_inference_ms: 0.9780691514459986
    mean_processing_ms: 1.3461239664715563
  time_since_restore: 195.73592567443848
  time_this_iter_s: 4.847502946853638
  time_total_s: 195.73592567443848
  timestamp: 1563365236
  timesteps_since_restore: 1029600
  timesteps_this_iter: 26400
  timesteps_total: 1029600
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 195 s, 39 iter, 1029600 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.439983228596176
  episode_reward_mean: 15.053401298296619
  episode_reward_min: -8.44193761314726
  episodes_this_iter: 176
  episodes_total: 7216
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2744.842
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4045794010162354
        kl: 0.011257975362241268
        policy_loss: -0.013431189581751823
        total_loss: 0.6679309606552124
        vf_explained_var: 0.9837640523910522
        vf_loss: 0.6799548864364624
    load_time_ms: 0.76
    num_steps_sampled: 1082400
    num_steps_trained: 1066000
    sample_time_ms: 2158.304
    update_time_ms: 4.064
  iterations_since_restore: 41
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.599854218657345
    mean_inference_ms: 0.9773119146914315
    mean_processing_ms: 1.3464550198162655
  time_since_restore: 205.72880482673645
  time_this_iter_s: 5.009084939956665
  time_total_s: 205.72880482673645
  timestamp: 1563365246
  timesteps_since_restore: 1082400
  timesteps_this_iter: 26400
  timesteps_total: 1082400
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 205 s, 41 iter, 1082400 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.05870077706682
  episode_reward_mean: 14.052905865977165
  episode_reward_min: -7.019157883564287
  episodes_this_iter: 176
  episodes_total: 7392
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2741.031
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3955438137054443
        kl: 0.010861673392355442
        policy_loss: -0.012584015727043152
        total_loss: 0.6771720051765442
        vf_explained_var: 0.9815269708633423
        vf_loss: 0.6883982419967651
    load_time_ms: 0.766
    num_steps_sampled: 1108800
    num_steps_trained: 1092000
    sample_time_ms: 2213.158
    update_time_ms: 4.151
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5997613655131162
    mean_inference_ms: 0.9776923178204101
    mean_processing_ms: 1.3459347753301683
  time_since_restore: 210.88036394119263
  time_this_iter_s: 5.151559114456177
  time_total_s: 210.88036394119263
  timestamp: 1563365251
  timesteps_since_restore: 1108800
  timesteps_this_iter: 26400
  timesteps_total: 1108800
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 210 s, 42 iter, 1108800 ts, 14.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.830586284124415
  episode_reward_mean: 13.630421291859466
  episode_reward_min: -5.937940033060735
  episodes_this_iter: 176
  episodes_total: 7568
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2760.846
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3833906650543213
        kl: 0.011182411573827267
        policy_loss: -0.012702682055532932
        total_loss: 0.7145239114761353
        vf_explained_var: 0.9766674637794495
        vf_loss: 0.7258287072181702
    load_time_ms: 0.77
    num_steps_sampled: 1135200
    num_steps_trained: 1118000
    sample_time_ms: 2215.278
    update_time_ms: 4.191
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5995824169324047
    mean_inference_ms: 0.9767431826277835
    mean_processing_ms: 1.3461169894493275
  time_since_restore: 215.87826538085938
  time_this_iter_s: 4.997901439666748
  time_total_s: 215.87826538085938
  timestamp: 1563365256
  timesteps_since_restore: 1135200
  timesteps_this_iter: 26400
  timesteps_total: 1135200
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 215 s, 43 iter, 1135200 ts, 13.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.96987631540345
  episode_reward_mean: 14.694282715858693
  episode_reward_min: -8.001543489582376
  episodes_this_iter: 176
  episodes_total: 7920
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2747.83
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.355787515640259
        kl: 0.010415222495794296
        policy_loss: -0.012297599576413631
        total_loss: 0.6034255623817444
        vf_explained_var: 0.9822205305099487
        vf_loss: 0.6144212484359741
    load_time_ms: 0.774
    num_steps_sampled: 1188000
    num_steps_trained: 1170000
    sample_time_ms: 2222.105
    update_time_ms: 4.196
  iterations_since_restore: 45
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6000116436725826
    mean_inference_ms: 0.9768478095626217
    mean_processing_ms: 1.3448236068091115
  time_since_restore: 225.72076320648193
  time_this_iter_s: 4.921396732330322
  time_total_s: 225.72076320648193
  timestamp: 1563365266
  timesteps_since_restore: 1188000
  timesteps_this_iter: 26400
  timesteps_total: 1188000
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 225 s, 45 iter, 1188000 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-07-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.93169038334402
  episode_reward_mean: 15.389445295156866
  episode_reward_min: -4.800057816597924
  episodes_this_iter: 176
  episodes_total: 8096
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2749.46
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3479762077331543
        kl: 0.010544265620410442
        policy_loss: -0.014284636825323105
        total_loss: 0.5307063460350037
        vf_explained_var: 0.9867104291915894
        vf_loss: 0.5436729192733765
    load_time_ms: 0.737
    num_steps_sampled: 1214400
    num_steps_trained: 1196000
    sample_time_ms: 2242.8
    update_time_ms: 4.367
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6020183209699448
    mean_inference_ms: 0.9772202783790362
    mean_processing_ms: 1.3472969731339233
  time_since_restore: 230.9580340385437
  time_this_iter_s: 5.237270832061768
  time_total_s: 230.9580340385437
  timestamp: 1563365271
  timesteps_since_restore: 1214400
  timesteps_this_iter: 26400
  timesteps_total: 1214400
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 230 s, 46 iter, 1214400 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.84237016831198
  episode_reward_mean: 13.95070766783885
  episode_reward_min: -6.401644997376365
  episodes_this_iter: 176
  episodes_total: 8448
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2716.665
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3149712085723877
        kl: 0.01127595640718937
        policy_loss: -0.013343002647161484
        total_loss: 0.5528646111488342
        vf_explained_var: 0.9832044839859009
        vf_loss: 0.5647980570793152
    load_time_ms: 0.73
    num_steps_sampled: 1267200
    num_steps_trained: 1248000
    sample_time_ms: 2237.16
    update_time_ms: 4.698
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6033790338839973
    mean_inference_ms: 0.9773972884605939
    mean_processing_ms: 1.3478074909371671
  time_since_restore: 240.63550662994385
  time_this_iter_s: 4.869030952453613
  time_total_s: 240.63550662994385
  timestamp: 1563365281
  timesteps_since_restore: 1267200
  timesteps_this_iter: 26400
  timesteps_total: 1267200
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 240 s, 48 iter, 1267200 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.809596098572804
  episode_reward_mean: 15.537178104735903
  episode_reward_min: -6.0346764476770005
  episodes_this_iter: 176
  episodes_total: 8800
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2703.7
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.285984754562378
        kl: 0.011890645138919353
        policy_loss: -0.012352769263088703
        total_loss: 0.4695953130722046
        vf_explained_var: 0.9868056774139404
        vf_loss: 0.4804617166519165
    load_time_ms: 0.738
    num_steps_sampled: 1320000
    num_steps_trained: 1300000
    sample_time_ms: 2264.213
    update_time_ms: 4.642
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6058445510396362
    mean_inference_ms: 0.9771950603080068
    mean_processing_ms: 1.3505126773251244
  time_since_restore: 250.60568809509277
  time_this_iter_s: 5.119358062744141
  time_total_s: 250.60568809509277
  timestamp: 1563365291
  timesteps_since_restore: 1320000
  timesteps_this_iter: 26400
  timesteps_total: 1320000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 250 s, 50 iter, 1320000 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.383756345959874
  episode_reward_mean: 13.106201207161813
  episode_reward_min: -6.283372786322637
  episodes_this_iter: 176
  episodes_total: 8976
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2735.252
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2690396308898926
        kl: 0.01271612849086523
        policy_loss: -0.015395521186292171
        total_loss: 0.5059041380882263
        vf_explained_var: 0.9809291362762451
        vf_loss: 0.5197100639343262
    load_time_ms: 0.748
    num_steps_sampled: 1346400
    num_steps_trained: 1326000
    sample_time_ms: 2281.223
    update_time_ms: 4.619
  iterations_since_restore: 51
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6076885321603307
    mean_inference_ms: 0.9791057009374878
    mean_processing_ms: 1.3520687598390426
  time_since_restore: 256.1023631095886
  time_this_iter_s: 5.49667501449585
  time_total_s: 256.1023631095886
  timestamp: 1563365297
  timesteps_since_restore: 1346400
  timesteps_this_iter: 26400
  timesteps_total: 1346400
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 256 s, 51 iter, 1346400 ts, 13.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.145080268479404
  episode_reward_mean: 15.904026635808169
  episode_reward_min: -5.028333883958156
  episodes_this_iter: 176
  episodes_total: 9152
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2755.642
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2528011798858643
        kl: 0.011634575203061104
        policy_loss: -0.01177945639938116
        total_loss: 0.4849916994571686
        vf_explained_var: 0.9867414832115173
        vf_loss: 0.4953168034553528
    load_time_ms: 0.747
    num_steps_sampled: 1372800
    num_steps_trained: 1352000
    sample_time_ms: 2255.816
    update_time_ms: 4.69
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6082484098587282
    mean_inference_ms: 0.9804505049858943
    mean_processing_ms: 1.3545955322907959
  time_since_restore: 261.204448223114
  time_this_iter_s: 5.102085113525391
  time_total_s: 261.204448223114
  timestamp: 1563365302
  timesteps_since_restore: 1372800
  timesteps_this_iter: 26400
  timesteps_total: 1372800
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 261 s, 52 iter, 1372800 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.740576028328235
  episode_reward_mean: 15.689474299663464
  episode_reward_min: -7.00229243281764
  episodes_this_iter: 176
  episodes_total: 9328
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2736.66
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.232504367828369
        kl: 0.012489219196140766
        policy_loss: -0.014764407649636269
        total_loss: 0.43875426054000854
        vf_explained_var: 0.9870631694793701
        vf_loss: 0.4519575238227844
    load_time_ms: 0.748
    num_steps_sampled: 1399200
    num_steps_trained: 1378000
    sample_time_ms: 2287.048
    update_time_ms: 4.655
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6086964600711793
    mean_inference_ms: 0.9811790093602446
    mean_processing_ms: 1.3537759970827044
  time_since_restore: 266.325492143631
  time_this_iter_s: 5.121043920516968
  time_total_s: 266.325492143631
  timestamp: 1563365307
  timesteps_since_restore: 1399200
  timesteps_this_iter: 26400
  timesteps_total: 1399200
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 266 s, 53 iter, 1399200 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.07716675224377
  episode_reward_mean: 14.908429323026617
  episode_reward_min: -5.300053727907657
  episodes_this_iter: 176
  episodes_total: 9504
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2761.486
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.218395233154297
        kl: 0.012181991711258888
        policy_loss: -0.01416821125894785
        total_loss: 0.3898908197879791
        vf_explained_var: 0.988300085067749
        vf_loss: 0.4025363326072693
    load_time_ms: 0.747
    num_steps_sampled: 1425600
    num_steps_trained: 1404000
    sample_time_ms: 2317.189
    update_time_ms: 4.486
  iterations_since_restore: 54
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.614520192608262
    mean_inference_ms: 0.984327808304295
    mean_processing_ms: 1.3596165395004682
  time_since_restore: 271.7930462360382
  time_this_iter_s: 5.467554092407227
  time_total_s: 271.7930462360382
  timestamp: 1563365312
  timesteps_since_restore: 1425600
  timesteps_this_iter: 26400
  timesteps_total: 1425600
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 271 s, 54 iter, 1425600 ts, 14.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.60344711104477
  episode_reward_mean: 15.8492402608715
  episode_reward_min: -5.540898563940146
  episodes_this_iter: 176
  episodes_total: 9856
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2764.236
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1970393657684326
        kl: 0.011884639970958233
        policy_loss: -0.01172296330332756
        total_loss: 0.4154978096485138
        vf_explained_var: 0.988688051700592
        vf_loss: 0.425735205411911
    load_time_ms: 0.766
    num_steps_sampled: 1478400
    num_steps_trained: 1456000
    sample_time_ms: 2308.332
    update_time_ms: 4.337
  iterations_since_restore: 56
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.608091225467465
    mean_inference_ms: 0.9815869263445769
    mean_processing_ms: 1.3532162220138757
  time_since_restore: 281.88529229164124
  time_this_iter_s: 5.185672760009766
  time_total_s: 281.88529229164124
  timestamp: 1563365322
  timesteps_since_restore: 1478400
  timesteps_this_iter: 26400
  timesteps_total: 1478400
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 281 s, 56 iter, 1478400 ts, 15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-08-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.33897841780538
  episode_reward_mean: 15.578959821273584
  episode_reward_min: -5.9770801812280725
  episodes_this_iter: 176
  episodes_total: 10208
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2770.174
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.171057939529419
        kl: 0.013290369883179665
        policy_loss: -0.014626667834818363
        total_loss: 0.38831549882888794
        vf_explained_var: 0.9887489676475525
        vf_loss: 0.40128085017204285
    load_time_ms: 0.77
    num_steps_sampled: 1531200
    num_steps_trained: 1508000
    sample_time_ms: 2274.167
    update_time_ms: 3.827
  iterations_since_restore: 58
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6114458219197447
    mean_inference_ms: 0.9828748264285039
    mean_processing_ms: 1.3588489637999768
  time_since_restore: 291.2736074924469
  time_this_iter_s: 4.6373960971832275
  time_total_s: 291.2736074924469
  timestamp: 1563365332
  timesteps_since_restore: 1531200
  timesteps_this_iter: 26400
  timesteps_total: 1531200
  training_iteration: 58
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 291 s, 58 iter, 1531200 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.006442730705594
  episode_reward_mean: 16.0273010536309
  episode_reward_min: -4.794033842149629
  episodes_this_iter: 176
  episodes_total: 10560
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2781.802
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1458773612976074
        kl: 0.01286266278475523
        policy_loss: -0.015906108543276787
        total_loss: 0.3420419991016388
        vf_explained_var: 0.9905940294265747
        vf_loss: 0.3563402593135834
    load_time_ms: 0.771
    num_steps_sampled: 1584000
    num_steps_trained: 1560000
    sample_time_ms: 2262.915
    update_time_ms: 3.917
  iterations_since_restore: 60
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6100409425091655
    mean_inference_ms: 0.9815895229227108
    mean_processing_ms: 1.3567823420672689
  time_since_restore: 301.25303888320923
  time_this_iter_s: 5.200676441192627
  time_total_s: 301.25303888320923
  timestamp: 1563365342
  timesteps_since_restore: 1584000
  timesteps_this_iter: 26400
  timesteps_total: 1584000
  training_iteration: 60
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 301 s, 60 iter, 1584000 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.826476855501724
  episode_reward_mean: 14.037175463537004
  episode_reward_min: -3.5249751596334074
  episodes_this_iter: 176
  episodes_total: 10912
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2688.994
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.117274761199951
        kl: 0.013521040789783001
        policy_loss: -0.0176409799605608
        total_loss: 0.3220653533935547
        vf_explained_var: 0.9899714589118958
        vf_loss: 0.33801621198654175
    load_time_ms: 0.772
    num_steps_sampled: 1636800
    num_steps_trained: 1612000
    sample_time_ms: 2223.335
    update_time_ms: 3.761
  iterations_since_restore: 62
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6083829370591116
    mean_inference_ms: 0.9800652202241092
    mean_processing_ms: 1.3568683620629867
  time_since_restore: 310.51978397369385
  time_this_iter_s: 4.559995174407959
  time_total_s: 310.51978397369385
  timestamp: 1563365351
  timesteps_since_restore: 1636800
  timesteps_this_iter: 26400
  timesteps_total: 1636800
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 310 s, 62 iter, 1636800 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.66650621052203
  episode_reward_mean: 15.147802967253432
  episode_reward_min: -3.578705666409112
  episodes_this_iter: 176
  episodes_total: 11264
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2664.183
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0892317295074463
        kl: 0.0122667932882905
        policy_loss: -0.014908642508089542
        total_loss: 0.29537132382392883
        vf_explained_var: 0.9907444715499878
        vf_loss: 0.3087466359138489
    load_time_ms: 0.791
    num_steps_sampled: 1689600
    num_steps_trained: 1664000
    sample_time_ms: 2173.654
    update_time_ms: 3.747
  iterations_since_restore: 64
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6070443035229263
    mean_inference_ms: 0.9793068102581092
    mean_processing_ms: 1.35620521652023
  time_since_restore: 320.36403369903564
  time_this_iter_s: 4.971546411514282
  time_total_s: 320.36403369903564
  timestamp: 1563365361
  timesteps_since_restore: 1689600
  timesteps_this_iter: 26400
  timesteps_total: 1689600
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 320 s, 64 iter, 1689600 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.94019826879728
  episode_reward_mean: 17.4019940843339
  episode_reward_min: -4.612101075957538
  episodes_this_iter: 176
  episodes_total: 11440
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2682.032
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0726940631866455
        kl: 0.01337393932044506
        policy_loss: -0.017564501613378525
        total_loss: 0.29078158736228943
        vf_explained_var: 0.9921437501907349
        vf_loss: 0.30667436122894287
    load_time_ms: 0.782
    num_steps_sampled: 1716000
    num_steps_trained: 1690000
    sample_time_ms: 2187.231
    update_time_ms: 3.772
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6051363363117828
    mean_inference_ms: 0.9794511877445756
    mean_processing_ms: 1.354669045780534
  time_since_restore: 325.5868670940399
  time_this_iter_s: 5.2228333950042725
  time_total_s: 325.5868670940399
  timestamp: 1563365366
  timesteps_since_restore: 1716000
  timesteps_this_iter: 26400
  timesteps_total: 1716000
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 325 s, 65 iter, 1716000 ts, 17.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.050177723374105
  episode_reward_mean: 16.45785955742224
  episode_reward_min: -10.68765917382228
  episodes_this_iter: 176
  episodes_total: 11616
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2694.014
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.059579610824585
        kl: 0.014034600928425789
        policy_loss: -0.014835860580205917
        total_loss: 0.31783267855644226
        vf_explained_var: 0.991324782371521
        vf_loss: 0.3309142291545868
    load_time_ms: 0.776
    num_steps_sampled: 1742400
    num_steps_trained: 1716000
    sample_time_ms: 2182.34
    update_time_ms: 3.894
  iterations_since_restore: 66
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.606717449303673
    mean_inference_ms: 0.9798542876122596
    mean_processing_ms: 1.3554192632363442
  time_since_restore: 330.84645819664
  time_this_iter_s: 5.259591102600098
  time_total_s: 330.84645819664
  timestamp: 1563365372
  timesteps_since_restore: 1742400
  timesteps_this_iter: 26400
  timesteps_total: 1742400
  training_iteration: 66
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 330 s, 66 iter, 1742400 ts, 16.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.88159269339538
  episode_reward_mean: 16.20599747912998
  episode_reward_min: -6.1241593536046555
  episodes_this_iter: 176
  episodes_total: 11792
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2700.325
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0503323078155518
        kl: 0.012515892274677753
        policy_loss: -0.015433011576533318
        total_loss: 0.3112032115459442
        vf_explained_var: 0.9906550645828247
        vf_loss: 0.325071781873703
    load_time_ms: 0.775
    num_steps_sampled: 1768800
    num_steps_trained: 1742000
    sample_time_ms: 2208.106
    update_time_ms: 3.947
  iterations_since_restore: 67
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6085925557121115
    mean_inference_ms: 0.9800491642684118
    mean_processing_ms: 1.3564914663070995
  time_since_restore: 335.91882586479187
  time_this_iter_s: 5.0723676681518555
  time_total_s: 335.91882586479187
  timestamp: 1563365377
  timesteps_since_restore: 1768800
  timesteps_this_iter: 26400
  timesteps_total: 1768800
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 335 s, 67 iter, 1768800 ts, 16.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.72006915867219
  episode_reward_mean: 17.348685365087576
  episode_reward_min: -4.538401571396306
  episodes_this_iter: 176
  episodes_total: 11968
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2744.585
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.032092809677124
        kl: 0.013216452673077583
        policy_loss: -0.01573359966278076
        total_loss: 0.274596244096756
        vf_explained_var: 0.9923256635665894
        vf_loss: 0.28867781162261963
    load_time_ms: 0.776
    num_steps_sampled: 1795200
    num_steps_trained: 1768000
    sample_time_ms: 2278.544
    update_time_ms: 3.963
  iterations_since_restore: 68
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6065614360179277
    mean_inference_ms: 0.9813851158585245
    mean_processing_ms: 1.3557663512914218
  time_since_restore: 341.70765376091003
  time_this_iter_s: 5.788827896118164
  time_total_s: 341.70765376091003
  timestamp: 1563365382
  timesteps_since_restore: 1795200
  timesteps_this_iter: 26400
  timesteps_total: 1795200
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 341 s, 68 iter, 1795200 ts, 17.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.906217136091314
  episode_reward_mean: 15.735348251023687
  episode_reward_min: -5.417367900481495
  episodes_this_iter: 176
  episodes_total: 12144
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2760.338
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.018012285232544
        kl: 0.01331979688256979
        policy_loss: -0.017394132912158966
        total_loss: 0.31433597207069397
        vf_explained_var: 0.9908180236816406
        vf_loss: 0.33006513118743896
    load_time_ms: 0.774
    num_steps_sampled: 1821600
    num_steps_trained: 1794000
    sample_time_ms: 2307.487
    update_time_ms: 4.171
  iterations_since_restore: 69
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6105390330854858
    mean_inference_ms: 0.9832491802907427
    mean_processing_ms: 1.3588418442347956
  time_since_restore: 346.9310562610626
  time_this_iter_s: 5.223402500152588
  time_total_s: 346.9310562610626
  timestamp: 1563365388
  timesteps_since_restore: 1821600
  timesteps_this_iter: 26400
  timesteps_total: 1821600
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 346 s, 69 iter, 1821600 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.26617168831556
  episode_reward_mean: 15.276753293817876
  episode_reward_min: -2.991593033477893
  episodes_this_iter: 176
  episodes_total: 12320
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2767.065
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.006995916366577
        kl: 0.014048987068235874
        policy_loss: -0.01675284095108509
        total_loss: 0.23524335026741028
        vf_explained_var: 0.9930196404457092
        vf_loss: 0.2502400577068329
    load_time_ms: 0.791
    num_steps_sampled: 1848000
    num_steps_trained: 1820000
    sample_time_ms: 2303.927
    update_time_ms: 4.082
  iterations_since_restore: 70
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6144018015528956
    mean_inference_ms: 0.984493543743092
    mean_processing_ms: 1.3611256975197188
  time_since_restore: 352.16308641433716
  time_this_iter_s: 5.232030153274536
  time_total_s: 352.16308641433716
  timestamp: 1563365393
  timesteps_since_restore: 1848000
  timesteps_this_iter: 26400
  timesteps_total: 1848000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 352 s, 70 iter, 1848000 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-09-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.916884883380106
  episode_reward_mean: 15.881727936057045
  episode_reward_min: -7.555536151160325
  episodes_this_iter: 176
  episodes_total: 12496
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2812.378
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.994377613067627
        kl: 0.011145555414259434
        policy_loss: -0.008422104641795158
        total_loss: 0.8855184316635132
        vf_explained_var: 0.9760229587554932
        vf_loss: 0.8925473690032959
    load_time_ms: 0.787
    num_steps_sampled: 1874400
    num_steps_trained: 1846000
    sample_time_ms: 2298.095
    update_time_ms: 4.063
  iterations_since_restore: 71
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6096409691398392
    mean_inference_ms: 0.9828848450655951
    mean_processing_ms: 1.358572014159701
  time_since_restore: 357.26695942878723
  time_this_iter_s: 5.103873014450073
  time_total_s: 357.26695942878723
  timestamp: 1563365398
  timesteps_since_restore: 1874400
  timesteps_this_iter: 26400
  timesteps_total: 1874400
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 357 s, 71 iter, 1874400 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.82501235813717
  episode_reward_mean: 16.52247909958907
  episode_reward_min: -3.877067945497671
  episodes_this_iter: 176
  episodes_total: 12672
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2826.09
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.975087285041809
        kl: 0.013636932708323002
        policy_loss: -0.015123877674341202
        total_loss: 0.25278663635253906
        vf_explained_var: 0.9925562143325806
        vf_loss: 0.26620587706565857
    load_time_ms: 0.781
    num_steps_sampled: 1900800
    num_steps_trained: 1872000
    sample_time_ms: 2333.753
    update_time_ms: 4.109
  iterations_since_restore: 72
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.611550000119904
    mean_inference_ms: 0.9842016268417236
    mean_processing_ms: 1.3608057392434132
  time_since_restore: 362.3222210407257
  time_this_iter_s: 5.055261611938477
  time_total_s: 362.3222210407257
  timestamp: 1563365403
  timesteps_since_restore: 1900800
  timesteps_this_iter: 26400
  timesteps_total: 1900800
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 362 s, 72 iter, 1900800 ts, 16.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.19492075936865
  episode_reward_mean: 14.856497156560078
  episode_reward_min: -5.682892302439964
  episodes_this_iter: 176
  episodes_total: 12848
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2840.821
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9628981351852417
        kl: 0.01274836715310812
        policy_loss: -0.013871150091290474
        total_loss: 0.24252860248088837
        vf_explained_var: 0.9917235970497131
        vf_loss: 0.25480619072914124
    load_time_ms: 0.774
    num_steps_sampled: 1927200
    num_steps_trained: 1898000
    sample_time_ms: 2346.536
    update_time_ms: 4.193
  iterations_since_restore: 73
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6137394814585642
    mean_inference_ms: 0.9841816350336281
    mean_processing_ms: 1.3605533927913127
  time_since_restore: 367.4738099575043
  time_this_iter_s: 5.1515889167785645
  time_total_s: 367.4738099575043
  timestamp: 1563365408
  timesteps_since_restore: 1927200
  timesteps_this_iter: 26400
  timesteps_total: 1927200
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 367 s, 73 iter, 1927200 ts, 14.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.12875415876811
  episode_reward_mean: 16.83421852508575
  episode_reward_min: -12.331851305790229
  episodes_this_iter: 176
  episodes_total: 13200
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2805.051
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9428539276123047
        kl: 0.01202339120209217
        policy_loss: -0.01323715876787901
        total_loss: 0.7030777335166931
        vf_explained_var: 0.9829093217849731
        vf_loss: 0.7148119807243347
    load_time_ms: 0.757
    num_steps_sampled: 1980000
    num_steps_trained: 1950000
    sample_time_ms: 2352.364
    update_time_ms: 4.486
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6129251467882433
    mean_inference_ms: 0.9850050615041048
    mean_processing_ms: 1.3616988648115396
  time_since_restore: 377.3729009628296
  time_this_iter_s: 4.951905012130737
  time_total_s: 377.3729009628296
  timestamp: 1563365418
  timesteps_since_restore: 1980000
  timesteps_this_iter: 26400
  timesteps_total: 1980000
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 377 s, 75 iter, 1980000 ts, 16.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.00342370756266
  episode_reward_mean: 17.950069020819715
  episode_reward_min: -3.2896322798160083
  episodes_this_iter: 176
  episodes_total: 13376
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2792.174
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9273077249526978
        kl: 0.013459782116115093
        policy_loss: -0.013007585890591145
        total_loss: 0.2134096920490265
        vf_explained_var: 0.9944210052490234
        vf_loss: 0.22473478317260742
    load_time_ms: 0.764
    num_steps_sampled: 2006400
    num_steps_trained: 1976000
    sample_time_ms: 2348.741
    update_time_ms: 4.394
  iterations_since_restore: 76
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6123674006494648
    mean_inference_ms: 0.984877787211014
    mean_processing_ms: 1.360245055291873
  time_since_restore: 382.46678280830383
  time_this_iter_s: 5.093881845474243
  time_total_s: 382.46678280830383
  timestamp: 1563365423
  timesteps_since_restore: 2006400
  timesteps_this_iter: 26400
  timesteps_total: 2006400
  training_iteration: 76
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 382 s, 76 iter, 2006400 ts, 18 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.89822856354854
  episode_reward_mean: 16.134084129272964
  episode_reward_min: -2.7332156542340553
  episodes_this_iter: 176
  episodes_total: 13728
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2747.986
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.900747299194336
        kl: 0.014487534761428833
        policy_loss: -0.0155789190903306
        total_loss: 0.1993672251701355
        vf_explained_var: 0.9936407804489136
        vf_loss: 0.21313521265983582
    load_time_ms: 0.757
    num_steps_sampled: 2059200
    num_steps_trained: 2028000
    sample_time_ms: 2293.596
    update_time_ms: 4.474
  iterations_since_restore: 78
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6146508633858347
    mean_inference_ms: 0.9854309919288742
    mean_processing_ms: 1.3624835637363242
  time_since_restore: 392.3336536884308
  time_this_iter_s: 4.943371057510376
  time_total_s: 392.3336536884308
  timestamp: 1563365433
  timesteps_since_restore: 2059200
  timesteps_this_iter: 26400
  timesteps_total: 2059200
  training_iteration: 78
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 392 s, 78 iter, 2059200 ts, 16.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.68043998276565
  episode_reward_mean: 16.840758364767947
  episode_reward_min: -1.8422061298833687
  episodes_this_iter: 176
  episodes_total: 13904
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2738.094
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8848974704742432
        kl: 0.01495725754648447
        policy_loss: -0.018994508311152458
        total_loss: 0.17515629529953003
        vf_explained_var: 0.9946416020393372
        vf_loss: 0.19228115677833557
    load_time_ms: 0.748
    num_steps_sampled: 2085600
    num_steps_trained: 2054000
    sample_time_ms: 2283.734
    update_time_ms: 4.343
  iterations_since_restore: 79
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.613912927993747
    mean_inference_ms: 0.9863588190741239
    mean_processing_ms: 1.3610815456344894
  time_since_restore: 397.3611743450165
  time_this_iter_s: 5.027520656585693
  time_total_s: 397.3611743450165
  timestamp: 1563365438
  timesteps_since_restore: 2085600
  timesteps_this_iter: 26400
  timesteps_total: 2085600
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 397 s, 79 iter, 2085600 ts, 16.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.000997103108155
  episode_reward_mean: 16.699667667757556
  episode_reward_min: -1.3636647450827843
  episodes_this_iter: 176
  episodes_total: 14080
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2751.714
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8710118532180786
        kl: 0.015668436884880066
        policy_loss: -0.018711717799305916
        total_loss: 0.201897531747818
        vf_explained_var: 0.9939753413200378
        vf_loss: 0.218650683760643
    load_time_ms: 0.74
    num_steps_sampled: 2112000
    num_steps_trained: 2080000
    sample_time_ms: 2285.291
    update_time_ms: 4.413
  iterations_since_restore: 80
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6158822209039785
    mean_inference_ms: 0.9856623410442676
    mean_processing_ms: 1.3637091023569885
  time_since_restore: 402.74699211120605
  time_this_iter_s: 5.385817766189575
  time_total_s: 402.74699211120605
  timestamp: 1563365444
  timesteps_since_restore: 2112000
  timesteps_this_iter: 26400
  timesteps_total: 2112000
  training_iteration: 80
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 402 s, 80 iter, 2112000 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.96791425909894
  episode_reward_mean: 17.081656543913592
  episode_reward_min: -3.440392455015597
  episodes_this_iter: 176
  episodes_total: 14256
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2765.744
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.851560115814209
        kl: 0.015304755419492722
        policy_loss: -0.01812201924622059
        total_loss: 0.17650601267814636
        vf_explained_var: 0.9945172667503357
        vf_loss: 0.19271494448184967
    load_time_ms: 0.749
    num_steps_sampled: 2138400
    num_steps_trained: 2106000
    sample_time_ms: 2288.563
    update_time_ms: 4.471
  iterations_since_restore: 81
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.615438503484146
    mean_inference_ms: 0.9857301381885517
    mean_processing_ms: 1.3618267700958449
  time_since_restore: 408.02659726142883
  time_this_iter_s: 5.279605150222778
  time_total_s: 408.02659726142883
  timestamp: 1563365449
  timesteps_since_restore: 2138400
  timesteps_this_iter: 26400
  timesteps_total: 2138400
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 408 s, 81 iter, 2138400 ts, 17.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-10-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.21082157286731
  episode_reward_mean: 17.512835925900855
  episode_reward_min: -3.2554461832128903
  episodes_this_iter: 176
  episodes_total: 14432
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2799.822
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8315818309783936
        kl: 0.014935069717466831
        policy_loss: -0.019904715940356255
        total_loss: 0.15656474232673645
        vf_explained_var: 0.9953774809837341
        vf_loss: 0.17460259795188904
    load_time_ms: 0.754
    num_steps_sampled: 2164800
    num_steps_trained: 2132000
    sample_time_ms: 2273.665
    update_time_ms: 4.479
  iterations_since_restore: 82
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.617432237294068
    mean_inference_ms: 0.9872845274845382
    mean_processing_ms: 1.363576647977763
  time_since_restore: 413.27363991737366
  time_this_iter_s: 5.247042655944824
  time_total_s: 413.27363991737366
  timestamp: 1563365454
  timesteps_since_restore: 2164800
  timesteps_this_iter: 26400
  timesteps_total: 2164800
  training_iteration: 82
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 413 s, 82 iter, 2164800 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.61661012539253
  episode_reward_mean: 17.445022997753313
  episode_reward_min: -3.1620257078481675
  episodes_this_iter: 176
  episodes_total: 14608
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2832.234
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8109805583953857
        kl: 0.016854528337717056
        policy_loss: -0.017096256837248802
        total_loss: 0.15076932311058044
        vf_explained_var: 0.99601149559021
        vf_loss: 0.16575877368450165
    load_time_ms: 0.747
    num_steps_sampled: 2191200
    num_steps_trained: 2158000
    sample_time_ms: 2281.858
    update_time_ms: 4.497
  iterations_since_restore: 83
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6170987696430734
    mean_inference_ms: 0.9858015534019415
    mean_processing_ms: 1.3640140403490646
  time_since_restore: 418.8276243209839
  time_this_iter_s: 5.5539844036102295
  time_total_s: 418.8276243209839
  timestamp: 1563365460
  timesteps_since_restore: 2191200
  timesteps_this_iter: 26400
  timesteps_total: 2191200
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 418 s, 83 iter, 2191200 ts, 17.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.27847514554151
  episode_reward_mean: 17.586751563533106
  episode_reward_min: -2.857579081636586
  episodes_this_iter: 176
  episodes_total: 14960
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2812.523
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7817916870117188
        kl: 0.015616267919540405
        policy_loss: -0.01779348775744438
        total_loss: 0.13610602915287018
        vf_explained_var: 0.9957539439201355
        vf_loss: 0.151947483420372
    load_time_ms: 0.759
    num_steps_sampled: 2244000
    num_steps_trained: 2210000
    sample_time_ms: 2254.976
    update_time_ms: 4.262
  iterations_since_restore: 85
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6121546858352247
    mean_inference_ms: 0.9842603533419472
    mean_processing_ms: 1.3610723804509435
  time_since_restore: 428.2575144767761
  time_this_iter_s: 4.816617012023926
  time_total_s: 428.2575144767761
  timestamp: 1563365469
  timesteps_since_restore: 2244000
  timesteps_this_iter: 26400
  timesteps_total: 2244000
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 428 s, 85 iter, 2244000 ts, 17.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.46279058463266
  episode_reward_mean: 17.691659689396985
  episode_reward_min: -4.10605043392696
  episodes_this_iter: 176
  episodes_total: 15136
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2828.186
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7626855373382568
        kl: 0.01481584832072258
        policy_loss: -0.01806717738509178
        total_loss: 0.1188102513551712
        vf_explained_var: 0.9964287281036377
        vf_loss: 0.13502544164657593
    load_time_ms: 0.757
    num_steps_sampled: 2270400
    num_steps_trained: 2236000
    sample_time_ms: 2246.811
    update_time_ms: 4.307
  iterations_since_restore: 86
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.613313688931367
    mean_inference_ms: 0.9845786761224331
    mean_processing_ms: 1.3624963647930277
  time_since_restore: 433.42606687545776
  time_this_iter_s: 5.168552398681641
  time_total_s: 433.42606687545776
  timestamp: 1563365474
  timesteps_since_restore: 2270400
  timesteps_this_iter: 26400
  timesteps_total: 2270400
  training_iteration: 86
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 433 s, 86 iter, 2270400 ts, 17.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.16552375235892
  episode_reward_mean: 17.272624906224323
  episode_reward_min: -2.4731971677791353
  episodes_this_iter: 176
  episodes_total: 15488
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2870.308
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.728458285331726
        kl: 0.01557177398353815
        policy_loss: -0.017008930444717407
        total_loss: 0.11519281566143036
        vf_explained_var: 0.9964612722396851
        vf_loss: 0.13025528192520142
    load_time_ms: 0.773
    num_steps_sampled: 2323200
    num_steps_trained: 2288000
    sample_time_ms: 2232.826
    update_time_ms: 4.209
  iterations_since_restore: 88
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6158630614520981
    mean_inference_ms: 0.9855190642759853
    mean_processing_ms: 1.3621765269965342
  time_since_restore: 443.57230854034424
  time_this_iter_s: 5.182870149612427
  time_total_s: 443.57230854034424
  timestamp: 1563365484
  timesteps_since_restore: 2323200
  timesteps_this_iter: 26400
  timesteps_total: 2323200
  training_iteration: 88
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 443 s, 88 iter, 2323200 ts, 17.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.675720178296956
  episode_reward_mean: 17.65657594163844
  episode_reward_min: -1.9092710243009328
  episodes_this_iter: 176
  episodes_total: 15664
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2899.177
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.716584324836731
        kl: 0.016051406040787697
        policy_loss: -0.01719558611512184
        total_loss: 0.10730405151844025
        vf_explained_var: 0.9964948892593384
        vf_loss: 0.1224931925535202
    load_time_ms: 0.787
    num_steps_sampled: 2349600
    num_steps_trained: 2314000
    sample_time_ms: 2241.438
    update_time_ms: 4.201
  iterations_since_restore: 89
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6153131948049149
    mean_inference_ms: 0.9853860882291327
    mean_processing_ms: 1.3644871953059992
  time_since_restore: 448.9730098247528
  time_this_iter_s: 5.400701284408569
  time_total_s: 448.9730098247528
  timestamp: 1563365490
  timesteps_since_restore: 2349600
  timesteps_this_iter: 26400
  timesteps_total: 2349600
  training_iteration: 89
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 448 s, 89 iter, 2349600 ts, 17.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.72867346386948
  episode_reward_mean: 16.301237803210423
  episode_reward_min: -4.650416803854299
  episodes_this_iter: 176
  episodes_total: 15840
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2884.181
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7045842409133911
        kl: 0.015458031557500362
        policy_loss: -0.017547590658068657
        total_loss: 0.10296837985515594
        vf_explained_var: 0.9965220093727112
        vf_loss: 0.11858371645212173
    load_time_ms: 0.78
    num_steps_sampled: 2376000
    num_steps_trained: 2340000
    sample_time_ms: 2229.243
    update_time_ms: 4.104
  iterations_since_restore: 90
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6121393161496562
    mean_inference_ms: 0.9847085179399393
    mean_processing_ms: 1.36165455243756
  time_since_restore: 454.08433175086975
  time_this_iter_s: 5.111321926116943
  time_total_s: 454.08433175086975
  timestamp: 1563365495
  timesteps_since_restore: 2376000
  timesteps_this_iter: 26400
  timesteps_total: 2376000
  training_iteration: 90
  2019-07-17 14:11:51,339	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 454 s, 90 iter, 2376000 ts, 16.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.56897920567242
  episode_reward_mean: 17.484173779108122
  episode_reward_min: -2.2090333701915936
  episodes_this_iter: 176
  episodes_total: 16016
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2850.424
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6860600709915161
        kl: 0.0154747748747468
        policy_loss: -0.017619656398892403
        total_loss: 0.11017059534788132
        vf_explained_var: 0.9966002106666565
        vf_loss: 0.12585589289665222
    load_time_ms: 0.774
    num_steps_sampled: 2402400
    num_steps_trained: 2366000
    sample_time_ms: 2241.705
    update_time_ms: 4.157
  iterations_since_restore: 91
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6128367934488699
    mean_inference_ms: 0.9844063703488808
    mean_processing_ms: 1.3627628538063046
  time_since_restore: 459.1472272872925
  time_this_iter_s: 5.0628955364227295
  time_total_s: 459.1472272872925
  timestamp: 1563365500
  timesteps_since_restore: 2402400
  timesteps_this_iter: 26400
  timesteps_total: 2402400
  training_iteration: 91
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 459 s, 91 iter, 2402400 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.575291847981944
  episode_reward_mean: 17.956381060600577
  episode_reward_min: 0.12848842348375913
  episodes_this_iter: 176
  episodes_total: 16192
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2867.288
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.675360918045044
        kl: 0.015407920815050602
        policy_loss: -0.018055567517876625
        total_loss: 0.09510420262813568
        vf_explained_var: 0.9971152544021606
        vf_loss: 0.11123377829790115
    load_time_ms: 0.77
    num_steps_sampled: 2428800
    num_steps_trained: 2392000
    sample_time_ms: 2251.168
    update_time_ms: 4.068
  iterations_since_restore: 92
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6118766098978101
    mean_inference_ms: 0.9831728402267946
    mean_processing_ms: 1.3615124384731205
  time_since_restore: 464.6550862789154
  time_this_iter_s: 5.507858991622925
  time_total_s: 464.6550862789154
  timestamp: 1563365506
  timesteps_since_restore: 2428800
  timesteps_this_iter: 26400
  timesteps_total: 2428800
  training_iteration: 92
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32187], 464 s, 92 iter, 2428800 ts, 18 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-11-51
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 36.60217254131696
  episode_reward_mean: 19.044825603547135
  episode_reward_min: -3.334429552662748
  episodes_this_iter: 176
  episodes_total: 16368
  experiment_id: d08df8b5e13f4b3c8f275b54376bd89b
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2840.264
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6576424837112427
        kl: 0.01586449332535267
        policy_loss: -0.017473207786679268
        total_loss: 0.09247453510761261
        vf_explained_var: 0.9973089694976807
        vf_loss: 0.10796467959880829
    load_time_ms: 0.773
    num_steps_sampled: 2455200
    num_steps_trained: 2418000
    sample_time_ms: 2241.654
    update_time_ms: 4.117
  iterations_since_restore: 93
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32187
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.6143559285231062
    mean_inference_ms: 0.9849590882180302
    mean_processing_ms: 1.3635400283225163
  time_since_restore: 469.8441848754883
  time_this_iter_s: 5.189098596572876
  time_total_s: 469.8441848754883
  timestamp: 1563365511
  timesteps_since_restore: 2455200
  timesteps_this_iter: 26400
  timesteps_total: 2455200
  training_iteration: 93
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew
2019-07-17 14:11:51,350	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

[2m[36m(pid=32171)[0m 2019-07-17 14:11:51,464	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=32171)[0m [32m [     0.03995s,  INFO] TimeLimit:
[2m[36m(pid=32171)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32171)[0m - action_space = Box(2,)
[2m[36m(pid=32171)[0m - observation_space = Box(9,)
[2m[36m(pid=32171)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32171)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32171)[0m - _max_episode_steps = 150
[2m[36m(pid=32171)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.468296: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.476221: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.567543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.567932: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561c89d8baa0 executing computations on platform CUDA. Devices:
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.567959: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.591455: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.592236: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561c8a515950 executing computations on platform Host. Devices:
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.592269: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.592592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.592986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
[2m[36m(pid=32171)[0m name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
[2m[36m(pid=32171)[0m pciBusID: 0000:01:00.0
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.593214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.593330: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.593414: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.593508: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.593567: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.593630: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/amr/.mujoco/mujoco200/bin:/home/amr/.mujoco/mujoco200/bin
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.597446: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.597521: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.597553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.597565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:51.597572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[2m[36m(pid=32171)[0m W0717 14:11:51.602988 139871847769536 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32171)[0m Instructions for updating:
[2m[36m(pid=32171)[0m Use keras.layers.dense instead.
[2m[36m(pid=32171)[0m W0717 14:11:51.964976 139871847769536 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32171)[0m Instructions for updating:
[2m[36m(pid=32171)[0m Use `tf.cast` instead.
[2m[36m(pid=32171)[0m 2019-07-17 14:11:52.066180: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32171)[0m 2019-07-17 14:11:52,081	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32171)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32171)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=32171)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32171)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32171)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32171)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m W0717 14:11:52.118383 139871847769536 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32171)[0m Instructions for updating:
[2m[36m(pid=32171)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32171)[0m [32m [     1.34694s,  INFO] TimeLimit:
[2m[36m(pid=32171)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32171)[0m - action_space = Box(2,)
[2m[36m(pid=32171)[0m - observation_space = Box(9,)
[2m[36m(pid=32171)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32171)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32171)[0m - _max_episode_steps = 150
[2m[36m(pid=32171)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32171)[0m [32m [     1.34733s,  INFO] TimeLimit:
[2m[36m(pid=32171)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32171)[0m - action_space = Box(2,)
[2m[36m(pid=32171)[0m - observation_space = Box(9,)
[2m[36m(pid=32171)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32171)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32171)[0m - _max_episode_steps = 150
[2m[36m(pid=32171)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32171)[0m [32m [     1.34773s,  INFO] TimeLimit:
[2m[36m(pid=32171)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32171)[0m - action_space = Box(2,)
[2m[36m(pid=32171)[0m - observation_space = Box(9,)
[2m[36m(pid=32171)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32171)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32171)[0m - _max_episode_steps = 150
[2m[36m(pid=32171)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32171)[0m 2019-07-17 14:11:52,773	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f3492df3828>}
[2m[36m(pid=32171)[0m 2019-07-17 14:11:52,773	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f3492df3748>}
[2m[36m(pid=32171)[0m 2019-07-17 14:11:52,773	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52,849	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.871730: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32175)[0m [32m [     0.02703s,  INFO] TimeLimit:
[2m[36m(pid=32175)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32175)[0m - action_space = Box(2,)
[2m[36m(pid=32175)[0m - observation_space = Box(9,)
[2m[36m(pid=32175)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32175)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32175)[0m - _max_episode_steps = 150
[2m[36m(pid=32175)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52,855	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.871730: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32171)[0m 2019-07-17 14:11:52,839	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=32181)[0m [32m [     0.02708s,  INFO] TimeLimit:
[2m[36m(pid=32181)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32181)[0m - action_space = Box(2,)
[2m[36m(pid=32181)[0m - observation_space = Box(9,)
[2m[36m(pid=32181)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32181)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32181)[0m - _max_episode_steps = 150
[2m[36m(pid=32181)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52,884	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.904651: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.914062: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.917561: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.917617: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.917629: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.917741: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.917775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.917786: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.921392: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.921957: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559df9faa8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32172)[0m 2019-07-17 14:11:52.921987: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32172)[0m [32m [     0.03541s,  INFO] TimeLimit:
[2m[36m(pid=32172)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32172)[0m - action_space = Box(2,)
[2m[36m(pid=32172)[0m - observation_space = Box(9,)
[2m[36m(pid=32172)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32172)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32172)[0m - _max_episode_steps = 150
[2m[36m(pid=32172)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.880372: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.885350: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.885390: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.885402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.885539: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.885573: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.885584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.888214: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.888931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5627b9aa28d0 executing computations on platform Host. Devices:
[2m[36m(pid=32175)[0m 2019-07-17 14:11:52.888954: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32175)[0m W0717 14:11:52.893779 139922916931008 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32175)[0m Instructions for updating:
[2m[36m(pid=32175)[0m Use keras.layers.dense instead.
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.880407: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.885011: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.885057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.885066: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.885161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.885188: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.885195: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.903389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.904072: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5589cb8dc8d0 executing computations on platform Host. Devices:
[2m[36m(pid=32181)[0m 2019-07-17 14:11:52.904104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=32181)[0m W0717 14:11:52.909107 140381343561152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32181)[0m Instructions for updating:
[2m[36m(pid=32181)[0m Use keras.layers.dense instead.
[2m[36m(pid=32172)[0m W0717 14:11:52.929472 140047935804864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=32172)[0m Instructions for updating:
[2m[36m(pid=32172)[0m Use keras.layers.dense instead.
[2m[36m(pid=32175)[0m W0717 14:11:53.437810 139922916931008 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32175)[0m Instructions for updating:
[2m[36m(pid=32175)[0m Use `tf.cast` instead.
[2m[36m(pid=32175)[0m 2019-07-17 14:11:53.640659: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32181)[0m W0717 14:11:53.640906 140381343561152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32181)[0m Instructions for updating:
[2m[36m(pid=32181)[0m Use `tf.cast` instead.
[2m[36m(pid=32172)[0m W0717 14:11:53.685646 140047935804864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32172)[0m Instructions for updating:
[2m[36m(pid=32172)[0m Use `tf.cast` instead.
[2m[36m(pid=32175)[0m W0717 14:11:53.852986 139922916931008 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32175)[0m Instructions for updating:
[2m[36m(pid=32175)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32181)[0m 2019-07-17 14:11:53.843674: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32181)[0m 2019-07-17 14:11:53,893	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32181)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32181)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32181)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32181)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=32181)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32181)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32181)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32181)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32181)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32181)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32181)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m W0717 14:11:53.981485 140381343561152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32181)[0m Instructions for updating:
[2m[36m(pid=32181)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32172)[0m 2019-07-17 14:11:54.031531: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=32172)[0m W0717 14:11:54.262042 140047935804864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=32172)[0m Instructions for updating:
[2m[36m(pid=32172)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32172)[0m [32m [     3.51065s,  INFO] TimeLimit:
[2m[36m(pid=32172)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32172)[0m - action_space = Box(2,)
[2m[36m(pid=32172)[0m - observation_space = Box(9,)
[2m[36m(pid=32172)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32172)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32172)[0m - _max_episode_steps = 150
[2m[36m(pid=32172)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32172)[0m [32m [     3.51134s,  INFO] TimeLimit:
[2m[36m(pid=32172)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32172)[0m - action_space = Box(2,)
[2m[36m(pid=32172)[0m - observation_space = Box(9,)
[2m[36m(pid=32172)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32172)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32172)[0m - _max_episode_steps = 150
[2m[36m(pid=32172)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32172)[0m [32m [     3.51305s,  INFO] TimeLimit:
[2m[36m(pid=32172)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32172)[0m - action_space = Box(2,)
[2m[36m(pid=32172)[0m - observation_space = Box(9,)
[2m[36m(pid=32172)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32172)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32172)[0m - _max_episode_steps = 150
[2m[36m(pid=32172)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32175)[0m [32m [     3.65523s,  INFO] TimeLimit:
[2m[36m(pid=32175)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32175)[0m - action_space = Box(2,)
[2m[36m(pid=32175)[0m - observation_space = Box(9,)
[2m[36m(pid=32175)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32175)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32175)[0m - _max_episode_steps = 150
[2m[36m(pid=32175)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32175)[0m [32m [     3.65597s,  INFO] TimeLimit:
[2m[36m(pid=32175)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32175)[0m - action_space = Box(2,)
[2m[36m(pid=32175)[0m - observation_space = Box(9,)
[2m[36m(pid=32175)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32175)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32175)[0m - _max_episode_steps = 150
[2m[36m(pid=32175)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32175)[0m [32m [     3.65701s,  INFO] TimeLimit:
[2m[36m(pid=32175)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32175)[0m - action_space = Box(2,)
[2m[36m(pid=32175)[0m - observation_space = Box(9,)
[2m[36m(pid=32175)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32175)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32175)[0m - _max_episode_steps = 150
[2m[36m(pid=32175)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32181)[0m [32m [     3.63318s,  INFO] TimeLimit:
[2m[36m(pid=32181)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32181)[0m - action_space = Box(2,)
[2m[36m(pid=32181)[0m - observation_space = Box(9,)
[2m[36m(pid=32181)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32181)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32181)[0m - _max_episode_steps = 150
[2m[36m(pid=32181)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32181)[0m [32m [     3.63387s,  INFO] TimeLimit:
[2m[36m(pid=32181)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32181)[0m - action_space = Box(2,)
[2m[36m(pid=32181)[0m - observation_space = Box(9,)
[2m[36m(pid=32181)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32181)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32181)[0m - _max_episode_steps = 150
[2m[36m(pid=32181)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32181)[0m [32m [     3.63456s,  INFO] TimeLimit:
[2m[36m(pid=32181)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32181)[0m - action_space = Box(2,)
[2m[36m(pid=32181)[0m - observation_space = Box(9,)
[2m[36m(pid=32181)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32181)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32181)[0m - _max_episode_steps = 150
[2m[36m(pid=32181)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.478377: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.478916: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_1 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_1/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_1/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.479222: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_2 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_2/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_2/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.479491: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_3 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_3/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_3/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.479745: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_4 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_4/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_4/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.479952: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_5 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_5/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_5/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.480131: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_6 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_6/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_6/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:11:57.480287: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
[2m[36m(pid=32171)[0m   /job:localhost/replica:0/task:0/device:CPU:0].
[2m[36m(pid=32171)[0m See below for details of this colocation group:
[2m[36m(pid=32171)[0m Colocation Debug Info:
[2m[36m(pid=32171)[0m Colocation group had the following types and supported devices: 
[2m[36m(pid=32171)[0m Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
[2m[36m(pid=32171)[0m Assign: CPU 
[2m[36m(pid=32171)[0m Identity: CPU XLA_CPU XLA_GPU 
[2m[36m(pid=32171)[0m VariableV2: CPU 
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m Colocation members, user-requested devices, and framework assigned devices, if any:
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_7 (VariableV2) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_7/Assign (Assign) /device:GPU:0
[2m[36m(pid=32171)[0m   default_policy_1/tower_1/Variable_7/read (Identity) /device:GPU:0
[2m[36m(pid=32171)[0m 
[2m[36m(pid=2247)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2247)[0m W0717 14:11:58.514719 140025423832832 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2247)[0m Instructions for updating:
[2m[36m(pid=2247)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2249)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2249)[0m W0717 14:11:58.910600 139882941597440 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2249)[0m Instructions for updating:
[2m[36m(pid=2249)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2250)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2250)[0m W0717 14:11:58.931831 139903751984896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2250)[0m Instructions for updating:
[2m[36m(pid=2250)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2337)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2337)[0m W0717 14:11:59.017738 140700816918272 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2337)[0m Instructions for updating:
[2m[36m(pid=2337)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2243)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2243)[0m W0717 14:11:59.087526 140151007377152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2243)[0m Instructions for updating:
[2m[36m(pid=2243)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2340)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2340)[0m W0717 14:11:59.087206 140577693464320 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2340)[0m Instructions for updating:
[2m[36m(pid=2340)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2358)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2358)[0m W0717 14:11:59.136938 139685212931840 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2358)[0m Instructions for updating:
[2m[36m(pid=2358)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2341)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2341)[0m W0717 14:11:59.259658 140704343344896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2341)[0m Instructions for updating:
[2m[36m(pid=2341)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2344)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2344)[0m W0717 14:11:59.554772 140236830402304 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2344)[0m Instructions for updating:
[2m[36m(pid=2344)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2339)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2339)[0m W0717 14:11:59.574913 140248833279744 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2339)[0m Instructions for updating:
[2m[36m(pid=2339)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2347)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2347)[0m W0717 14:11:59.734749 140344984028928 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2347)[0m Instructions for updating:
[2m[36m(pid=2347)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2345)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2345)[0m W0717 14:11:59.713516 139622189217536 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2345)[0m Instructions for updating:
[2m[36m(pid=2345)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2338)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2338)[0m W0717 14:11:59.854559 140437901600512 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2338)[0m Instructions for updating:
[2m[36m(pid=2338)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2249)[0m 2019-07-17 14:11:59,926	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=2249)[0m [32m [     0.09380s,  INFO] TimeLimit:
[2m[36m(pid=2249)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2249)[0m - action_space = Box(2,)
[2m[36m(pid=2249)[0m - observation_space = Box(9,)
[2m[36m(pid=2249)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2249)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2249)[0m - _max_episode_steps = 150
[2m[36m(pid=2249)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2335)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2335)[0m W0717 14:11:59.949010 140173059376896 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2335)[0m Instructions for updating:
[2m[36m(pid=2335)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2343)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2343)[0m W0717 14:11:59.929781 140506692380416 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2343)[0m Instructions for updating:
[2m[36m(pid=2343)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2249)[0m 2019-07-17 14:11:59.981460: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.002996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.024684: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.024741: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.024755: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.024876: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.024915: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.024927: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.055592: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.056079: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559886ac1cf0 executing computations on platform Host. Devices:
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.056109: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=2249)[0m W0717 14:12:00.065060 139883298620864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=2249)[0m Instructions for updating:
[2m[36m(pid=2249)[0m Use keras.layers.dense instead.
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00,041	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.063760: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.074011: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.077814: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.077882: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.077895: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.078029: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.078063: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.078073: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=2250)[0m [32m [     0.04403s,  INFO] TimeLimit:
[2m[36m(pid=2250)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2250)[0m - action_space = Box(2,)
[2m[36m(pid=2250)[0m - observation_space = Box(9,)
[2m[36m(pid=2250)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2250)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2250)[0m - _max_episode_steps = 150
[2m[36m(pid=2250)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2336)[0m WARNING: Logging before flag parsing goes to stderr.
[2m[36m(pid=2336)[0m W0717 14:12:00.073594 140543306462976 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2336)[0m Instructions for updating:
[2m[36m(pid=2336)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00,086	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.113155: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.126073: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=2247)[0m [32m [     0.05266s,  INFO] TimeLimit:
[2m[36m(pid=2247)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2247)[0m - action_space = Box(2,)
[2m[36m(pid=2247)[0m - observation_space = Box(9,)
[2m[36m(pid=2247)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2247)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2247)[0m - _max_episode_steps = 150
[2m[36m(pid=2247)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.111346: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.112062: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5612011dc8d0 executing computations on platform Host. Devices:
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.112106: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=2250)[0m W0717 14:12:00.121957 139904108979648 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=2250)[0m Instructions for updating:
[2m[36m(pid=2250)[0m Use keras.layers.dense instead.
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.139384: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.139466: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.139483: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.139602: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.139648: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.139662: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.151729: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.152391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fa6fe778d0 executing computations on platform Host. Devices:
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.152431: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=2247)[0m W0717 14:12:00.160073 140025780819392 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=2247)[0m Instructions for updating:
[2m[36m(pid=2247)[0m Use keras.layers.dense instead.
[2m[36m(pid=2243)[0m [32m [     0.05710s,  INFO] TimeLimit:
[2m[36m(pid=2243)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2243)[0m - action_space = Box(2,)
[2m[36m(pid=2243)[0m - observation_space = Box(9,)
[2m[36m(pid=2243)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2243)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2243)[0m - _max_episode_steps = 150
[2m[36m(pid=2243)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00,317	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.340269: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.350403: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.354161: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.354225: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: navel-notebook-1
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.354236: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: navel-notebook-1
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.354354: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.354389: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.26.0
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.354400: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.26.0
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.357760: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.358384: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563fa0f0c8d0 executing computations on platform Host. Devices:
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.358430: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
[2m[36m(pid=2243)[0m W0717 14:12:00.367331 140151364367808 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
[2m[36m(pid=2243)[0m Instructions for updating:
[2m[36m(pid=2243)[0m Use keras.layers.dense instead.
[2m[36m(pid=2247)[0m W0717 14:12:00.638368 140025780819392 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2247)[0m Instructions for updating:
[2m[36m(pid=2247)[0m Use `tf.cast` instead.
[2m[36m(pid=2250)[0m W0717 14:12:00.622754 139904108979648 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2250)[0m Instructions for updating:
[2m[36m(pid=2250)[0m Use `tf.cast` instead.
[2m[36m(pid=2249)[0m W0717 14:12:00.641858 139883298620864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2249)[0m Instructions for updating:
[2m[36m(pid=2249)[0m Use `tf.cast` instead.
[2m[36m(pid=32171)[0m W0717 14:12:00.699209 139871847769536 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32171)[0m Instructions for updating:
[2m[36m(pid=32171)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=2247)[0m 2019-07-17 14:12:00.730682: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=2249)[0m 2019-07-17 14:12:00.728026: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=2250)[0m 2019-07-17 14:12:00.723345: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=2243)[0m W0717 14:12:00.749371 140151364367808 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/models/action_dist.py:172: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2243)[0m Instructions for updating:
[2m[36m(pid=2243)[0m Use `tf.cast` instead.
[2m[36m(pid=32172)[0m W0717 14:12:00.760563 140047935804864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32172)[0m Instructions for updating:
[2m[36m(pid=32172)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32175)[0m W0717 14:12:00.760465 139922916931008 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32175)[0m Instructions for updating:
[2m[36m(pid=32175)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32181)[0m W0717 14:12:00.760286 140381343561152 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=32181)[0m Instructions for updating:
[2m[36m(pid=32181)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=2249)[0m W0717 14:12:00.803002 139883298620864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2249)[0m Instructions for updating:
[2m[36m(pid=2249)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=2247)[0m W0717 14:12:00.805276 140025780819392 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2247)[0m Instructions for updating:
[2m[36m(pid=2247)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=2243)[0m 2019-07-17 14:12:00.830800: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[2m[36m(pid=2250)[0m W0717 14:12:00.823495 139904108979648 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2250)[0m Instructions for updating:
[2m[36m(pid=2250)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=2243)[0m W0717 14:12:00.909152 140151364367808 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
[2m[36m(pid=2243)[0m Instructions for updating:
[2m[36m(pid=2243)[0m Use tf.where in 2.0, which has the same broadcast rule as np.where
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,450	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,486	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.91, max=0.862, mean=0.049)},
[2m[36m(pid=32181)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.995, max=0.473, mean=-0.072)},
[2m[36m(pid=32181)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.867, max=0.676, mean=-0.112)},
[2m[36m(pid=32181)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.91, max=0.55, mean=-0.093)}}
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,486	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=32181)[0m   1: {'agent0': None},
[2m[36m(pid=32181)[0m   2: {'agent0': None},
[2m[36m(pid=32181)[0m   3: {'agent0': None}}
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,486	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.91, max=0.862, mean=0.049)
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,487	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,490	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32181)[0m                                   'env_id': 0,
[2m[36m(pid=32181)[0m                                   'info': None,
[2m[36m(pid=32181)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32181)[0m                                   'rnn_state': []},
[2m[36m(pid=32181)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32181)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32181)[0m                                   'env_id': 1,
[2m[36m(pid=32181)[0m                                   'info': None,
[2m[36m(pid=32181)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.393),
[2m[36m(pid=32181)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32181)[0m                                   'rnn_state': []},
[2m[36m(pid=32181)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32181)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32181)[0m                                   'env_id': 2,
[2m[36m(pid=32181)[0m                                   'info': None,
[2m[36m(pid=32181)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.15, max=1.092, mean=0.02),
[2m[36m(pid=32181)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32181)[0m                                   'rnn_state': []},
[2m[36m(pid=32181)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32181)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32181)[0m                                   'env_id': 3,
[2m[36m(pid=32181)[0m                                   'info': None,
[2m[36m(pid=32181)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.136, max=0.938, mean=-0.217),
[2m[36m(pid=32181)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32181)[0m                                   'rnn_state': []},
[2m[36m(pid=32181)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,490	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,576	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-2.199, max=0.955, mean=0.058),
[2m[36m(pid=32181)[0m                       [],
[2m[36m(pid=32181)[0m                       { 'action_prob': np.ndarray((4,), dtype=float32, min=0.008, max=0.119, mean=0.077),
[2m[36m(pid=32181)[0m                         'behaviour_logits': np.ndarray((4, 4), dtype=float32, min=-0.009, max=0.005, mean=-0.001),
[2m[36m(pid=32181)[0m                         'vf_preds': np.ndarray((4,), dtype=float32, min=-0.004, max=0.001, mean=-0.001)})}
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m 2019-07-17 14:12:01,967	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.001, max=0.159, mean=0.076),
[2m[36m(pid=32181)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-2.784, max=2.525, mean=0.033),
[2m[36m(pid=32181)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-21.114, max=20.591, mean=0.43),
[2m[36m(pid=32181)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.006, max=0.012, mean=0.003),
[2m[36m(pid=32181)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=32181)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1533411704.0, max=1533411704.0, mean=1533411704.0),
[2m[36m(pid=32181)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=32181)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-2.959, max=3.664, mean=0.316),
[2m[36m(pid=32181)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-2.959, max=3.664, mean=0.312),
[2m[36m(pid=32181)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-2.784, max=2.525, mean=0.035),
[2m[36m(pid=32181)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-2.239, max=3.466, mean=-0.055),
[2m[36m(pid=32181)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-2.239, max=3.466, mean=-0.063),
[2m[36m(pid=32181)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32181)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-21.114, max=20.601, mean=0.433),
[2m[36m(pid=32181)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.002, max=0.009, mean=0.003)},
[2m[36m(pid=32181)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=32181)[0m 
[2m[36m(pid=2247)[0m W0717 14:12:01.982257 140025780819392 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=2247)[0m Instructions for updating:
[2m[36m(pid=2247)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=2247)[0m [32m [     1.94614s,  INFO] TimeLimit:
[2m[36m(pid=2247)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2247)[0m - action_space = Box(2,)
[2m[36m(pid=2247)[0m - observation_space = Box(9,)
[2m[36m(pid=2247)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2247)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2247)[0m - _max_episode_steps = 150
[2m[36m(pid=2247)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2247)[0m [32m [     1.94679s,  INFO] TimeLimit:
[2m[36m(pid=2247)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2247)[0m - action_space = Box(2,)
[2m[36m(pid=2247)[0m - observation_space = Box(9,)
[2m[36m(pid=2247)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2247)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2247)[0m - _max_episode_steps = 150
[2m[36m(pid=2247)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2247)[0m [32m [     1.94734s,  INFO] TimeLimit:
[2m[36m(pid=2247)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2247)[0m - action_space = Box(2,)
[2m[36m(pid=2247)[0m - observation_space = Box(9,)
[2m[36m(pid=2247)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2247)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2247)[0m - _max_episode_steps = 150
[2m[36m(pid=2247)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2249)[0m W0717 14:12:01.995208 139883298620864 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=2249)[0m Instructions for updating:
[2m[36m(pid=2249)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=2249)[0m [32m [     2.15914s,  INFO] TimeLimit:
[2m[36m(pid=2249)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2249)[0m - action_space = Box(2,)
[2m[36m(pid=2249)[0m - observation_space = Box(9,)
[2m[36m(pid=2249)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2249)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2249)[0m - _max_episode_steps = 150
[2m[36m(pid=2249)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2249)[0m [32m [     2.16018s,  INFO] TimeLimit:
[2m[36m(pid=2249)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2249)[0m - action_space = Box(2,)
[2m[36m(pid=2249)[0m - observation_space = Box(9,)
[2m[36m(pid=2249)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2249)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2249)[0m - _max_episode_steps = 150
[2m[36m(pid=2249)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2249)[0m [32m [     2.16099s,  INFO] TimeLimit:
[2m[36m(pid=2249)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2249)[0m - action_space = Box(2,)
[2m[36m(pid=2249)[0m - observation_space = Box(9,)
[2m[36m(pid=2249)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2249)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2249)[0m - _max_episode_steps = 150
[2m[36m(pid=2249)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2250)[0m W0717 14:12:01.982031 139904108979648 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=2250)[0m Instructions for updating:
[2m[36m(pid=2250)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=2250)[0m [32m [     1.98231s,  INFO] TimeLimit:
[2m[36m(pid=2250)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2250)[0m - action_space = Box(2,)
[2m[36m(pid=2250)[0m - observation_space = Box(9,)
[2m[36m(pid=2250)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2250)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2250)[0m - _max_episode_steps = 150
[2m[36m(pid=2250)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2250)[0m [32m [     1.98300s,  INFO] TimeLimit:
[2m[36m(pid=2250)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2250)[0m - action_space = Box(2,)
[2m[36m(pid=2250)[0m - observation_space = Box(9,)
[2m[36m(pid=2250)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2250)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2250)[0m - _max_episode_steps = 150
[2m[36m(pid=2250)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2250)[0m [32m [     1.98358s,  INFO] TimeLimit:
[2m[36m(pid=2250)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2250)[0m - action_space = Box(2,)
[2m[36m(pid=2250)[0m - observation_space = Box(9,)
[2m[36m(pid=2250)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2250)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2250)[0m - _max_episode_steps = 150
[2m[36m(pid=2250)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2243)[0m [32m [     1.87521s,  INFO] TimeLimit:
[2m[36m(pid=2243)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2243)[0m - action_space = Box(2,)
[2m[36m(pid=2243)[0m - observation_space = Box(9,)
[2m[36m(pid=2243)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2243)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2243)[0m - _max_episode_steps = 150
[2m[36m(pid=2243)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2243)[0m [32m [     1.87604s,  INFO] TimeLimit:
[2m[36m(pid=2243)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2243)[0m - action_space = Box(2,)
[2m[36m(pid=2243)[0m - observation_space = Box(9,)
[2m[36m(pid=2243)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2243)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2243)[0m - _max_episode_steps = 150
[2m[36m(pid=2243)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2243)[0m [32m [     1.87675s,  INFO] TimeLimit:
[2m[36m(pid=2243)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=2243)[0m - action_space = Box(2,)
[2m[36m(pid=2243)[0m - observation_space = Box(9,)
[2m[36m(pid=2243)[0m - reward_range = (-inf, inf)
[2m[36m(pid=2243)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=2243)[0m - _max_episode_steps = 150
[2m[36m(pid=2243)[0m - _elapsed_steps = None [0m
[2m[36m(pid=2243)[0m W0717 14:12:02.137603 140151364367808 deprecation.py:323] From /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:542: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=2243)[0m Instructions for updating:
[2m[36m(pid=2243)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=32181)[0m 2019-07-17 14:12:02,421	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32181)[0m { 'data': { 'action_prob': np.ndarray((900,), dtype=float32, min=0.0, max=0.159, mean=0.08),
[2m[36m(pid=32181)[0m             'actions': np.ndarray((900, 2), dtype=float32, min=-2.985, max=3.715, mean=-0.005),
[2m[36m(pid=32181)[0m             'advantages': np.ndarray((900,), dtype=float32, min=-29.247, max=20.591, mean=-5.603),
[2m[36m(pid=32181)[0m             'agent_index': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32181)[0m             'behaviour_logits': np.ndarray((900, 4), dtype=float32, min=-0.015, max=0.02, mean=0.001),
[2m[36m(pid=32181)[0m             'dones': np.ndarray((900,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=32181)[0m             'eps_id': np.ndarray((900,), dtype=int64, min=507269185.0, max=1806999099.0, mean=1024076476.833),
[2m[36m(pid=32181)[0m             'infos': np.ndarray((900,), dtype=object, head={}),
[2m[36m(pid=32181)[0m             'new_obs': np.ndarray((900, 9), dtype=float32, min=-4.268, max=3.746, mean=0.023),
[2m[36m(pid=32181)[0m             'obs': np.ndarray((900, 9), dtype=float32, min=-4.268, max=3.746, mean=0.021),
[2m[36m(pid=32181)[0m             'prev_actions': np.ndarray((900, 2), dtype=float32, min=-2.985, max=3.715, mean=-0.005),
[2m[36m(pid=32181)[0m             'prev_rewards': np.ndarray((900,), dtype=float32, min=-3.286, max=3.466, mean=-0.074),
[2m[36m(pid=32181)[0m             'rewards': np.ndarray((900,), dtype=float32, min=-3.286, max=3.466, mean=-0.073),
[2m[36m(pid=32181)[0m             't': np.ndarray((900,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32181)[0m             'unroll_id': np.ndarray((900,), dtype=int64, min=0.0, max=1.0, mean=0.333),
[2m[36m(pid=32181)[0m             'value_targets': np.ndarray((900,), dtype=float32, min=-29.252, max=20.601, mean=-5.603),
[2m[36m(pid=32181)[0m             'vf_preds': np.ndarray((900,), dtype=float32, min=-0.011, max=0.009, mean=0.001)},
[2m[36m(pid=32181)[0m   'type': 'SampleBatch'}
[2m[36m(pid=32181)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:12:04,854	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m { 'inputs': [ np.ndarray((26100, 2), dtype=float32, min=-3.884, max=4.556, mean=-0.0),
[2m[36m(pid=32171)[0m               np.ndarray((26100,), dtype=float32, min=-28.408, max=25.882, mean=-0.098),
[2m[36m(pid=32171)[0m               np.ndarray((26100, 9), dtype=float32, min=-11.601, max=7.671, mean=-0.004),
[2m[36m(pid=32171)[0m               np.ndarray((26100, 2), dtype=float32, min=-3.884, max=4.556, mean=-0.001),
[2m[36m(pid=32171)[0m               np.ndarray((26100,), dtype=float32, min=-5.848, max=3.88, mean=0.0),
[2m[36m(pid=32171)[0m               np.ndarray((26100, 4), dtype=float32, min=-0.019, max=0.02, mean=0.0),
[2m[36m(pid=32171)[0m               np.ndarray((26100,), dtype=float32, min=-71.478, max=36.732, mean=-6.424),
[2m[36m(pid=32171)[0m               np.ndarray((26100,), dtype=float32, min=-0.011, max=0.012, mean=-0.0)],
[2m[36m(pid=32171)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=32171)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=32171)[0m   'state_inputs': []}
[2m[36m(pid=32171)[0m 
[2m[36m(pid=32171)[0m 2019-07-17 14:12:04,855	INFO multi_gpu_impl.py:191 -- Divided 26100 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.148709650685856
  episode_reward_mean: -14.697129491180641
  episode_reward_min: -93.04767627122605
  episodes_this_iter: 174
  episodes_total: 174
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 3155.949
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.8393969535827637
        kl: 0.0014633851824328303
        policy_loss: -0.004899776540696621
        total_loss: 111.47364044189453
        vf_explained_var: 0.11760923266410828
        vf_loss: 111.47708129882812
    load_time_ms: 42.7
    num_steps_sampled: 26100
    num_steps_trained: 26000
    sample_time_ms: 3458.272
    update_time_ms: 629.746
  iterations_since_restore: 1
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9398073679032103
    mean_inference_ms: 1.1002526314494077
    mean_processing_ms: 0.7691860267299279
  time_since_restore: 7.338279724121094
  time_this_iter_s: 7.338279724121094
  time_total_s: 7.338279724121094
  timestamp: 1563365528
  timesteps_since_restore: 26100
  timesteps_this_iter: 26100
  timesteps_total: 26100
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 7 s, 1 iter, 26100 ts, -14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 13.908293087054057
  episode_reward_mean: -14.253351130257464
  episode_reward_min: -48.17222623456476
  episodes_this_iter: 174
  episodes_total: 348
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2987.179
    learner:
      default_policy:
        cur_kl_coeff: 0.5
        cur_lr: 9.999999747378752e-05
        entropy: 2.8458032608032227
        kl: 0.0018103912007063627
        policy_loss: -0.003784694941714406
        total_loss: 68.13816833496094
        vf_explained_var: 0.3400036096572876
        vf_loss: 68.14103698730469
    load_time_ms: 21.73
    num_steps_sampled: 52200
    num_steps_trained: 52000
    sample_time_ms: 3083.481
    update_time_ms: 316.599
  iterations_since_restore: 2
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9304218858107811
    mean_inference_ms: 1.0650501751321664
    mean_processing_ms: 0.7748927771542942
  time_since_restore: 12.8842613697052
  time_this_iter_s: 5.5459816455841064
  time_total_s: 12.8842613697052
  timestamp: 1563365533
  timesteps_since_restore: 52200
  timesteps_this_iter: 26100
  timesteps_total: 52200
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 12 s, 2 iter, 52200 ts, -14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 14.540867039954781
  episode_reward_mean: -13.785814262328248
  episode_reward_min: -55.45024890685527
  episodes_this_iter: 174
  episodes_total: 522
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2949.88
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.8323466777801514
        kl: 0.004302468150854111
        policy_loss: -0.004632697440683842
        total_loss: 57.43545150756836
        vf_explained_var: 0.37292569875717163
        vf_loss: 57.43901062011719
    load_time_ms: 14.728
    num_steps_sampled: 78300
    num_steps_trained: 78000
    sample_time_ms: 2946.92
    update_time_ms: 212.047
  iterations_since_restore: 3
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9268910089954605
    mean_inference_ms: 1.0371083766488813
    mean_processing_ms: 0.776774934814817
  time_since_restore: 18.451321840286255
  time_this_iter_s: 5.567060470581055
  time_total_s: 18.451321840286255
  timestamp: 1563365539
  timesteps_since_restore: 78300
  timesteps_this_iter: 26100
  timesteps_total: 78300
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 18 s, 3 iter, 78300 ts, -13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.332472263715744
  episode_reward_mean: -13.01314068484373
  episode_reward_min: -50.54568813113139
  episodes_this_iter: 174
  episodes_total: 696
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2867.503
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8272974491119385
        kl: 0.007652771193534136
        policy_loss: -0.006380161270499229
        total_loss: 50.84646224975586
        vf_explained_var: 0.41019919514656067
        vf_loss: 50.85188674926758
    load_time_ms: 11.24
    num_steps_sampled: 104400
    num_steps_trained: 104000
    sample_time_ms: 2890.237
    update_time_ms: 159.936
  iterations_since_restore: 4
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9084737121911198
    mean_inference_ms: 1.0065225064283814
    mean_processing_ms: 0.7657694669398611
  time_since_restore: 23.809531927108765
  time_this_iter_s: 5.35821008682251
  time_total_s: 23.809531927108765
  timestamp: 1563365544
  timesteps_since_restore: 104400
  timesteps_this_iter: 26100
  timesteps_total: 104400
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 23 s, 4 iter, 104400 ts, -13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.097495927808993
  episode_reward_mean: -9.588257067735649
  episode_reward_min: -39.63882769323166
  episodes_this_iter: 174
  episodes_total: 870
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2854.144
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8229753971099854
        kl: 0.005310505628585815
        policy_loss: -0.005056632217019796
        total_loss: 41.68843078613281
        vf_explained_var: 0.4139834940433502
        vf_loss: 41.69282531738281
    load_time_ms: 9.142
    num_steps_sampled: 130500
    num_steps_trained: 130000
    sample_time_ms: 2858.954
    update_time_ms: 128.605
  iterations_since_restore: 5
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8979613128706875
    mean_inference_ms: 0.9903901539105253
    mean_processing_ms: 0.7583522920483076
  time_since_restore: 29.36181902885437
  time_this_iter_s: 5.5522871017456055
  time_total_s: 29.36181902885437
  timestamp: 1563365550
  timesteps_since_restore: 130500
  timesteps_this_iter: 26100
  timesteps_total: 130500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 29 s, 5 iter, 130500 ts, -9.59 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.494118775005578
  episode_reward_mean: -8.386512087963814
  episode_reward_min: -47.81330872157785
  episodes_this_iter: 174
  episodes_total: 1044
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2835.828
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.810835123062134
        kl: 0.007076544687151909
        policy_loss: -0.008168316446244717
        total_loss: 39.31626892089844
        vf_explained_var: 0.41589105129241943
        vf_loss: 39.323551177978516
    load_time_ms: 7.738
    num_steps_sampled: 156600
    num_steps_trained: 156000
    sample_time_ms: 2805.404
    update_time_ms: 107.809
  iterations_since_restore: 6
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8935892990281423
    mean_inference_ms: 0.9808042328214888
    mean_processing_ms: 0.7577570182234913
  time_since_restore: 34.662224769592285
  time_this_iter_s: 5.300405740737915
  time_total_s: 34.662224769592285
  timestamp: 1563365555
  timesteps_since_restore: 156600
  timesteps_this_iter: 26100
  timesteps_total: 156600
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 34 s, 6 iter, 156600 ts, -8.39 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.234596445468497
  episode_reward_mean: -6.485236649873584
  episode_reward_min: -36.704652595909806
  episodes_this_iter: 174
  episodes_total: 1218
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2779.081
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8012325763702393
        kl: 0.007101044990122318
        policy_loss: -0.0079262750223279
        total_loss: 34.95801544189453
        vf_explained_var: 0.3825313448905945
        vf_loss: 34.965057373046875
    load_time_ms: 6.733
    num_steps_sampled: 182700
    num_steps_trained: 182000
    sample_time_ms: 2795.693
    update_time_ms: 92.946
  iterations_since_restore: 7
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8870594171535061
    mean_inference_ms: 0.9717415317921669
    mean_processing_ms: 0.7540719894290404
  time_since_restore: 39.85727000236511
  time_this_iter_s: 5.195045232772827
  time_total_s: 39.85727000236511
  timestamp: 1563365560
  timesteps_since_restore: 182700
  timesteps_this_iter: 26100
  timesteps_total: 182700
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 39 s, 7 iter, 182700 ts, -6.49 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.884725922209135
  episode_reward_mean: -5.812715880623101
  episode_reward_min: -40.572545459132485
  episodes_this_iter: 174
  episodes_total: 1392
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2748.928
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.790107250213623
        kl: 0.007375613786280155
        policy_loss: -0.0076170433312654495
        total_loss: 29.070466995239258
        vf_explained_var: 0.4588485360145569
        vf_loss: 29.07716178894043
    load_time_ms: 5.996
    num_steps_sampled: 208800
    num_steps_trained: 208000
    sample_time_ms: 2766.575
    update_time_ms: 81.878
  iterations_since_restore: 8
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8860872265838464
    mean_inference_ms: 0.9677563636723534
    mean_processing_ms: 0.752980658132774
  time_since_restore: 44.97772550582886
  time_this_iter_s: 5.120455503463745
  time_total_s: 44.97772550582886
  timestamp: 1563365565
  timesteps_since_restore: 208800
  timesteps_this_iter: 26100
  timesteps_total: 208800
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 44 s, 8 iter, 208800 ts, -5.81 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.359717805624392
  episode_reward_mean: -3.9572672680301917
  episode_reward_min: -31.61779051607358
  episodes_this_iter: 174
  episodes_total: 1566
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2742.813
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.77996563911438
        kl: 0.007746451068669558
        policy_loss: -0.008566038683056831
        total_loss: 23.009225845336914
        vf_explained_var: 0.48238471150398254
        vf_loss: 23.01682472229004
    load_time_ms: 5.415
    num_steps_sampled: 234900
    num_steps_trained: 234000
    sample_time_ms: 2776.782
    update_time_ms: 73.189
  iterations_since_restore: 9
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8862858713733621
    mean_inference_ms: 0.9685184046365878
    mean_processing_ms: 0.7546020650456171
  time_since_restore: 50.54964256286621
  time_this_iter_s: 5.5719170570373535
  time_total_s: 50.54964256286621
  timestamp: 1563365571
  timesteps_since_restore: 234900
  timesteps_this_iter: 26100
  timesteps_total: 234900
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 50 s, 9 iter, 234900 ts, -3.96 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-12-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.656581216071363
  episode_reward_mean: -2.0353467786937105
  episode_reward_min: -34.54433295131369
  episodes_this_iter: 174
  episodes_total: 1740
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2718.805
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7734646797180176
        kl: 0.007791581097990274
        policy_loss: -0.009033768437802792
        total_loss: 20.568315505981445
        vf_explained_var: 0.5679324865341187
        vf_loss: 20.57637596130371
    load_time_ms: 4.952
    num_steps_sampled: 261000
    num_steps_trained: 260000
    sample_time_ms: 2760.329
    update_time_ms: 66.298
  iterations_since_restore: 10
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8875694705944339
    mean_inference_ms: 0.9704927833146914
    mean_processing_ms: 0.7556430693639671
  time_since_restore: 55.683878898620605
  time_this_iter_s: 5.1342363357543945
  time_total_s: 55.683878898620605
  timestamp: 1563365576
  timesteps_since_restore: 261000
  timesteps_this_iter: 26100
  timesteps_total: 261000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 55 s, 10 iter, 261000 ts, -2.04 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.346372296702
  episode_reward_mean: -0.3357099494888096
  episode_reward_min: -30.826194903460227
  episodes_this_iter: 174
  episodes_total: 1914
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2652.75
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7578110694885254
        kl: 0.008877944201231003
        policy_loss: -0.010859785601496696
        total_loss: 18.29648780822754
        vf_explained_var: 0.598389744758606
        vf_loss: 18.306236267089844
    load_time_ms: 0.772
    num_steps_sampled: 287100
    num_steps_trained: 286000
    sample_time_ms: 2688.32
    update_time_ms: 3.676
  iterations_since_restore: 11
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8889329170979684
    mean_inference_ms: 0.9737842296023113
    mean_processing_ms: 0.7570902222867696
  time_since_restore: 60.937228202819824
  time_this_iter_s: 5.253349304199219
  time_total_s: 60.937228202819824
  timestamp: 1563365581
  timesteps_since_restore: 287100
  timesteps_this_iter: 26100
  timesteps_total: 287100
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 60 s, 11 iter, 287100 ts, -0.336 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.782959187199356
  episode_reward_mean: 0.0014546188428352824
  episode_reward_min: -28.39476952906529
  episodes_this_iter: 174
  episodes_total: 2088
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2643.583
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.739231586456299
        kl: 0.007776208687573671
        policy_loss: -0.009136369451880455
        total_loss: 16.315370559692383
        vf_explained_var: 0.6333242654800415
        vf_loss: 16.32353401184082
    load_time_ms: 0.776
    num_steps_sampled: 313200
    num_steps_trained: 312000
    sample_time_ms: 2677.26
    update_time_ms: 3.683
  iterations_since_restore: 12
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8881167927093585
    mean_inference_ms: 0.9709612211622572
    mean_processing_ms: 0.7578850871677705
  time_since_restore: 66.28206253051758
  time_this_iter_s: 5.344834327697754
  time_total_s: 66.28206253051758
  timestamp: 1563365587
  timesteps_since_restore: 313200
  timesteps_this_iter: 26100
  timesteps_total: 313200
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 66 s, 12 iter, 313200 ts, 0.00145 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.539318523865436
  episode_reward_mean: 2.3502478582488093
  episode_reward_min: -48.00028396096285
  episodes_this_iter: 174
  episodes_total: 2262
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2643.339
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7271456718444824
        kl: 0.008180168457329273
        policy_loss: -0.00996799673885107
        total_loss: 16.258394241333008
        vf_explained_var: 0.6461268067359924
        vf_loss: 16.267337799072266
    load_time_ms: 0.775
    num_steps_sampled: 339300
    num_steps_trained: 338000
    sample_time_ms: 2695.606
    update_time_ms: 3.749
  iterations_since_restore: 13
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8884688673943861
    mean_inference_ms: 0.9710317633611423
    mean_processing_ms: 0.7582173382989789
  time_since_restore: 72.03557205200195
  time_this_iter_s: 5.753509521484375
  time_total_s: 72.03557205200195
  timestamp: 1563365592
  timesteps_since_restore: 339300
  timesteps_this_iter: 26100
  timesteps_total: 339300
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 72 s, 13 iter, 339300 ts, 2.35 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.02966157808515
  episode_reward_mean: 4.169722307316583
  episode_reward_min: -24.964004541213246
  episodes_this_iter: 174
  episodes_total: 2436
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2643.302
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7167587280273438
        kl: 0.008933999575674534
        policy_loss: -0.010174809955060482
        total_loss: 14.605846405029297
        vf_explained_var: 0.7050503492355347
        vf_loss: 14.614903450012207
    load_time_ms: 0.773
    num_steps_sampled: 365400
    num_steps_trained: 364000
    sample_time_ms: 2682.967
    update_time_ms: 3.836
  iterations_since_restore: 14
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.889337737159518
    mean_inference_ms: 0.9694101154594263
    mean_processing_ms: 0.7591418304762078
  time_since_restore: 77.26973152160645
  time_this_iter_s: 5.234159469604492
  time_total_s: 77.26973152160645
  timestamp: 1563365598
  timesteps_since_restore: 365400
  timesteps_this_iter: 26100
  timesteps_total: 365400
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 77 s, 14 iter, 365400 ts, 4.17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.534302783576294
  episode_reward_mean: 4.053490364342774
  episode_reward_min: -22.680652899513092
  episodes_this_iter: 174
  episodes_total: 2610
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2636.49
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7026546001434326
        kl: 0.008174209855496883
        policy_loss: -0.010410904884338379
        total_loss: 12.391862869262695
        vf_explained_var: 0.7398604154586792
        vf_loss: 12.401253700256348
    load_time_ms: 0.77
    num_steps_sampled: 391500
    num_steps_trained: 390000
    sample_time_ms: 2686.63
    update_time_ms: 3.941
  iterations_since_restore: 15
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8895545294865539
    mean_inference_ms: 0.9681111385669628
    mean_processing_ms: 0.7599221651511524
  time_since_restore: 82.79312515258789
  time_this_iter_s: 5.523393630981445
  time_total_s: 82.79312515258789
  timestamp: 1563365603
  timesteps_since_restore: 391500
  timesteps_this_iter: 26100
  timesteps_total: 391500
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 82 s, 15 iter, 391500 ts, 4.05 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.579823349887732
  episode_reward_mean: 4.660630143526164
  episode_reward_min: -25.625641603142856
  episodes_this_iter: 174
  episodes_total: 2784
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2653.504
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.683753490447998
        kl: 0.008953881449997425
        policy_loss: -0.01054510846734047
        total_loss: 10.279753684997559
        vf_explained_var: 0.7785302400588989
        vf_loss: 10.289179801940918
    load_time_ms: 0.784
    num_steps_sampled: 417600
    num_steps_trained: 416000
    sample_time_ms: 2689.725
    update_time_ms: 3.963
  iterations_since_restore: 16
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8892581768863043
    mean_inference_ms: 0.9658119841640398
    mean_processing_ms: 0.7602683165895473
  time_since_restore: 88.29748749732971
  time_this_iter_s: 5.504362344741821
  time_total_s: 88.29748749732971
  timestamp: 1563365609
  timesteps_since_restore: 417600
  timesteps_this_iter: 26100
  timesteps_total: 417600
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 88 s, 16 iter, 417600 ts, 4.66 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.134853688520014
  episode_reward_mean: 5.5060682093677125
  episode_reward_min: -18.61971395636451
  episodes_this_iter: 174
  episodes_total: 2958
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2717.369
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6692137718200684
        kl: 0.008606364950537682
        policy_loss: -0.010943249799311161
        total_loss: 10.091182708740234
        vf_explained_var: 0.7856900095939636
        vf_loss: 10.101049423217773
    load_time_ms: 0.788
    num_steps_sampled: 443700
    num_steps_trained: 442000
    sample_time_ms: 2709.229
    update_time_ms: 4.17
  iterations_since_restore: 17
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8910930041184786
    mean_inference_ms: 0.9682205908050592
    mean_processing_ms: 0.7622361027271791
  time_since_restore: 94.33239340782166
  time_this_iter_s: 6.034905910491943
  time_total_s: 94.33239340782166
  timestamp: 1563365615
  timesteps_since_restore: 443700
  timesteps_this_iter: 26100
  timesteps_total: 443700
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 94 s, 17 iter, 443700 ts, 5.51 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.383965569211586
  episode_reward_mean: 7.104261421696223
  episode_reward_min: -19.116185440487413
  episodes_this_iter: 174
  episodes_total: 3132
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2755.953
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.655822515487671
        kl: 0.00911397859454155
        policy_loss: -0.011371603235602379
        total_loss: 7.728070259094238
        vf_explained_var: 0.8297289609909058
        vf_loss: 7.738302230834961
    load_time_ms: 0.778
    num_steps_sampled: 469800
    num_steps_trained: 468000
    sample_time_ms: 2762.378
    update_time_ms: 4.175
  iterations_since_restore: 18
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8948200860340678
    mean_inference_ms: 0.9722331717370659
    mean_processing_ms: 0.7648052566653981
  time_since_restore: 100.37093830108643
  time_this_iter_s: 6.0385448932647705
  time_total_s: 100.37093830108643
  timestamp: 1563365621
  timesteps_since_restore: 469800
  timesteps_this_iter: 26100
  timesteps_total: 469800
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 100 s, 18 iter, 469800 ts, 7.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.770832562155622
  episode_reward_mean: 7.387748559819384
  episode_reward_min: -14.812870402089215
  episodes_this_iter: 174
  episodes_total: 3306
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2760.467
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.646462917327881
        kl: 0.008362940512597561
        policy_loss: -0.011463496834039688
        total_loss: 6.432583808898926
        vf_explained_var: 0.8388038873672485
        vf_loss: 6.443001747131348
    load_time_ms: 0.773
    num_steps_sampled: 495900
    num_steps_trained: 494000
    sample_time_ms: 2749.741
    update_time_ms: 4.222
  iterations_since_restore: 19
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.896992797742033
    mean_inference_ms: 0.9750382088154239
    mean_processing_ms: 0.7667396864794932
  time_since_restore: 105.86667704582214
  time_this_iter_s: 5.495738744735718
  time_total_s: 105.86667704582214
  timestamp: 1563365626
  timesteps_since_restore: 495900
  timesteps_this_iter: 26100
  timesteps_total: 495900
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 105 s, 19 iter, 495900 ts, 7.39 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.047406564241463
  episode_reward_mean: 8.640618969519386
  episode_reward_min: -12.534409096906986
  episodes_this_iter: 174
  episodes_total: 3480
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2794.01
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6352920532226562
        kl: 0.009757042862474918
        policy_loss: -0.010672896169126034
        total_loss: 4.596615791320801
        vf_explained_var: 0.8894307613372803
        vf_loss: 4.606069087982178
    load_time_ms: 0.769
    num_steps_sampled: 522000
    num_steps_trained: 520000
    sample_time_ms: 2786.151
    update_time_ms: 4.304
  iterations_since_restore: 20
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8999364443940474
    mean_inference_ms: 0.9786070053825389
    mean_processing_ms: 0.7686876200033832
  time_since_restore: 111.70086979866028
  time_this_iter_s: 5.834192752838135
  time_total_s: 111.70086979866028
  timestamp: 1563365632
  timesteps_since_restore: 522000
  timesteps_this_iter: 26100
  timesteps_total: 522000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 111 s, 20 iter, 522000 ts, 8.64 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-13-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.80014296569633
  episode_reward_mean: 8.187585060405537
  episode_reward_min: -12.071458994779485
  episodes_this_iter: 174
  episodes_total: 3654
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2823.397
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6252005100250244
        kl: 0.01029707957059145
        policy_loss: -0.01081397756934166
        total_loss: 4.2474589347839355
        vf_explained_var: 0.87940913438797
        vf_loss: 4.256985664367676
    load_time_ms: 0.764
    num_steps_sampled: 548100
    num_steps_trained: 546000
    sample_time_ms: 2787.181
    update_time_ms: 4.346
  iterations_since_restore: 21
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8978789971001524
    mean_inference_ms: 0.9753193099719312
    mean_processing_ms: 0.7673758671645418
  time_since_restore: 117.25953793525696
  time_this_iter_s: 5.55866813659668
  time_total_s: 117.25953793525696
  timestamp: 1563365638
  timesteps_since_restore: 548100
  timesteps_this_iter: 26100
  timesteps_total: 548100
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 117 s, 21 iter, 548100 ts, 8.19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.318163359997754
  episode_reward_mean: 9.145074283475378
  episode_reward_min: -12.421685073352752
  episodes_this_iter: 174
  episodes_total: 3828
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2807.4
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6148273944854736
        kl: 0.010332420468330383
        policy_loss: -0.011977351270616055
        total_loss: 3.5234696865081787
        vf_explained_var: 0.9085098505020142
        vf_loss: 3.5341551303863525
    load_time_ms: 0.767
    num_steps_sampled: 574200
    num_steps_trained: 572000
    sample_time_ms: 2811.77
    update_time_ms: 4.44
  iterations_since_restore: 22
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9014622834431916
    mean_inference_ms: 0.980538048058338
    mean_processing_ms: 0.7708596426573445
  time_since_restore: 122.69093370437622
  time_this_iter_s: 5.431395769119263
  time_total_s: 122.69093370437622
  timestamp: 1563365643
  timesteps_since_restore: 574200
  timesteps_this_iter: 26100
  timesteps_total: 574200
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 122 s, 22 iter, 574200 ts, 9.15 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.90667053959093
  episode_reward_mean: 9.384003005438744
  episode_reward_min: -11.4245719227289
  episodes_this_iter: 174
  episodes_total: 4002
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2800.237
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.608987331390381
        kl: 0.008381599560379982
        policy_loss: -0.010374536737799644
        total_loss: 3.250772714614868
        vf_explained_var: 0.9141650795936584
        vf_loss: 3.260099172592163
    load_time_ms: 0.774
    num_steps_sampled: 600300
    num_steps_trained: 598000
    sample_time_ms: 2805.677
    update_time_ms: 4.46
  iterations_since_restore: 23
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9007119899154378
    mean_inference_ms: 0.9784752338450949
    mean_processing_ms: 0.7701977894597098
  time_since_restore: 128.30757403373718
  time_this_iter_s: 5.616640329360962
  time_total_s: 128.30757403373718
  timestamp: 1563365649
  timesteps_since_restore: 600300
  timesteps_this_iter: 26100
  timesteps_total: 600300
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 128 s, 23 iter, 600300 ts, 9.38 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.620304882047048
  episode_reward_mean: 8.85044391664486
  episode_reward_min: -14.032883384671266
  episodes_this_iter: 174
  episodes_total: 4176
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2804.04
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5991857051849365
        kl: 0.007994351908564568
        policy_loss: -0.010733772069215775
        total_loss: 2.4169487953186035
        vf_explained_var: 0.9344663619995117
        vf_loss: 2.426683187484741
    load_time_ms: 0.775
    num_steps_sampled: 626400
    num_steps_trained: 624000
    sample_time_ms: 2826.918
    update_time_ms: 4.385
  iterations_since_restore: 24
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.8999924766104384
    mean_inference_ms: 0.9773828211430148
    mean_processing_ms: 0.7698262539625208
  time_since_restore: 133.79407262802124
  time_this_iter_s: 5.486498594284058
  time_total_s: 133.79407262802124
  timestamp: 1563365654
  timesteps_since_restore: 626400
  timesteps_this_iter: 26100
  timesteps_total: 626400
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 133 s, 24 iter, 626400 ts, 8.85 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.85440995123024
  episode_reward_mean: 11.129539037750252
  episode_reward_min: -9.311703981341962
  episodes_this_iter: 174
  episodes_total: 4350
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2820.305
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.589076042175293
        kl: 0.00786326453089714
        policy_loss: -0.010293484665453434
        total_loss: 2.2442448139190674
        vf_explained_var: 0.9367945194244385
        vf_loss: 2.2535555362701416
    load_time_ms: 0.767
    num_steps_sampled: 652500
    num_steps_trained: 650000
    sample_time_ms: 2840.372
    update_time_ms: 4.664
  iterations_since_restore: 25
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9003392288540736
    mean_inference_ms: 0.9765729184652792
    mean_processing_ms: 0.7700791830081798
  time_since_restore: 139.61893558502197
  time_this_iter_s: 5.824862957000732
  time_total_s: 139.61893558502197
  timestamp: 1563365660
  timesteps_since_restore: 652500
  timesteps_this_iter: 26100
  timesteps_total: 652500
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 139 s, 25 iter, 652500 ts, 11.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.5637488260436
  episode_reward_mean: 10.550718638021817
  episode_reward_min: -10.740097968338079
  episodes_this_iter: 174
  episodes_total: 4524
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2807.831
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5778770446777344
        kl: 0.009722105227410793
        policy_loss: -0.011607687920331955
        total_loss: 2.1413302421569824
        vf_explained_var: 0.94338458776474
        vf_loss: 2.1517226696014404
    load_time_ms: 0.752
    num_steps_sampled: 678600
    num_steps_trained: 676000
    sample_time_ms: 2854.111
    update_time_ms: 4.777
  iterations_since_restore: 26
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9020096355424466
    mean_inference_ms: 0.9782054162597782
    mean_processing_ms: 0.7714956867542657
  time_since_restore: 145.1341769695282
  time_this_iter_s: 5.515241384506226
  time_total_s: 145.1341769695282
  timestamp: 1563365666
  timesteps_since_restore: 678600
  timesteps_this_iter: 26100
  timesteps_total: 678600
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 145 s, 26 iter, 678600 ts, 10.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.263746706558795
  episode_reward_mean: 11.924948204079467
  episode_reward_min: -10.307364402082559
  episodes_this_iter: 174
  episodes_total: 4698
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2771.424
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5681381225585938
        kl: 0.007372508756816387
        policy_loss: -0.010158734396100044
        total_loss: 1.8852986097335815
        vf_explained_var: 0.949796736240387
        vf_loss: 1.894535779953003
    load_time_ms: 0.781
    num_steps_sampled: 704700
    num_steps_trained: 702000
    sample_time_ms: 2867.733
    update_time_ms: 4.609
  iterations_since_restore: 27
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9029241886118546
    mean_inference_ms: 0.9787246448755764
    mean_processing_ms: 0.7719139728261181
  time_since_restore: 150.93857955932617
  time_this_iter_s: 5.804402589797974
  time_total_s: 150.93857955932617
  timestamp: 1563365672
  timesteps_since_restore: 704700
  timesteps_this_iter: 26100
  timesteps_total: 704700
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 150 s, 27 iter, 704700 ts, 11.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.203098627581703
  episode_reward_mean: 11.708954127710587
  episode_reward_min: -9.324166372569339
  episodes_this_iter: 174
  episodes_total: 4872
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2745.218
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.556304693222046
        kl: 0.009781597182154655
        policy_loss: -0.011895888485014439
        total_loss: 1.8158934116363525
        vf_explained_var: 0.9535951018333435
        vf_loss: 1.8265666961669922
    load_time_ms: 0.794
    num_steps_sampled: 730800
    num_steps_trained: 728000
    sample_time_ms: 2827.804
    update_time_ms: 4.565
  iterations_since_restore: 28
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9033083931019522
    mean_inference_ms: 0.979560065126978
    mean_processing_ms: 0.7721399633133764
  time_since_restore: 156.31527638435364
  time_this_iter_s: 5.376696825027466
  time_total_s: 156.31527638435364
  timestamp: 1563365677
  timesteps_since_restore: 730800
  timesteps_this_iter: 26100
  timesteps_total: 730800
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 156 s, 28 iter, 730800 ts, 11.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.162029319105415
  episode_reward_mean: 11.295842917505974
  episode_reward_min: -9.283778289024628
  episodes_this_iter: 174
  episodes_total: 5046
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2733.548
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.540971040725708
        kl: 0.009302768856287003
        policy_loss: -0.010625637136399746
        total_loss: 1.2308862209320068
        vf_explained_var: 0.9616016149520874
        vf_loss: 1.2403490543365479
    load_time_ms: 0.793
    num_steps_sampled: 756900
    num_steps_trained: 754000
    sample_time_ms: 2824.877
    update_time_ms: 4.546
  iterations_since_restore: 29
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9049291328914787
    mean_inference_ms: 0.9798243344868263
    mean_processing_ms: 0.773266451596084
  time_since_restore: 161.66140723228455
  time_this_iter_s: 5.346130847930908
  time_total_s: 161.66140723228455
  timestamp: 1563365682
  timesteps_since_restore: 756900
  timesteps_this_iter: 26100
  timesteps_total: 756900
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 161 s, 29 iter, 756900 ts, 11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.01260764167742
  episode_reward_mean: 11.517135755583938
  episode_reward_min: -16.499165871925424
  episodes_this_iter: 174
  episodes_total: 5220
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2737.097
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5413894653320312
        kl: 0.008922304958105087
        policy_loss: -0.010049819014966488
        total_loss: 3.5522255897521973
        vf_explained_var: 0.9120476245880127
        vf_loss: 3.5611605644226074
    load_time_ms: 0.786
    num_steps_sampled: 783000
    num_steps_trained: 780000
    sample_time_ms: 2793.851
    update_time_ms: 4.454
  iterations_since_restore: 30
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9047509870893718
    mean_inference_ms: 0.9795971181747822
    mean_processing_ms: 0.7736442725834272
  time_since_restore: 167.22208666801453
  time_this_iter_s: 5.5606794357299805
  time_total_s: 167.22208666801453
  timestamp: 1563365688
  timesteps_since_restore: 783000
  timesteps_this_iter: 26100
  timesteps_total: 783000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 167 s, 30 iter, 783000 ts, 11.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.947881426352556
  episode_reward_mean: 11.553157865433825
  episode_reward_min: -10.547066628125043
  episodes_this_iter: 174
  episodes_total: 5394
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2755.625
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5329537391662598
        kl: 0.00979998055845499
        policy_loss: -0.011541896499693394
        total_loss: 1.357527256011963
        vf_explained_var: 0.9610977172851562
        vf_loss: 1.3678442239761353
    load_time_ms: 0.769
    num_steps_sampled: 809100
    num_steps_trained: 806000
    sample_time_ms: 2790.279
    update_time_ms: 4.493
  iterations_since_restore: 31
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9056281424032271
    mean_inference_ms: 0.9798858724337268
    mean_processing_ms: 0.7741767405733787
  time_since_restore: 172.9292290210724
  time_this_iter_s: 5.707142353057861
  time_total_s: 172.9292290210724
  timestamp: 1563365694
  timesteps_since_restore: 809100
  timesteps_this_iter: 26100
  timesteps_total: 809100
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 172 s, 31 iter, 809100 ts, 11.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-14-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.835328110312535
  episode_reward_mean: 11.318604089141404
  episode_reward_min: -8.215161590410347
  episodes_this_iter: 174
  episodes_total: 5568
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2778.022
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.519874095916748
        kl: 0.010080158710479736
        policy_loss: -0.01190502755343914
        total_loss: 1.1856472492218018
        vf_explained_var: 0.9622322916984558
        vf_loss: 1.1962924003601074
    load_time_ms: 0.756
    num_steps_sampled: 835200
    num_steps_trained: 832000
    sample_time_ms: 2792.34
    update_time_ms: 4.469
  iterations_since_restore: 32
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9065174744731209
    mean_inference_ms: 0.9804834317373694
    mean_processing_ms: 0.7745676431658808
  time_since_restore: 178.60439443588257
  time_this_iter_s: 5.675165414810181
  time_total_s: 178.60439443588257
  timestamp: 1563365699
  timesteps_since_restore: 835200
  timesteps_this_iter: 26100
  timesteps_total: 835200
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 178 s, 32 iter, 835200 ts, 11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.891456143324504
  episode_reward_mean: 11.923985815290978
  episode_reward_min: -15.304542270541189
  episodes_this_iter: 174
  episodes_total: 5742
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2763.032
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.501959800720215
        kl: 0.010705526918172836
        policy_loss: -0.010780738666653633
        total_loss: 1.0956103801727295
        vf_explained_var: 0.9685724973678589
        vf_loss: 1.1050528287887573
    load_time_ms: 0.752
    num_steps_sampled: 861300
    num_steps_trained: 858000
    sample_time_ms: 2781.905
    update_time_ms: 4.554
  iterations_since_restore: 33
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9069902950546985
    mean_inference_ms: 0.9805744545398292
    mean_processing_ms: 0.7746609720799438
  time_since_restore: 183.9672553539276
  time_this_iter_s: 5.362860918045044
  time_total_s: 183.9672553539276
  timestamp: 1563365705
  timesteps_since_restore: 861300
  timesteps_this_iter: 26100
  timesteps_total: 861300
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 183 s, 33 iter, 861300 ts, 11.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.976838279485058
  episode_reward_mean: 11.384453610976985
  episode_reward_min: -8.590218234704286
  episodes_this_iter: 174
  episodes_total: 5916
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2752.26
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.485517740249634
        kl: 0.009102968499064445
        policy_loss: -0.013689005747437477
        total_loss: 0.9249608516693115
        vf_explained_var: 0.9719701409339905
        vf_loss: 0.9375118613243103
    load_time_ms: 0.747
    num_steps_sampled: 887400
    num_steps_trained: 884000
    sample_time_ms: 2788.945
    update_time_ms: 4.551
  iterations_since_restore: 34
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9064530736231882
    mean_inference_ms: 0.9802460572292747
    mean_processing_ms: 0.7748727625850597
  time_since_restore: 189.41298961639404
  time_this_iter_s: 5.445734262466431
  time_total_s: 189.41298961639404
  timestamp: 1563365710
  timesteps_since_restore: 887400
  timesteps_this_iter: 26100
  timesteps_total: 887400
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 189 s, 34 iter, 887400 ts, 11.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.08299082185726
  episode_reward_mean: 12.040753001581697
  episode_reward_min: -10.38455939108061
  episodes_this_iter: 174
  episodes_total: 6090
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2743.762
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.476936101913452
        kl: 0.009931200183928013
        policy_loss: -0.012163536623120308
        total_loss: 0.9167446494102478
        vf_explained_var: 0.9738226532936096
        vf_loss: 0.9276667833328247
    load_time_ms: 0.765
    num_steps_sampled: 913500
    num_steps_trained: 910000
    sample_time_ms: 2786.83
    update_time_ms: 4.217
  iterations_since_restore: 35
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9065151486236592
    mean_inference_ms: 0.979441514228288
    mean_processing_ms: 0.7745943541454047
  time_since_restore: 195.12687754631042
  time_this_iter_s: 5.713887929916382
  time_total_s: 195.12687754631042
  timestamp: 1563365716
  timesteps_since_restore: 913500
  timesteps_this_iter: 26100
  timesteps_total: 913500
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 195 s, 35 iter, 913500 ts, 12 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.79827078836012
  episode_reward_mean: 12.35262661809829
  episode_reward_min: -7.388353330440861
  episodes_this_iter: 174
  episodes_total: 6264
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2733.324
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4594790935516357
        kl: 0.009707819670438766
        policy_loss: -0.012343080714344978
        total_loss: 0.85878586769104
        vf_explained_var: 0.9754586815834045
        vf_loss: 0.8699154257774353
    load_time_ms: 0.766
    num_steps_sampled: 939600
    num_steps_trained: 936000
    sample_time_ms: 2786.649
    update_time_ms: 4.087
  iterations_since_restore: 36
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9071438415919103
    mean_inference_ms: 0.9798631036936198
    mean_processing_ms: 0.7749548798796219
  time_since_restore: 200.53522324562073
  time_this_iter_s: 5.408345699310303
  time_total_s: 200.53522324562073
  timestamp: 1563365721
  timesteps_since_restore: 939600
  timesteps_this_iter: 26100
  timesteps_total: 939600
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 200 s, 36 iter, 939600 ts, 12.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.942319363506368
  episode_reward_mean: 12.584301354119413
  episode_reward_min: -6.996212547064439
  episodes_this_iter: 174
  episodes_total: 6438
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2722.513
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4438860416412354
        kl: 0.009297002106904984
        policy_loss: -0.012502104043960571
        total_loss: 0.7748693823814392
        vf_explained_var: 0.9772434830665588
        vf_loss: 0.7862094044685364
    load_time_ms: 0.734
    num_steps_sampled: 965700
    num_steps_trained: 962000
    sample_time_ms: 2768.858
    update_time_ms: 4.044
  iterations_since_restore: 37
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9073432257124058
    mean_inference_ms: 0.9797502142545877
    mean_processing_ms: 0.7753169572017727
  time_since_restore: 206.04855370521545
  time_this_iter_s: 5.513330459594727
  time_total_s: 206.04855370521545
  timestamp: 1563365727
  timesteps_since_restore: 965700
  timesteps_this_iter: 26100
  timesteps_total: 965700
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 206 s, 37 iter, 965700 ts, 12.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.830056858283264
  episode_reward_mean: 14.63974227985834
  episode_reward_min: -8.896929943087658
  episodes_this_iter: 174
  episodes_total: 6612
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2715.843
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.433318614959717
        kl: 0.010089186951518059
        policy_loss: -0.01237463392317295
        total_loss: 0.718347430229187
        vf_explained_var: 0.9802223443984985
        vf_loss: 0.7294607162475586
    load_time_ms: 0.73
    num_steps_sampled: 991800
    num_steps_trained: 988000
    sample_time_ms: 2790.754
    update_time_ms: 4.189
  iterations_since_restore: 38
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9095488784742487
    mean_inference_ms: 0.9837996279749472
    mean_processing_ms: 0.777770417914996
  time_since_restore: 211.57798290252686
  time_this_iter_s: 5.529429197311401
  time_total_s: 211.57798290252686
  timestamp: 1563365732
  timesteps_since_restore: 991800
  timesteps_this_iter: 26100
  timesteps_total: 991800
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 211 s, 38 iter, 991800 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.4779566837352
  episode_reward_mean: 13.447771939615595
  episode_reward_min: -7.844033001211211
  episodes_this_iter: 174
  episodes_total: 6786
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2737.968
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4193806648254395
        kl: 0.009789516218006611
        policy_loss: -0.01387722697108984
        total_loss: 0.7605339288711548
        vf_explained_var: 0.9796583652496338
        vf_loss: 0.773187518119812
    load_time_ms: 0.734
    num_steps_sampled: 1017900
    num_steps_trained: 1014000
    sample_time_ms: 2807.239
    update_time_ms: 4.196
  iterations_since_restore: 39
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9101174869910676
    mean_inference_ms: 0.9838587871279274
    mean_processing_ms: 0.7776233860445754
  time_since_restore: 217.30788350105286
  time_this_iter_s: 5.729900598526001
  time_total_s: 217.30788350105286
  timestamp: 1563365738
  timesteps_since_restore: 1017900
  timesteps_this_iter: 26100
  timesteps_total: 1017900
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 217 s, 39 iter, 1017900 ts, 13.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.2824664573576
  episode_reward_mean: 12.315620497639475
  episode_reward_min: -6.145046959644861
  episodes_this_iter: 174
  episodes_total: 6960
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2758.048
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.40388822555542
        kl: 0.009923956356942654
        policy_loss: -0.012412026524543762
        total_loss: 0.7422707080841064
        vf_explained_var: 0.9768009781837463
        vf_loss: 0.7534422278404236
    load_time_ms: 0.736
    num_steps_sampled: 1044000
    num_steps_trained: 1040000
    sample_time_ms: 2806.364
    update_time_ms: 4.162
  iterations_since_restore: 40
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.910021651809946
    mean_inference_ms: 0.9836192649787348
    mean_processing_ms: 0.7778326484798356
  time_since_restore: 223.05949187278748
  time_this_iter_s: 5.751608371734619
  time_total_s: 223.05949187278748
  timestamp: 1563365744
  timesteps_since_restore: 1044000
  timesteps_this_iter: 26100
  timesteps_total: 1044000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 223 s, 40 iter, 1044000 ts, 12.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.425922299808754
  episode_reward_mean: 12.191224899578906
  episode_reward_min: -9.034286928342373
  episodes_this_iter: 174
  episodes_total: 7134
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2720.049
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.393728494644165
        kl: 0.011147702112793922
        policy_loss: -0.014215628616511822
        total_loss: 0.7413813471794128
        vf_explained_var: 0.9770547151565552
        vf_loss: 0.7542034983634949
    load_time_ms: 0.757
    num_steps_sampled: 1070100
    num_steps_trained: 1066000
    sample_time_ms: 2791.396
    update_time_ms: 4.165
  iterations_since_restore: 41
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9087042462826265
    mean_inference_ms: 0.9818395798352614
    mean_processing_ms: 0.7766065012665182
  time_since_restore: 228.2358160018921
  time_this_iter_s: 5.176324129104614
  time_total_s: 228.2358160018921
  timestamp: 1563365749
  timesteps_since_restore: 1070100
  timesteps_this_iter: 26100
  timesteps_total: 1070100
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 228 s, 41 iter, 1070100 ts, 12.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-15-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.05442536645853
  episode_reward_mean: 14.684341235801295
  episode_reward_min: -7.776486984111589
  episodes_this_iter: 174
  episodes_total: 7308
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2694.506
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3776872158050537
        kl: 0.009670157916843891
        policy_loss: -0.014029926620423794
        total_loss: 0.6031888723373413
        vf_explained_var: 0.9835906624794006
        vf_loss: 0.6160100698471069
    load_time_ms: 0.759
    num_steps_sampled: 1096200
    num_steps_trained: 1092000
    sample_time_ms: 2758.59
    update_time_ms: 4.182
  iterations_since_restore: 42
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9083607348804204
    mean_inference_ms: 0.9813086933983173
    mean_processing_ms: 0.7760447343687878
  time_since_restore: 233.326678276062
  time_this_iter_s: 5.090862274169922
  time_total_s: 233.326678276062
  timestamp: 1563365754
  timesteps_since_restore: 1096200
  timesteps_this_iter: 26100
  timesteps_total: 1096200
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 233 s, 42 iter, 1096200 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.357448999214505
  episode_reward_mean: 12.834492244578907
  episode_reward_min: -9.080420465636514
  episodes_this_iter: 174
  episodes_total: 7482
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2719.632
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.365097761154175
        kl: 0.009050888940691948
        policy_loss: -0.012431863695383072
        total_loss: 0.6777623295783997
        vf_explained_var: 0.9816114902496338
        vf_loss: 0.689062774181366
    load_time_ms: 0.758
    num_steps_sampled: 1122300
    num_steps_trained: 1118000
    sample_time_ms: 2759.782
    update_time_ms: 4.066
  iterations_since_restore: 43
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9067605696132706
    mean_inference_ms: 0.9794465540356526
    mean_processing_ms: 0.7745376526575487
  time_since_restore: 238.9547734260559
  time_this_iter_s: 5.6280951499938965
  time_total_s: 238.9547734260559
  timestamp: 1563365760
  timesteps_since_restore: 1122300
  timesteps_this_iter: 26100
  timesteps_total: 1122300
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 238 s, 43 iter, 1122300 ts, 12.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.016188813902296
  episode_reward_mean: 14.621588763304185
  episode_reward_min: -10.619009729224285
  episodes_this_iter: 174
  episodes_total: 7656
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2745.259
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.346546173095703
        kl: 0.010782747529447079
        policy_loss: -0.014402970671653748
        total_loss: 0.6163874268531799
        vf_explained_var: 0.983579695224762
        vf_loss: 0.6294425129890442
    load_time_ms: 0.76
    num_steps_sampled: 1148400
    num_steps_trained: 1144000
    sample_time_ms: 2741.648
    update_time_ms: 4.196
  iterations_since_restore: 44
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9071340771711321
    mean_inference_ms: 0.9800204326579071
    mean_processing_ms: 0.7751737784829619
  time_since_restore: 244.4783411026001
  time_this_iter_s: 5.5235676765441895
  time_total_s: 244.4783411026001
  timestamp: 1563365765
  timesteps_since_restore: 1148400
  timesteps_this_iter: 26100
  timesteps_total: 1148400
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 244 s, 44 iter, 1148400 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.73413595992994
  episode_reward_mean: 14.256339651776777
  episode_reward_min: -7.922570748666818
  episodes_this_iter: 174
  episodes_total: 7830
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2734.924
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.331735849380493
        kl: 0.010041794739663601
        policy_loss: -0.015604688785970211
        total_loss: 0.5890048742294312
        vf_explained_var: 0.9815613031387329
        vf_loss: 0.6033543348312378
    load_time_ms: 0.765
    num_steps_sampled: 1174500
    num_steps_trained: 1170000
    sample_time_ms: 2721.17
    update_time_ms: 4.223
  iterations_since_restore: 45
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9076454243608073
    mean_inference_ms: 0.9798018751234926
    mean_processing_ms: 0.7751839466377463
  time_since_restore: 249.88388109207153
  time_this_iter_s: 5.4055399894714355
  time_total_s: 249.88388109207153
  timestamp: 1563365771
  timesteps_since_restore: 1174500
  timesteps_this_iter: 26100
  timesteps_total: 1174500
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 249 s, 45 iter, 1174500 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.154643396704266
  episode_reward_mean: 13.649203078748343
  episode_reward_min: -9.522681233532165
  episodes_this_iter: 174
  episodes_total: 8004
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2727.317
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3121564388275146
        kl: 0.010744478553533554
        policy_loss: -0.01315921451896429
        total_loss: 0.5943955183029175
        vf_explained_var: 0.9841004014015198
        vf_loss: 0.6062116026878357
    load_time_ms: 0.778
    num_steps_sampled: 1200600
    num_steps_trained: 1196000
    sample_time_ms: 2738.824
    update_time_ms: 4.258
  iterations_since_restore: 46
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9074453989763238
    mean_inference_ms: 0.9797092469572093
    mean_processing_ms: 0.775466948626118
  time_since_restore: 255.39379954338074
  time_this_iter_s: 5.509918451309204
  time_total_s: 255.39379954338074
  timestamp: 1563365776
  timesteps_since_restore: 1200600
  timesteps_this_iter: 26100
  timesteps_total: 1200600
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 255 s, 46 iter, 1200600 ts, 13.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.02218973994556
  episode_reward_mean: 14.524036864426261
  episode_reward_min: -4.395863735092274
  episodes_this_iter: 174
  episodes_total: 8178
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2746.537
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2926149368286133
        kl: 0.012024111114442348
        policy_loss: -0.013900822959840298
        total_loss: 0.5136259198188782
        vf_explained_var: 0.9851245284080505
        vf_loss: 0.5260237455368042
    load_time_ms: 0.785
    num_steps_sampled: 1226700
    num_steps_trained: 1222000
    sample_time_ms: 2737.765
    update_time_ms: 4.246
  iterations_since_restore: 47
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9076876764258028
    mean_inference_ms: 0.979478879691327
    mean_processing_ms: 0.7753242290281946
  time_since_restore: 261.08996534347534
  time_this_iter_s: 5.6961658000946045
  time_total_s: 261.08996534347534
  timestamp: 1563365782
  timesteps_since_restore: 1226700
  timesteps_this_iter: 26100
  timesteps_total: 1226700
  training_iteration: 47
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 261 s, 47 iter, 1226700 ts, 14.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.24400356135139
  episode_reward_mean: 14.483648530382279
  episode_reward_min: -6.298442316592934
  episodes_this_iter: 174
  episodes_total: 8352
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2773.241
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2730512619018555
        kl: 0.010849947109818459
        policy_loss: -0.01454233005642891
        total_loss: 0.5457942485809326
        vf_explained_var: 0.9854964017868042
        vf_loss: 0.5589803457260132
    load_time_ms: 0.78
    num_steps_sampled: 1252800
    num_steps_trained: 1248000
    sample_time_ms: 2711.437
    update_time_ms: 4.11
  iterations_since_restore: 48
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9075480324223718
    mean_inference_ms: 0.979491878666493
    mean_processing_ms: 0.7755540059322664
  time_since_restore: 266.62396121025085
  time_this_iter_s: 5.533995866775513
  time_total_s: 266.62396121025085
  timestamp: 1563365787
  timesteps_since_restore: 1252800
  timesteps_this_iter: 26100
  timesteps_total: 1252800
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 266 s, 48 iter, 1252800 ts, 14.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.98960565244516
  episode_reward_mean: 14.490002998665487
  episode_reward_min: -7.016122995763238
  episodes_this_iter: 174
  episodes_total: 8526
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2765.654
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2569680213928223
        kl: 0.01070378627628088
        policy_loss: -0.0159206073731184
        total_loss: 0.5046388506889343
        vf_explained_var: 0.9864052534103394
        vf_loss: 0.5192214846611023
    load_time_ms: 0.777
    num_steps_sampled: 1278900
    num_steps_trained: 1274000
    sample_time_ms: 2712.299
    update_time_ms: 4.118
  iterations_since_restore: 49
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9095739752287026
    mean_inference_ms: 0.9827720658916238
    mean_processing_ms: 0.777599450721861
  time_since_restore: 272.288063287735
  time_this_iter_s: 5.664102077484131
  time_total_s: 272.288063287735
  timestamp: 1563365793
  timesteps_since_restore: 1278900
  timesteps_this_iter: 26100
  timesteps_total: 1278900
  training_iteration: 49
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 272 s, 49 iter, 1278900 ts, 14.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.01452662548281
  episode_reward_mean: 14.31784035379097
  episode_reward_min: -5.740905411955717
  episodes_this_iter: 174
  episodes_total: 8700
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2742.441
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.24128794670105
        kl: 0.00968256127089262
        policy_loss: -0.014908362179994583
        total_loss: 0.47137293219566345
        vf_explained_var: 0.9868606328964233
        vf_loss: 0.48507097363471985
    load_time_ms: 0.786
    num_steps_sampled: 1305000
    num_steps_trained: 1300000
    sample_time_ms: 2726.309
    update_time_ms: 4.093
  iterations_since_restore: 50
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9084959491951774
    mean_inference_ms: 0.9805473202730409
    mean_processing_ms: 0.7767791939656934
  time_since_restore: 277.94905710220337
  time_this_iter_s: 5.660993814468384
  time_total_s: 277.94905710220337
  timestamp: 1563365799
  timesteps_since_restore: 1305000
  timesteps_this_iter: 26100
  timesteps_total: 1305000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 277 s, 50 iter, 1305000 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.13567276133131
  episode_reward_mean: 14.584949157960066
  episode_reward_min: -4.234357167181523
  episodes_this_iter: 174
  episodes_total: 8874
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2765.482
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2289488315582275
        kl: 0.009890768676996231
        policy_loss: -0.013567849062383175
        total_loss: 0.4553190767765045
        vf_explained_var: 0.9864112138748169
        vf_loss: 0.4676505923271179
    load_time_ms: 0.766
    num_steps_sampled: 1331100
    num_steps_trained: 1326000
    sample_time_ms: 2740.522
    update_time_ms: 4.143
  iterations_since_restore: 51
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9088971762566618
    mean_inference_ms: 0.9808182010389809
    mean_processing_ms: 0.7771321530862708
  time_since_restore: 283.4992301464081
  time_this_iter_s: 5.550173044204712
  time_total_s: 283.4992301464081
  timestamp: 1563365804
  timesteps_since_restore: 1331100
  timesteps_this_iter: 26100
  timesteps_total: 1331100
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 283 s, 51 iter, 1331100 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.95428202205152
  episode_reward_mean: 14.404073847659864
  episode_reward_min: -12.157163588047052
  episodes_this_iter: 174
  episodes_total: 9048
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2782.119
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2174153327941895
        kl: 0.011596865020692348
        policy_loss: -0.015400763601064682
        total_loss: 0.49577099084854126
        vf_explained_var: 0.9861676096916199
        vf_loss: 0.5097221732139587
    load_time_ms: 0.773
    num_steps_sampled: 1357200
    num_steps_trained: 1352000
    sample_time_ms: 2769.196
    update_time_ms: 4.038
  iterations_since_restore: 52
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9085734878760319
    mean_inference_ms: 0.9807301065620382
    mean_processing_ms: 0.7768116929668218
  time_since_restore: 289.0429358482361
  time_this_iter_s: 5.543705701828003
  time_total_s: 289.0429358482361
  timestamp: 1563365810
  timesteps_since_restore: 1357200
  timesteps_this_iter: 26100
  timesteps_total: 1357200
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 289 s, 52 iter, 1357200 ts, 14.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-16-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.43647783552869
  episode_reward_mean: 14.826865050471696
  episode_reward_min: -5.129764078600534
  episodes_this_iter: 174
  episodes_total: 9222
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2773.992
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.199321985244751
        kl: 0.011649813503026962
        policy_loss: -0.01592966541647911
        total_loss: 0.46059030294418335
        vf_explained_var: 0.9881230592727661
        vf_loss: 0.4750637412071228
    load_time_ms: 0.772
    num_steps_sampled: 1383300
    num_steps_trained: 1378000
    sample_time_ms: 2757.343
    update_time_ms: 4.071
  iterations_since_restore: 53
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9080188603251912
    mean_inference_ms: 0.980427871500597
    mean_processing_ms: 0.776355422825636
  time_since_restore: 294.4669635295868
  time_this_iter_s: 5.424027681350708
  time_total_s: 294.4669635295868
  timestamp: 1563365815
  timesteps_since_restore: 1383300
  timesteps_this_iter: 26100
  timesteps_total: 1383300
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 294 s, 53 iter, 1383300 ts, 14.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.22833600235881
  episode_reward_mean: 14.247519399212823
  episode_reward_min: -5.345226779455403
  episodes_this_iter: 174
  episodes_total: 9396
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2771.48
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.189893960952759
        kl: 0.011113607324659824
        policy_loss: -0.01456443127244711
        total_loss: 0.41366612911224365
        vf_explained_var: 0.9876231551170349
        vf_loss: 0.4268413782119751
    load_time_ms: 0.768
    num_steps_sampled: 1409400
    num_steps_trained: 1404000
    sample_time_ms: 2761.709
    update_time_ms: 3.899
  iterations_since_restore: 54
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9067482444301175
    mean_inference_ms: 0.9795874487275323
    mean_processing_ms: 0.7755725303860814
  time_since_restore: 300.0060648918152
  time_this_iter_s: 5.5391013622283936
  time_total_s: 300.0060648918152
  timestamp: 1563365821
  timesteps_since_restore: 1409400
  timesteps_this_iter: 26100
  timesteps_total: 1409400
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 300 s, 54 iter, 1409400 ts, 14.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.63040772502037
  episode_reward_mean: 14.857608335352209
  episode_reward_min: -5.352381117016248
  episodes_this_iter: 174
  episodes_total: 9570
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2744.991
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.16957688331604
        kl: 0.01081843115389347
        policy_loss: -0.016471628099679947
        total_loss: 0.43809372186660767
        vf_explained_var: 0.9874974489212036
        vf_loss: 0.45321303606033325
    load_time_ms: 0.754
    num_steps_sampled: 1435500
    num_steps_trained: 1430000
    sample_time_ms: 2768.232
    update_time_ms: 3.971
  iterations_since_restore: 55
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9057583765801223
    mean_inference_ms: 0.9780282245029809
    mean_processing_ms: 0.7748755701572531
  time_since_restore: 305.2118570804596
  time_this_iter_s: 5.205792188644409
  time_total_s: 305.2118570804596
  timestamp: 1563365826
  timesteps_since_restore: 1435500
  timesteps_this_iter: 26100
  timesteps_total: 1435500
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 305 s, 55 iter, 1435500 ts, 14.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.2895676290632
  episode_reward_mean: 15.618972177983835
  episode_reward_min: -5.834916994896554
  episodes_this_iter: 174
  episodes_total: 9744
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2732.967
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1526002883911133
        kl: 0.012152926065027714
        policy_loss: -0.016387922689318657
        total_loss: 0.39828282594680786
        vf_explained_var: 0.989693284034729
        vf_loss: 0.4131515920162201
    load_time_ms: 0.738
    num_steps_sampled: 1461600
    num_steps_trained: 1456000
    sample_time_ms: 2754.279
    update_time_ms: 3.965
  iterations_since_restore: 56
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9048960290211931
    mean_inference_ms: 0.9767340450810277
    mean_processing_ms: 0.7738733729563415
  time_since_restore: 310.4611999988556
  time_this_iter_s: 5.249342918395996
  time_total_s: 310.4611999988556
  timestamp: 1563365831
  timesteps_since_restore: 1461600
  timesteps_this_iter: 26100
  timesteps_total: 1461600
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 310 s, 56 iter, 1461600 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.33441321507485
  episode_reward_mean: 13.770418598281323
  episode_reward_min: -10.296360274506956
  episodes_this_iter: 174
  episodes_total: 9918
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2711.467
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.139302968978882
        kl: 0.010663943365216255
        policy_loss: -0.015314633958041668
        total_loss: 0.3492467701435089
        vf_explained_var: 0.9886093139648438
        vf_loss: 0.36322835087776184
    load_time_ms: 0.757
    num_steps_sampled: 1487700
    num_steps_trained: 1482000
    sample_time_ms: 2736.88
    update_time_ms: 4.027
  iterations_since_restore: 57
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9039822715833578
    mean_inference_ms: 0.9753428218245809
    mean_processing_ms: 0.7732017754984959
  time_since_restore: 315.77285718917847
  time_this_iter_s: 5.311657190322876
  time_total_s: 315.77285718917847
  timestamp: 1563365837
  timesteps_since_restore: 1487700
  timesteps_this_iter: 26100
  timesteps_total: 1487700
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 315 s, 57 iter, 1487700 ts, 13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.02650518600671
  episode_reward_mean: 15.296091130069955
  episode_reward_min: -5.134320416060642
  episodes_this_iter: 174
  episodes_total: 10266
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2632.767
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1075265407562256
        kl: 0.011429651640355587
        policy_loss: -0.015328540466725826
        total_loss: 0.34074071049690247
        vf_explained_var: 0.9904305338859558
        vf_loss: 0.35464051365852356
    load_time_ms: 0.764
    num_steps_sampled: 1539900
    num_steps_trained: 1534000
    sample_time_ms: 2709.387
    update_time_ms: 4.105
  iterations_since_restore: 59
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9025758003588633
    mean_inference_ms: 0.9736785831152003
    mean_processing_ms: 0.7718789229625945
  time_since_restore: 325.9063401222229
  time_this_iter_s: 5.173614740371704
  time_total_s: 325.9063401222229
  timestamp: 1563365847
  timesteps_since_restore: 1539900
  timesteps_this_iter: 26100
  timesteps_total: 1539900
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 325 s, 59 iter, 1539900 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.93898344369313
  episode_reward_mean: 16.512539829368265
  episode_reward_min: -6.189650915049233
  episodes_this_iter: 174
  episodes_total: 10440
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2614.092
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0954596996307373
        kl: 0.011304219253361225
        policy_loss: -0.015857206657528877
        total_loss: 0.3606577515602112
        vf_explained_var: 0.9903078079223633
        vf_loss: 0.3751019835472107
    load_time_ms: 0.758
    num_steps_sampled: 1566000
    num_steps_trained: 1560000
    sample_time_ms: 2706.589
    update_time_ms: 4.097
  iterations_since_restore: 60
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.903302422254539
    mean_inference_ms: 0.9763448780404866
    mean_processing_ms: 0.7728624203688643
  time_since_restore: 331.3509829044342
  time_this_iter_s: 5.444642782211304
  time_total_s: 331.3509829044342
  timestamp: 1563365852
  timesteps_since_restore: 1566000
  timesteps_this_iter: 26100
  timesteps_total: 1566000
  training_iteration: 60
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 331 s, 60 iter, 1566000 ts, 16.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.4380526518139
  episode_reward_mean: 15.049666450813303
  episode_reward_min: -12.50165159545851
  episodes_this_iter: 174
  episodes_total: 10614
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2611.389
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.076897144317627
        kl: 0.012058485299348831
        policy_loss: -0.01758500374853611
        total_loss: 0.37152212858200073
        vf_explained_var: 0.9899717569351196
        vf_loss: 0.38759979605674744
    load_time_ms: 0.769
    num_steps_sampled: 1592100
    num_steps_trained: 1586000
    sample_time_ms: 2715.901
    update_time_ms: 4.083
  iterations_since_restore: 61
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9027883758327964
    mean_inference_ms: 0.9754333392956683
    mean_processing_ms: 0.7724320792977559
  time_since_restore: 336.97149872779846
  time_this_iter_s: 5.620515823364258
  time_total_s: 336.97149872779846
  timestamp: 1563365858
  timesteps_since_restore: 1592100
  timesteps_this_iter: 26100
  timesteps_total: 1592100
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 336 s, 61 iter, 1592100 ts, 15 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.381022386826025
  episode_reward_mean: 15.587500262259752
  episode_reward_min: -5.766993299405398
  episodes_this_iter: 174
  episodes_total: 10788
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2632.451
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0609633922576904
        kl: 0.0122677618637681
        policy_loss: -0.01750790886580944
        total_loss: 0.31096240878105164
        vf_explained_var: 0.9919881820678711
        vf_loss: 0.32693687081336975
    load_time_ms: 0.771
    num_steps_sampled: 1618200
    num_steps_trained: 1612000
    sample_time_ms: 2703.611
    update_time_ms: 4.288
  iterations_since_restore: 62
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9031205340104782
    mean_inference_ms: 0.97572261580502
    mean_processing_ms: 0.7729284685971607
  time_since_restore: 342.6072897911072
  time_this_iter_s: 5.635791063308716
  time_total_s: 342.6072897911072
  timestamp: 1563365864
  timesteps_since_restore: 1618200
  timesteps_this_iter: 26100
  timesteps_total: 1618200
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 342 s, 62 iter, 1618200 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.76750817602043
  episode_reward_mean: 15.969529479428536
  episode_reward_min: -4.57041743474089
  episodes_this_iter: 174
  episodes_total: 10962
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2622.047
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.039912223815918
        kl: 0.013509483076632023
        policy_loss: -0.01799020543694496
        total_loss: 0.2865336239337921
        vf_explained_var: 0.9919885396957397
        vf_loss: 0.3028351366519928
    load_time_ms: 0.771
    num_steps_sampled: 1644300
    num_steps_trained: 1638000
    sample_time_ms: 2720.895
    update_time_ms: 4.343
  iterations_since_restore: 63
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9038229658428499
    mean_inference_ms: 0.9760762960251271
    mean_processing_ms: 0.7729775151167892
  time_since_restore: 348.10431575775146
  time_this_iter_s: 5.497025966644287
  time_total_s: 348.10431575775146
  timestamp: 1563365869
  timesteps_since_restore: 1644300
  timesteps_this_iter: 26100
  timesteps_total: 1644300
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 348 s, 63 iter, 1644300 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-17-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.16503554429739
  episode_reward_mean: 15.523015428682106
  episode_reward_min: -13.489803341266974
  episodes_this_iter: 174
  episodes_total: 11136
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2625.635
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.020779609680176
        kl: 0.012254338711500168
        policy_loss: -0.017423687502741814
        total_loss: 0.2844933569431305
        vf_explained_var: 0.9915742874145508
        vf_loss: 0.3003852665424347
    load_time_ms: 0.773
    num_steps_sampled: 1670400
    num_steps_trained: 1664000
    sample_time_ms: 2730.874
    update_time_ms: 4.401
  iterations_since_restore: 64
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9043659557480702
    mean_inference_ms: 0.9771501415310365
    mean_processing_ms: 0.7734206327237129
  time_since_restore: 353.77938079833984
  time_this_iter_s: 5.675065040588379
  time_total_s: 353.77938079833984
  timestamp: 1563365875
  timesteps_since_restore: 1670400
  timesteps_this_iter: 26100
  timesteps_total: 1670400
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 353 s, 64 iter, 1670400 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.941940412500436
  episode_reward_mean: 15.451027986415058
  episode_reward_min: -6.19636591002121
  episodes_this_iter: 174
  episodes_total: 11310
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2676.986
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.010646104812622
        kl: 0.013800972141325474
        policy_loss: -0.014538021758198738
        total_loss: 0.2685399055480957
        vf_explained_var: 0.9916397333145142
        vf_loss: 0.28135278820991516
    load_time_ms: 0.772
    num_steps_sampled: 1696500
    num_steps_trained: 1690000
    sample_time_ms: 2745.611
    update_time_ms: 4.433
  iterations_since_restore: 65
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9039923765237794
    mean_inference_ms: 0.9759426714343674
    mean_processing_ms: 0.773096399238365
  time_since_restore: 359.6464512348175
  time_this_iter_s: 5.867070436477661
  time_total_s: 359.6464512348175
  timestamp: 1563365881
  timesteps_since_restore: 1696500
  timesteps_this_iter: 26100
  timesteps_total: 1696500
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 359 s, 65 iter, 1696500 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.406165646636374
  episode_reward_mean: 15.21607942163796
  episode_reward_min: -8.934440060858238
  episodes_this_iter: 174
  episodes_total: 11484
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2683.602
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9923497438430786
        kl: 0.012742366641759872
        policy_loss: -0.014660179615020752
        total_loss: 0.283904105424881
        vf_explained_var: 0.9918833374977112
        vf_loss: 0.29697149991989136
    load_time_ms: 0.781
    num_steps_sampled: 1722600
    num_steps_trained: 1716000
    sample_time_ms: 2735.306
    update_time_ms: 4.44
  iterations_since_restore: 66
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9045885134356425
    mean_inference_ms: 0.9760553882688687
    mean_processing_ms: 0.773333377188766
  time_since_restore: 364.8590476512909
  time_this_iter_s: 5.212596416473389
  time_total_s: 364.8590476512909
  timestamp: 1563365886
  timesteps_since_restore: 1722600
  timesteps_this_iter: 26100
  timesteps_total: 1722600
  training_iteration: 66
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 364 s, 66 iter, 1722600 ts, 15.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.77936145770256
  episode_reward_mean: 15.067955585674763
  episode_reward_min: -4.193611884017641
  episodes_this_iter: 174
  episodes_total: 11658
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2698.225
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.972337007522583
        kl: 0.012548795901238918
        policy_loss: -0.016624409705400467
        total_loss: 0.27386024594306946
        vf_explained_var: 0.9914332032203674
        vf_loss: 0.28891605138778687
    load_time_ms: 0.76
    num_steps_sampled: 1748700
    num_steps_trained: 1742000
    sample_time_ms: 2743.339
    update_time_ms: 4.396
  iterations_since_restore: 67
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9039408387689408
    mean_inference_ms: 0.9761638315449861
    mean_processing_ms: 0.7732919775602729
  time_since_restore: 370.39266657829285
  time_this_iter_s: 5.533618927001953
  time_total_s: 370.39266657829285
  timestamp: 1563365891
  timesteps_since_restore: 1748700
  timesteps_this_iter: 26100
  timesteps_total: 1748700
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 370 s, 67 iter, 1748700 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.634463095707524
  episode_reward_mean: 15.469968022569702
  episode_reward_min: -17.287002854441855
  episodes_this_iter: 174
  episodes_total: 11832
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2751.617
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9591782093048096
        kl: 0.012520155869424343
        policy_loss: -0.015863636508584023
        total_loss: 0.2908661365509033
        vf_explained_var: 0.9920616745948792
        vf_loss: 0.30516475439071655
    load_time_ms: 0.765
    num_steps_sampled: 1774800
    num_steps_trained: 1768000
    sample_time_ms: 2756.094
    update_time_ms: 4.182
  iterations_since_restore: 68
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.903951482914335
    mean_inference_ms: 0.9753333953315263
    mean_processing_ms: 0.7731374982464797
  time_since_restore: 376.0132112503052
  time_this_iter_s: 5.620544672012329
  time_total_s: 376.0132112503052
  timestamp: 1563365897
  timesteps_since_restore: 1774800
  timesteps_this_iter: 26100
  timesteps_total: 1774800
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 376 s, 68 iter, 1774800 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.68126919442308
  episode_reward_mean: 15.725797755039121
  episode_reward_min: -5.222974046771373
  episodes_this_iter: 174
  episodes_total: 12006
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2796.351
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9413301944732666
        kl: 0.011302134022116661
        policy_loss: -0.017645137384533882
        total_loss: 0.283722460269928
        vf_explained_var: 0.9920943379402161
        vf_loss: 0.2999548316001892
    load_time_ms: 0.766
    num_steps_sampled: 1800900
    num_steps_trained: 1794000
    sample_time_ms: 2782.034
    update_time_ms: 4.277
  iterations_since_restore: 69
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9044455241918266
    mean_inference_ms: 0.9759719411710573
    mean_processing_ms: 0.7736120426091307
  time_since_restore: 381.896035194397
  time_this_iter_s: 5.882823944091797
  time_total_s: 381.896035194397
  timestamp: 1563365903
  timesteps_since_restore: 1800900
  timesteps_this_iter: 26100
  timesteps_total: 1800900
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 381 s, 69 iter, 1800900 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.971493069379264
  episode_reward_mean: 15.465052140424415
  episode_reward_min: -1.8124237068176072
  episodes_this_iter: 174
  episodes_total: 12180
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2816.965
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.921631932258606
        kl: 0.012633605860173702
        policy_loss: -0.017236076295375824
        total_loss: 0.2354830950498581
        vf_explained_var: 0.991978645324707
        vf_loss: 0.25113993883132935
    load_time_ms: 0.76
    num_steps_sampled: 1827000
    num_steps_trained: 1820000
    sample_time_ms: 2775.891
    update_time_ms: 4.371
  iterations_since_restore: 70
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9050656024432785
    mean_inference_ms: 0.9767162138835799
    mean_processing_ms: 0.7743730576232564
  time_since_restore: 387.4900143146515
  time_this_iter_s: 5.593979120254517
  time_total_s: 387.4900143146515
  timestamp: 1563365909
  timesteps_since_restore: 1827000
  timesteps_this_iter: 26100
  timesteps_total: 1827000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 387 s, 70 iter, 1827000 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.987488798599855
  episode_reward_mean: 16.785807895117493
  episode_reward_min: -4.559324997681744
  episodes_this_iter: 174
  episodes_total: 12354
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2805.908
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8995387554168701
        kl: 0.012726272456347942
        policy_loss: -0.01703754812479019
        total_loss: 0.2192690223455429
        vf_explained_var: 0.9941622018814087
        vf_loss: 0.2347157895565033
    load_time_ms: 0.754
    num_steps_sampled: 1853100
    num_steps_trained: 1846000
    sample_time_ms: 2787.527
    update_time_ms: 4.332
  iterations_since_restore: 71
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9061361521184963
    mean_inference_ms: 0.9788581951371828
    mean_processing_ms: 0.7754405952064255
  time_since_restore: 393.11101937294006
  time_this_iter_s: 5.621005058288574
  time_total_s: 393.11101937294006
  timestamp: 1563365914
  timesteps_since_restore: 1853100
  timesteps_this_iter: 26100
  timesteps_total: 1853100
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 393 s, 71 iter, 1853100 ts, 16.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.57713589999989
  episode_reward_mean: 16.184530424554726
  episode_reward_min: -5.144426522180088
  episodes_this_iter: 174
  episodes_total: 12528
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2762.349
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8782597780227661
        kl: 0.013426253572106361
        policy_loss: -0.017913714051246643
        total_loss: 0.2257329374551773
        vf_explained_var: 0.992759108543396
        vf_loss: 0.2419683337211609
    load_time_ms: 0.754
    num_steps_sampled: 1879200
    num_steps_trained: 1872000
    sample_time_ms: 2793.899
    update_time_ms: 4.153
  iterations_since_restore: 72
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9056997609644307
    mean_inference_ms: 0.9781015023582008
    mean_processing_ms: 0.775104346445639
  time_since_restore: 398.3709981441498
  time_this_iter_s: 5.259978771209717
  time_total_s: 398.3709981441498
  timestamp: 1563365920
  timesteps_since_restore: 1879200
  timesteps_this_iter: 26100
  timesteps_total: 1879200
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 398 s, 72 iter, 1879200 ts, 16.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.064644101036144
  episode_reward_mean: 16.65774978473884
  episode_reward_min: -3.4934128082034785
  episodes_this_iter: 174
  episodes_total: 12702
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2762.074
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8662285804748535
        kl: 0.012405757792294025
        policy_loss: -0.01720411330461502
        total_loss: 0.19834798574447632
        vf_explained_var: 0.9943094849586487
        vf_loss: 0.2140013575553894
    load_time_ms: 0.756
    num_steps_sampled: 1905300
    num_steps_trained: 1898000
    sample_time_ms: 2777.366
    update_time_ms: 4.109
  iterations_since_restore: 73
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9055809679584212
    mean_inference_ms: 0.976702935098217
    mean_processing_ms: 0.7748593753304642
  time_since_restore: 403.6983916759491
  time_this_iter_s: 5.327393531799316
  time_total_s: 403.6983916759491
  timestamp: 1563365925
  timesteps_since_restore: 1905300
  timesteps_this_iter: 26100
  timesteps_total: 1905300
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 403 s, 73 iter, 1905300 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.48794631535161
  episode_reward_mean: 16.680027626414972
  episode_reward_min: -3.658155202735844
  episodes_this_iter: 174
  episodes_total: 12876
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2760.234
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8379724025726318
        kl: 0.013592883944511414
        policy_loss: -0.019285377115011215
        total_loss: 0.20088666677474976
        vf_explained_var: 0.9938128590583801
        vf_loss: 0.2184729427099228
    load_time_ms: 0.762
    num_steps_sampled: 1931400
    num_steps_trained: 1924000
    sample_time_ms: 2776.326
    update_time_ms: 4.141
  iterations_since_restore: 74
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9053548329097942
    mean_inference_ms: 0.9762113899674002
    mean_processing_ms: 0.7747797717144684
  time_since_restore: 409.3445816040039
  time_this_iter_s: 5.64618992805481
  time_total_s: 409.3445816040039
  timestamp: 1563365931
  timesteps_since_restore: 1931400
  timesteps_this_iter: 26100
  timesteps_total: 1931400
  training_iteration: 74
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 409 s, 74 iter, 1931400 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-18-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.31185554139697
  episode_reward_mean: 15.900765719657086
  episode_reward_min: -7.9376579655193895
  episodes_this_iter: 174
  episodes_total: 13050
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2720.751
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8180028200149536
        kl: 0.015200331807136536
        policy_loss: -0.019049236550927162
        total_loss: 0.17799583077430725
        vf_explained_var: 0.9943087697029114
        vf_loss: 0.19514499604701996
    load_time_ms: 0.77
    num_steps_sampled: 1957500
    num_steps_trained: 1950000
    sample_time_ms: 2735.839
    update_time_ms: 3.977
  iterations_since_restore: 75
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9052745693013708
    mean_inference_ms: 0.9768982890896811
    mean_processing_ms: 0.7747706826248361
  time_since_restore: 414.40985679626465
  time_this_iter_s: 5.065275192260742
  time_total_s: 414.40985679626465
  timestamp: 1563365936
  timesteps_since_restore: 1957500
  timesteps_this_iter: 26100
  timesteps_total: 1957500
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 414 s, 75 iter, 1957500 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.76837547931492
  episode_reward_mean: 16.712199341532244
  episode_reward_min: -6.927188209228326
  episodes_this_iter: 174
  episodes_total: 13224
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2729.1
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.798162579536438
        kl: 0.013740639202296734
        policy_loss: -0.018189260736107826
        total_loss: 0.17527826130390167
        vf_explained_var: 0.9949145913124084
        vf_loss: 0.19174990057945251
    load_time_ms: 0.774
    num_steps_sampled: 1983600
    num_steps_trained: 1976000
    sample_time_ms: 2762.263
    update_time_ms: 3.929
  iterations_since_restore: 76
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.904745828092884
    mean_inference_ms: 0.9755085553172465
    mean_processing_ms: 0.7741190994139208
  time_since_restore: 419.97071528434753
  time_this_iter_s: 5.560858488082886
  time_total_s: 419.97071528434753
  timestamp: 1563365941
  timesteps_since_restore: 1983600
  timesteps_this_iter: 26100
  timesteps_total: 1983600
  training_iteration: 76
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 419 s, 76 iter, 1983600 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.97756060608722
  episode_reward_mean: 17.928778589000697
  episode_reward_min: -4.856369948349874
  episodes_this_iter: 174
  episodes_total: 13398
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2746.896
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7882722616195679
        kl: 0.014535929076373577
        policy_loss: -0.017987241968512535
        total_loss: 0.15451018512248993
        vf_explained_var: 0.9954230785369873
        vf_loss: 0.170680433511734
    load_time_ms: 0.769
    num_steps_sampled: 2009700
    num_steps_trained: 2002000
    sample_time_ms: 2761.554
    update_time_ms: 3.965
  iterations_since_restore: 77
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9038490129897971
    mean_inference_ms: 0.975193730230683
    mean_processing_ms: 0.7741045249918388
  time_since_restore: 425.67691802978516
  time_this_iter_s: 5.706202745437622
  time_total_s: 425.67691802978516
  timestamp: 1563365947
  timesteps_since_restore: 2009700
  timesteps_this_iter: 26100
  timesteps_total: 2009700
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 425 s, 77 iter, 2009700 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.71739952166119
  episode_reward_mean: 17.201760145164375
  episode_reward_min: -3.7391235145875847
  episodes_this_iter: 174
  episodes_total: 13572
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2708.659
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.777368426322937
        kl: 0.013143236748874187
        policy_loss: -0.017224855720996857
        total_loss: 0.16985124349594116
        vf_explained_var: 0.9955663681030273
        vf_loss: 0.18543317914009094
    load_time_ms: 0.769
    num_steps_sampled: 2035800
    num_steps_trained: 2028000
    sample_time_ms: 2760.662
    update_time_ms: 4.143
  iterations_since_restore: 78
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9037977197152779
    mean_inference_ms: 0.9747929068143335
    mean_processing_ms: 0.7736839542443614
  time_since_restore: 430.9074544906616
  time_this_iter_s: 5.230536460876465
  time_total_s: 430.9074544906616
  timestamp: 1563365952
  timesteps_since_restore: 2035800
  timesteps_this_iter: 26100
  timesteps_total: 2035800
  training_iteration: 78
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 430 s, 78 iter, 2035800 ts, 17.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.00221408278461
  episode_reward_mean: 17.153294304652594
  episode_reward_min: -3.193410052259764
  episodes_this_iter: 174
  episodes_total: 13746
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2690.354
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7585148811340332
        kl: 0.015102817676961422
        policy_loss: -0.019591625779867172
        total_loss: 0.15706011652946472
        vf_explained_var: 0.9951740503311157
        vf_loss: 0.17476388812065125
    load_time_ms: 0.755
    num_steps_sampled: 2061900
    num_steps_trained: 2054000
    sample_time_ms: 2724.972
    update_time_ms: 4.076
  iterations_since_restore: 79
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9037714501933194
    mean_inference_ms: 0.9743063423868505
    mean_processing_ms: 0.7737640453709879
  time_since_restore: 436.2502167224884
  time_this_iter_s: 5.342762231826782
  time_total_s: 436.2502167224884
  timestamp: 1563365957
  timesteps_since_restore: 2061900
  timesteps_this_iter: 26100
  timesteps_total: 2061900
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 436 s, 79 iter, 2061900 ts, 17.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.80198744705735
  episode_reward_mean: 17.27322186611818
  episode_reward_min: -3.2041473701651366
  episodes_this_iter: 174
  episodes_total: 13920
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2691.451
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7438507080078125
        kl: 0.013689907267689705
        policy_loss: -0.01941031590104103
        total_loss: 0.13507282733917236
        vf_explained_var: 0.9959439635276794
        vf_loss: 0.15277191996574402
    load_time_ms: 0.765
    num_steps_sampled: 2088000
    num_steps_trained: 2080000
    sample_time_ms: 2752.609
    update_time_ms: 4.013
  iterations_since_restore: 80
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9039258441248359
    mean_inference_ms: 0.974326305781212
    mean_processing_ms: 0.7738793288472985
  time_since_restore: 442.12984132766724
  time_this_iter_s: 5.879624605178833
  time_total_s: 442.12984132766724
  timestamp: 1563365963
  timesteps_since_restore: 2088000
  timesteps_this_iter: 26100
  timesteps_total: 2088000
  training_iteration: 80
  2019-07-17 14:19:40,768	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 442 s, 80 iter, 2088000 ts, 17.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.81177185574809
  episode_reward_mean: 17.491130847844786
  episode_reward_min: -3.188232512492354
  episodes_this_iter: 174
  episodes_total: 14094
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2732.521
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7322317361831665
        kl: 0.013423153199255466
        policy_loss: -0.017809420824050903
        total_loss: 0.1408034712076187
        vf_explained_var: 0.9959441423416138
        vf_loss: 0.1569349765777588
    load_time_ms: 0.776
    num_steps_sampled: 2114100
    num_steps_trained: 2106000
    sample_time_ms: 2720.662
    update_time_ms: 4.1
  iterations_since_restore: 81
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9039327473885609
    mean_inference_ms: 0.9747279346514034
    mean_processing_ms: 0.7741992037201965
  time_since_restore: 447.843878030777
  time_this_iter_s: 5.714036703109741
  time_total_s: 447.843878030777
  timestamp: 1563365969
  timesteps_since_restore: 2114100
  timesteps_this_iter: 26100
  timesteps_total: 2114100
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 447 s, 81 iter, 2114100 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.170993259011524
  episode_reward_mean: 17.74852909901534
  episode_reward_min: -2.162030317853708
  episodes_this_iter: 174
  episodes_total: 14268
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2758.036
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7162922620773315
        kl: 0.014415026642382145
        policy_loss: -0.019306378439068794
        total_loss: 0.13061140477657318
        vf_explained_var: 0.9960430860519409
        vf_loss: 0.14811588823795319
    load_time_ms: 0.776
    num_steps_sampled: 2140200
    num_steps_trained: 2132000
    sample_time_ms: 2725.265
    update_time_ms: 4.19
  iterations_since_restore: 82
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9037864548071831
    mean_inference_ms: 0.9743580161830999
    mean_processing_ms: 0.7741346019817958
  time_since_restore: 453.4065327644348
  time_this_iter_s: 5.562654733657837
  time_total_s: 453.4065327644348
  timestamp: 1563365975
  timesteps_since_restore: 2140200
  timesteps_this_iter: 26100
  timesteps_total: 2140200
  training_iteration: 82
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=32171], 453 s, 82 iter, 2140200 ts, 17.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-17_14-19-40
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 37.324899158853256
  episode_reward_mean: 19.61213361977675
  episode_reward_min: -7.085011485068859
  episodes_this_iter: 174
  episodes_total: 14442
  experiment_id: 09ee7ea2cadd473391a74d0294d7617e
  hostname: navel-notebook-1
  info:
    grad_time_ms: 2770.11
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7004773616790771
        kl: 0.014649581164121628
        policy_loss: -0.019517749547958374
        total_loss: 0.10477656871080399
        vf_explained_var: 0.9972174167633057
        vf_loss: 0.12246309965848923
    load_time_ms: 0.774
    num_steps_sampled: 2166300
    num_steps_trained: 2158000
    sample_time_ms: 2737.201
    update_time_ms: 4.195
  iterations_since_restore: 83
  node_ip: 10.16.128.38
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32171
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.9040803032977306
    mean_inference_ms: 0.9747234172364433
    mean_processing_ms: 0.7745523789886094
  time_since_restore: 458.9745383262634
  time_this_iter_s: 5.568005561828613
  time_total_s: 458.9745383262634
  timestamp: 1563365980
  timesteps_since_restore: 2166300
  timesteps_this_iter: 26100
  timesteps_total: 2166300
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 6})
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32171], 458 s, 83 iter, 2166300 ts, 19.6 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 6 ({'TERMINATED': 6})
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29871], 426 s, 77 iter, 2795100 ts, 18.2 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=29869], 430 s, 95 iter, 2508000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30796], 441 s, 90 iter, 2349000 ts, 18.1 rew
 - PPO_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32075], 436 s, 85 iter, 2244000 ts, 18.4 rew
 - PPO_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32187], 469 s, 93 iter, 2455200 ts, 19 rew
 - PPO_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=32171], 458 s, 83 iter, 2166300 ts, 19.6 rew

[32m [  2721.72438s,  INFO] Experiment took 2721.41293 seconds | 45.35688 minutes | 0.75595 hours [0m
