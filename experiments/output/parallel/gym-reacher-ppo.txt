2019-07-24 01:23:48,320	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-24_01-23-48_320705_30485/logs.
2019-07-24 01:23:48,426	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-24 01:23:48,536	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-24 01:23:48,540	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-24 01:23:48,562	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-24_01-23-48_320705_30485/logs.
2019-07-24 01:23:48,562	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-24 01:23:48,662	INFO tune.py:65 -- Did not find checkpoint file in /home/amr/kayray_results/parallel/gym-reacher-ppo.
2019-07-24 01:23:48,663	INFO tune.py:233 -- Starting a new experiment.
2019-07-24 01:23:48,723	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-24 01:23:48,831	WARNING util.py:64 -- The `start_trial` operation took 0.14428925514221191 seconds to complete, which may be a performance bottleneck.
[32m [     0.20977s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.21008s,  INFO] Experiment configs: 
 {
  "gym-reacher-ppo": {
    "env": "RoboschoolReacher-v1",
    "run": "PPO",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 50,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 21,
      "timesteps_total": 10000000
    },
    "config": {
      "env_config": {
        "env_type": "openai"
      },
      "gamma": 0.995,
      "kl_coeff": 1.0,
      "num_sgd_iter": 20,
      "lr": 0.0001,
      "sgd_minibatch_size": 1000,
      "train_batch_size": 25000,
      "model": {
        "free_log_std": true
      },
      "num_gpus": 1,
      "num_workers": 11,
      "num_envs_per_worker": {
        "grid_search": [
          16,
          8,
          4
        ]
      },
      "batch_mode": "complete_episodes",
      "observation_filter": "MeanStdFilter"
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 1.1/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 1.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING

[2m[36m(pid=30530)[0m [32m [     0.01534s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m 2019-07-24 01:23:50,573	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=30530)[0m 2019-07-24 01:23:50.576016: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30530)[0m 2019-07-24 01:23:50,663	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=30530)[0m 
[2m[36m(pid=30530)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30530)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30530)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=30530)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30530)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30530)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30530)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=30530)[0m 
[2m[36m(pid=30530)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30530)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30530)[0m [32m [     0.63617s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63656s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63694s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63732s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63769s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63807s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63854s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63892s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63930s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.63967s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.64004s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.64042s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.64080s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m 2019-07-24 01:23:51,195	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fe1a6d656a0>}
[2m[36m(pid=30530)[0m 2019-07-24 01:23:51,195	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe1a738a1d0>}
[2m[36m(pid=30530)[0m 2019-07-24 01:23:51,195	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.64118s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m [32m [     0.64155s,  INFO] TimeLimit:
[2m[36m(pid=30530)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30530)[0m - action_space = Box(2,)
[2m[36m(pid=30530)[0m - observation_space = Box(9,)
[2m[36m(pid=30530)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30530)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30530)[0m - _max_episode_steps = 150
[2m[36m(pid=30530)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30530)[0m 2019-07-24 01:23:51,287	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=30534)[0m 2019-07-24 01:23:53,724	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30534)[0m 2019-07-24 01:23:53.725133: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30534)[0m [32m [     0.03712s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     0.03774s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m 2019-07-24 01:23:53,778	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30536)[0m 2019-07-24 01:23:53.779556: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30730)[0m 2019-07-24 01:23:53,752	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30730)[0m 2019-07-24 01:23:53.753664: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30730)[0m [32m [     0.03744s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m 2019-07-24 01:23:53,841	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30528)[0m 2019-07-24 01:23:53.841922: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30528)[0m [32m [     0.03719s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     0.03733s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m 2019-07-24 01:23:53,817	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30533)[0m 2019-07-24 01:23:53.818242: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30538)[0m 2019-07-24 01:23:53,824	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30538)[0m 2019-07-24 01:23:53.825532: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30538)[0m [32m [     0.03760s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m 2019-07-24 01:23:53,824	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30537)[0m 2019-07-24 01:23:53.825534: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30537)[0m [32m [     0.03752s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m 2019-07-24 01:23:53,815	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30532)[0m 2019-07-24 01:23:53.816211: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30532)[0m [32m [     0.03920s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     0.03817s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m 2019-07-24 01:23:53,897	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30531)[0m 2019-07-24 01:23:53.898367: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30535)[0m [32m [     0.03844s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m 2019-07-24 01:23:53,878	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30535)[0m 2019-07-24 01:23:53.879175: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30539)[0m [32m [     0.03851s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m 2019-07-24 01:23:53,919	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30539)[0m 2019-07-24 01:23:53.919806: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30730)[0m 2019-07-24 01:23:53,967	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30730)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30730)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30730)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30730)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=30730)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30730)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30730)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30730)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30730)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30730)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30730)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30534)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30534)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30730)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30730)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30538)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30538)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30537)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30537)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30532)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30532)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30536)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30536)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30528)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30528)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30533)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30533)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30531)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30531)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30535)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30535)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30539)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30539)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30534)[0m [32m [     1.56555s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.56638s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.56715s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.56795s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.56883s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.56967s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57049s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57130s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57207s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57306s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57392s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57477s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57564s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57647s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30534)[0m [32m [     1.57730s,  INFO] TimeLimit:
[2m[36m(pid=30534)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30534)[0m - action_space = Box(2,)
[2m[36m(pid=30534)[0m - observation_space = Box(9,)
[2m[36m(pid=30534)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30534)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30534)[0m - _max_episode_steps = 150
[2m[36m(pid=30534)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63388s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63468s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63552s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63634s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63716s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63798s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63882s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.63967s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.64050s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.64145s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.64222s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63335s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63415s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63499s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63581s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63662s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63742s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63820s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63899s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.63983s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.64084s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.64172s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.64249s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.64333s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m [32m [     1.64304s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.64383s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.64470s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.64411s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m [32m [     1.64504s,  INFO] TimeLimit:
[2m[36m(pid=30730)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30730)[0m - action_space = Box(2,)
[2m[36m(pid=30730)[0m - observation_space = Box(9,)
[2m[36m(pid=30730)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30730)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30730)[0m - _max_episode_steps = 150
[2m[36m(pid=30730)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30536)[0m [32m [     1.64548s,  INFO] TimeLimit:
[2m[36m(pid=30536)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30536)[0m - action_space = Box(2,)
[2m[36m(pid=30536)[0m - observation_space = Box(9,)
[2m[36m(pid=30536)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30536)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30536)[0m - _max_episode_steps = 150
[2m[36m(pid=30536)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62210s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62299s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62380s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62461s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62536s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62617s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62704s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62790s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62880s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.62960s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.63046s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.63144s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.63228s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m [32m [     1.65277s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65360s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65453s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65555s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65651s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65763s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65845s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.63832s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.63928s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64010s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64096s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64179s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64253s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64331s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64419s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64506s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64613s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64693s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64778s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64869s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m [32m [     1.60805s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.60892s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.60975s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61060s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61143s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61224s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61306s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61389s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61479s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61576s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61654s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61732s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61813s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m [32m [     1.63402s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63482s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63560s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63636s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63714s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63796s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63889s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.63975s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64055s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64153s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64249s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64342s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64432s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.63349s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30528)[0m [32m [     1.63437s,  INFO] TimeLimit:
[2m[36m(pid=30528)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30528)[0m - action_space = Box(2,)
[2m[36m(pid=30528)[0m - observation_space = Box(9,)
[2m[36m(pid=30528)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30528)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30528)[0m - _max_episode_steps = 150
[2m[36m(pid=30528)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.65926s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66010s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.64967s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30538)[0m [32m [     1.65074s,  INFO] TimeLimit:
[2m[36m(pid=30538)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30538)[0m - action_space = Box(2,)
[2m[36m(pid=30538)[0m - observation_space = Box(9,)
[2m[36m(pid=30538)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30538)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30538)[0m - _max_episode_steps = 150
[2m[36m(pid=30538)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61892s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30537)[0m [32m [     1.61975s,  INFO] TimeLimit:
[2m[36m(pid=30537)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30537)[0m - action_space = Box(2,)
[2m[36m(pid=30537)[0m - observation_space = Box(9,)
[2m[36m(pid=30537)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30537)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30537)[0m - _max_episode_steps = 150
[2m[36m(pid=30537)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64530s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30532)[0m [32m [     1.64626s,  INFO] TimeLimit:
[2m[36m(pid=30532)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30532)[0m - action_space = Box(2,)
[2m[36m(pid=30532)[0m - observation_space = Box(9,)
[2m[36m(pid=30532)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30532)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30532)[0m - _max_episode_steps = 150
[2m[36m(pid=30532)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66119s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66209s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66299s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66389s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66477s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30533)[0m [32m [     1.66566s,  INFO] TimeLimit:
[2m[36m(pid=30533)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30533)[0m - action_space = Box(2,)
[2m[36m(pid=30533)[0m - observation_space = Box(9,)
[2m[36m(pid=30533)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30533)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30533)[0m - _max_episode_steps = 150
[2m[36m(pid=30533)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,445	INFO rollout_worker.py:428 -- Generating sample batch of size 3200
[2m[36m(pid=30531)[0m [32m [     1.66759s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.66824s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.66912s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67000s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67060s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67136s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67212s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67273s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67334s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67425s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67518s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67610s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67709s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m [32m [     1.68330s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.68452s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.68569s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.68670s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.68766s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.68870s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.68984s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69077s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69180s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69293s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69398s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69503s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69598s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m [32m [     1.62182s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62277s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62381s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62484s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62587s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62705s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62805s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.62898s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63004s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63140s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63237s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63341s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63452s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67804s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30531)[0m [32m [     1.67899s,  INFO] TimeLimit:
[2m[36m(pid=30531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30531)[0m - action_space = Box(2,)
[2m[36m(pid=30531)[0m - observation_space = Box(9,)
[2m[36m(pid=30531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30531)[0m - _max_episode_steps = 150
[2m[36m(pid=30531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69696s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30535)[0m [32m [     1.69786s,  INFO] TimeLimit:
[2m[36m(pid=30535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30535)[0m - action_space = Box(2,)
[2m[36m(pid=30535)[0m - observation_space = Box(9,)
[2m[36m(pid=30535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30535)[0m - _max_episode_steps = 150
[2m[36m(pid=30535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63562s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30539)[0m [32m [     1.63683s,  INFO] TimeLimit:
[2m[36m(pid=30539)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30539)[0m - action_space = Box(2,)
[2m[36m(pid=30539)[0m - observation_space = Box(9,)
[2m[36m(pid=30539)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30539)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30539)[0m - _max_episode_steps = 150
[2m[36m(pid=30539)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,611	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.836, max=0.549, mean=-0.082)},
[2m[36m(pid=30730)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.0, max=0.144, mean=-0.159)},
[2m[36m(pid=30730)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.595, max=0.804, mean=0.023)},
[2m[36m(pid=30730)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.923, max=0.995, mean=-0.008)},
[2m[36m(pid=30730)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.879, max=1.009, mean=-0.037)},
[2m[36m(pid=30730)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.529, max=0.849, mean=0.045)},
[2m[36m(pid=30730)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.681, max=0.733, mean=0.099)},
[2m[36m(pid=30730)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.993, max=0.341, mean=-0.137)},
[2m[36m(pid=30730)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.575, max=1.0, mean=0.048)},
[2m[36m(pid=30730)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.215, max=0.996, mean=0.168)},
[2m[36m(pid=30730)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.709, max=0.996, mean=0.056)},
[2m[36m(pid=30730)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.973, max=0.126, mean=-0.234)},
[2m[36m(pid=30730)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.071, max=0.977, mean=0.193)},
[2m[36m(pid=30730)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.078, max=0.852, mean=0.246)},
[2m[36m(pid=30730)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.979, max=0.173, mean=-0.198)},
[2m[36m(pid=30730)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.244, max=0.96, mean=0.261)}}
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,611	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=30730)[0m   1: {'agent0': None},
[2m[36m(pid=30730)[0m   2: {'agent0': None},
[2m[36m(pid=30730)[0m   3: {'agent0': None},
[2m[36m(pid=30730)[0m   4: {'agent0': None},
[2m[36m(pid=30730)[0m   5: {'agent0': None},
[2m[36m(pid=30730)[0m   6: {'agent0': None},
[2m[36m(pid=30730)[0m   7: {'agent0': None},
[2m[36m(pid=30730)[0m   8: {'agent0': None},
[2m[36m(pid=30730)[0m   9: {'agent0': None},
[2m[36m(pid=30730)[0m   10: {'agent0': None},
[2m[36m(pid=30730)[0m   11: {'agent0': None},
[2m[36m(pid=30730)[0m   12: {'agent0': None},
[2m[36m(pid=30730)[0m   13: {'agent0': None},
[2m[36m(pid=30730)[0m   14: {'agent0': None},
[2m[36m(pid=30730)[0m   15: {'agent0': None}}
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,612	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.836, max=0.549, mean=-0.082)
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,612	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,623	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 0,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 1,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.079),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 2,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.343, max=0.799, mean=0.261),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 3,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.248, max=1.345, mean=-0.022),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 4,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.927, max=1.595, mean=0.011),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 5,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.665, max=1.462, mean=0.071),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 6,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.594, max=1.056, mean=0.288),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 7,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.191, max=1.885, mean=-0.132),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 8,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-2.214, max=1.849, mean=0.104),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 9,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.542, max=1.518, mean=0.261),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 10,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.977, max=1.31, mean=0.125),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 11,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.477, max=1.626, mean=-0.341),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 12,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.695, max=1.73, mean=0.437),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 13,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.084, max=1.364, mean=0.31),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 14,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.843, max=1.307, mean=-0.285),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30730)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30730)[0m                                   'env_id': 15,
[2m[36m(pid=30730)[0m                                   'info': None,
[2m[36m(pid=30730)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.505, max=1.447, mean=0.324),
[2m[36m(pid=30730)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30730)[0m                                   'rnn_state': []},
[2m[36m(pid=30730)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,624	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=30730)[0m 2019-07-24 01:23:55,664	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-3.19, max=2.317, mean=-0.466),
[2m[36m(pid=30730)[0m                       [],
[2m[36m(pid=30730)[0m                       { 'action_prob': np.ndarray((16,), dtype=float32, min=0.0, max=0.157, mean=0.064),
[2m[36m(pid=30730)[0m                         'behaviour_logits': np.ndarray((16, 4), dtype=float32, min=-0.006, max=0.006, mean=0.001),
[2m[36m(pid=30730)[0m                         'vf_preds': np.ndarray((16,), dtype=float32, min=-0.007, max=0.011, mean=0.001)})}
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m 2019-07-24 01:23:56,786	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.0, max=0.156, mean=0.078),
[2m[36m(pid=30730)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-3.108, max=3.293, mean=0.125),
[2m[36m(pid=30730)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-49.352, max=-0.031, mean=-24.016),
[2m[36m(pid=30730)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.009, max=0.006, mean=0.0),
[2m[36m(pid=30730)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=30730)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=592891198.0, max=592891198.0, mean=592891198.0),
[2m[36m(pid=30730)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=30730)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-4.077, max=4.82, mean=0.187),
[2m[36m(pid=30730)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-4.077, max=4.82, mean=0.183),
[2m[36m(pid=30730)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-3.108, max=3.293, mean=0.122),
[2m[36m(pid=30730)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-6.662, max=6.425, mean=-0.265),
[2m[36m(pid=30730)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-6.662, max=6.425, mean=-0.265),
[2m[36m(pid=30730)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=30730)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-49.359, max=-0.028, mean=-24.016),
[2m[36m(pid=30730)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.009, max=0.008, mean=-0.001)},
[2m[36m(pid=30730)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m 2019-07-24 01:23:58,296	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30730)[0m { 'data': { 'action_prob': np.ndarray((3300,), dtype=float32, min=0.0, max=0.159, mean=0.078),
[2m[36m(pid=30730)[0m             'actions': np.ndarray((3300, 2), dtype=float32, min=-3.854, max=3.494, mean=0.012),
[2m[36m(pid=30730)[0m             'advantages': np.ndarray((3300,), dtype=float32, min=-49.352, max=30.482, mean=-7.808),
[2m[36m(pid=30730)[0m             'agent_index': np.ndarray((3300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30730)[0m             'behaviour_logits': np.ndarray((3300, 4), dtype=float32, min=-0.009, max=0.009, mean=0.0),
[2m[36m(pid=30730)[0m             'dones': np.ndarray((3300,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=30730)[0m             'eps_id': np.ndarray((3300,), dtype=int64, min=76670720.0, max=1878207551.0, mean=948352595.455),
[2m[36m(pid=30730)[0m             'infos': np.ndarray((3300,), dtype=object, head={}),
[2m[36m(pid=30730)[0m             'new_obs': np.ndarray((3300, 9), dtype=float32, min=-5.273, max=4.842, mean=-0.009),
[2m[36m(pid=30730)[0m             'obs': np.ndarray((3300, 9), dtype=float32, min=-5.273, max=4.842, mean=-0.01),
[2m[36m(pid=30730)[0m             'prev_actions': np.ndarray((3300, 2), dtype=float32, min=-3.854, max=3.494, mean=0.011),
[2m[36m(pid=30730)[0m             'prev_rewards': np.ndarray((3300,), dtype=float32, min=-6.662, max=6.425, mean=-0.119),
[2m[36m(pid=30730)[0m             'rewards': np.ndarray((3300,), dtype=float32, min=-6.662, max=6.425, mean=-0.12),
[2m[36m(pid=30730)[0m             't': np.ndarray((3300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=30730)[0m             'unroll_id': np.ndarray((3300,), dtype=int64, min=0.0, max=1.0, mean=0.273),
[2m[36m(pid=30730)[0m             'value_targets': np.ndarray((3300,), dtype=float32, min=-49.359, max=30.484, mean=-7.808),
[2m[36m(pid=30730)[0m             'vf_preds': np.ndarray((3300,), dtype=float32, min=-0.012, max=0.014, mean=-0.0)},
[2m[36m(pid=30730)[0m   'type': 'SampleBatch'}
[2m[36m(pid=30730)[0m 
[2m[36m(pid=30530)[0m 2019-07-24 01:23:58,336	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=30530)[0m 
[2m[36m(pid=30530)[0m { 'inputs': [ np.ndarray((36300, 2), dtype=float32, min=-4.917, max=4.238, mean=0.002),
[2m[36m(pid=30530)[0m               np.ndarray((36300,), dtype=float32, min=-26.342, max=27.883, mean=-0.101),
[2m[36m(pid=30530)[0m               np.ndarray((36300, 9), dtype=float32, min=-11.873, max=9.294, mean=0.001),
[2m[36m(pid=30530)[0m               np.ndarray((36300, 2), dtype=float32, min=-4.917, max=4.238, mean=0.002),
[2m[36m(pid=30530)[0m               np.ndarray((36300,), dtype=float32, min=-5.845, max=3.58, mean=0.0),
[2m[36m(pid=30530)[0m               np.ndarray((36300, 4), dtype=float32, min=-0.01, max=0.011, mean=0.0),
[2m[36m(pid=30530)[0m               np.ndarray((36300,), dtype=float32, min=-70.428, max=31.661, mean=-7.119),
[2m[36m(pid=30530)[0m               np.ndarray((36300,), dtype=float32, min=-0.013, max=0.014, mean=0.0)],
[2m[36m(pid=30530)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30530)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=30530)[0m   'state_inputs': []}
[2m[36m(pid=30530)[0m 
[2m[36m(pid=30530)[0m 2019-07-24 01:23:58,336	INFO multi_gpu_impl.py:191 -- Divided 36300 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.392199596236452
  episode_reward_mean: -15.168709571526257
  episode_reward_min: -81.18821584853313
  episodes_this_iter: 242
  episodes_total: 242
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5777.559
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.837555408477783
        kl: 0.001111507648602128
        policy_loss: -0.003738071769475937
        total_loss: 97.61965942382812
        vf_explained_var: 0.1822831630706787
        vf_loss: 97.62228393554688
    load_time_ms: 21.633
    num_steps_sampled: 36300
    num_steps_trained: 36000
    sample_time_ms: 2909.482
    update_time_ms: 406.125
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.43397141482239
    mean_inference_ms: 1.4792928808011239
    mean_processing_ms: 3.2398306830200827
  time_since_restore: 9.166616678237915
  time_this_iter_s: 9.166616678237915
  time_total_s: 9.166616678237915
  timestamp: 1563924244
  timesteps_since_restore: 36300
  timesteps_this_iter: 36300
  timesteps_total: 36300
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 9 s, 1 iter, 36300 ts, -15.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.74019341828706
  episode_reward_mean: -14.640017251386464
  episode_reward_min: -54.72865124012045
  episodes_this_iter: 242
  episodes_total: 484
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5648.788
    learner:
      default_policy:
        cur_kl_coeff: 0.5
        cur_lr: 9.999999747378752e-05
        entropy: 2.8358280658721924
        kl: 0.0018137701554223895
        policy_loss: -0.0017638909630477428
        total_loss: 76.03860473632812
        vf_explained_var: 0.29439231753349304
        vf_loss: 76.03945922851562
    load_time_ms: 11.537
    num_steps_sampled: 72600
    num_steps_trained: 72000
    sample_time_ms: 2126.67
    update_time_ms: 204.932
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.284777004039713
    mean_inference_ms: 1.441375114965333
    mean_processing_ms: 3.2358064787975604
  time_since_restore: 16.055012702941895
  time_this_iter_s: 6.8883960247039795
  time_total_s: 16.055012702941895
  timestamp: 1563924251
  timesteps_since_restore: 72600
  timesteps_this_iter: 36300
  timesteps_total: 72600
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 16 s, 2 iter, 72600 ts, -14.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.87816272847427
  episode_reward_mean: -14.389102154031473
  episode_reward_min: -51.55281167625815
  episodes_this_iter: 242
  episodes_total: 726
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5602.115
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.8301939964294434
        kl: 0.004884408321231604
        policy_loss: -0.006193851586431265
        total_loss: 65.7116470336914
        vf_explained_var: 0.381081223487854
        vf_loss: 65.71662139892578
    load_time_ms: 8.241
    num_steps_sampled: 108900
    num_steps_trained: 108000
    sample_time_ms: 2318.522
    update_time_ms: 137.895
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.186741305153734
    mean_inference_ms: 1.40861090556312
    mean_processing_ms: 3.2697163775676876
  time_since_restore: 24.29028868675232
  time_this_iter_s: 8.235275983810425
  time_total_s: 24.29028868675232
  timestamp: 1563924259
  timesteps_since_restore: 108900
  timesteps_this_iter: 36300
  timesteps_total: 108900
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 24 s, 3 iter, 108900 ts, -14.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.560001141885266
  episode_reward_mean: -13.078471061182228
  episode_reward_min: -45.44008873486797
  episodes_this_iter: 242
  episodes_total: 968
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5586.05
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8216497898101807
        kl: 0.00694745359942317
        policy_loss: -0.005349579267203808
        total_loss: 64.3908920288086
        vf_explained_var: 0.3869531750679016
        vf_loss: 64.39537048339844
    load_time_ms: 6.589
    num_steps_sampled: 145200
    num_steps_trained: 144000
    sample_time_ms: 2077.441
    update_time_ms: 104.275
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.160703004350721
    mean_inference_ms: 1.3992832594906066
    mean_processing_ms: 3.282637832359858
  time_since_restore: 31.207651376724243
  time_this_iter_s: 6.917362689971924
  time_total_s: 31.207651376724243
  timestamp: 1563924266
  timesteps_since_restore: 145200
  timesteps_this_iter: 36300
  timesteps_total: 145200
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 31 s, 4 iter, 145200 ts, -13.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.4318799650494
  episode_reward_mean: -12.707572614885109
  episode_reward_min: -49.85387026595998
  episodes_this_iter: 242
  episodes_total: 1210
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5970.001
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8125345706939697
        kl: 0.006125519517809153
        policy_loss: -0.005677313543856144
        total_loss: 48.857765197753906
        vf_explained_var: 0.4303567111492157
        vf_loss: 48.862674713134766
    load_time_ms: 5.641
    num_steps_sampled: 181500
    num_steps_trained: 180000
    sample_time_ms: 1931.65
    update_time_ms: 84.153
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.141909328982364
    mean_inference_ms: 1.3914516811273512
    mean_processing_ms: 3.2931250608582956
  time_since_restore: 40.091437578201294
  time_this_iter_s: 8.88378620147705
  time_total_s: 40.091437578201294
  timestamp: 1563924275
  timesteps_since_restore: 181500
  timesteps_this_iter: 36300
  timesteps_total: 181500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 40 s, 5 iter, 181500 ts, -12.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.215437212206204
  episode_reward_mean: -10.062697169120883
  episode_reward_min: -38.42983313685274
  episodes_this_iter: 242
  episodes_total: 1452
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5892.514
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.801438808441162
        kl: 0.007205640897154808
        policy_loss: -0.00662405276671052
        total_loss: 44.212215423583984
        vf_explained_var: 0.43042975664138794
        vf_loss: 44.21794128417969
    load_time_ms: 4.968
    num_steps_sampled: 217800
    num_steps_trained: 216000
    sample_time_ms: 2052.413
    update_time_ms: 70.905
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.109752843184407
    mean_inference_ms: 1.3878346933393488
    mean_processing_ms: 3.2889583888746
  time_since_restore: 48.2795205116272
  time_this_iter_s: 8.188082933425903
  time_total_s: 48.2795205116272
  timestamp: 1563924283
  timesteps_since_restore: 217800
  timesteps_this_iter: 36300
  timesteps_total: 217800
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 48 s, 6 iter, 217800 ts, -10.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.62615364536849
  episode_reward_mean: -9.78104188822863
  episode_reward_min: -45.234398898510356
  episodes_this_iter: 242
  episodes_total: 1694
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5868.032
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7971324920654297
        kl: 0.007562245707958937
        policy_loss: -0.0057951160706579685
        total_loss: 41.9487190246582
        vf_explained_var: 0.4551251232624054
        vf_loss: 41.95356750488281
    load_time_ms: 4.494
    num_steps_sampled: 254100
    num_steps_trained: 252000
    sample_time_ms: 1952.44
    update_time_ms: 61.248
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.09991360975636
    mean_inference_ms: 1.3847577781970666
    mean_processing_ms: 3.296541854315918
  time_since_restore: 55.378315925598145
  time_this_iter_s: 7.098795413970947
  time_total_s: 55.378315925598145
  timestamp: 1563924290
  timesteps_since_restore: 254100
  timesteps_this_iter: 36300
  timesteps_total: 254100
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 55 s, 7 iter, 254100 ts, -9.78 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-24-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.501791519827833
  episode_reward_mean: -7.672947879373445
  episode_reward_min: -36.43695083176593
  episodes_this_iter: 242
  episodes_total: 1936
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5930.727
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7934627532958984
        kl: 0.008338209241628647
        policy_loss: -0.008700271137058735
        total_loss: 31.826242446899414
        vf_explained_var: 0.5159571766853333
        vf_loss: 31.83390235900879
    load_time_ms: 4.155
    num_steps_sampled: 290400
    num_steps_trained: 288000
    sample_time_ms: 1876.514
    update_time_ms: 54.069
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0901109750992495
    mean_inference_ms: 1.3819392652581437
    mean_processing_ms: 3.2954265758740626
  time_since_restore: 63.12003755569458
  time_this_iter_s: 7.7417216300964355
  time_total_s: 63.12003755569458
  timestamp: 1563924298
  timesteps_since_restore: 290400
  timesteps_this_iter: 36300
  timesteps_total: 290400
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 63 s, 8 iter, 290400 ts, -7.67 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.295031780217467
  episode_reward_mean: -5.069104878473709
  episode_reward_min: -36.21887031158383
  episodes_this_iter: 242
  episodes_total: 2178
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5883.159
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7825498580932617
        kl: 0.006952609401196241
        policy_loss: -0.007786408066749573
        total_loss: 24.551921844482422
        vf_explained_var: 0.6029002666473389
        vf_loss: 24.558841705322266
    load_time_ms: 3.872
    num_steps_sampled: 326700
    num_steps_trained: 324000
    sample_time_ms: 1966.902
    update_time_ms: 48.484
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.077358029438728
    mean_inference_ms: 1.3796020257362456
    mean_processing_ms: 3.2993421367483333
  time_since_restore: 71.33852458000183
  time_this_iter_s: 8.218487024307251
  time_total_s: 71.33852458000183
  timestamp: 1563924306
  timesteps_since_restore: 326700
  timesteps_this_iter: 36300
  timesteps_total: 326700
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 71 s, 9 iter, 326700 ts, -5.07 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.29030608367786
  episode_reward_mean: -4.846644047477425
  episode_reward_min: -41.18533998097239
  episodes_this_iter: 242
  episodes_total: 2420
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5846.523
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7670888900756836
        kl: 0.010484411381185055
        policy_loss: -0.008459445089101791
        total_loss: 26.407405853271484
        vf_explained_var: 0.584267795085907
        vf_loss: 26.414554595947266
    load_time_ms: 3.642
    num_steps_sampled: 363000
    num_steps_trained: 360000
    sample_time_ms: 1905.984
    update_time_ms: 44.034
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.075422006798278
    mean_inference_ms: 1.3781276910195979
    mean_processing_ms: 3.3002256248427573
  time_since_restore: 78.23844194412231
  time_this_iter_s: 6.899917364120483
  time_total_s: 78.23844194412231
  timestamp: 1563924313
  timesteps_since_restore: 363000
  timesteps_this_iter: 36300
  timesteps_total: 363000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 78 s, 10 iter, 363000 ts, -4.85 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.832031974709306
  episode_reward_mean: -1.2556941123993262
  episode_reward_min: -29.34976249951786
  episodes_this_iter: 242
  episodes_total: 2662
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5923.842
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.75490665435791
        kl: 0.008159193210303783
        policy_loss: -0.00838145986199379
        total_loss: 21.651283264160156
        vf_explained_var: 0.6166914105415344
        vf_loss: 21.65864372253418
    load_time_ms: 1.648
    num_steps_sampled: 399300
    num_steps_trained: 396000
    sample_time_ms: 1884.032
    update_time_ms: 3.757
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.072763761763255
    mean_inference_ms: 1.376639356364368
    mean_processing_ms: 3.307372237961591
  time_since_restore: 87.50484728813171
  time_this_iter_s: 9.2664053440094
  time_total_s: 87.50484728813171
  timestamp: 1563924322
  timesteps_since_restore: 399300
  timesteps_this_iter: 36300
  timesteps_total: 399300
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 87 s, 11 iter, 399300 ts, -1.26 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.370931503805565
  episode_reward_mean: -1.8084840907399287
  episode_reward_min: -30.579142735559227
  episodes_this_iter: 242
  episodes_total: 2904
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6120.548
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.747321844100952
        kl: 0.008607005700469017
        policy_loss: -0.008017061278223991
        total_loss: 19.69047737121582
        vf_explained_var: 0.6475461721420288
        vf_loss: 19.697416305541992
    load_time_ms: 1.667
    num_steps_sampled: 435600
    num_steps_trained: 432000
    sample_time_ms: 1882.867
    update_time_ms: 3.735
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0653955698106765
    mean_inference_ms: 1.3741404471294252
    mean_processing_ms: 3.305594684316301
  time_since_restore: 96.35429644584656
  time_this_iter_s: 8.849449157714844
  time_total_s: 96.35429644584656
  timestamp: 1563924331
  timesteps_since_restore: 435600
  timesteps_this_iter: 36300
  timesteps_total: 435600
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 96 s, 12 iter, 435600 ts, -1.81 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.672511113054565
  episode_reward_mean: 0.33907878341151204
  episode_reward_min: -27.248498843167962
  episodes_this_iter: 242
  episodes_total: 3146
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6258.52
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7338175773620605
        kl: 0.010058272629976273
        policy_loss: -0.008572597056627274
        total_loss: 18.990867614746094
        vf_explained_var: 0.6303492784500122
        vf_loss: 18.998180389404297
    load_time_ms: 1.668
    num_steps_sampled: 471900
    num_steps_trained: 468000
    sample_time_ms: 1745.498
    update_time_ms: 3.804
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.058850737992815
    mean_inference_ms: 1.3725133606796214
    mean_processing_ms: 3.3017704228535396
  time_since_restore: 104.60096168518066
  time_this_iter_s: 8.246665239334106
  time_total_s: 104.60096168518066
  timestamp: 1563924339
  timesteps_since_restore: 471900
  timesteps_this_iter: 36300
  timesteps_total: 471900
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 104 s, 13 iter, 471900 ts, 0.339 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.63520556319812
  episode_reward_mean: 1.639194473772675
  episode_reward_min: -25.265973930274026
  episodes_this_iter: 242
  episodes_total: 3388
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6337.347
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.727325916290283
        kl: 0.008220144547522068
        policy_loss: -0.008511900901794434
        total_loss: 16.493539810180664
        vf_explained_var: 0.6630051136016846
        vf_loss: 16.501026153564453
    load_time_ms: 1.668
    num_steps_sampled: 508200
    num_steps_trained: 504000
    sample_time_ms: 1877.25
    update_time_ms: 3.905
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.052928112936353
    mean_inference_ms: 1.3710974722505718
    mean_processing_ms: 3.3020470309996366
  time_since_restore: 113.62693786621094
  time_this_iter_s: 9.025976181030273
  time_total_s: 113.62693786621094
  timestamp: 1563924348
  timesteps_since_restore: 508200
  timesteps_this_iter: 36300
  timesteps_total: 508200
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 113 s, 14 iter, 508200 ts, 1.64 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-25-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.159442999040312
  episode_reward_mean: 4.744722949845778
  episode_reward_min: -25.155595481626357
  episodes_this_iter: 242
  episodes_total: 3630
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6334.601
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.718801975250244
        kl: 0.009526706300675869
        policy_loss: -0.01134925615042448
        total_loss: 16.358863830566406
        vf_explained_var: 0.6842280030250549
        vf_loss: 16.369022369384766
    load_time_ms: 1.641
    num_steps_sampled: 544500
    num_steps_trained: 540000
    sample_time_ms: 1876.581
    update_time_ms: 3.978
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.047433460739988
    mean_inference_ms: 1.3713027954871964
    mean_processing_ms: 3.3009960112047985
  time_since_restore: 122.47739124298096
  time_this_iter_s: 8.85045337677002
  time_total_s: 122.47739124298096
  timestamp: 1563924357
  timesteps_since_restore: 544500
  timesteps_this_iter: 36300
  timesteps_total: 544500
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 122 s, 15 iter, 544500 ts, 4.74 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.175254666904536
  episode_reward_mean: 5.3572062194190515
  episode_reward_min: -29.398654340457238
  episodes_this_iter: 242
  episodes_total: 3872
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6336.9
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.709839344024658
        kl: 0.009404268115758896
        policy_loss: -0.010516729205846786
        total_loss: 12.359260559082031
        vf_explained_var: 0.7502275109291077
        vf_loss: 12.36860179901123
    load_time_ms: 1.649
    num_steps_sampled: 580800
    num_steps_trained: 576000
    sample_time_ms: 1744.193
    update_time_ms: 4.058
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.042828389342941
    mean_inference_ms: 1.370717840217552
    mean_processing_ms: 3.2983071998860303
  time_since_restore: 129.36464619636536
  time_this_iter_s: 6.887254953384399
  time_total_s: 129.36464619636536
  timestamp: 1563924364
  timesteps_since_restore: 580800
  timesteps_this_iter: 36300
  timesteps_total: 580800
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 129 s, 16 iter, 580800 ts, 5.36 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.148033179784548
  episode_reward_mean: 6.382254667324695
  episode_reward_min: -27.434282012059274
  episodes_this_iter: 242
  episodes_total: 4114
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6318.045
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6945769786834717
        kl: 0.009098153561353683
        policy_loss: -0.010006820783019066
        total_loss: 9.60438060760498
        vf_explained_var: 0.7900134921073914
        vf_loss: 9.613248825073242
    load_time_ms: 1.654
    num_steps_sampled: 617100
    num_steps_trained: 612000
    sample_time_ms: 1879.108
    update_time_ms: 4.061
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.043641669059653
    mean_inference_ms: 1.3708402958163282
    mean_processing_ms: 3.301067993438721
  time_since_restore: 137.62425231933594
  time_this_iter_s: 8.259606122970581
  time_total_s: 137.62425231933594
  timestamp: 1563924372
  timesteps_since_restore: 617100
  timesteps_this_iter: 36300
  timesteps_total: 617100
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 137 s, 17 iter, 617100 ts, 6.38 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.88567344498276
  episode_reward_mean: 8.77769335417544
  episode_reward_min: -18.11174691311051
  episodes_this_iter: 242
  episodes_total: 4356
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6294.176
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.685276508331299
        kl: 0.010488968342542648
        policy_loss: -0.011684213764965534
        total_loss: 7.4927191734313965
        vf_explained_var: 0.82329922914505
        vf_loss: 7.503092288970947
    load_time_ms: 1.638
    num_steps_sampled: 653400
    num_steps_trained: 648000
    sample_time_ms: 1881.074
    update_time_ms: 4.05
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.042944984762393
    mean_inference_ms: 1.3705452293817029
    mean_processing_ms: 3.3025793524096554
  time_since_restore: 145.1469075679779
  time_this_iter_s: 7.522655248641968
  time_total_s: 145.1469075679779
  timestamp: 1563924380
  timesteps_since_restore: 653400
  timesteps_this_iter: 36300
  timesteps_total: 653400
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 145 s, 18 iter, 653400 ts, 8.78 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.97841707723748
  episode_reward_mean: 8.978577497405093
  episode_reward_min: -33.68686346408608
  episodes_this_iter: 242
  episodes_total: 4598
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6411.967
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.671236991882324
        kl: 0.009006187319755554
        policy_loss: -0.008259696885943413
        total_loss: 6.532156467437744
        vf_explained_var: 0.8508027195930481
        vf_loss: 6.539290428161621
    load_time_ms: 1.64
    num_steps_sampled: 689700
    num_steps_trained: 684000
    sample_time_ms: 1881.506
    update_time_ms: 4.061
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.041354014868635
    mean_inference_ms: 1.3691691580127123
    mean_processing_ms: 3.304838735612504
  time_since_restore: 154.5461175441742
  time_this_iter_s: 9.399209976196289
  time_total_s: 154.5461175441742
  timestamp: 1563924389
  timesteps_since_restore: 689700
  timesteps_this_iter: 36300
  timesteps_total: 689700
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 154 s, 19 iter, 689700 ts, 8.98 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.141390989028334
  episode_reward_mean: 8.787658225001916
  episode_reward_min: -12.816608547284945
  episodes_this_iter: 242
  episodes_total: 4840
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6607.861
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6591475009918213
        kl: 0.009552281349897385
        policy_loss: -0.01104883011430502
        total_loss: 5.067827224731445
        vf_explained_var: 0.8678145408630371
        vf_loss: 5.077682018280029
    load_time_ms: 1.65
    num_steps_sampled: 726000
    num_steps_trained: 720000
    sample_time_ms: 1881.321
    update_time_ms: 4.025
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0400835821328345
    mean_inference_ms: 1.368332836830258
    mean_processing_ms: 3.3068570502326327
  time_since_restore: 163.4071490764618
  time_this_iter_s: 8.861031532287598
  time_total_s: 163.4071490764618
  timestamp: 1563924398
  timesteps_since_restore: 726000
  timesteps_this_iter: 36300
  timesteps_total: 726000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 163 s, 20 iter, 726000 ts, 8.79 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.114235553772303
  episode_reward_mean: 9.511898937895323
  episode_reward_min: -12.523917032622236
  episodes_this_iter: 242
  episodes_total: 5082
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6708.981
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6429481506347656
        kl: 0.009796182624995708
        policy_loss: -0.009328174404799938
        total_loss: 3.920100688934326
        vf_explained_var: 0.8913025259971619
        vf_loss: 3.9282052516937256
    load_time_ms: 1.659
    num_steps_sampled: 762300
    num_steps_trained: 756000
    sample_time_ms: 1745.386
    update_time_ms: 4.187
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0373722132686805
    mean_inference_ms: 1.367638202325387
    mean_processing_ms: 3.3050770421937856
  time_since_restore: 172.3327329158783
  time_this_iter_s: 8.925583839416504
  time_total_s: 172.3327329158783
  timestamp: 1563924407
  timesteps_since_restore: 762300
  timesteps_this_iter: 36300
  timesteps_total: 762300
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 172 s, 21 iter, 762300 ts, 9.51 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-26-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.27510623225596
  episode_reward_mean: 9.692477616869601
  episode_reward_min: -12.262075051909548
  episodes_this_iter: 242
  episodes_total: 5324
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6706.72
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.632026195526123
        kl: 0.008907991461455822
        policy_loss: -0.009827375411987305
        total_loss: 2.6580300331115723
        vf_explained_var: 0.9279459714889526
        vf_loss: 2.666743755340576
    load_time_ms: 1.667
    num_steps_sampled: 798600
    num_steps_trained: 792000
    sample_time_ms: 1880.263
    update_time_ms: 4.33
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.035066927768573
    mean_inference_ms: 1.3684408395485776
    mean_processing_ms: 3.3041729989229554
  time_since_restore: 182.5062596797943
  time_this_iter_s: 10.173526763916016
  time_total_s: 182.5062596797943
  timestamp: 1563924417
  timesteps_since_restore: 798600
  timesteps_this_iter: 36300
  timesteps_total: 798600
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 182 s, 22 iter, 798600 ts, 9.69 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.016536143113132
  episode_reward_mean: 10.167420700981346
  episode_reward_min: -14.688797586722552
  episodes_this_iter: 242
  episodes_total: 5566
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6571.24
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6246461868286133
        kl: 0.009616486728191376
        policy_loss: -0.009165345691144466
        total_loss: 2.55495548248291
        vf_explained_var: 0.925048291683197
        vf_loss: 2.5629186630249023
    load_time_ms: 1.67
    num_steps_sampled: 834900
    num_steps_trained: 828000
    sample_time_ms: 1879.191
    update_time_ms: 4.34
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.031566319311509
    mean_inference_ms: 1.367000807476302
    mean_processing_ms: 3.3026129784408615
  time_since_restore: 189.38381934165955
  time_this_iter_s: 6.877559661865234
  time_total_s: 189.38381934165955
  timestamp: 1563924424
  timesteps_since_restore: 834900
  timesteps_this_iter: 36300
  timesteps_total: 834900
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 189 s, 23 iter, 834900 ts, 10.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.813373755687422
  episode_reward_mean: 10.345364656400339
  episode_reward_min: -13.142815083600286
  episodes_this_iter: 242
  episodes_total: 5808
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6491.17
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6106786727905273
        kl: 0.009562131017446518
        policy_loss: -0.009180929511785507
        total_loss: 2.2078301906585693
        vf_explained_var: 0.9346440434455872
        vf_loss: 2.215815782546997
    load_time_ms: 1.669
    num_steps_sampled: 871200
    num_steps_trained: 864000
    sample_time_ms: 1747.263
    update_time_ms: 4.296
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.030879881135914
    mean_inference_ms: 1.3669919031680817
    mean_processing_ms: 3.3041573707273915
  time_since_restore: 196.28647351264954
  time_this_iter_s: 6.90265417098999
  time_total_s: 196.28647351264954
  timestamp: 1563924431
  timesteps_since_restore: 871200
  timesteps_this_iter: 36300
  timesteps_total: 871200
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 196 s, 24 iter, 871200 ts, 10.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.015864606480136
  episode_reward_mean: 11.286737612075244
  episode_reward_min: -10.292795960178236
  episodes_this_iter: 242
  episodes_total: 6050
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6496.348
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5928525924682617
        kl: 0.009328337386250496
        policy_loss: -0.009902115911245346
        total_loss: 1.85639488697052
        vf_explained_var: 0.9483924508094788
        vf_loss: 1.86513090133667
    load_time_ms: 1.68
    num_steps_sampled: 907500
    num_steps_trained: 900000
    sample_time_ms: 1888.866
    update_time_ms: 4.252
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.031656632985012
    mean_inference_ms: 1.3761727842638458
    mean_processing_ms: 3.3042681510052065
  time_since_restore: 206.60356903076172
  time_this_iter_s: 10.317095518112183
  time_total_s: 206.60356903076172
  timestamp: 1563924441
  timesteps_since_restore: 907500
  timesteps_this_iter: 36300
  timesteps_total: 907500
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 206 s, 25 iter, 907500 ts, 11.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.30292248591635
  episode_reward_mean: 10.790780242066411
  episode_reward_min: -9.05366300777744
  episodes_this_iter: 242
  episodes_total: 6292
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6516.153
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.578179121017456
        kl: 0.010382203385233879
        policy_loss: -0.011019420810043812
        total_loss: 1.7594972848892212
        vf_explained_var: 0.9501726627349854
        vf_loss: 1.769218921661377
    load_time_ms: 1.679
    num_steps_sampled: 943800
    num_steps_trained: 936000
    sample_time_ms: 1891.853
    update_time_ms: 4.227
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.029334262641727
    mean_inference_ms: 1.3792094234680945
    mean_processing_ms: 3.3015638273330667
  time_since_restore: 213.71914768218994
  time_this_iter_s: 7.115578651428223
  time_total_s: 213.71914768218994
  timestamp: 1563924449
  timesteps_since_restore: 943800
  timesteps_this_iter: 36300
  timesteps_total: 943800
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 213 s, 26 iter, 943800 ts, 10.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.894366099373126
  episode_reward_mean: 11.510597203558037
  episode_reward_min: -12.485357445235719
  episodes_this_iter: 242
  episodes_total: 6534
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6572.36
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.561411142349243
        kl: 0.010963205248117447
        policy_loss: -0.010392315685749054
        total_loss: 1.7879475355148315
        vf_explained_var: 0.9497225880622864
        vf_loss: 1.7969692945480347
    load_time_ms: 1.667
    num_steps_sampled: 980100
    num_steps_trained: 972000
    sample_time_ms: 1889.827
    update_time_ms: 4.275
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.029315351076913
    mean_inference_ms: 1.3774875034943554
    mean_processing_ms: 3.304730129124846
  time_since_restore: 222.5229012966156
  time_this_iter_s: 8.80375361442566
  time_total_s: 222.5229012966156
  timestamp: 1563924457
  timesteps_since_restore: 980100
  timesteps_this_iter: 36300
  timesteps_total: 980100
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 222 s, 27 iter, 980100 ts, 11.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.893208162320068
  episode_reward_mean: 11.988393938426574
  episode_reward_min: -10.37484352595034
  episodes_this_iter: 242
  episodes_total: 6776
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6573.338
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5542187690734863
        kl: 0.010631274431943893
        policy_loss: -0.00941892433911562
        total_loss: 1.3571966886520386
        vf_explained_var: 0.9604504108428955
        vf_loss: 1.3652867078781128
    load_time_ms: 1.665
    num_steps_sampled: 1016400
    num_steps_trained: 1008000
    sample_time_ms: 1886.768
    update_time_ms: 4.296
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.027821119316799
    mean_inference_ms: 1.3771098432912667
    mean_processing_ms: 3.3051848117060647
  time_since_restore: 230.02264666557312
  time_this_iter_s: 7.4997453689575195
  time_total_s: 230.02264666557312
  timestamp: 1563924465
  timesteps_since_restore: 1016400
  timesteps_this_iter: 36300
  timesteps_total: 1016400
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 230 s, 28 iter, 1016400 ts, 12 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-27-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.088808535276975
  episode_reward_mean: 11.89324172247609
  episode_reward_min: -11.919264000261066
  episodes_this_iter: 242
  episodes_total: 7018
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6650.011
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.531182050704956
        kl: 0.010970077477395535
        policy_loss: -0.011547734029591084
        total_loss: 1.4604203701019287
        vf_explained_var: 0.9574716091156006
        vf_loss: 1.4705969095230103
    load_time_ms: 1.666
    num_steps_sampled: 1052700
    num_steps_trained: 1044000
    sample_time_ms: 1750.019
    update_time_ms: 4.242
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.026167700925671
    mean_inference_ms: 1.3762745255729472
    mean_processing_ms: 3.3053966824799006
  time_since_restore: 238.82780599594116
  time_this_iter_s: 8.805159330368042
  time_total_s: 238.82780599594116
  timestamp: 1563924474
  timesteps_since_restore: 1052700
  timesteps_this_iter: 36300
  timesteps_total: 1052700
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 238 s, 29 iter, 1052700 ts, 11.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.3158356120315
  episode_reward_mean: 11.642651265508398
  episode_reward_min: -9.911577691372624
  episodes_this_iter: 242
  episodes_total: 7260
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6652.274
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5170998573303223
        kl: 0.009623990394175053
        policy_loss: -0.009043719619512558
        total_loss: 1.3020435571670532
        vf_explained_var: 0.959208607673645
        vf_loss: 1.3098843097686768
    load_time_ms: 1.666
    num_steps_sampled: 1089000
    num_steps_trained: 1080000
    sample_time_ms: 1883.438
    update_time_ms: 4.34
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.026309558911173
    mean_inference_ms: 1.375649529908523
    mean_processing_ms: 3.3060313973837747
  time_since_restore: 249.04558491706848
  time_this_iter_s: 10.21777892112732
  time_total_s: 249.04558491706848
  timestamp: 1563924484
  timesteps_since_restore: 1089000
  timesteps_this_iter: 36300
  timesteps_total: 1089000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 249 s, 30 iter, 1089000 ts, 11.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.19926441476001
  episode_reward_mean: 12.755781103364196
  episode_reward_min: -8.123145825866601
  episodes_this_iter: 242
  episodes_total: 7502
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6445.067
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5130600929260254
        kl: 0.009344743564724922
        policy_loss: -0.009580236859619617
        total_loss: 1.1580604314804077
        vf_explained_var: 0.965260922908783
        vf_loss: 1.1664725542068481
    load_time_ms: 1.654
    num_steps_sampled: 1125300
    num_steps_trained: 1116000
    sample_time_ms: 1882.127
    update_time_ms: 4.324
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.024331467296889
    mean_inference_ms: 1.3749492882227432
    mean_processing_ms: 3.3043686119031532
  time_since_restore: 255.8793261051178
  time_this_iter_s: 6.833741188049316
  time_total_s: 255.8793261051178
  timestamp: 1563924491
  timesteps_since_restore: 1125300
  timesteps_this_iter: 36300
  timesteps_total: 1125300
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 255 s, 31 iter, 1125300 ts, 12.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.96134550178328
  episode_reward_mean: 13.328914541184414
  episode_reward_min: -6.498146944710472
  episodes_this_iter: 242
  episodes_total: 7744
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6447.231
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4954421520233154
        kl: 0.009751716628670692
        policy_loss: -0.010908666998147964
        total_loss: 1.0786898136138916
        vf_explained_var: 0.9682870507240295
        vf_loss: 1.0883792638778687
    load_time_ms: 1.645
    num_steps_sampled: 1161600
    num_steps_trained: 1152000
    sample_time_ms: 1749.968
    update_time_ms: 4.182
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.024108370236553
    mean_inference_ms: 1.374598448199456
    mean_processing_ms: 3.3040180100701306
  time_since_restore: 264.7540850639343
  time_this_iter_s: 8.874758958816528
  time_total_s: 264.7540850639343
  timestamp: 1563924500
  timesteps_since_restore: 1161600
  timesteps_this_iter: 36300
  timesteps_total: 1161600
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 264 s, 32 iter, 1161600 ts, 13.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.21594866790204
  episode_reward_mean: 13.389339914465449
  episode_reward_min: -9.286272685096952
  episodes_this_iter: 242
  episodes_total: 7986
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6644.148
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4778337478637695
        kl: 0.008556680753827095
        policy_loss: -0.009691250510513783
        total_loss: 1.0283653736114502
        vf_explained_var: 0.9702222347259521
        vf_loss: 1.036987066268921
    load_time_ms: 1.648
    num_steps_sampled: 1197900
    num_steps_trained: 1188000
    sample_time_ms: 1885.011
    update_time_ms: 4.254
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.023389035741369
    mean_inference_ms: 1.3743038556643334
    mean_processing_ms: 3.3048150084332817
  time_since_restore: 274.9575126171112
  time_this_iter_s: 10.20342755317688
  time_total_s: 274.9575126171112
  timestamp: 1563924510
  timesteps_since_restore: 1197900
  timesteps_this_iter: 36300
  timesteps_total: 1197900
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 274 s, 33 iter, 1197900 ts, 13.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.32390496113801
  episode_reward_mean: 13.436257879785462
  episode_reward_min: -7.599508486714982
  episodes_this_iter: 242
  episodes_total: 8228
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6645.821
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4644179344177246
        kl: 0.011259272694587708
        policy_loss: -0.010301758535206318
        total_loss: 0.9735428690910339
        vf_explained_var: 0.9716165065765381
        vf_loss: 0.982437252998352
    load_time_ms: 1.641
    num_steps_sampled: 1234200
    num_steps_trained: 1224000
    sample_time_ms: 1882.286
    update_time_ms: 4.4
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.021207200043947
    mean_inference_ms: 1.3733732191762993
    mean_processing_ms: 3.304001739715802
  time_since_restore: 281.85256695747375
  time_this_iter_s: 6.895054340362549
  time_total_s: 281.85256695747375
  timestamp: 1563924517
  timesteps_since_restore: 1234200
  timesteps_this_iter: 36300
  timesteps_total: 1234200
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 281 s, 34 iter, 1234200 ts, 13.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.7606794650996
  episode_reward_mean: 12.825531751930056
  episode_reward_min: -6.979499499762432
  episodes_this_iter: 242
  episodes_total: 8470
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6503.944
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4436867237091064
        kl: 0.011187099851667881
        policy_loss: -0.010511821135878563
        total_loss: 0.9266300201416016
        vf_explained_var: 0.9719370603561401
        vf_loss: 0.9357433319091797
    load_time_ms: 1.646
    num_steps_sampled: 1270500
    num_steps_trained: 1260000
    sample_time_ms: 1875.337
    update_time_ms: 4.354
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.022237951176055
    mean_inference_ms: 1.3735033606272844
    mean_processing_ms: 3.3063433979711068
  time_since_restore: 290.67973279953003
  time_this_iter_s: 8.827165842056274
  time_total_s: 290.67973279953003
  timestamp: 1563924526
  timesteps_since_restore: 1270500
  timesteps_this_iter: 36300
  timesteps_total: 1270500
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 290 s, 35 iter, 1270500 ts, 12.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-28-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.56167614490569
  episode_reward_mean: 14.489095527602489
  episode_reward_min: -7.471610147983992
  episodes_this_iter: 242
  episodes_total: 8712
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6680.841
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.43243145942688
        kl: 0.01249998901039362
        policy_loss: -0.011408093385398388
        total_loss: 0.8417004942893982
        vf_explained_var: 0.97645503282547
        vf_loss: 0.851546049118042
    load_time_ms: 1.64
    num_steps_sampled: 1306800
    num_steps_trained: 1296000
    sample_time_ms: 1873.165
    update_time_ms: 4.274
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.020876827883317
    mean_inference_ms: 1.3728696633543382
    mean_processing_ms: 3.3057192452575626
  time_since_restore: 299.5443630218506
  time_this_iter_s: 8.864630222320557
  time_total_s: 299.5443630218506
  timestamp: 1563924535
  timesteps_since_restore: 1306800
  timesteps_this_iter: 36300
  timesteps_total: 1306800
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 299 s, 36 iter, 1306800 ts, 14.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.320682196864375
  episode_reward_mean: 14.129744315063602
  episode_reward_min: -8.967839336676475
  episodes_this_iter: 242
  episodes_total: 8954
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6625.996
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.420628547668457
        kl: 0.010966569185256958
        policy_loss: -0.010619105771183968
        total_loss: 0.8216482996940613
        vf_explained_var: 0.9755477905273438
        vf_loss: 0.8308964371681213
    load_time_ms: 1.651
    num_steps_sampled: 1343100
    num_steps_trained: 1332000
    sample_time_ms: 1736.74
    update_time_ms: 4.444
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.018882805357681
    mean_inference_ms: 1.3723453555451197
    mean_processing_ms: 3.305225445062984
  time_since_restore: 306.43491673469543
  time_this_iter_s: 6.890553712844849
  time_total_s: 306.43491673469543
  timestamp: 1563924541
  timesteps_since_restore: 1343100
  timesteps_this_iter: 36300
  timesteps_total: 1343100
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 306 s, 37 iter, 1343100 ts, 14.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.30404587047637
  episode_reward_mean: 13.537091172507944
  episode_reward_min: -7.953674604142646
  episodes_this_iter: 242
  episodes_total: 9196
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6760.758
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4064180850982666
        kl: 0.009891562163829803
        policy_loss: -0.012479608878493309
        total_loss: 0.7588953375816345
        vf_explained_var: 0.9773488640785217
        vf_loss: 0.7701385021209717
    load_time_ms: 1.655
    num_steps_sampled: 1379400
    num_steps_trained: 1368000
    sample_time_ms: 1873.273
    update_time_ms: 4.371
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0192307667094305
    mean_inference_ms: 1.3723635163488577
    mean_processing_ms: 3.3079154742679866
  time_since_restore: 316.6520619392395
  time_this_iter_s: 10.217145204544067
  time_total_s: 316.6520619392395
  timestamp: 1563924552
  timesteps_since_restore: 1379400
  timesteps_this_iter: 36300
  timesteps_total: 1379400
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 316 s, 38 iter, 1379400 ts, 13.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.65517446927786
  episode_reward_mean: 13.766282768222917
  episode_reward_min: -7.636078197885905
  episodes_this_iter: 242
  episodes_total: 9438
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6655.847
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.386915445327759
        kl: 0.012713511474430561
        policy_loss: -0.012107852846384048
        total_loss: 0.7690309286117554
        vf_explained_var: 0.977509617805481
        vf_loss: 0.7795495986938477
    load_time_ms: 1.658
    num_steps_sampled: 1415700
    num_steps_trained: 1404000
    sample_time_ms: 1873.588
    update_time_ms: 4.511
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.018309996316465
    mean_inference_ms: 1.3720474135500993
    mean_processing_ms: 3.307482229083537
  time_since_restore: 324.4081337451935
  time_this_iter_s: 7.7560718059539795
  time_total_s: 324.4081337451935
  timestamp: 1563924559
  timesteps_since_restore: 1415700
  timesteps_this_iter: 36300
  timesteps_total: 1415700
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 324 s, 39 iter, 1415700 ts, 13.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.63026922234128
  episode_reward_mean: 14.409887331444684
  episode_reward_min: -6.2150624955508915
  episodes_this_iter: 242
  episodes_total: 9680
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6459.087
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.371410608291626
        kl: 0.01220010407269001
        policy_loss: -0.011295945383608341
        total_loss: 0.7157502770423889
        vf_explained_var: 0.979167640209198
        vf_loss: 0.7255210280418396
    load_time_ms: 1.651
    num_steps_sampled: 1452000
    num_steps_trained: 1440000
    sample_time_ms: 1738.016
    update_time_ms: 4.425
  iterations_since_restore: 40
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.017075327969699
    mean_inference_ms: 1.3715251194183116
    mean_processing_ms: 3.3073830227070053
  time_since_restore: 331.2986309528351
  time_this_iter_s: 6.890497207641602
  time_total_s: 331.2986309528351
  timestamp: 1563924566
  timesteps_since_restore: 1452000
  timesteps_this_iter: 36300
  timesteps_total: 1452000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 331 s, 40 iter, 1452000 ts, 14.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.065842998012016
  episode_reward_mean: 13.386230677435735
  episode_reward_min: -6.485014529623197
  episodes_this_iter: 242
  episodes_total: 9922
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6660.561
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.354201078414917
        kl: 0.009844460524618626
        policy_loss: -0.01167106069624424
        total_loss: 0.7269986271858215
        vf_explained_var: 0.9776487350463867
        vf_loss: 0.7374391555786133
    load_time_ms: 1.649
    num_steps_sampled: 1488300
    num_steps_trained: 1476000
    sample_time_ms: 1875.783
    update_time_ms: 4.327
  iterations_since_restore: 41
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.017858806069947
    mean_inference_ms: 1.3718741105433971
    mean_processing_ms: 3.3091762416921235
  time_since_restore: 341.52830266952515
  time_this_iter_s: 10.229671716690063
  time_total_s: 341.52830266952515
  timestamp: 1563924577
  timesteps_since_restore: 1488300
  timesteps_this_iter: 36300
  timesteps_total: 1488300
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 341 s, 41 iter, 1488300 ts, 13.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.5460591860771
  episode_reward_mean: 15.651710752559012
  episode_reward_min: -5.0787379531366605
  episodes_this_iter: 242
  episodes_total: 10164
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6483.958
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3419995307922363
        kl: 0.011197886429727077
        policy_loss: -0.011129659600555897
        total_loss: 0.6408182978630066
        vf_explained_var: 0.9825127124786377
        vf_loss: 0.6505481004714966
    load_time_ms: 1.654
    num_steps_sampled: 1524600
    num_steps_trained: 1512000
    sample_time_ms: 1870.968
    update_time_ms: 4.403
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.016456943680197
    mean_inference_ms: 1.3713394961210796
    mean_processing_ms: 3.3083693301693518
  time_since_restore: 348.58692717552185
  time_this_iter_s: 7.058624505996704
  time_total_s: 348.58692717552185
  timestamp: 1563924584
  timesteps_since_restore: 1524600
  timesteps_this_iter: 36300
  timesteps_total: 1524600
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 348 s, 42 iter, 1524600 ts, 15.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-29-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.13727501316338
  episode_reward_mean: 14.46202876739017
  episode_reward_min: -7.250451446036694
  episodes_this_iter: 242
  episodes_total: 10406
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6481.898
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3295845985412598
        kl: 0.010546073317527771
        policy_loss: -0.011479911394417286
        total_loss: 0.68222576379776
        vf_explained_var: 0.9796478748321533
        vf_loss: 0.6923874020576477
    load_time_ms: 1.654
    num_steps_sampled: 1560900
    num_steps_trained: 1548000
    sample_time_ms: 1872.521
    update_time_ms: 4.233
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.01670366576519
    mean_inference_ms: 1.3709355524689593
    mean_processing_ms: 3.3098024924688456
  time_since_restore: 358.7828137874603
  time_this_iter_s: 10.195886611938477
  time_total_s: 358.7828137874603
  timestamp: 1563924594
  timesteps_since_restore: 1560900
  timesteps_this_iter: 36300
  timesteps_total: 1560900
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 358 s, 43 iter, 1560900 ts, 14.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.683167772365884
  episode_reward_mean: 14.125881285119968
  episode_reward_min: -6.258208580948644
  episodes_this_iter: 242
  episodes_total: 10648
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6478.857
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3151400089263916
        kl: 0.01225653663277626
        policy_loss: -0.012261860072612762
        total_loss: 0.6540019512176514
        vf_explained_var: 0.9805543422698975
        vf_loss: 0.6647317409515381
    load_time_ms: 1.66
    num_steps_sampled: 1597200
    num_steps_trained: 1584000
    sample_time_ms: 1873.324
    update_time_ms: 4.224
  iterations_since_restore: 44
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.015430962761926
    mean_inference_ms: 1.3706427956049987
    mean_processing_ms: 3.309014116841032
  time_since_restore: 365.6547648906708
  time_this_iter_s: 6.871951103210449
  time_total_s: 365.6547648906708
  timestamp: 1563924601
  timesteps_since_restore: 1597200
  timesteps_this_iter: 36300
  timesteps_total: 1597200
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 365 s, 44 iter, 1597200 ts, 14.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.13228846502215
  episode_reward_mean: 14.00630206840404
  episode_reward_min: -5.307019474165445
  episodes_this_iter: 242
  episodes_total: 10890
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6438.331
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.300528049468994
        kl: 0.012359478510916233
        policy_loss: -0.013508312404155731
        total_loss: 0.5992205142974854
        vf_explained_var: 0.9811556339263916
        vf_loss: 0.6111838817596436
    load_time_ms: 1.647
    num_steps_sampled: 1633500
    num_steps_trained: 1620000
    sample_time_ms: 1740.197
    update_time_ms: 4.252
  iterations_since_restore: 45
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.015494893211963
    mean_inference_ms: 1.3706328247576198
    mean_processing_ms: 3.309405819074592
  time_since_restore: 372.7424247264862
  time_this_iter_s: 7.08765983581543
  time_total_s: 372.7424247264862
  timestamp: 1563924608
  timesteps_since_restore: 1633500
  timesteps_this_iter: 36300
  timesteps_total: 1633500
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 372 s, 45 iter, 1633500 ts, 14 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.84787962179083
  episode_reward_mean: 14.62527234158149
  episode_reward_min: -4.986413975423486
  episodes_this_iter: 242
  episodes_total: 11132
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6422.781
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2752082347869873
        kl: 0.012177512049674988
        policy_loss: -0.012773755937814713
        total_loss: 0.6323296427726746
        vf_explained_var: 0.982376217842102
        vf_loss: 0.643581211566925
    load_time_ms: 1.662
    num_steps_sampled: 1669800
    num_steps_trained: 1656000
    sample_time_ms: 1877.032
    update_time_ms: 4.139
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.015997044432475
    mean_inference_ms: 1.3704286494534412
    mean_processing_ms: 3.310338366618713
  time_since_restore: 382.81718134880066
  time_this_iter_s: 10.074756622314453
  time_total_s: 382.81718134880066
  timestamp: 1563924618
  timesteps_since_restore: 1669800
  timesteps_this_iter: 36300
  timesteps_total: 1669800
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 382 s, 46 iter, 1669800 ts, 14.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.477269299160426
  episode_reward_mean: 13.884147197979303
  episode_reward_min: -5.7178974646014495
  episodes_this_iter: 242
  episodes_total: 11374
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6419.109
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.267416477203369
        kl: 0.011687024496495724
        policy_loss: -0.010965794324874878
        total_loss: 0.5819576978683472
        vf_explained_var: 0.9808380603790283
        vf_loss: 0.5914625525474548
    load_time_ms: 1.664
    num_steps_sampled: 1706100
    num_steps_trained: 1692000
    sample_time_ms: 1877.928
    update_time_ms: 3.993
  iterations_since_restore: 47
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.014363808878671
    mean_inference_ms: 1.3701274221811517
    mean_processing_ms: 3.310352468000254
  time_since_restore: 389.67816734313965
  time_this_iter_s: 6.860985994338989
  time_total_s: 389.67816734313965
  timestamp: 1563924625
  timesteps_since_restore: 1706100
  timesteps_this_iter: 36300
  timesteps_total: 1706100
  training_iteration: 47
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 389 s, 47 iter, 1706100 ts, 13.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.054062107512195
  episode_reward_mean: 14.779585286249622
  episode_reward_min: -7.371781255575123
  episodes_this_iter: 242
  episodes_total: 11616
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6357.488
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2460432052612305
        kl: 0.012375760823488235
        policy_loss: -0.011637888848781586
        total_loss: 0.5725505948066711
        vf_explained_var: 0.9831910729408264
        vf_loss: 0.5826415419578552
    load_time_ms: 1.67
    num_steps_sampled: 1742400
    num_steps_trained: 1728000
    sample_time_ms: 1745.272
    update_time_ms: 4.012
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.014435387212874
    mean_inference_ms: 1.3700636203918337
    mean_processing_ms: 3.3110480817238988
  time_since_restore: 397.94979977607727
  time_this_iter_s: 8.271632432937622
  time_total_s: 397.94979977607727
  timestamp: 1563924633
  timesteps_since_restore: 1742400
  timesteps_this_iter: 36300
  timesteps_total: 1742400
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 397 s, 48 iter, 1742400 ts, 14.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.05141667476095
  episode_reward_mean: 14.664327138927378
  episode_reward_min: -6.79671126402178
  episodes_this_iter: 242
  episodes_total: 11858
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6269.68
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2299370765686035
        kl: 0.010875104926526546
        policy_loss: -0.010967861860990524
        total_loss: 0.5192200541496277
        vf_explained_var: 0.9833507537841797
        vf_loss: 0.5288283824920654
    load_time_ms: 1.671
    num_steps_sampled: 1778700
    num_steps_trained: 1764000
    sample_time_ms: 1880.706
    update_time_ms: 3.926
  iterations_since_restore: 49
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.014807009798735
    mean_inference_ms: 1.3693326183180083
    mean_processing_ms: 3.3116131669572355
  time_since_restore: 406.1791763305664
  time_this_iter_s: 8.229376554489136
  time_total_s: 406.1791763305664
  timestamp: 1563924641
  timesteps_since_restore: 1778700
  timesteps_this_iter: 36300
  timesteps_total: 1778700
  training_iteration: 49
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 406 s, 49 iter, 1778700 ts, 14.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.74061585894457
  episode_reward_mean: 14.216380354366965
  episode_reward_min: -6.156088831105074
  episodes_this_iter: 242
  episodes_total: 12100
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6290.368
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2163257598876953
        kl: 0.011816736310720444
        policy_loss: -0.011213401332497597
        total_loss: 0.5274612307548523
        vf_explained_var: 0.9833469390869141
        vf_loss: 0.537197470664978
    load_time_ms: 1.677
    num_steps_sampled: 1815000
    num_steps_trained: 1800000
    sample_time_ms: 1882.537
    update_time_ms: 3.892
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.015096020549249
    mean_inference_ms: 1.3690830651712935
    mean_processing_ms: 3.3125882426776574
  time_since_restore: 413.29615807533264
  time_this_iter_s: 7.116981744766235
  time_total_s: 413.29615807533264
  timestamp: 1563924649
  timesteps_since_restore: 1815000
  timesteps_this_iter: 36300
  timesteps_total: 1815000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 413 s, 50 iter, 1815000 ts, 14.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-30-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.43058034570888
  episode_reward_mean: 15.431927183459038
  episode_reward_min: -5.405533014457144
  episodes_this_iter: 242
  episodes_total: 12342
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6177.193
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.194269895553589
        kl: 0.01173827238380909
        policy_loss: -0.012793543748557568
        total_loss: 0.5007764101028442
        vf_explained_var: 0.9848076701164246
        vf_loss: 0.5121026039123535
    load_time_ms: 1.687
    num_steps_sampled: 1851300
    num_steps_trained: 1836000
    sample_time_ms: 1880.762
    update_time_ms: 3.917
  iterations_since_restore: 51
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.01529246412547
    mean_inference_ms: 1.3691169678925268
    mean_processing_ms: 3.313180576361939
  time_since_restore: 422.37479758262634
  time_this_iter_s: 9.078639507293701
  time_total_s: 422.37479758262634
  timestamp: 1563924658
  timesteps_since_restore: 1851300
  timesteps_this_iter: 36300
  timesteps_total: 1851300
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 422 s, 51 iter, 1851300 ts, 15.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.337068799845675
  episode_reward_mean: 15.940729691111466
  episode_reward_min: -5.220058071211105
  episodes_this_iter: 242
  episodes_total: 12584
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6157.576
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.184004545211792
        kl: 0.014002063311636448
        policy_loss: -0.011122006922960281
        total_loss: 0.465563029050827
        vf_explained_var: 0.9860413074493408
        vf_loss: 0.47493478655815125
    load_time_ms: 1.68
    num_steps_sampled: 1887600
    num_steps_trained: 1872000
    sample_time_ms: 1882.611
    update_time_ms: 3.884
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.014336392897365
    mean_inference_ms: 1.3687077866218973
    mean_processing_ms: 3.3124658568203955
  time_since_restore: 429.25426149368286
  time_this_iter_s: 6.8794639110565186
  time_total_s: 429.25426149368286
  timestamp: 1563924665
  timesteps_since_restore: 1887600
  timesteps_this_iter: 36300
  timesteps_total: 1887600
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 429 s, 52 iter, 1887600 ts, 15.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.25331694282502
  episode_reward_mean: 16.051070450287316
  episode_reward_min: -4.025431722548841
  episodes_this_iter: 242
  episodes_total: 12826
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6158.176
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.167389154434204
        kl: 0.013548348098993301
        policy_loss: -0.0127870487049222
        total_loss: 0.43166691064834595
        vf_explained_var: 0.9883612394332886
        vf_loss: 0.4427603781223297
    load_time_ms: 1.683
    num_steps_sampled: 1923900
    num_steps_trained: 1908000
    sample_time_ms: 1749.815
    update_time_ms: 3.86
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.014580923571559
    mean_inference_ms: 1.3686048686754237
    mean_processing_ms: 3.312876775116477
  time_since_restore: 438.1292417049408
  time_this_iter_s: 8.874980211257935
  time_total_s: 438.1292417049408
  timestamp: 1563924673
  timesteps_since_restore: 1923900
  timesteps_this_iter: 36300
  timesteps_total: 1923900
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 438 s, 53 iter, 1923900 ts, 16.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.679822847072984
  episode_reward_mean: 14.803280274959727
  episode_reward_min: -5.873921588484046
  episodes_this_iter: 242
  episodes_total: 13068
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6355.491
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.148850679397583
        kl: 0.012476934120059013
        policy_loss: -0.01376377884298563
        total_loss: 0.4354371726512909
        vf_explained_var: 0.9863213300704956
        vf_loss: 0.4476413130760193
    load_time_ms: 1.684
    num_steps_sampled: 1960200
    num_steps_trained: 1944000
    sample_time_ms: 1883.06
    update_time_ms: 3.82
  iterations_since_restore: 54
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.014308978467494
    mean_inference_ms: 1.3682085663202301
    mean_processing_ms: 3.311988906915217
  time_since_restore: 448.311856508255
  time_this_iter_s: 10.182614803314209
  time_total_s: 448.311856508255
  timestamp: 1563924684
  timesteps_since_restore: 1960200
  timesteps_this_iter: 36300
  timesteps_total: 1960200
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 448 s, 54 iter, 1960200 ts, 14.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.08475098248779
  episode_reward_mean: 15.381841344601233
  episode_reward_min: -6.0528360225839695
  episodes_this_iter: 242
  episodes_total: 13310
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6423.479
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.142449378967285
        kl: 0.014130507595837116
        policy_loss: -0.012445165775716305
        total_loss: 0.4260748624801636
        vf_explained_var: 0.9875032901763916
        vf_loss: 0.4367537200450897
    load_time_ms: 1.689
    num_steps_sampled: 1996500
    num_steps_trained: 1980000
    sample_time_ms: 1879.706
    update_time_ms: 3.868
  iterations_since_restore: 55
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.013130884991578
    mean_inference_ms: 1.3677068753626553
    mean_processing_ms: 3.3112756212316334
  time_since_restore: 456.0489196777344
  time_this_iter_s: 7.73706316947937
  time_total_s: 456.0489196777344
  timestamp: 1563924691
  timesteps_since_restore: 1996500
  timesteps_this_iter: 36300
  timesteps_total: 1996500
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 456 s, 55 iter, 1996500 ts, 15.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.157340247715766
  episode_reward_mean: 14.66525279249437
  episode_reward_min: -4.83921872790231
  episodes_this_iter: 242
  episodes_total: 13552
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6243.069
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.121066093444824
        kl: 0.013935287483036518
        policy_loss: -0.012897747568786144
        total_loss: 0.4319238066673279
        vf_explained_var: 0.9858378171920776
        vf_loss: 0.4430796205997467
    load_time_ms: 1.676
    num_steps_sampled: 2032800
    num_steps_trained: 2016000
    sample_time_ms: 1741.365
    update_time_ms: 3.906
  iterations_since_restore: 56
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.012020866241588
    mean_inference_ms: 1.3674830864235388
    mean_processing_ms: 3.3106950291580697
  time_since_restore: 462.93480610847473
  time_this_iter_s: 6.8858864307403564
  time_total_s: 462.93480610847473
  timestamp: 1563924698
  timesteps_since_restore: 2032800
  timesteps_this_iter: 36300
  timesteps_total: 2032800
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 462 s, 56 iter, 2032800 ts, 14.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.80505343817509
  episode_reward_mean: 15.558090867926914
  episode_reward_min: -4.920771944236724
  episodes_this_iter: 242
  episodes_total: 13794
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6352.685
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.101820707321167
        kl: 0.010504143312573433
        policy_loss: -0.013309524394571781
        total_loss: 0.38588231801986694
        vf_explained_var: 0.9879496693611145
        vf_loss: 0.39787882566452026
    load_time_ms: 1.669
    num_steps_sampled: 2069100
    num_steps_trained: 2052000
    sample_time_ms: 1880.093
    update_time_ms: 3.852
  iterations_since_restore: 57
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.01293651924558
    mean_inference_ms: 1.3675481292933893
    mean_processing_ms: 3.3119648294689314
  time_since_restore: 472.2815148830414
  time_this_iter_s: 9.34670877456665
  time_total_s: 472.2815148830414
  timestamp: 1563924708
  timesteps_since_restore: 2069100
  timesteps_this_iter: 36300
  timesteps_total: 2069100
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 472 s, 57 iter, 2069100 ts, 15.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-31-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.49047182775946
  episode_reward_mean: 15.808812839645373
  episode_reward_min: -5.806794230082342
  episodes_this_iter: 242
  episodes_total: 14036
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6411.957
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.092102289199829
        kl: 0.012881714850664139
        policy_loss: -0.012435177341103554
        total_loss: 0.3877629041671753
        vf_explained_var: 0.9888027906417847
        vf_loss: 0.3985878527164459
    load_time_ms: 1.663
    num_steps_sampled: 2105400
    num_steps_trained: 2088000
    sample_time_ms: 1878.376
    update_time_ms: 4.003
  iterations_since_restore: 58
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.012447993514468
    mean_inference_ms: 1.3673146994391274
    mean_processing_ms: 3.3114064579680558
  time_since_restore: 481.1315219402313
  time_this_iter_s: 8.850007057189941
  time_total_s: 481.1315219402313
  timestamp: 1563924716
  timesteps_since_restore: 2105400
  timesteps_this_iter: 36300
  timesteps_total: 2105400
  training_iteration: 58
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 481 s, 58 iter, 2105400 ts, 15.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-32-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.35787329368312
  episode_reward_mean: 16.19220961554326
  episode_reward_min: -5.567228865228576
  episodes_this_iter: 242
  episodes_total: 14278
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6612.516
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.076244831085205
        kl: 0.012678753584623337
        policy_loss: -0.01374292466789484
        total_loss: 0.3658629357814789
        vf_explained_var: 0.9894796013832092
        vf_loss: 0.3780210316181183
    load_time_ms: 1.662
    num_steps_sampled: 2141700
    num_steps_trained: 2124000
    sample_time_ms: 1876.017
    update_time_ms: 4.202
  iterations_since_restore: 59
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.012213553527823
    mean_inference_ms: 1.3668781882305516
    mean_processing_ms: 3.311518626001598
  time_since_restore: 491.35041189193726
  time_this_iter_s: 10.218889951705933
  time_total_s: 491.35041189193726
  timestamp: 1563924727
  timesteps_since_restore: 2141700
  timesteps_this_iter: 36300
  timesteps_total: 2141700
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 491 s, 59 iter, 2141700 ts, 16.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-32-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.71468828100627
  episode_reward_mean: 17.036704902728417
  episode_reward_min: -4.733257454500902
  episodes_this_iter: 242
  episodes_total: 14520
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6787.184
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0681161880493164
        kl: 0.013879581354558468
        policy_loss: -0.013772091828286648
        total_loss: 0.36258232593536377
        vf_explained_var: 0.9899734854698181
        vf_loss: 0.3746194839477539
    load_time_ms: 1.661
    num_steps_sampled: 2178000
    num_steps_trained: 2160000
    sample_time_ms: 1873.411
    update_time_ms: 4.378
  iterations_since_restore: 60
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.011346022372722
    mean_inference_ms: 1.3666142892409638
    mean_processing_ms: 3.3107103342528137
  time_since_restore: 500.19216561317444
  time_this_iter_s: 8.841753721237183
  time_total_s: 500.19216561317444
  timestamp: 1563924736
  timesteps_since_restore: 2178000
  timesteps_this_iter: 36300
  timesteps_total: 2178000
  training_iteration: 60
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 500 s, 60 iter, 2178000 ts, 17 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-32-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.87956709739961
  episode_reward_mean: 15.741149171653095
  episode_reward_min: -5.207001840993337
  episodes_this_iter: 242
  episodes_total: 14762
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6899.617
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0486929416656494
        kl: 0.014748419634997845
        policy_loss: -0.013436013832688332
        total_loss: 0.33637312054634094
        vf_explained_var: 0.9900397062301636
        vf_loss: 0.3479655683040619
    load_time_ms: 1.649
    num_steps_sampled: 2214300
    num_steps_trained: 2196000
    sample_time_ms: 1738.174
    update_time_ms: 4.423
  iterations_since_restore: 61
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.010432410378294
    mean_inference_ms: 1.3666559248353602
    mean_processing_ms: 3.309755422810114
  time_since_restore: 509.04526686668396
  time_this_iter_s: 8.853101253509521
  time_total_s: 509.04526686668396
  timestamp: 1563924744
  timesteps_since_restore: 2214300
  timesteps_this_iter: 36300
  timesteps_total: 2214300
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 509 s, 61 iter, 2214300 ts, 15.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-32-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.976701660533365
  episode_reward_mean: 16.65862031288824
  episode_reward_min: -4.029546886549044
  episodes_this_iter: 242
  episodes_total: 15004
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7097.108
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0380029678344727
        kl: 0.01297248899936676
        policy_loss: -0.014115293510258198
        total_loss: 0.3158830404281616
        vf_explained_var: 0.9904458522796631
        vf_loss: 0.32837677001953125
    load_time_ms: 1.646
    num_steps_sampled: 2250600
    num_steps_trained: 2232000
    sample_time_ms: 1873.666
    update_time_ms: 4.539
  iterations_since_restore: 62
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.010645853376961
    mean_inference_ms: 1.3665552468103
    mean_processing_ms: 3.3095486441588045
  time_since_restore: 519.2593252658844
  time_this_iter_s: 10.21405839920044
  time_total_s: 519.2593252658844
  timestamp: 1563924755
  timesteps_since_restore: 2250600
  timesteps_this_iter: 36300
  timesteps_total: 2250600
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 519 s, 62 iter, 2250600 ts, 16.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-32-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.476287010585004
  episode_reward_mean: 15.914857544815328
  episode_reward_min: -3.745753887589724
  episodes_this_iter: 242
  episodes_total: 15246
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7008.321
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0253193378448486
        kl: 0.01403515413403511
        policy_loss: -0.013062726706266403
        total_loss: 0.3128114640712738
        vf_explained_var: 0.9904562830924988
        vf_loss: 0.32411983609199524
    load_time_ms: 1.644
    num_steps_sampled: 2286900
    num_steps_trained: 2268000
    sample_time_ms: 1872.074
    update_time_ms: 4.707
  iterations_since_restore: 63
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.010124385106546
    mean_inference_ms: 1.366582322928288
    mean_processing_ms: 3.3088604259860985
  time_since_restore: 527.2282383441925
  time_this_iter_s: 7.9689130783081055
  time_total_s: 527.2282383441925
  timestamp: 1563924763
  timesteps_since_restore: 2286900
  timesteps_this_iter: 36300
  timesteps_total: 2286900
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 527 s, 63 iter, 2286900 ts, 15.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-32-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.153512346609915
  episode_reward_mean: 16.00481842024735
  episode_reward_min: -3.5475304233285927
  episodes_this_iter: 242
  episodes_total: 15488
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7011.224
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0135393142700195
        kl: 0.012655479833483696
        policy_loss: -0.011998389847576618
        total_loss: 0.30723845958709717
        vf_explained_var: 0.9904458522796631
        vf_loss: 0.31765487790107727
    load_time_ms: 1.646
    num_steps_sampled: 2323200
    num_steps_trained: 2304000
    sample_time_ms: 1739.003
    update_time_ms: 4.7
  iterations_since_restore: 64
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009281589630395
    mean_inference_ms: 1.3665265536083515
    mean_processing_ms: 3.308478005353003
  time_since_restore: 536.1054146289825
  time_this_iter_s: 8.877176284790039
  time_total_s: 536.1054146289825
  timestamp: 1563924772
  timesteps_since_restore: 2323200
  timesteps_this_iter: 36300
  timesteps_total: 2323200
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 536 s, 64 iter, 2323200 ts, 16 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.33570085858399
  episode_reward_mean: 15.935580842493486
  episode_reward_min: -2.194017219161982
  episodes_this_iter: 242
  episodes_total: 15730
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6925.369
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9949626922607422
        kl: 0.012556735426187515
        policy_loss: -0.015432540327310562
        total_loss: 0.30358797311782837
        vf_explained_var: 0.9905422925949097
        vf_loss: 0.3174508810043335
    load_time_ms: 1.649
    num_steps_sampled: 2359500
    num_steps_trained: 2340000
    sample_time_ms: 1873.679
    update_time_ms: 4.597
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009441422327359
    mean_inference_ms: 1.3663796236591605
    mean_processing_ms: 3.3085415350163605
  time_since_restore: 544.3287827968597
  time_this_iter_s: 8.223368167877197
  time_total_s: 544.3287827968597
  timestamp: 1563924780
  timesteps_since_restore: 2359500
  timesteps_this_iter: 36300
  timesteps_total: 2359500
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 544 s, 65 iter, 2359500 ts, 15.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.74553141527134
  episode_reward_mean: 16.519756242192624
  episode_reward_min: -4.586798834877421
  episodes_this_iter: 242
  episodes_total: 15972
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7006.556
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9808096885681152
        kl: 0.01474466547369957
        policy_loss: -0.013421852141618729
        total_loss: 0.29183024168014526
        vf_explained_var: 0.9918693900108337
        vf_loss: 0.30340903997421265
    load_time_ms: 1.646
    num_steps_sampled: 2395800
    num_steps_trained: 2376000
    sample_time_ms: 1876.221
    update_time_ms: 4.626
  iterations_since_restore: 66
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009699219872554
    mean_inference_ms: 1.366236370216561
    mean_processing_ms: 3.3090011722988617
  time_since_restore: 552.0546872615814
  time_this_iter_s: 7.72590446472168
  time_total_s: 552.0546872615814
  timestamp: 1563924788
  timesteps_since_restore: 2395800
  timesteps_this_iter: 36300
  timesteps_total: 2395800
  training_iteration: 66
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 552 s, 66 iter, 2395800 ts, 16.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.283935178077755
  episode_reward_mean: 17.108777499214657
  episode_reward_min: -3.4939583427661693
  episodes_this_iter: 242
  episodes_total: 16214
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7093.287
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9615817070007324
        kl: 0.014872604049742222
        policy_loss: -0.013445505872368813
        total_loss: 0.2817192077636719
        vf_explained_var: 0.991841197013855
        vf_loss: 0.2933056950569153
    load_time_ms: 1.646
    num_steps_sampled: 2432100
    num_steps_trained: 2412000
    sample_time_ms: 1871.691
    update_time_ms: 4.638
  iterations_since_restore: 67
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009578260948899
    mean_inference_ms: 1.366288136168478
    mean_processing_ms: 3.3090865764050794
  time_since_restore: 562.2252657413483
  time_this_iter_s: 10.170578479766846
  time_total_s: 562.2252657413483
  timestamp: 1563924798
  timesteps_since_restore: 2432100
  timesteps_this_iter: 36300
  timesteps_total: 2432100
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 562 s, 67 iter, 2432100 ts, 17.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.85196481898093
  episode_reward_mean: 17.193287709019685
  episode_reward_min: -4.171771745415262
  episodes_this_iter: 242
  episodes_total: 16456
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6937.198
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9463762044906616
        kl: 0.014704629778862
        policy_loss: -0.015720555558800697
        total_loss: 0.25752002000808716
        vf_explained_var: 0.9929001331329346
        vf_loss: 0.271402508020401
    load_time_ms: 1.648
    num_steps_sampled: 2468400
    num_steps_trained: 2448000
    sample_time_ms: 1870.804
    update_time_ms: 4.639
  iterations_since_restore: 68
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009354182241005
    mean_inference_ms: 1.3659841073878864
    mean_processing_ms: 3.3082716087380994
  time_since_restore: 569.5027101039886
  time_this_iter_s: 7.277444362640381
  time_total_s: 569.5027101039886
  timestamp: 1563924805
  timesteps_since_restore: 2468400
  timesteps_this_iter: 36300
  timesteps_total: 2468400
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 569 s, 68 iter, 2468400 ts, 17.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.245038679695554
  episode_reward_mean: 15.638411419085834
  episode_reward_min: -3.6653700424544793
  episodes_this_iter: 242
  episodes_total: 16698
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6754.322
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9392600059509277
        kl: 0.014971788972616196
        policy_loss: -0.013227368704974651
        total_loss: 0.270618200302124
        vf_explained_var: 0.9917699098587036
        vf_loss: 0.28197410702705383
    load_time_ms: 1.646
    num_steps_sampled: 2504700
    num_steps_trained: 2484000
    sample_time_ms: 1737.884
    update_time_ms: 4.453
  iterations_since_restore: 69
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008889343549645
    mean_inference_ms: 1.365697914448303
    mean_processing_ms: 3.3081212067701253
  time_since_restore: 576.5574951171875
  time_this_iter_s: 7.0547850131988525
  time_total_s: 576.5574951171875
  timestamp: 1563924812
  timesteps_since_restore: 2504700
  timesteps_this_iter: 36300
  timesteps_total: 2504700
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 576 s, 69 iter, 2504700 ts, 15.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.63224502251979
  episode_reward_mean: 16.532790946094906
  episode_reward_min: -3.302010801819725
  episodes_this_iter: 242
  episodes_total: 16940
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6756.788
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9162946939468384
        kl: 0.013771881349384785
        policy_loss: -0.014469403773546219
        total_loss: 0.2538498640060425
        vf_explained_var: 0.9926718473434448
        vf_loss: 0.2665977478027344
    load_time_ms: 1.644
    num_steps_sampled: 2541000
    num_steps_trained: 2520000
    sample_time_ms: 1874.096
    update_time_ms: 4.328
  iterations_since_restore: 70
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0094875490820865
    mean_inference_ms: 1.365656379373532
    mean_processing_ms: 3.308378846728129
  time_since_restore: 586.784175157547
  time_this_iter_s: 10.226680040359497
  time_total_s: 586.784175157547
  timestamp: 1563924822
  timesteps_since_restore: 2541000
  timesteps_this_iter: 36300
  timesteps_total: 2541000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 586 s, 70 iter, 2541000 ts, 16.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.08865186753784
  episode_reward_mean: 16.17720712671207
  episode_reward_min: -3.3314591367685162
  episodes_this_iter: 242
  episodes_total: 17182
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6559.909
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9009013175964355
        kl: 0.015315352007746696
        policy_loss: -0.014877875335514545
        total_loss: 0.24257493019104004
        vf_explained_var: 0.9927464723587036
        vf_loss: 0.25553834438323975
    load_time_ms: 1.654
    num_steps_sampled: 2577300
    num_steps_trained: 2556000
    sample_time_ms: 1873.701
    update_time_ms: 4.42
  iterations_since_restore: 71
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008594203762404
    mean_inference_ms: 1.3655455130989855
    mean_processing_ms: 3.3080668345931987
  time_since_restore: 593.660612821579
  time_this_iter_s: 6.876437664031982
  time_total_s: 593.660612821579
  timestamp: 1563924829
  timesteps_since_restore: 2577300
  timesteps_this_iter: 36300
  timesteps_total: 2577300
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 593 s, 71 iter, 2577300 ts, 16.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-33-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.711646597449295
  episode_reward_mean: 16.48414688073485
  episode_reward_min: -4.496901159562377
  episodes_this_iter: 242
  episodes_total: 17424
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6363.267
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8793449401855469
        kl: 0.01516731083393097
        policy_loss: -0.014450336806476116
        total_loss: 0.24561254680156708
        vf_explained_var: 0.9925581812858582
        vf_loss: 0.25816696882247925
    load_time_ms: 1.661
    num_steps_sampled: 2613600
    num_steps_trained: 2592000
    sample_time_ms: 1741.18
    update_time_ms: 4.244
  iterations_since_restore: 72
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008968749730432
    mean_inference_ms: 1.3654792865809662
    mean_processing_ms: 3.3086153798619358
  time_since_restore: 600.5779964923859
  time_this_iter_s: 6.917383670806885
  time_total_s: 600.5779964923859
  timestamp: 1563924836
  timesteps_since_restore: 2613600
  timesteps_this_iter: 36300
  timesteps_total: 2613600
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 600 s, 72 iter, 2613600 ts, 16.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.47685637576775
  episode_reward_mean: 17.152910865271313
  episode_reward_min: -4.567211537811518
  episodes_this_iter: 242
  episodes_total: 17666
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6275.754
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8574557304382324
        kl: 0.013939721509814262
        policy_loss: -0.015025160275399685
        total_loss: 0.21550652384757996
        vf_explained_var: 0.9936813712120056
        vf_loss: 0.22878922522068024
    load_time_ms: 1.659
    num_steps_sampled: 2649900
    num_steps_trained: 2628000
    sample_time_ms: 1877.558
    update_time_ms: 4.112
  iterations_since_restore: 73
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009783725396449
    mean_inference_ms: 1.3656009167058873
    mean_processing_ms: 3.309907806831558
  time_since_restore: 609.0322155952454
  time_this_iter_s: 8.454219102859497
  time_total_s: 609.0322155952454
  timestamp: 1563924845
  timesteps_since_restore: 2649900
  timesteps_this_iter: 36300
  timesteps_total: 2649900
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 609 s, 73 iter, 2649900 ts, 17.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.15728435255794
  episode_reward_mean: 17.301050368713362
  episode_reward_min: -3.3434403712826013
  episodes_this_iter: 242
  episodes_total: 17908
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6275.805
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8430126905441284
        kl: 0.01546908263117075
        policy_loss: -0.013720574788749218
        total_loss: 0.20979969203472137
        vf_explained_var: 0.9939997792243958
        vf_loss: 0.22158664464950562
    load_time_ms: 1.653
    num_steps_sampled: 2686200
    num_steps_trained: 2664000
    sample_time_ms: 1880.352
    update_time_ms: 4.032
  iterations_since_restore: 74
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009756776274004
    mean_inference_ms: 1.3655983883463212
    mean_processing_ms: 3.309892452624903
  time_since_restore: 617.940358877182
  time_this_iter_s: 8.908143281936646
  time_total_s: 617.940358877182
  timestamp: 1563924854
  timesteps_since_restore: 2686200
  timesteps_this_iter: 36300
  timesteps_total: 2686200
  training_iteration: 74
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 617 s, 74 iter, 2686200 ts, 17.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.120732582693854
  episode_reward_mean: 17.57435728374771
  episode_reward_min: -1.8576638447415068
  episodes_this_iter: 242
  episodes_total: 18150
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6369.457
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8271909952163696
        kl: 0.013595137745141983
        policy_loss: -0.013806723989546299
        total_loss: 0.19360440969467163
        vf_explained_var: 0.9942691326141357
        vf_loss: 0.2057117372751236
    load_time_ms: 1.667
    num_steps_sampled: 2722500
    num_steps_trained: 2700000
    sample_time_ms: 1880.632
    update_time_ms: 4.225
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009572951045437
    mean_inference_ms: 1.3658572034907335
    mean_processing_ms: 3.3093249180070305
  time_since_restore: 627.1067614555359
  time_this_iter_s: 9.166402578353882
  time_total_s: 627.1067614555359
  timestamp: 1563924863
  timesteps_since_restore: 2722500
  timesteps_this_iter: 36300
  timesteps_total: 2722500
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 627 s, 75 iter, 2722500 ts, 17.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.00738716125841
  episode_reward_mean: 17.849530998312805
  episode_reward_min: -2.0705038378797376
  episodes_this_iter: 242
  episodes_total: 18392
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6486.095
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.816791296005249
        kl: 0.015406223013997078
        policy_loss: -0.014078895561397076
        total_loss: 0.18811888992786407
        vf_explained_var: 0.994500458240509
        vf_loss: 0.20027202367782593
    load_time_ms: 1.675
    num_steps_sampled: 2758800
    num_steps_trained: 2736000
    sample_time_ms: 1880.417
    update_time_ms: 4.233
  iterations_since_restore: 76
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009069507677324
    mean_inference_ms: 1.3655890115927958
    mean_processing_ms: 3.3092785056076965
  time_since_restore: 635.9991192817688
  time_this_iter_s: 8.89235782623291
  time_total_s: 635.9991192817688
  timestamp: 1563924872
  timesteps_since_restore: 2758800
  timesteps_this_iter: 36300
  timesteps_total: 2758800
  training_iteration: 76
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 635 s, 76 iter, 2758800 ts, 17.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.82198226517824
  episode_reward_mean: 16.14326543228553
  episode_reward_min: -1.038393336916893
  episodes_this_iter: 242
  episodes_total: 18634
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6289.012
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7989834547042847
        kl: 0.01516804751008749
        policy_loss: -0.014700528234243393
        total_loss: 0.18149973452091217
        vf_explained_var: 0.9938077926635742
        vf_loss: 0.19430424273014069
    load_time_ms: 1.673
    num_steps_sampled: 2795100
    num_steps_trained: 2772000
    sample_time_ms: 1746.778
    update_time_ms: 4.396
  iterations_since_restore: 77
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008492831555838
    mean_inference_ms: 1.3652076905807655
    mean_processing_ms: 3.308186607481445
  time_since_restore: 642.8596677780151
  time_this_iter_s: 6.860548496246338
  time_total_s: 642.8596677780151
  timestamp: 1563924879
  timesteps_since_restore: 2795100
  timesteps_this_iter: 36300
  timesteps_total: 2795100
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 642 s, 77 iter, 2795100 ts, 16.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.738719033792606
  episode_reward_mean: 17.94455600322803
  episode_reward_min: -2.4060833688371415
  episodes_this_iter: 242
  episodes_total: 18876
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6247.985
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7788594961166382
        kl: 0.016030417755246162
        policy_loss: -0.01640445739030838
        total_loss: 0.16354553401470184
        vf_explained_var: 0.9956585764884949
        vf_loss: 0.17794618010520935
    load_time_ms: 1.678
    num_steps_sampled: 2831400
    num_steps_trained: 2808000
    sample_time_ms: 1882.959
    update_time_ms: 4.295
  iterations_since_restore: 78
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00916443101417
    mean_inference_ms: 1.3653965526552363
    mean_processing_ms: 3.3086879993576597
  time_since_restore: 651.0876610279083
  time_this_iter_s: 8.227993249893188
  time_total_s: 651.0876610279083
  timestamp: 1563924887
  timesteps_since_restore: 2831400
  timesteps_this_iter: 36300
  timesteps_total: 2831400
  training_iteration: 78
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 651 s, 78 iter, 2831400 ts, 17.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-34-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.48829046241491
  episode_reward_mean: 16.962205418692854
  episode_reward_min: -2.5332259672250768
  episodes_this_iter: 242
  episodes_total: 19118
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6228.163
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7684022188186646
        kl: 0.016828281804919243
        policy_loss: -0.015335907228291035
        total_loss: 0.16341058909893036
        vf_explained_var: 0.9947583675384521
        vf_loss: 0.17664296925067902
    load_time_ms: 1.682
    num_steps_sampled: 2867700
    num_steps_trained: 2844000
    sample_time_ms: 1885.026
    update_time_ms: 4.283
  iterations_since_restore: 79
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009326550006407
    mean_inference_ms: 1.3653459694285832
    mean_processing_ms: 3.3089987219359904
  time_since_restore: 657.9641005992889
  time_this_iter_s: 6.876439571380615
  time_total_s: 657.9641005992889
  timestamp: 1563924894
  timesteps_since_restore: 2867700
  timesteps_this_iter: 36300
  timesteps_total: 2867700
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 657 s, 79 iter, 2867700 ts, 17 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.97544530289744
  episode_reward_mean: 16.736525055428356
  episode_reward_min: -2.1174710845890843
  episodes_this_iter: 242
  episodes_total: 19360
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6224.369
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7485712766647339
        kl: 0.01671847701072693
        policy_loss: -0.014680854044854641
        total_loss: 0.14744630455970764
        vf_explained_var: 0.9949643611907959
        vf_loss: 0.1600373536348343
    load_time_ms: 1.693
    num_steps_sampled: 2904000
    num_steps_trained: 2880000
    sample_time_ms: 1752.072
    update_time_ms: 4.255
  iterations_since_restore: 80
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0097525476652125
    mean_inference_ms: 1.3652364355367501
    mean_processing_ms: 3.308799836607624
  time_since_restore: 666.8227376937866
  time_this_iter_s: 8.85863709449768
  time_total_s: 666.8227376937866
  timestamp: 1563924903
  timesteps_since_restore: 2904000
  timesteps_this_iter: 36300
  timesteps_total: 2904000
  training_iteration: 80
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 666 s, 80 iter, 2904000 ts, 16.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.413771317731836
  episode_reward_mean: 18.89621803643901
  episode_reward_min: -2.3536768201570593
  episodes_this_iter: 242
  episodes_total: 19602
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6243.521
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7298076152801514
        kl: 0.013938850723206997
        policy_loss: -0.014138367027044296
        total_loss: 0.1839652955532074
        vf_explained_var: 0.9951599836349487
        vf_loss: 0.19636130332946777
    load_time_ms: 1.684
    num_steps_sampled: 2940300
    num_steps_trained: 2916000
    sample_time_ms: 1888.385
    update_time_ms: 4.178
  iterations_since_restore: 81
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009519883050909
    mean_inference_ms: 1.365226164125813
    mean_processing_ms: 3.308592370753209
  time_since_restore: 675.2537462711334
  time_this_iter_s: 8.431008577346802
  time_total_s: 675.2537462711334
  timestamp: 1563924911
  timesteps_since_restore: 2940300
  timesteps_this_iter: 36300
  timesteps_total: 2940300
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 675 s, 81 iter, 2940300 ts, 18.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.250663566642075
  episode_reward_mean: 16.46342289365687
  episode_reward_min: -2.367442025608508
  episodes_this_iter: 242
  episodes_total: 19844
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6261.573
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7144665718078613
        kl: 0.014868593774735928
        policy_loss: -0.014585067518055439
        total_loss: 0.15414464473724365
        vf_explained_var: 0.9947351813316345
        vf_loss: 0.16687113046646118
    load_time_ms: 1.697
    num_steps_sampled: 2976600
    num_steps_trained: 2952000
    sample_time_ms: 1887.323
    update_time_ms: 4.174
  iterations_since_restore: 82
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0094319966009175
    mean_inference_ms: 1.3653195959662154
    mean_processing_ms: 3.3089112258839855
  time_since_restore: 682.3416368961334
  time_this_iter_s: 7.087890625
  time_total_s: 682.3416368961334
  timestamp: 1563924918
  timesteps_since_restore: 2976600
  timesteps_this_iter: 36300
  timesteps_total: 2976600
  training_iteration: 82
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 682 s, 82 iter, 2976600 ts, 16.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.41533003177204
  episode_reward_mean: 17.600805795912763
  episode_reward_min: -2.101149772615108
  episodes_this_iter: 242
  episodes_total: 20086
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6437.232
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6995688676834106
        kl: 0.015458215028047562
        policy_loss: -0.015897296369075775
        total_loss: 0.11887236684560776
        vf_explained_var: 0.9960757493972778
        vf_loss: 0.13283738493919373
    load_time_ms: 1.7
    num_steps_sampled: 3012900
    num_steps_trained: 2988000
    sample_time_ms: 1887.744
    update_time_ms: 4.107
  iterations_since_restore: 83
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009694553449442
    mean_inference_ms: 1.3654146048663602
    mean_processing_ms: 3.3089557159834087
  time_since_restore: 692.5597505569458
  time_this_iter_s: 10.218113660812378
  time_total_s: 692.5597505569458
  timestamp: 1563924928
  timesteps_since_restore: 3012900
  timesteps_this_iter: 36300
  timesteps_total: 3012900
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 692 s, 83 iter, 3012900 ts, 17.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.792497844256076
  episode_reward_mean: 17.530731154166453
  episode_reward_min: -1.2052256588857837
  episodes_this_iter: 242
  episodes_total: 20328
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6276.656
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6847355365753174
        kl: 0.016135701909661293
        policy_loss: -0.015796875581145287
        total_loss: 0.12179316580295563
        vf_explained_var: 0.9959861040115356
        vf_loss: 0.13557305932044983
    load_time_ms: 1.718
    num_steps_sampled: 3049200
    num_steps_trained: 3024000
    sample_time_ms: 1884.315
    update_time_ms: 4.238
  iterations_since_restore: 84
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.009001321462423
    mean_inference_ms: 1.3653988737577483
    mean_processing_ms: 3.3082127494502513
  time_since_restore: 699.8250224590302
  time_this_iter_s: 7.265271902084351
  time_total_s: 699.8250224590302
  timestamp: 1563924936
  timesteps_since_restore: 3049200
  timesteps_this_iter: 36300
  timesteps_total: 3049200
  training_iteration: 84
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 699 s, 84 iter, 3049200 ts, 17.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.94025595875759
  episode_reward_mean: 17.851683742102615
  episode_reward_min: -2.1673494936510482
  episodes_this_iter: 242
  episodes_total: 20570
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6377.97
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6627700328826904
        kl: 0.018730325624346733
        policy_loss: -0.015329746529459953
        total_loss: 0.12432479858398438
        vf_explained_var: 0.9964167475700378
        vf_loss: 0.13731327652931213
    load_time_ms: 1.706
    num_steps_sampled: 3085500
    num_steps_trained: 3060000
    sample_time_ms: 1751.634
    update_time_ms: 4.125
  iterations_since_restore: 85
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008760889982325
    mean_inference_ms: 1.365196691218425
    mean_processing_ms: 3.3080502000381795
  time_since_restore: 708.6800079345703
  time_this_iter_s: 8.854985475540161
  time_total_s: 708.6800079345703
  timestamp: 1563924944
  timesteps_since_restore: 3085500
  timesteps_this_iter: 36300
  timesteps_total: 3085500
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 708 s, 85 iter, 3085500 ts, 17.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-35-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.87095439023846
  episode_reward_mean: 17.977619372238465
  episode_reward_min: -10.57786158730953
  episodes_this_iter: 242
  episodes_total: 20812
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6178.917
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6435987949371338
        kl: 0.014766201376914978
        policy_loss: -0.015501950867474079
        total_loss: 0.1216876357793808
        vf_explained_var: 0.9961235523223877
        vf_loss: 0.13534381985664368
    load_time_ms: 1.709
    num_steps_sampled: 3121800
    num_steps_trained: 3096000
    sample_time_ms: 1881.448
    update_time_ms: 4.147
  iterations_since_restore: 86
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0087310185321865
    mean_inference_ms: 1.3649872299665167
    mean_processing_ms: 3.3086759800374543
  time_since_restore: 716.8756251335144
  time_this_iter_s: 8.195617198944092
  time_total_s: 716.8756251335144
  timestamp: 1563924953
  timesteps_since_restore: 3121800
  timesteps_this_iter: 36300
  timesteps_total: 3121800
  training_iteration: 86
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 716 s, 86 iter, 3121800 ts, 18 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.29696617193689
  episode_reward_mean: 17.299644147941223
  episode_reward_min: -3.107116994117479
  episodes_this_iter: 242
  episodes_total: 21054
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6381.512
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6320246458053589
        kl: 0.015395622700452805
        policy_loss: -0.015832040458917618
        total_loss: 0.11527743935585022
        vf_explained_var: 0.9961208701133728
        vf_loss: 0.1291850209236145
    load_time_ms: 1.711
    num_steps_sampled: 3158100
    num_steps_trained: 3132000
    sample_time_ms: 1883.863
    update_time_ms: 4.001
  iterations_since_restore: 87
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008712396064413
    mean_inference_ms: 1.3650894278221393
    mean_processing_ms: 3.3086342467245298
  time_since_restore: 725.7897937297821
  time_this_iter_s: 8.9141685962677
  time_total_s: 725.7897937297821
  timestamp: 1563924962
  timesteps_since_restore: 3158100
  timesteps_this_iter: 36300
  timesteps_total: 3158100
  training_iteration: 87
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 725 s, 87 iter, 3158100 ts, 17.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.3362489079269
  episode_reward_mean: 17.734149585238576
  episode_reward_min: -2.998463558958651
  episodes_this_iter: 242
  episodes_total: 21296
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6582.057
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6123565435409546
        kl: 0.015658114105463028
        policy_loss: -0.01536637730896473
        total_loss: 0.10609312355518341
        vf_explained_var: 0.9967410564422607
        vf_loss: 0.1195022389292717
    load_time_ms: 1.7
    num_steps_sampled: 3194400
    num_steps_trained: 3168000
    sample_time_ms: 1747.872
    update_time_ms: 4.037
  iterations_since_restore: 88
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008352843372251
    mean_inference_ms: 1.3650390443532439
    mean_processing_ms: 3.3083505275744907
  time_since_restore: 734.6675612926483
  time_this_iter_s: 8.877767562866211
  time_total_s: 734.6675612926483
  timestamp: 1563924970
  timesteps_since_restore: 3194400
  timesteps_this_iter: 36300
  timesteps_total: 3194400
  training_iteration: 88
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 734 s, 88 iter, 3194400 ts, 17.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.72263336865237
  episode_reward_mean: 19.34438502060419
  episode_reward_min: -3.177230275355068
  episodes_this_iter: 242
  episodes_total: 21538
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6691.355
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5972305536270142
        kl: 0.015869610011577606
        policy_loss: -0.01775429956614971
        total_loss: 0.1003144234418869
        vf_explained_var: 0.9970369338989258
        vf_loss: 0.11608501523733139
    load_time_ms: 1.698
    num_steps_sampled: 3230700
    num_steps_trained: 3204000
    sample_time_ms: 1877.954
    update_time_ms: 4.167
  iterations_since_restore: 89
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0080478013631655
    mean_inference_ms: 1.3647983271314883
    mean_processing_ms: 3.3075557481806706
  time_since_restore: 743.9415929317474
  time_this_iter_s: 9.274031639099121
  time_total_s: 743.9415929317474
  timestamp: 1563924980
  timesteps_since_restore: 3230700
  timesteps_this_iter: 36300
  timesteps_total: 3230700
  training_iteration: 89
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 743 s, 89 iter, 3230700 ts, 19.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.95621321645595
  episode_reward_mean: 18.49579599678736
  episode_reward_min: -3.471076129135234
  episodes_this_iter: 242
  episodes_total: 21780
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6498.556
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5829119682312012
        kl: 0.017272144556045532
        policy_loss: -0.020125772804021835
        total_loss: 0.11118006706237793
        vf_explained_var: 0.9964969158172607
        vf_loss: 0.12914684414863586
    load_time_ms: 1.691
    num_steps_sampled: 3267000
    num_steps_trained: 3240000
    sample_time_ms: 1875.366
    update_time_ms: 4.272
  iterations_since_restore: 90
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0075130546449715
    mean_inference_ms: 1.3645003752936797
    mean_processing_ms: 3.307412629446519
  time_since_restore: 750.8443155288696
  time_this_iter_s: 6.902722597122192
  time_total_s: 750.8443155288696
  timestamp: 1563924987
  timesteps_since_restore: 3267000
  timesteps_this_iter: 36300
  timesteps_total: 3267000
  training_iteration: 90
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 750 s, 90 iter, 3267000 ts, 18.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.43416896030191
  episode_reward_mean: 18.088533700482866
  episode_reward_min: -1.1590168839614994
  episodes_this_iter: 242
  episodes_total: 22022
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6479.049
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5679538249969482
        kl: 0.01639675348997116
        policy_loss: -0.014729239977896214
        total_loss: 0.09808200597763062
        vf_explained_var: 0.9968741536140442
        vf_loss: 0.11076164245605469
    load_time_ms: 1.692
    num_steps_sampled: 3303300
    num_steps_trained: 3276000
    sample_time_ms: 1875.51
    update_time_ms: 4.199
  iterations_since_restore: 91
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00809955451518
    mean_inference_ms: 1.364608785262125
    mean_processing_ms: 3.3081371745645543
  time_since_restore: 759.0798316001892
  time_this_iter_s: 8.23551607131958
  time_total_s: 759.0798316001892
  timestamp: 1563924995
  timesteps_since_restore: 3303300
  timesteps_this_iter: 36300
  timesteps_total: 3303300
  training_iteration: 91
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 759 s, 91 iter, 3303300 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.40630254337415
  episode_reward_mean: 17.85712491172215
  episode_reward_min: -2.292176267747364
  episodes_this_iter: 242
  episodes_total: 22264
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6519.295
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5598680973052979
        kl: 0.016948729753494263
        policy_loss: -0.016880055889487267
        total_loss: 0.09007187187671661
        vf_explained_var: 0.9970089793205261
        vf_loss: 0.10483333468437195
    load_time_ms: 1.678
    num_steps_sampled: 3339600
    num_steps_trained: 3312000
    sample_time_ms: 1876.047
    update_time_ms: 4.222
  iterations_since_restore: 92
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008291212068594
    mean_inference_ms: 1.3644966419817095
    mean_processing_ms: 3.308474704963183
  time_since_restore: 766.5759904384613
  time_this_iter_s: 7.496158838272095
  time_total_s: 766.5759904384613
  timestamp: 1563925002
  timesteps_since_restore: 3339600
  timesteps_this_iter: 36300
  timesteps_total: 3339600
  training_iteration: 92
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 766 s, 92 iter, 3339600 ts, 17.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.633739462055686
  episode_reward_mean: 18.091665307499472
  episode_reward_min: -3.4332358537082577
  episodes_this_iter: 242
  episodes_total: 22506
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6360.621
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5462371110916138
        kl: 0.018158914521336555
        policy_loss: -0.017349854111671448
        total_loss: 0.08298200368881226
        vf_explained_var: 0.9972436428070068
        vf_loss: 0.09806197881698608
    load_time_ms: 1.682
    num_steps_sampled: 3375900
    num_steps_trained: 3348000
    sample_time_ms: 1738.985
    update_time_ms: 4.285
  iterations_since_restore: 93
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008081829400967
    mean_inference_ms: 1.3643448300357595
    mean_processing_ms: 3.3084275945320867
  time_since_restore: 773.8345854282379
  time_this_iter_s: 7.258594989776611
  time_total_s: 773.8345854282379
  timestamp: 1563925010
  timesteps_since_restore: 3375900
  timesteps_this_iter: 36300
  timesteps_total: 3375900
  training_iteration: 93
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 773 s, 93 iter, 3375900 ts, 18.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-36-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.34812708827106
  episode_reward_mean: 19.12041451203999
  episode_reward_min: -2.1263575353912754
  episodes_this_iter: 242
  episodes_total: 22748
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6461.466
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5300275087356567
        kl: 0.014422636479139328
        policy_loss: -0.017896020784974098
        total_loss: 0.08984440565109253
        vf_explained_var: 0.9973589777946472
        vf_loss: 0.10593759268522263
    load_time_ms: 1.672
    num_steps_sampled: 3412200
    num_steps_trained: 3384000
    sample_time_ms: 1873.563
    update_time_ms: 4.133
  iterations_since_restore: 94
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.008119874339076
    mean_inference_ms: 1.3643670757076798
    mean_processing_ms: 3.309176605487588
  time_since_restore: 783.4522135257721
  time_this_iter_s: 9.61762809753418
  time_total_s: 783.4522135257721
  timestamp: 1563925019
  timesteps_since_restore: 3412200
  timesteps_this_iter: 36300
  timesteps_total: 3412200
  training_iteration: 94
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 783 s, 94 iter, 3412200 ts, 19.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.062007569589035
  episode_reward_mean: 18.452168812059817
  episode_reward_min: -2.111491436176721
  episodes_this_iter: 242
  episodes_total: 22990
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6463.443
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5174052715301514
        kl: 0.01852489449083805
        policy_loss: -0.016296161338686943
        total_loss: 0.07916294038295746
        vf_explained_var: 0.9973878860473633
        vf_loss: 0.09314349293708801
    load_time_ms: 1.663
    num_steps_sampled: 3448500
    num_steps_trained: 3420000
    sample_time_ms: 1871.212
    update_time_ms: 4.11
  iterations_since_restore: 95
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.007264651114199
    mean_inference_ms: 1.3644060546188728
    mean_processing_ms: 3.3088359882353093
  time_since_restore: 792.3003931045532
  time_this_iter_s: 8.848179578781128
  time_total_s: 792.3003931045532
  timestamp: 1563925028
  timesteps_since_restore: 3448500
  timesteps_this_iter: 36300
  timesteps_total: 3448500
  training_iteration: 95
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 792 s, 95 iter, 3448500 ts, 18.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.814812274108824
  episode_reward_mean: 19.452862031869373
  episode_reward_min: -1.754132397597086
  episodes_this_iter: 242
  episodes_total: 23232
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6627.71
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4997268915176392
        kl: 0.017981333658099174
        policy_loss: -0.017845390364527702
        total_loss: 0.07377222180366516
        vf_explained_var: 0.9978050589561462
        vf_loss: 0.08936993032693863
    load_time_ms: 1.655
    num_steps_sampled: 3484800
    num_steps_trained: 3456000
    sample_time_ms: 1740.461
    update_time_ms: 4.161
  iterations_since_restore: 96
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00693301628339
    mean_inference_ms: 1.3643620832533705
    mean_processing_ms: 3.308181269331699
  time_since_restore: 800.8349571228027
  time_this_iter_s: 8.534564018249512
  time_total_s: 800.8349571228027
  timestamp: 1563925037
  timesteps_since_restore: 3484800
  timesteps_this_iter: 36300
  timesteps_total: 3484800
  training_iteration: 96
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 800 s, 96 iter, 3484800 ts, 19.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.03986971492724
  episode_reward_mean: 18.795600644428433
  episode_reward_min: -0.42693201489924537
  episodes_this_iter: 242
  episodes_total: 23474
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6625.055
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4685174226760864
        kl: 0.016187770292162895
        policy_loss: -0.019211312755942345
        total_loss: 0.06708426028490067
        vf_explained_var: 0.9977384209632874
        vf_loss: 0.0842721089720726
    load_time_ms: 1.663
    num_steps_sampled: 3521100
    num_steps_trained: 3492000
    sample_time_ms: 1870.982
    update_time_ms: 4.269
  iterations_since_restore: 97
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006756366623617
    mean_inference_ms: 1.3642081175501701
    mean_processing_ms: 3.308595899512062
  time_since_restore: 811.0286295413971
  time_this_iter_s: 10.19367241859436
  time_total_s: 811.0286295413971
  timestamp: 1563925047
  timesteps_since_restore: 3521100
  timesteps_this_iter: 36300
  timesteps_total: 3521100
  training_iteration: 97
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 811 s, 97 iter, 3521100 ts, 18.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.313389610607565
  episode_reward_mean: 18.492154562381252
  episode_reward_min: -0.5843167519825907
  episodes_this_iter: 242
  episodes_total: 23716
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6426.583
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4599833488464355
        kl: 0.017705483362078667
        policy_loss: -0.01824689656496048
        total_loss: 0.06441560387611389
        vf_explained_var: 0.9978145956993103
        vf_loss: 0.08044932037591934
    load_time_ms: 1.677
    num_steps_sampled: 3557400
    num_steps_trained: 3528000
    sample_time_ms: 1868.748
    update_time_ms: 4.398
  iterations_since_restore: 98
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0063676564779005
    mean_inference_ms: 1.3642152415190076
    mean_processing_ms: 3.3078133193239734
  time_since_restore: 817.8979043960571
  time_this_iter_s: 6.869274854660034
  time_total_s: 817.8979043960571
  timestamp: 1563925054
  timesteps_since_restore: 3557400
  timesteps_this_iter: 36300
  timesteps_total: 3557400
  training_iteration: 98
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 817 s, 98 iter, 3557400 ts, 18.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.13137432082684
  episode_reward_mean: 19.375460393270192
  episode_reward_min: -4.075589723416145
  episodes_this_iter: 242
  episodes_total: 23958
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6396.559
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4463071823120117
        kl: 0.016645019873976707
        policy_loss: -0.017262490466237068
        total_loss: 0.07286258786916733
        vf_explained_var: 0.997772753238678
        vf_loss: 0.08804444968700409
    load_time_ms: 1.679
    num_steps_sampled: 3593700
    num_steps_trained: 3564000
    sample_time_ms: 1871.171
    update_time_ms: 4.214
  iterations_since_restore: 99
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006962851466055
    mean_inference_ms: 1.3644466029495086
    mean_processing_ms: 3.30840161946926
  time_since_restore: 826.8937954902649
  time_this_iter_s: 8.995891094207764
  time_total_s: 826.8937954902649
  timestamp: 1563925063
  timesteps_since_restore: 3593700
  timesteps_this_iter: 36300
  timesteps_total: 3593700
  training_iteration: 99
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 826 s, 99 iter, 3593700 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.89958720202564
  episode_reward_mean: 19.00169703809972
  episode_reward_min: -13.398780330085806
  episodes_this_iter: 242
  episodes_total: 24200
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6477.052
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4353282451629639
        kl: 0.0186200812458992
        policy_loss: -0.015278526581823826
        total_loss: 0.07619074732065201
        vf_explained_var: 0.9976465702056885
        vf_loss: 0.08914177119731903
    load_time_ms: 1.682
    num_steps_sampled: 3630000
    num_steps_trained: 3600000
    sample_time_ms: 1871.449
    update_time_ms: 4.178
  iterations_since_restore: 100
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0064594801138655
    mean_inference_ms: 1.36415460053161
    mean_processing_ms: 3.3082729544749037
  time_since_restore: 834.6060256958008
  time_this_iter_s: 7.712230205535889
  time_total_s: 834.6060256958008
  timestamp: 1563925071
  timesteps_since_restore: 3630000
  timesteps_this_iter: 36300
  timesteps_total: 3630000
  training_iteration: 100
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 834 s, 100 iter, 3630000 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-37-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.644511200484814
  episode_reward_mean: 17.735635086279647
  episode_reward_min: -8.899515117377963
  episodes_this_iter: 242
  episodes_total: 24442
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6476.629
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4213366508483887
        kl: 0.01785009354352951
        policy_loss: -0.012602642178535461
        total_loss: 0.06441626697778702
        vf_explained_var: 0.9977919459342957
        vf_loss: 0.07478765398263931
    load_time_ms: 1.681
    num_steps_sampled: 3666300
    num_steps_trained: 3636000
    sample_time_ms: 1736.898
    update_time_ms: 4.186
  iterations_since_restore: 101
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006059236889189
    mean_inference_ms: 1.3640976276053678
    mean_processing_ms: 3.3080589575794566
  time_since_restore: 841.4918074607849
  time_this_iter_s: 6.885781764984131
  time_total_s: 841.4918074607849
  timestamp: 1563925078
  timesteps_since_restore: 3666300
  timesteps_this_iter: 36300
  timesteps_total: 3666300
  training_iteration: 101
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 841 s, 101 iter, 3666300 ts, 17.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.488785752330735
  episode_reward_mean: 17.676603460243733
  episode_reward_min: -2.0403808919374327
  episodes_this_iter: 242
  episodes_total: 24684
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6613.34
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4014954566955566
        kl: 0.0151658421382308
        policy_loss: -0.015534141100943089
        total_loss: 0.05954676866531372
        vf_explained_var: 0.9979027509689331
        vf_loss: 0.07318518310785294
    load_time_ms: 1.682
    num_steps_sampled: 3702600
    num_steps_trained: 3672000
    sample_time_ms: 1870.901
    update_time_ms: 4.178
  iterations_since_restore: 102
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006726464873486
    mean_inference_ms: 1.3641657389811825
    mean_processing_ms: 3.308714216215092
  time_since_restore: 851.6988527774811
  time_this_iter_s: 10.207045316696167
  time_total_s: 851.6988527774811
  timestamp: 1563925088
  timesteps_since_restore: 3702600
  timesteps_this_iter: 36300
  timesteps_total: 3702600
  training_iteration: 102
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 851 s, 102 iter, 3702600 ts, 17.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.85093698641998
  episode_reward_mean: 17.583157426679882
  episode_reward_min: -2.2459593790542995
  episodes_this_iter: 242
  episodes_total: 24926
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6596.694
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.396398663520813
        kl: 0.01802603155374527
        policy_loss: -0.015392846427857876
        total_loss: 0.06847911328077316
        vf_explained_var: 0.9975936412811279
        vf_loss: 0.08161871135234833
    load_time_ms: 1.674
    num_steps_sampled: 3738900
    num_steps_trained: 3708000
    sample_time_ms: 1871.794
    update_time_ms: 4.358
  iterations_since_restore: 103
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0062920968077345
    mean_inference_ms: 1.364189919986208
    mean_processing_ms: 3.308070399359265
  time_since_restore: 858.8012478351593
  time_this_iter_s: 7.102395057678223
  time_total_s: 858.8012478351593
  timestamp: 1563925095
  timesteps_since_restore: 3738900
  timesteps_this_iter: 36300
  timesteps_total: 3738900
  training_iteration: 103
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 858 s, 103 iter, 3738900 ts, 17.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.14536018069633
  episode_reward_mean: 19.087904947214895
  episode_reward_min: -4.738927557621105
  episodes_this_iter: 242
  episodes_total: 25168
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6457.265
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.373853087425232
        kl: 0.02011396363377571
        policy_loss: -0.01754024438560009
        total_loss: 0.05118178203701973
        vf_explained_var: 0.9982396364212036
        vf_loss: 0.06620778143405914
    load_time_ms: 1.664
    num_steps_sampled: 3775200
    num_steps_trained: 3744000
    sample_time_ms: 1739.464
    update_time_ms: 4.293
  iterations_since_restore: 104
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006030782730407
    mean_inference_ms: 1.364156106499444
    mean_processing_ms: 3.308176969568558
  time_since_restore: 865.7001600265503
  time_this_iter_s: 6.898912191390991
  time_total_s: 865.7001600265503
  timestamp: 1563925102
  timesteps_since_restore: 3775200
  timesteps_this_iter: 36300
  timesteps_total: 3775200
  training_iteration: 104
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 865 s, 104 iter, 3775200 ts, 19.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.69505730093114
  episode_reward_mean: 19.18512574044067
  episode_reward_min: 0.4218824097438703
  episodes_this_iter: 242
  episodes_total: 25410
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6443.068
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.3546347618103027
        kl: 0.013967772014439106
        policy_loss: -0.020161349326372147
        total_loss: 0.04248789697885513
        vf_explained_var: 0.9983539581298828
        vf_loss: 0.060030288994312286
    load_time_ms: 1.666
    num_steps_sampled: 3811500
    num_steps_trained: 3780000
    sample_time_ms: 1877.222
    update_time_ms: 4.227
  iterations_since_restore: 105
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0064442198152594
    mean_inference_ms: 1.3642616178223903
    mean_processing_ms: 3.308478216494546
  time_since_restore: 875.7813444137573
  time_this_iter_s: 10.081184387207031
  time_total_s: 875.7813444137573
  timestamp: 1563925112
  timesteps_since_restore: 3811500
  timesteps_this_iter: 36300
  timesteps_total: 3811500
  training_iteration: 105
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 875 s, 105 iter, 3811500 ts, 19.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.35302240985017
  episode_reward_mean: 19.103521263130723
  episode_reward_min: -6.033828763823251
  episodes_this_iter: 242
  episodes_total: 25652
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6315.184
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.3480913639068604
        kl: 0.016474805772304535
        policy_loss: -0.015761159360408783
        total_loss: 0.05132251977920532
        vf_explained_var: 0.9982631206512451
        vf_loss: 0.06399465352296829
    load_time_ms: 1.673
    num_steps_sampled: 3847800
    num_steps_trained: 3816000
    sample_time_ms: 1876.096
    update_time_ms: 4.162
  iterations_since_restore: 106
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.005931822127831
    mean_inference_ms: 1.363990442545733
    mean_processing_ms: 3.308124997398081
  time_since_restore: 883.0217621326447
  time_this_iter_s: 7.240417718887329
  time_total_s: 883.0217621326447
  timestamp: 1563925119
  timesteps_since_restore: 3847800
  timesteps_this_iter: 36300
  timesteps_total: 3847800
  training_iteration: 106
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 883 s, 106 iter, 3847800 ts, 19.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.03602726987581
  episode_reward_mean: 19.042970578751508
  episode_reward_min: -0.44709963491298893
  episodes_this_iter: 242
  episodes_total: 25894
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6317.105
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.327767014503479
        kl: 0.01673518866300583
        policy_loss: -0.019460249692201614
        total_loss: 0.0362812839448452
        vf_explained_var: 0.998582124710083
        vf_loss: 0.05260368436574936
    load_time_ms: 1.663
    num_steps_sampled: 3884100
    num_steps_trained: 3852000
    sample_time_ms: 1878.508
    update_time_ms: 4.046
  iterations_since_restore: 107
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.006147307517349
    mean_inference_ms: 1.3639818445572276
    mean_processing_ms: 3.308608650657921
  time_since_restore: 893.2570443153381
  time_this_iter_s: 10.235282182693481
  time_total_s: 893.2570443153381
  timestamp: 1563925129
  timesteps_since_restore: 3884100
  timesteps_this_iter: 36300
  timesteps_total: 3884100
  training_iteration: 107
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 893 s, 107 iter, 3884100 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-38-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.588899136297464
  episode_reward_mean: 17.90662411837892
  episode_reward_min: -1.2987216779556208
  episodes_this_iter: 242
  episodes_total: 26136
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6518.019
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.32222318649292
        kl: 0.017333103343844414
        policy_loss: -0.017455855384469032
        total_loss: 0.039474476128816605
        vf_explained_var: 0.9983875751495361
        vf_loss: 0.05368037894368172
    load_time_ms: 1.649
    num_steps_sampled: 3920400
    num_steps_trained: 3888000
    sample_time_ms: 1878.955
    update_time_ms: 3.914
  iterations_since_restore: 108
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.005603069852021
    mean_inference_ms: 1.3639748877825328
    mean_processing_ms: 3.308199036752226
  time_since_restore: 902.1421828269958
  time_this_iter_s: 8.885138511657715
  time_total_s: 902.1421828269958
  timestamp: 1563925138
  timesteps_since_restore: 3920400
  timesteps_this_iter: 36300
  timesteps_total: 3920400
  training_iteration: 108
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 902 s, 108 iter, 3920400 ts, 17.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.180095654512286
  episode_reward_mean: 19.594730044778522
  episode_reward_min: -1.3624764164012002
  episodes_this_iter: 242
  episodes_total: 26378
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6440.432
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.3094582557678223
        kl: 0.018011443316936493
        policy_loss: -0.017325544729828835
        total_loss: 0.03616016358137131
        vf_explained_var: 0.9988033771514893
        vf_loss: 0.050108566880226135
    load_time_ms: 1.651
    num_steps_sampled: 3956700
    num_steps_trained: 3924000
    sample_time_ms: 1743.833
    update_time_ms: 4.099
  iterations_since_restore: 109
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.005236385837772
    mean_inference_ms: 1.3639135395896198
    mean_processing_ms: 3.3077455717189803
  time_since_restore: 909.010541677475
  time_this_iter_s: 6.868358850479126
  time_total_s: 909.010541677475
  timestamp: 1563925145
  timesteps_since_restore: 3956700
  timesteps_this_iter: 36300
  timesteps_total: 3956700
  training_iteration: 109
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 909 s, 109 iter, 3956700 ts, 19.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.20231797070632
  episode_reward_mean: 19.239920578801012
  episode_reward_min: -0.754023968357448
  episodes_this_iter: 242
  episodes_total: 26620
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6538.583
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2928953170776367
        kl: 0.015406278893351555
        policy_loss: -0.019188961014151573
        total_loss: 0.03209333494305611
        vf_explained_var: 0.9987929463386536
        vf_loss: 0.048393622040748596
    load_time_ms: 1.641
    num_steps_sampled: 3993000
    num_steps_trained: 3960000
    sample_time_ms: 1880.563
    update_time_ms: 4.019
  iterations_since_restore: 110
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.005731014553449
    mean_inference_ms: 1.3641943455276662
    mean_processing_ms: 3.3079051296896456
  time_since_restore: 919.0685679912567
  time_this_iter_s: 10.058026313781738
  time_total_s: 919.0685679912567
  timestamp: 1563925155
  timesteps_since_restore: 3993000
  timesteps_this_iter: 36300
  timesteps_total: 3993000
  training_iteration: 110
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 919 s, 110 iter, 3993000 ts, 19.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.80947845343404
  episode_reward_mean: 19.49650971690309
  episode_reward_min: -1.9737376017777744
  episodes_this_iter: 242
  episodes_total: 26862
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6620.736
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2885655164718628
        kl: 0.016966313123703003
        policy_loss: -0.018026305362582207
        total_loss: 0.029783418402075768
        vf_explained_var: 0.9988686442375183
        vf_loss: 0.04462854191660881
    load_time_ms: 1.643
    num_steps_sampled: 4029300
    num_steps_trained: 3996000
    sample_time_ms: 1879.265
    update_time_ms: 3.967
  iterations_since_restore: 111
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.005097843697372
    mean_inference_ms: 1.364095447850689
    mean_processing_ms: 3.3075969503588265
  time_since_restore: 926.7653892040253
  time_this_iter_s: 7.696821212768555
  time_total_s: 926.7653892040253
  timestamp: 1563925163
  timesteps_since_restore: 4029300
  timesteps_this_iter: 36300
  timesteps_total: 4029300
  training_iteration: 111
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 926 s, 111 iter, 4029300 ts, 19.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.149684199406785
  episode_reward_mean: 18.567208481057584
  episode_reward_min: 1.3594002168519128
  episodes_this_iter: 242
  episodes_total: 27104
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6620.771
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2727810144424438
        kl: 0.017145948484539986
        policy_loss: -0.017773358151316643
        total_loss: 0.035816386342048645
        vf_explained_var: 0.9985172748565674
        vf_loss: 0.05037487670779228
    load_time_ms: 1.638
    num_steps_sampled: 4065600
    num_steps_trained: 4032000
    sample_time_ms: 1743.059
    update_time_ms: 4.009
  iterations_since_restore: 112
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.004702972119136
    mean_inference_ms: 1.3639121416996547
    mean_processing_ms: 3.307513680343172
  time_since_restore: 935.6117866039276
  time_this_iter_s: 8.846397399902344
  time_total_s: 935.6117866039276
  timestamp: 1563925172
  timesteps_since_restore: 4065600
  timesteps_this_iter: 36300
  timesteps_total: 4065600
  training_iteration: 112
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 935 s, 112 iter, 4065600 ts, 18.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.91643709165986
  episode_reward_mean: 19.84178962724087
  episode_reward_min: -1.2748860250735192
  episodes_this_iter: 242
  episodes_total: 27346
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6619.43
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2601737976074219
        kl: 0.015967344865202904
        policy_loss: -0.018519025295972824
        total_loss: 0.028651509433984756
        vf_explained_var: 0.9988752007484436
        vf_loss: 0.04417665675282478
    load_time_ms: 1.641
    num_steps_sampled: 4101900
    num_steps_trained: 4068000
    sample_time_ms: 1874.41
    update_time_ms: 3.978
  iterations_since_restore: 113
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.004301625771226
    mean_inference_ms: 1.3638958253589315
    mean_processing_ms: 3.3075147538881806
  time_since_restore: 944.0138487815857
  time_this_iter_s: 8.402062177658081
  time_total_s: 944.0138487815857
  timestamp: 1563925180
  timesteps_since_restore: 4101900
  timesteps_this_iter: 36300
  timesteps_total: 4101900
  training_iteration: 113
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 944 s, 113 iter, 4101900 ts, 19.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.112213760439815
  episode_reward_mean: 19.362362051156783
  episode_reward_min: -9.376221256866387
  episodes_this_iter: 242
  episodes_total: 27588
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6815.872
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2482471466064453
        kl: 0.01693485490977764
        policy_loss: -0.01755434274673462
        total_loss: 0.0310432780534029
        vf_explained_var: 0.9988102316856384
        vf_loss: 0.04542233422398567
    load_time_ms: 1.652
    num_steps_sampled: 4138200
    num_steps_trained: 4104000
    sample_time_ms: 1873.565
    update_time_ms: 4.017
  iterations_since_restore: 114
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.004274951046406
    mean_inference_ms: 1.3637719214049757
    mean_processing_ms: 3.3078413795484938
  time_since_restore: 952.8745229244232
  time_this_iter_s: 8.860674142837524
  time_total_s: 952.8745229244232
  timestamp: 1563925189
  timesteps_since_restore: 4138200
  timesteps_this_iter: 36300
  timesteps_total: 4138200
  training_iteration: 114
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 952 s, 114 iter, 4138200 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-39-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.13254552465257
  episode_reward_mean: 20.207887394960522
  episode_reward_min: -1.3696359716275586
  episodes_this_iter: 242
  episodes_total: 27830
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6830.63
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.229398250579834
        kl: 0.016557689756155014
        policy_loss: -0.02173589915037155
        total_loss: 0.01883716695010662
        vf_explained_var: 0.9990816116333008
        vf_loss: 0.03746849671006203
    load_time_ms: 1.657
    num_steps_sampled: 4174500
    num_steps_trained: 4140000
    sample_time_ms: 1870.572
    update_time_ms: 4.182
  iterations_since_restore: 115
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.004263468843674
    mean_inference_ms: 1.3638563169686369
    mean_processing_ms: 3.3075262297665646
  time_since_restore: 963.0792660713196
  time_this_iter_s: 10.204743146896362
  time_total_s: 963.0792660713196
  timestamp: 1563925199
  timesteps_since_restore: 4174500
  timesteps_this_iter: 36300
  timesteps_total: 4174500
  training_iteration: 115
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 963 s, 115 iter, 4174500 ts, 20.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-40-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.284716520409276
  episode_reward_mean: 19.2461196294744
  episode_reward_min: -1.9734661755167953
  episodes_this_iter: 242
  episodes_total: 28072
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6987.797
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.225034236907959
        kl: 0.018455270677804947
        policy_loss: -0.019474070519208908
        total_loss: 0.039282288402318954
        vf_explained_var: 0.9986308217048645
        vf_loss: 0.05529598891735077
    load_time_ms: 1.648
    num_steps_sampled: 4210800
    num_steps_trained: 4176000
    sample_time_ms: 1870.909
    update_time_ms: 4.193
  iterations_since_restore: 116
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003805163289447
    mean_inference_ms: 1.3638591316270796
    mean_processing_ms: 3.306936900466353
  time_since_restore: 971.9000458717346
  time_this_iter_s: 8.820779800415039
  time_total_s: 971.9000458717346
  timestamp: 1563925208
  timesteps_since_restore: 4210800
  timesteps_this_iter: 36300
  timesteps_total: 4210800
  training_iteration: 116
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 971 s, 116 iter, 4210800 ts, 19.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-40-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.54239562454783
  episode_reward_mean: 19.37015298021548
  episode_reward_min: -0.7261213596106315
  episodes_this_iter: 242
  episodes_total: 28314
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6988.189
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2095911502838135
        kl: 0.018552206456661224
        policy_loss: -0.018424786627292633
        total_loss: 0.024614769965410233
        vf_explained_var: 0.9989436268806458
        vf_loss: 0.03956101834774017
    load_time_ms: 1.663
    num_steps_sampled: 4247100
    num_steps_trained: 4212000
    sample_time_ms: 1736.32
    update_time_ms: 4.341
  iterations_since_restore: 117
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003399416691739
    mean_inference_ms: 1.363918557733652
    mean_processing_ms: 3.3065375774999746
  time_since_restore: 980.7941901683807
  time_this_iter_s: 8.894144296646118
  time_total_s: 980.7941901683807
  timestamp: 1563925217
  timesteps_since_restore: 4247100
  timesteps_this_iter: 36300
  timesteps_total: 4247100
  training_iteration: 117
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 980 s, 117 iter, 4247100 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-40-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.37050579063053
  episode_reward_mean: 18.414652915879977
  episode_reward_min: -1.3507353882737356
  episodes_this_iter: 242
  episodes_total: 28556
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6849.504
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1943849325180054
        kl: 0.015325618907809258
        policy_loss: -0.019492903724312782
        total_loss: 0.017582915723323822
        vf_explained_var: 0.9990736842155457
        vf_loss: 0.03420226648449898
    load_time_ms: 1.672
    num_steps_sampled: 4283400
    num_steps_trained: 4248000
    sample_time_ms: 1870.717
    update_time_ms: 4.491
  iterations_since_restore: 118
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0033718998921
    mean_inference_ms: 1.3636604690692171
    mean_processing_ms: 3.306619745886139
  time_since_restore: 989.6345312595367
  time_this_iter_s: 8.840341091156006
  time_total_s: 989.6345312595367
  timestamp: 1563925226
  timesteps_since_restore: 4283400
  timesteps_this_iter: 36300
  timesteps_total: 4283400
  training_iteration: 118
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 989 s, 118 iter, 4283400 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-40-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.320235011795994
  episode_reward_mean: 17.967080538360698
  episode_reward_min: -4.18514031788515
  episodes_this_iter: 242
  episodes_total: 28798
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7051.873
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1831828355789185
        kl: 0.017943719401955605
        policy_loss: -0.01857168786227703
        total_loss: 0.021596724167466164
        vf_explained_var: 0.9989365339279175
        vf_loss: 0.0368039570748806
    load_time_ms: 1.671
    num_steps_sampled: 4319700
    num_steps_trained: 4284000
    sample_time_ms: 1871.516
    update_time_ms: 4.349
  iterations_since_restore: 119
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003152470346733
    mean_inference_ms: 1.363532151323094
    mean_processing_ms: 3.3064649449478596
  time_since_restore: 998.5390074253082
  time_this_iter_s: 8.904476165771484
  time_total_s: 998.5390074253082
  timestamp: 1563925235
  timesteps_since_restore: 4319700
  timesteps_this_iter: 36300
  timesteps_total: 4319700
  training_iteration: 119
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 998 s, 119 iter, 4319700 ts, 18 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-40-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.60750130837922
  episode_reward_mean: 19.428013880956037
  episode_reward_min: -0.5845967301034737
  episodes_this_iter: 242
  episodes_total: 29040
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 7067.831
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1707690954208374
        kl: 0.01742940954864025
        policy_loss: -0.020109161734580994
        total_loss: 0.01570347510278225
        vf_explained_var: 0.9991447925567627
        vf_loss: 0.03254461660981178
    load_time_ms: 1.677
    num_steps_sampled: 4356000
    num_steps_trained: 4320000
    sample_time_ms: 1734.179
    update_time_ms: 4.493
  iterations_since_restore: 120
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002881342330489
    mean_inference_ms: 1.3635269612429974
    mean_processing_ms: 3.3060274979492887
  time_since_restore: 1007.389238357544
  time_this_iter_s: 8.850230932235718
  time_total_s: 1007.389238357544
  timestamp: 1563925244
  timesteps_since_restore: 4356000
  timesteps_this_iter: 36300
  timesteps_total: 4356000
  training_iteration: 120
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1007 s, 120 iter, 4356000 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-40-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.34437079223408
  episode_reward_mean: 18.561241274127056
  episode_reward_min: -0.9791558709794344
  episodes_this_iter: 242
  episodes_total: 29282
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6988.944
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1602262258529663
        kl: 0.015218045562505722
        policy_loss: -0.020776497200131416
        total_loss: 0.016511384397745132
        vf_explained_var: 0.9990102052688599
        vf_loss: 0.034434493631124496
    load_time_ms: 1.677
    num_steps_sampled: 4392300
    num_steps_trained: 4356000
    sample_time_ms: 1868.41
    update_time_ms: 4.703
  iterations_since_restore: 121
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00310511283821
    mean_inference_ms: 1.3634744785969763
    mean_processing_ms: 3.3061029700267057
  time_since_restore: 1015.6390347480774
  time_this_iter_s: 8.249796390533447
  time_total_s: 1015.6390347480774
  timestamp: 1563925252
  timesteps_since_restore: 4392300
  timesteps_this_iter: 36300
  timesteps_total: 4392300
  training_iteration: 121
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1015 s, 121 iter, 4392300 ts, 18.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.25380359796095
  episode_reward_mean: 19.15728499031209
  episode_reward_min: -0.5957074951033458
  episodes_this_iter: 242
  episodes_total: 29524
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6989.92
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1587529182434082
        kl: 0.036342982202768326
        policy_loss: -0.02083386667072773
        total_loss: 0.5470583438873291
        vf_explained_var: 0.986974835395813
        vf_loss: 0.5610777735710144
    load_time_ms: 1.677
    num_steps_sampled: 4428600
    num_steps_trained: 4392000
    sample_time_ms: 1870.949
    update_time_ms: 4.694
  iterations_since_restore: 122
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003359158181322
    mean_inference_ms: 1.3633569703735682
    mean_processing_ms: 3.3064170357639258
  time_since_restore: 1024.5176775455475
  time_this_iter_s: 8.878642797470093
  time_total_s: 1024.5176775455475
  timestamp: 1563925261
  timesteps_since_restore: 4428600
  timesteps_this_iter: 36300
  timesteps_total: 4428600
  training_iteration: 122
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1024 s, 122 iter, 4428600 ts, 19.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.25542975752363
  episode_reward_mean: 19.411504046248627
  episode_reward_min: -0.3510886620917475
  episodes_this_iter: 242
  episodes_total: 29766
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6970.727
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.1408019065856934
        kl: 0.019374815747141838
        policy_loss: -0.018322013318538666
        total_loss: 0.021460264921188354
        vf_explained_var: 0.9991498589515686
        vf_loss: 0.03433311730623245
    load_time_ms: 1.674
    num_steps_sampled: 4464900
    num_steps_trained: 4428000
    sample_time_ms: 1872.364
    update_time_ms: 4.603
  iterations_since_restore: 123
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003351690072997
    mean_inference_ms: 1.3634074452528437
    mean_processing_ms: 3.306474045695279
  time_since_restore: 1032.7397882938385
  time_this_iter_s: 8.222110748291016
  time_total_s: 1032.7397882938385
  timestamp: 1563925269
  timesteps_since_restore: 4464900
  timesteps_this_iter: 36300
  timesteps_total: 4464900
  training_iteration: 123
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1032 s, 123 iter, 4464900 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.59549604966113
  episode_reward_mean: 18.69583428596585
  episode_reward_min: -0.2987116913461426
  episodes_this_iter: 242
  episodes_total: 30008
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6906.898
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.137619972229004
        kl: 0.018528567627072334
        policy_loss: -0.019478991627693176
        total_loss: 0.02321811579167843
        vf_explained_var: 0.9989659786224365
        vf_loss: 0.037485942244529724
    load_time_ms: 1.674
    num_steps_sampled: 4501200
    num_steps_trained: 4464000
    sample_time_ms: 1873.955
    update_time_ms: 4.584
  iterations_since_restore: 124
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003426411833772
    mean_inference_ms: 1.363460802165739
    mean_processing_ms: 3.3067676118349834
  time_since_restore: 1040.9738969802856
  time_this_iter_s: 8.234108686447144
  time_total_s: 1040.9738969802856
  timestamp: 1563925277
  timesteps_since_restore: 4501200
  timesteps_this_iter: 36300
  timesteps_total: 4501200
  training_iteration: 124
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1040 s, 124 iter, 4501200 ts, 18.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.827778842927756
  episode_reward_mean: 18.42321547883669
  episode_reward_min: 0.15450185856346238
  episodes_this_iter: 242
  episodes_total: 30250
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6905.514
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.1174890995025635
        kl: 0.016942333430051804
        policy_loss: -0.01906665787100792
        total_loss: 0.015886250883340836
        vf_explained_var: 0.9991519451141357
        vf_loss: 0.030187878757715225
    load_time_ms: 1.683
    num_steps_sampled: 4537500
    num_steps_trained: 4500000
    sample_time_ms: 1740.466
    update_time_ms: 4.541
  iterations_since_restore: 125
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003071689962235
    mean_inference_ms: 1.3635941929390802
    mean_processing_ms: 3.306656148139487
  time_since_restore: 1049.8279988765717
  time_this_iter_s: 8.85410189628601
  time_total_s: 1049.8279988765717
  timestamp: 1563925286
  timesteps_since_restore: 4537500
  timesteps_this_iter: 36300
  timesteps_total: 4537500
  training_iteration: 125
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1049 s, 125 iter, 4537500 ts, 18.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.84918449451014
  episode_reward_mean: 18.825307012526576
  episode_reward_min: -5.825430777316838
  episodes_this_iter: 242
  episodes_total: 30492
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6909.653
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.1045031547546387
        kl: 0.013330868445336819
        policy_loss: -0.021128691732883453
        total_loss: 0.01674916408956051
        vf_explained_var: 0.9990594387054443
        vf_loss: 0.034128542989492416
    load_time_ms: 1.686
    num_steps_sampled: 4573800
    num_steps_trained: 4536000
    sample_time_ms: 1874.771
    update_time_ms: 4.562
  iterations_since_restore: 126
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003093656942917
    mean_inference_ms: 1.3635710144035
    mean_processing_ms: 3.306326543836277
  time_since_restore: 1060.033225774765
  time_this_iter_s: 10.20522689819336
  time_total_s: 1060.033225774765
  timestamp: 1563925296
  timesteps_since_restore: 4573800
  timesteps_this_iter: 36300
  timesteps_total: 4573800
  training_iteration: 126
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1060 s, 126 iter, 4573800 ts, 18.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.739713647891705
  episode_reward_mean: 19.014819541832864
  episode_reward_min: -0.6803831296390939
  episodes_this_iter: 242
  episodes_total: 30734
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6708.657
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0957216024398804
        kl: 0.016603341326117516
        policy_loss: -0.017925681546330452
        total_loss: 0.015953052788972855
        vf_explained_var: 0.9991672039031982
        vf_loss: 0.02920905128121376
    load_time_ms: 1.671
    num_steps_sampled: 4610100
    num_steps_trained: 4572000
    sample_time_ms: 1873.744
    update_time_ms: 4.523
  iterations_since_restore: 127
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002894862338261
    mean_inference_ms: 1.363465735458053
    mean_processing_ms: 3.3055027874049103
  time_since_restore: 1066.9029829502106
  time_this_iter_s: 6.869757175445557
  time_total_s: 1066.9029829502106
  timestamp: 1563925303
  timesteps_since_restore: 4610100
  timesteps_this_iter: 36300
  timesteps_total: 4610100
  training_iteration: 127
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1066 s, 127 iter, 4610100 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.762247341436485
  episode_reward_mean: 18.558415981892654
  episode_reward_min: -1.6775298011496458
  episodes_this_iter: 242
  episodes_total: 30976
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6708.34
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0940423011779785
        kl: 0.014585327357053757
        policy_loss: -0.016169710084795952
        total_loss: 0.02179778553545475
        vf_explained_var: 0.9990460872650146
        vf_loss: 0.03386537730693817
    load_time_ms: 1.662
    num_steps_sampled: 4646400
    num_steps_trained: 4608000
    sample_time_ms: 1741.746
    update_time_ms: 4.273
  iterations_since_restore: 128
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002971395096274
    mean_inference_ms: 1.3633885165835582
    mean_processing_ms: 3.305610603789031
  time_since_restore: 1074.4181571006775
  time_this_iter_s: 7.515174150466919
  time_total_s: 1074.4181571006775
  timestamp: 1563925311
  timesteps_since_restore: 4646400
  timesteps_this_iter: 36300
  timesteps_total: 4646400
  training_iteration: 128
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1074 s, 128 iter, 4646400 ts, 18.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-41-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.20488791174802
  episode_reward_mean: 18.170806572528473
  episode_reward_min: -1.2967154283700584
  episodes_this_iter: 242
  episodes_total: 31218
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6508.158
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0746104717254639
        kl: 0.014232673682272434
        policy_loss: -0.019494811072945595
        total_loss: 0.012366744689643383
        vf_explained_var: 0.9991993308067322
        vf_loss: 0.02785862237215042
    load_time_ms: 1.652
    num_steps_sampled: 4682700
    num_steps_trained: 4644000
    sample_time_ms: 1876.723
    update_time_ms: 4.272
  iterations_since_restore: 129
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003071422853481
    mean_inference_ms: 1.3634126607366412
    mean_processing_ms: 3.306064592692493
  time_since_restore: 1082.6639828681946
  time_this_iter_s: 8.24582576751709
  time_total_s: 1082.6639828681946
  timestamp: 1563925319
  timesteps_since_restore: 4682700
  timesteps_this_iter: 36300
  timesteps_total: 4682700
  training_iteration: 129
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1082 s, 129 iter, 4682700 ts, 18.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.289213397770595
  episode_reward_mean: 19.405244680840603
  episode_reward_min: 0.16270745912396956
  episodes_this_iter: 242
  episodes_total: 31460
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6395.004
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0718708038330078
        kl: 0.01539670117199421
        policy_loss: -0.017843224108219147
        total_loss: 0.016529472544789314
        vf_explained_var: 0.9991827011108398
        vf_loss: 0.030042370781302452
    load_time_ms: 1.645
    num_steps_sampled: 4719000
    num_steps_trained: 4680000
    sample_time_ms: 1879.162
    update_time_ms: 4.139
  iterations_since_restore: 130
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003200205667632
    mean_inference_ms: 1.3634020710666606
    mean_processing_ms: 3.3060674422192347
  time_since_restore: 1090.402702331543
  time_this_iter_s: 7.738719463348389
  time_total_s: 1090.402702331543
  timestamp: 1563925327
  timesteps_since_restore: 4719000
  timesteps_this_iter: 36300
  timesteps_total: 4719000
  training_iteration: 130
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1090 s, 130 iter, 4719000 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.17910317756417
  episode_reward_mean: 18.766856029485293
  episode_reward_min: -0.6258368351117518
  episodes_this_iter: 242
  episodes_total: 31702
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6586.668
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0664992332458496
        kl: 0.014983813278377056
        policy_loss: -0.021536335349082947
        total_loss: 0.011085500940680504
        vf_explained_var: 0.9991917014122009
        vf_loss: 0.028407633304595947
    load_time_ms: 1.651
    num_steps_sampled: 4755300
    num_steps_trained: 4716000
    sample_time_ms: 1879.5
    update_time_ms: 3.972
  iterations_since_restore: 131
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002991702670998
    mean_inference_ms: 1.3635499074065591
    mean_processing_ms: 3.306429189463805
  time_since_restore: 1100.5753219127655
  time_this_iter_s: 10.172619581222534
  time_total_s: 1100.5753219127655
  timestamp: 1563925337
  timesteps_since_restore: 4755300
  timesteps_this_iter: 36300
  timesteps_total: 4755300
  training_iteration: 131
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1100 s, 131 iter, 4755300 ts, 18.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.456429029886394
  episode_reward_mean: 18.692451252874424
  episode_reward_min: -1.007748663246223
  episodes_this_iter: 242
  episodes_total: 31944
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6577.488
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0592888593673706
        kl: 0.015659671276807785
        policy_loss: -0.016645558178424835
        total_loss: 0.014312045648694038
        vf_explained_var: 0.9992716908454895
        vf_loss: 0.026553325355052948
    load_time_ms: 1.664
    num_steps_sampled: 4791600
    num_steps_trained: 4752000
    sample_time_ms: 1876.828
    update_time_ms: 4.041
  iterations_since_restore: 132
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002776742586859
    mean_inference_ms: 1.3634153891000185
    mean_processing_ms: 3.3061109745305304
  time_since_restore: 1109.3341920375824
  time_this_iter_s: 8.758870124816895
  time_total_s: 1109.3341920375824
  timestamp: 1563925346
  timesteps_since_restore: 4791600
  timesteps_this_iter: 36300
  timesteps_total: 4791600
  training_iteration: 132
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1109 s, 132 iter, 4791600 ts, 18.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.551612395951835
  episode_reward_mean: 18.481967741100252
  episode_reward_min: -0.6266400857877066
  episodes_this_iter: 242
  episodes_total: 32186
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6596.598
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0473238229751587
        kl: 0.018309984356164932
        policy_loss: -0.021032003685832024
        total_loss: 0.008767369203269482
        vf_explained_var: 0.99931401014328
        vf_loss: 0.024649687111377716
    load_time_ms: 1.658
    num_steps_sampled: 4827900
    num_steps_trained: 4788000
    sample_time_ms: 1741.485
    update_time_ms: 3.992
  iterations_since_restore: 133
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002223863663934
    mean_inference_ms: 1.3633650528139503
    mean_processing_ms: 3.3058075730493908
  time_since_restore: 1116.3932254314423
  time_this_iter_s: 7.059033393859863
  time_total_s: 1116.3932254314423
  timestamp: 1563925353
  timesteps_since_restore: 4827900
  timesteps_this_iter: 36300
  timesteps_total: 4827900
  training_iteration: 133
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1116 s, 133 iter, 4827900 ts, 18.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.41067683540795
  episode_reward_mean: 18.897508823592048
  episode_reward_min: -2.4625171581944234
  episodes_this_iter: 242
  episodes_total: 32428
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6661.502
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0396491289138794
        kl: 0.013142239302396774
        policy_loss: -0.01834128610789776
        total_loss: 0.012231743894517422
        vf_explained_var: 0.999263346195221
        vf_loss: 0.026876773685216904
    load_time_ms: 1.65
    num_steps_sampled: 4864200
    num_steps_trained: 4824000
    sample_time_ms: 1875.378
    update_time_ms: 3.944
  iterations_since_restore: 134
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002530475115482
    mean_inference_ms: 1.363494903482126
    mean_processing_ms: 3.306333886736315
  time_since_restore: 1126.6186175346375
  time_this_iter_s: 10.22539210319519
  time_total_s: 1126.6186175346375
  timestamp: 1563925363
  timesteps_since_restore: 4864200
  timesteps_this_iter: 36300
  timesteps_total: 4864200
  training_iteration: 134
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1126 s, 134 iter, 4864200 ts, 18.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.93255930852226
  episode_reward_mean: 19.40807225175183
  episode_reward_min: -0.8053453263528207
  episodes_this_iter: 242
  episodes_total: 32670
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6462.603
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0341486930847168
        kl: 0.015788260847330093
        policy_loss: -0.020006319507956505
        total_loss: 0.011661986820399761
        vf_explained_var: 0.999291181564331
        vf_loss: 0.0272278543561697
    load_time_ms: 1.636
    num_steps_sampled: 4900500
    num_steps_trained: 4860000
    sample_time_ms: 1874.835
    update_time_ms: 4.004
  iterations_since_restore: 135
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002283278471483
    mean_inference_ms: 1.3635702438411916
    mean_processing_ms: 3.3060938410132517
  time_since_restore: 1133.4742558002472
  time_this_iter_s: 6.855638265609741
  time_total_s: 1133.4742558002472
  timestamp: 1563925370
  timesteps_since_restore: 4900500
  timesteps_this_iter: 36300
  timesteps_total: 4900500
  training_iteration: 135
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1133 s, 135 iter, 4900500 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-42-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.36266538627973
  episode_reward_mean: 19.410282885743396
  episode_reward_min: -1.0452048257799882
  episodes_this_iter: 242
  episodes_total: 32912
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6264.311
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.019279956817627
        kl: 0.015150229446589947
        policy_loss: -0.018585367128252983
        total_loss: 0.01071943063288927
        vf_explained_var: 0.9993665218353271
        vf_loss: 0.025043796747922897
    load_time_ms: 1.635
    num_steps_sampled: 4936800
    num_steps_trained: 4896000
    sample_time_ms: 1743.953
    update_time_ms: 3.938
  iterations_since_restore: 136
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0026856339713675
    mean_inference_ms: 1.363575815311101
    mean_processing_ms: 3.3064177585806918
  time_since_restore: 1140.382268667221
  time_this_iter_s: 6.908012866973877
  time_total_s: 1140.382268667221
  timestamp: 1563925377
  timesteps_since_restore: 4936800
  timesteps_this_iter: 36300
  timesteps_total: 4936800
  training_iteration: 136
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1140 s, 136 iter, 4936800 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.528776487001004
  episode_reward_mean: 20.045171906590987
  episode_reward_min: 0.08689456958891202
  episodes_this_iter: 242
  episodes_total: 33154
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6398.677
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.005617380142212
        kl: 0.015713641420006752
        policy_loss: -0.021741056814789772
        total_loss: 0.00597128551453352
        vf_explained_var: 0.9994326829910278
        vf_loss: 0.02329287864267826
    load_time_ms: 1.652
    num_steps_sampled: 4973100
    num_steps_trained: 4932000
    sample_time_ms: 1879.625
    update_time_ms: 3.822
  iterations_since_restore: 137
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003044028096865
    mean_inference_ms: 1.3636894005836282
    mean_processing_ms: 3.3071379812457598
  time_since_restore: 1149.9557149410248
  time_this_iter_s: 9.573446273803711
  time_total_s: 1149.9557149410248
  timestamp: 1563925387
  timesteps_since_restore: 4973100
  timesteps_this_iter: 36300
  timesteps_total: 4973100
  training_iteration: 137
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1149 s, 137 iter, 4973100 ts, 20 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.31390728696629
  episode_reward_mean: 19.482077579452717
  episode_reward_min: -0.9997370645767233
  episodes_this_iter: 242
  episodes_total: 33396
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6337.744
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 1.0006194114685059
        kl: 0.0174447949975729
        policy_loss: -0.0205693356692791
        total_loss: 0.006784850265830755
        vf_explained_var: 0.9993994832038879
        vf_loss: 0.02244783751666546
    load_time_ms: 1.653
    num_steps_sampled: 5009400
    num_steps_trained: 4968000
    sample_time_ms: 1877.715
    update_time_ms: 3.92
  iterations_since_restore: 138
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002793222144573
    mean_inference_ms: 1.3636572976805634
    mean_processing_ms: 3.306907732186617
  time_since_restore: 1156.8417689800262
  time_this_iter_s: 6.886054039001465
  time_total_s: 1156.8417689800262
  timestamp: 1563925393
  timesteps_since_restore: 5009400
  timesteps_this_iter: 36300
  timesteps_total: 5009400
  training_iteration: 138
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1156 s, 138 iter, 5009400 ts, 19.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.58720947150649
  episode_reward_mean: 19.371676171663797
  episode_reward_min: -0.6161951208832958
  episodes_this_iter: 242
  episodes_total: 33638
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6535.553
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9919260740280151
        kl: 0.015963444486260414
        policy_loss: -0.020394325256347656
        total_loss: 0.007147158030420542
        vf_explained_var: 0.9993895292282104
        vf_loss: 0.023051762953400612
    load_time_ms: 1.659
    num_steps_sampled: 5045700
    num_steps_trained: 5004000
    sample_time_ms: 1878.576
    update_time_ms: 3.901
  iterations_since_restore: 139
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003315159069894
    mean_inference_ms: 1.3635185141735466
    mean_processing_ms: 3.307081796627388
  time_since_restore: 1167.0790402889252
  time_this_iter_s: 10.237271308898926
  time_total_s: 1167.0790402889252
  timestamp: 1563925404
  timesteps_since_restore: 5045700
  timesteps_this_iter: 36300
  timesteps_total: 5045700
  training_iteration: 139
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1167 s, 139 iter, 5045700 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.64358298564886
  episode_reward_mean: 19.741464137115845
  episode_reward_min: -8.702321480301247
  episodes_this_iter: 242
  episodes_total: 33880
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6648.736
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.987310528755188
        kl: 0.01707020029425621
        policy_loss: -0.018886346369981766
        total_loss: 0.010681774467229843
        vf_explained_var: 0.9993630647659302
        vf_loss: 0.024767132475972176
    load_time_ms: 1.668
    num_steps_sampled: 5082000
    num_steps_trained: 5040000
    sample_time_ms: 1876.311
    update_time_ms: 4.065
  iterations_since_restore: 140
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00298825331054
    mean_inference_ms: 1.3634896171398025
    mean_processing_ms: 3.3067675699427
  time_since_restore: 1175.932246685028
  time_this_iter_s: 8.853206396102905
  time_total_s: 1175.932246685028
  timestamp: 1563925413
  timesteps_since_restore: 5082000
  timesteps_this_iter: 36300
  timesteps_total: 5082000
  training_iteration: 140
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1175 s, 140 iter, 5082000 ts, 19.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.44043093527509
  episode_reward_mean: 18.889277176532502
  episode_reward_min: 1.276007213254552
  episodes_this_iter: 242
  episodes_total: 34122
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6454.358
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9822868704795837
        kl: 0.017655279487371445
        policy_loss: -0.019222073256969452
        total_loss: 0.009482408873736858
        vf_explained_var: 0.9993445873260498
        vf_loss: 0.023738937452435493
    load_time_ms: 1.659
    num_steps_sampled: 5118300
    num_steps_trained: 5076000
    sample_time_ms: 1743.257
    update_time_ms: 4.164
  iterations_since_restore: 141
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002668291891637
    mean_inference_ms: 1.3635274146628924
    mean_processing_ms: 3.306380793268437
  time_since_restore: 1182.8276724815369
  time_this_iter_s: 6.895425796508789
  time_total_s: 1182.8276724815369
  timestamp: 1563925419
  timesteps_since_restore: 5118300
  timesteps_this_iter: 36300
  timesteps_total: 5118300
  training_iteration: 141
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1182 s, 141 iter, 5118300 ts, 18.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.488402334825956
  episode_reward_mean: 19.39424843808871
  episode_reward_min: -0.16774649602341923
  episodes_this_iter: 242
  episodes_total: 34364
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6463.393
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9685221910476685
        kl: 0.013675798662006855
        policy_loss: -0.01797233335673809
        total_loss: 0.009864074178040028
        vf_explained_var: 0.9993897676467896
        vf_loss: 0.02399008721113205
    load_time_ms: 1.65
    num_steps_sampled: 5154600
    num_steps_trained: 5112000
    sample_time_ms: 1878.362
    update_time_ms: 4.124
  iterations_since_restore: 142
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003092845965332
    mean_inference_ms: 1.3634110439506755
    mean_processing_ms: 3.306686591617858
  time_since_restore: 1193.0307171344757
  time_this_iter_s: 10.203044652938843
  time_total_s: 1193.0307171344757
  timestamp: 1563925430
  timesteps_since_restore: 5154600
  timesteps_this_iter: 36300
  timesteps_total: 5154600
  training_iteration: 142
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1193 s, 142 iter, 5154600 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-43-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.46953192196365
  episode_reward_mean: 20.141815711711534
  episode_reward_min: -7.505848205743683
  episodes_this_iter: 242
  episodes_total: 34606
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6526.953
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9641014933586121
        kl: 0.016603467985987663
        policy_loss: -0.021481994539499283
        total_loss: 0.010715584270656109
        vf_explained_var: 0.9993190765380859
        vf_loss: 0.027527855709195137
    load_time_ms: 1.65
    num_steps_sampled: 5190900
    num_steps_trained: 5148000
    sample_time_ms: 1878.678
    update_time_ms: 4.224
  iterations_since_restore: 143
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002866549425885
    mean_inference_ms: 1.3633224116785194
    mean_processing_ms: 3.3064467930253865
  time_since_restore: 1200.7318291664124
  time_this_iter_s: 7.7011120319366455
  time_total_s: 1200.7318291664124
  timestamp: 1563925437
  timesteps_since_restore: 5190900
  timesteps_this_iter: 36300
  timesteps_total: 5190900
  training_iteration: 143
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1200 s, 143 iter, 5190900 ts, 20.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.54289744844298
  episode_reward_mean: 19.5523604299572
  episode_reward_min: -1.4833977445216713
  episodes_this_iter: 242
  episodes_total: 34848
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6328.803
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9528184533119202
        kl: 0.015573537908494473
        policy_loss: -0.019680682569742203
        total_loss: 0.00673808716237545
        vf_explained_var: 0.9994127750396729
        vf_loss: 0.022038711234927177
    load_time_ms: 1.669
    num_steps_sampled: 5227200
    num_steps_trained: 5184000
    sample_time_ms: 1743.511
    update_time_ms: 4.303
  iterations_since_restore: 144
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002585552162367
    mean_inference_ms: 1.3633504260030795
    mean_processing_ms: 3.306387075840813
  time_since_restore: 1207.6192078590393
  time_this_iter_s: 6.887378692626953
  time_total_s: 1207.6192078590393
  timestamp: 1563925444
  timesteps_since_restore: 5227200
  timesteps_this_iter: 36300
  timesteps_total: 5227200
  training_iteration: 144
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1207 s, 144 iter, 5227200 ts, 19.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.66715194431487
  episode_reward_mean: 19.02129113494063
  episode_reward_min: -0.8727234710282529
  episodes_this_iter: 242
  episodes_total: 35090
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6497.842
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9349915385246277
        kl: 0.01631232537329197
        policy_loss: -0.02051573619246483
        total_loss: 0.0061045195907354355
        vf_explained_var: 0.9994137287139893
        vf_loss: 0.02203240990638733
    load_time_ms: 1.676
    num_steps_sampled: 5263500
    num_steps_trained: 5220000
    sample_time_ms: 1878.614
    update_time_ms: 4.161
  iterations_since_restore: 145
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003037117827625
    mean_inference_ms: 1.363324626158757
    mean_processing_ms: 3.3067500651494828
  time_since_restore: 1217.5185317993164
  time_this_iter_s: 9.8993239402771
  time_total_s: 1217.5185317993164
  timestamp: 1563925454
  timesteps_since_restore: 5263500
  timesteps_this_iter: 36300
  timesteps_total: 5263500
  training_iteration: 145
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1217 s, 145 iter, 5263500 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.755239344463135
  episode_reward_mean: 20.140007677218062
  episode_reward_min: -0.4767497019398509
  episodes_this_iter: 242
  episodes_total: 35332
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6496.506
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9346622824668884
        kl: 0.016222147271037102
        policy_loss: -0.0180944986641407
        total_loss: 0.014134117402136326
        vf_explained_var: 0.9993412494659424
        vf_loss: 0.027666134759783745
    load_time_ms: 1.676
    num_steps_sampled: 5299800
    num_steps_trained: 5256000
    sample_time_ms: 1874.59
    update_time_ms: 4.312
  iterations_since_restore: 146
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002710474660632
    mean_inference_ms: 1.363304148224662
    mean_processing_ms: 3.3060694752571487
  time_since_restore: 1224.375201702118
  time_this_iter_s: 6.856669902801514
  time_total_s: 1224.375201702118
  timestamp: 1563925461
  timesteps_since_restore: 5299800
  timesteps_this_iter: 36300
  timesteps_total: 5299800
  training_iteration: 146
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1224 s, 146 iter, 5299800 ts, 20.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.4712170414865
  episode_reward_mean: 19.352201632996568
  episode_reward_min: -0.3631838030303982
  episodes_this_iter: 242
  episodes_total: 35574
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6560.654
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9240434169769287
        kl: 0.01593458279967308
        policy_loss: -0.020084379240870476
        total_loss: 0.005770623683929443
        vf_explained_var: 0.9994362592697144
        vf_loss: 0.021373404189944267
    load_time_ms: 1.667
    num_steps_sampled: 5336100
    num_steps_trained: 5292000
    sample_time_ms: 1873.689
    update_time_ms: 4.293
  iterations_since_restore: 147
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.003038524128973
    mean_inference_ms: 1.3634233281611265
    mean_processing_ms: 3.3065211720567276
  time_since_restore: 1234.581871509552
  time_this_iter_s: 10.206669807434082
  time_total_s: 1234.581871509552
  timestamp: 1563925471
  timesteps_since_restore: 5336100
  timesteps_this_iter: 36300
  timesteps_total: 5336100
  training_iteration: 147
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1234 s, 147 iter, 5336100 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.387856806235284
  episode_reward_mean: 18.993147494791614
  episode_reward_min: -0.09941862448973135
  episodes_this_iter: 242
  episodes_total: 35816
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6757.299
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9182373881340027
        kl: 0.017512891441583633
        policy_loss: -0.02018064260482788
        total_loss: 0.004715505056083202
        vf_explained_var: 0.999471127986908
        vf_loss: 0.019970646128058434
    load_time_ms: 1.676
    num_steps_sampled: 5372400
    num_steps_trained: 5328000
    sample_time_ms: 1872.535
    update_time_ms: 4.384
  iterations_since_restore: 148
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002777238502429
    mean_inference_ms: 1.3633530446705224
    mean_processing_ms: 3.306167466552835
  time_since_restore: 1243.4272861480713
  time_this_iter_s: 8.845414638519287
  time_total_s: 1243.4272861480713
  timestamp: 1563925480
  timesteps_since_restore: 5372400
  timesteps_this_iter: 36300
  timesteps_total: 5372400
  training_iteration: 148
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1243 s, 148 iter, 5372400 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.70542256041524
  episode_reward_mean: 18.52771113005556
  episode_reward_min: -3.2655043583710364
  episodes_this_iter: 242
  episodes_total: 36058
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6753.876
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9092972874641418
        kl: 0.01689082942903042
        policy_loss: -0.019701234996318817
        total_loss: 0.011751892045140266
        vf_explained_var: 0.9992205500602722
        vf_loss: 0.026702584698796272
    load_time_ms: 1.67
    num_steps_sampled: 5408700
    num_steps_trained: 5364000
    sample_time_ms: 1736.237
    update_time_ms: 4.507
  iterations_since_restore: 149
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002539100052542
    mean_inference_ms: 1.3633009817638166
    mean_processing_ms: 3.305865938454476
  time_since_restore: 1252.268672466278
  time_this_iter_s: 8.841386318206787
  time_total_s: 1252.268672466278
  timestamp: 1563925489
  timesteps_since_restore: 5408700
  timesteps_this_iter: 36300
  timesteps_total: 5408700
  training_iteration: 149
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1252 s, 149 iter, 5408700 ts, 18.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-44-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.687530679014735
  episode_reward_mean: 19.513860791697038
  episode_reward_min: -1.2746678222416044
  episodes_this_iter: 242
  episodes_total: 36300
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6617.241
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8968871235847473
        kl: 0.01708395592868328
        policy_loss: -0.022091323509812355
        total_loss: 0.0034063076600432396
        vf_explained_var: 0.9994468688964844
        vf_loss: 0.020692767575383186
    load_time_ms: 1.668
    num_steps_sampled: 5445000
    num_steps_trained: 5400000
    sample_time_ms: 1868.736
    update_time_ms: 4.414
  iterations_since_restore: 150
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0025566931088985
    mean_inference_ms: 1.3631778311303429
    mean_processing_ms: 3.3057231926194803
  time_since_restore: 1261.075427532196
  time_this_iter_s: 8.806755065917969
  time_total_s: 1261.075427532196
  timestamp: 1563925498
  timesteps_since_restore: 5445000
  timesteps_this_iter: 36300
  timesteps_total: 5445000
  training_iteration: 150
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1261 s, 150 iter, 5445000 ts, 19.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.84755395071526
  episode_reward_mean: 18.98261280637296
  episode_reward_min: 0.06783924376006889
  episodes_this_iter: 242
  episodes_total: 36542
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6617.255
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8923170566558838
        kl: 0.017674937844276428
        policy_loss: -0.019835568964481354
        total_loss: 0.003720786888152361
        vf_explained_var: 0.999521791934967
        vf_loss: 0.01858527958393097
    load_time_ms: 1.671
    num_steps_sampled: 5481300
    num_steps_trained: 5436000
    sample_time_ms: 1868.31
    update_time_ms: 4.326
  iterations_since_restore: 151
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002350801607569
    mean_inference_ms: 1.3631487267405493
    mean_processing_ms: 3.3056217400015147
  time_since_restore: 1267.9648537635803
  time_this_iter_s: 6.889426231384277
  time_total_s: 1267.9648537635803
  timestamp: 1563925505
  timesteps_since_restore: 5481300
  timesteps_this_iter: 36300
  timesteps_total: 5481300
  training_iteration: 151
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1267 s, 151 iter, 5481300 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.705871667410825
  episode_reward_mean: 20.02774979372798
  episode_reward_min: 0.6438855622098867
  episodes_this_iter: 242
  episodes_total: 36784
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6549.536
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8798652291297913
        kl: 0.01656937226653099
        policy_loss: -0.01922079734504223
        total_loss: 0.003394300816580653
        vf_explained_var: 0.9995486736297607
        vf_loss: 0.017954956740140915
    load_time_ms: 1.679
    num_steps_sampled: 5517600
    num_steps_trained: 5472000
    sample_time_ms: 1736.857
    update_time_ms: 4.341
  iterations_since_restore: 152
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002508241784515
    mean_inference_ms: 1.3631894869910863
    mean_processing_ms: 3.30573824916469
  time_since_restore: 1276.1745328903198
  time_this_iter_s: 8.209679126739502
  time_total_s: 1276.1745328903198
  timestamp: 1563925513
  timesteps_since_restore: 5517600
  timesteps_this_iter: 36300
  timesteps_total: 5517600
  training_iteration: 152
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1276 s, 152 iter, 5517600 ts, 20 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.645210996767815
  episode_reward_mean: 19.886580269429945
  episode_reward_min: -14.214993182096388
  episodes_this_iter: 242
  episodes_total: 37026
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6467.382
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8793294429779053
        kl: 0.016343511641025543
        policy_loss: -0.018561504781246185
        total_loss: 0.009351043961942196
        vf_explained_var: 0.9994151592254639
        vf_loss: 0.023315932601690292
    load_time_ms: 1.681
    num_steps_sampled: 5553900
    num_steps_trained: 5508000
    sample_time_ms: 1871.035
    update_time_ms: 4.304
  iterations_since_restore: 153
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002541894012709
    mean_inference_ms: 1.363087461793471
    mean_processing_ms: 3.3061038656128274
  time_since_restore: 1284.3942391872406
  time_this_iter_s: 8.219706296920776
  time_total_s: 1284.3942391872406
  timestamp: 1563925521
  timesteps_since_restore: 5553900
  timesteps_this_iter: 36300
  timesteps_total: 5553900
  training_iteration: 153
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1284 s, 153 iter, 5553900 ts, 19.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.368452658245424
  episode_reward_mean: 18.287121128884873
  episode_reward_min: -0.7318521839491727
  episodes_this_iter: 242
  episodes_total: 37268
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6469.265
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8755829334259033
        kl: 0.01905139721930027
        policy_loss: -0.022270912304520607
        total_loss: 0.006791094783693552
        vf_explained_var: 0.9992983341217041
        vf_loss: 0.02370380237698555
    load_time_ms: 1.669
    num_steps_sampled: 5590200
    num_steps_trained: 5544000
    sample_time_ms: 1871.981
    update_time_ms: 4.276
  iterations_since_restore: 154
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002640283338364
    mean_inference_ms: 1.36312908199641
    mean_processing_ms: 3.306167634305396
  time_since_restore: 1291.3107287883759
  time_this_iter_s: 6.916489601135254
  time_total_s: 1291.3107287883759
  timestamp: 1563925528
  timesteps_since_restore: 5590200
  timesteps_this_iter: 36300
  timesteps_total: 5590200
  training_iteration: 154
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1291 s, 154 iter, 5590200 ts, 18.3 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.73438026878232
  episode_reward_mean: 19.610712308080682
  episode_reward_min: 0.19016345848064245
  episodes_this_iter: 242
  episodes_total: 37510
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6467.653
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.863361120223999
        kl: 0.017758037894964218
        policy_loss: -0.018437469378113747
        total_loss: 0.005591870751231909
        vf_explained_var: 0.9995338916778564
        vf_loss: 0.019034897908568382
    load_time_ms: 1.661
    num_steps_sampled: 5626500
    num_steps_trained: 5580000
    sample_time_ms: 1872.733
    update_time_ms: 4.263
  iterations_since_restore: 155
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002966178420586
    mean_inference_ms: 1.363191965816268
    mean_processing_ms: 3.30649438854451
  time_since_restore: 1301.2021763324738
  time_this_iter_s: 9.8914475440979
  time_total_s: 1301.2021763324738
  timestamp: 1563925538
  timesteps_since_restore: 5626500
  timesteps_this_iter: 36300
  timesteps_total: 5626500
  training_iteration: 155
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1301 s, 155 iter, 5626500 ts, 19.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.42327823961355
  episode_reward_mean: 20.037673995675956
  episode_reward_min: -0.724429233040788
  episodes_this_iter: 242
  episodes_total: 37752
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6667.234
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8517947793006897
        kl: 0.016470078378915787
        policy_loss: -0.017796821892261505
        total_loss: 0.0045199282467365265
        vf_explained_var: 0.999557614326477
        vf_loss: 0.017684543505311012
    load_time_ms: 1.672
    num_steps_sampled: 5662800
    num_steps_trained: 5616000
    sample_time_ms: 1873.074
    update_time_ms: 4.224
  iterations_since_restore: 156
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002642667620066
    mean_inference_ms: 1.3631355801427223
    mean_processing_ms: 3.3063222645014974
  time_since_restore: 1310.0608174800873
  time_this_iter_s: 8.858641147613525
  time_total_s: 1310.0608174800873
  timestamp: 1563925547
  timesteps_since_restore: 5662800
  timesteps_this_iter: 36300
  timesteps_total: 5662800
  training_iteration: 156
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1310 s, 156 iter, 5662800 ts, 20 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-45-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.111159468989605
  episode_reward_mean: 19.838145129311684
  episode_reward_min: 1.1373095904396266
  episodes_this_iter: 242
  episodes_total: 37994
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6662.669
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8424124717712402
        kl: 0.019039414823055267
        policy_loss: -0.02172117307782173
        total_loss: 0.00016364414477720857
        vf_explained_var: 0.9995550513267517
        vf_loss: 0.016529984772205353
    load_time_ms: 1.664
    num_steps_sampled: 5699100
    num_steps_trained: 5652000
    sample_time_ms: 1737.602
    update_time_ms: 4.433
  iterations_since_restore: 157
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002292988297388
    mean_inference_ms: 1.3630728578719087
    mean_processing_ms: 3.3059210151031824
  time_since_restore: 1318.8685445785522
  time_this_iter_s: 8.807727098464966
  time_total_s: 1318.8685445785522
  timestamp: 1563925556
  timesteps_since_restore: 5699100
  timesteps_this_iter: 36300
  timesteps_total: 5699100
  training_iteration: 157
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1318 s, 157 iter, 5699100 ts, 19.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.85185552549166
  episode_reward_mean: 19.188930854054057
  episode_reward_min: -0.9433798229001003
  episodes_this_iter: 242
  episodes_total: 38236
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6661.698
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8346267342567444
        kl: 0.01668461039662361
        policy_loss: -0.019630052149295807
        total_loss: 0.0014265450881794095
        vf_explained_var: 0.9995459318161011
        vf_loss: 0.016364047303795815
    load_time_ms: 1.673
    num_steps_sampled: 5735400
    num_steps_trained: 5688000
    sample_time_ms: 1870.797
    update_time_ms: 4.36
  iterations_since_restore: 158
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002115422912465
    mean_inference_ms: 1.3631427743502504
    mean_processing_ms: 3.3056055396804145
  time_since_restore: 1329.036957502365
  time_this_iter_s: 10.168412923812866
  time_total_s: 1329.036957502365
  timestamp: 1563925566
  timesteps_since_restore: 5735400
  timesteps_this_iter: 36300
  timesteps_total: 5735400
  training_iteration: 158
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1329 s, 158 iter, 5735400 ts, 19.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.22523149323317
  episode_reward_mean: 19.98867436188153
  episode_reward_min: -0.430621773918274
  episodes_this_iter: 242
  episodes_total: 38478
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6662.331
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8319266438484192
        kl: 0.017704060301184654
        policy_loss: -0.021973218768835068
        total_loss: 0.0008946580346673727
        vf_explained_var: 0.9995834827423096
        vf_loss: 0.017888614907860756
    load_time_ms: 1.677
    num_steps_sampled: 5771700
    num_steps_trained: 5724000
    sample_time_ms: 1870.289
    update_time_ms: 4.329
  iterations_since_restore: 159
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.001905001812974
    mean_inference_ms: 1.3630605479677638
    mean_processing_ms: 3.305259393633127
  time_since_restore: 1337.8804965019226
  time_this_iter_s: 8.843538999557495
  time_total_s: 1337.8804965019226
  timestamp: 1563925575
  timesteps_since_restore: 5771700
  timesteps_this_iter: 36300
  timesteps_total: 5771700
  training_iteration: 159
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1337 s, 159 iter, 5771700 ts, 20 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.272666547887276
  episode_reward_mean: 19.35811882897127
  episode_reward_min: -7.280546005930335
  episodes_this_iter: 242
  episodes_total: 38720
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6602.893
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.815477728843689
        kl: 0.01606660895049572
        policy_loss: -0.020336708053946495
        total_loss: 0.0023344706278294325
        vf_explained_var: 0.9995362758636475
        vf_loss: 0.018152443692088127
    load_time_ms: 1.678
    num_steps_sampled: 5808000
    num_steps_trained: 5760000
    sample_time_ms: 1738.044
    update_time_ms: 4.384
  iterations_since_restore: 160
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0016609991811265
    mean_inference_ms: 1.3630730609455644
    mean_processing_ms: 3.3048952255775075
  time_since_restore: 1344.769434928894
  time_this_iter_s: 6.8889384269714355
  time_total_s: 1344.769434928894
  timestamp: 1563925582
  timesteps_since_restore: 5808000
  timesteps_this_iter: 36300
  timesteps_total: 5808000
  training_iteration: 160
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1344 s, 160 iter, 5808000 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.67026806348128
  episode_reward_mean: 19.11727368765726
  episode_reward_min: 0.7277360225786903
  episodes_this_iter: 242
  episodes_total: 38962
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6641.041
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8049823045730591
        kl: 0.015826769173145294
        policy_loss: -0.022609051316976547
        total_loss: 0.0007637820672243834
        vf_explained_var: 0.9995259046554565
        vf_loss: 0.01892155222594738
    load_time_ms: 1.675
    num_steps_sampled: 5844300
    num_steps_trained: 5796000
    sample_time_ms: 1873.62
    update_time_ms: 4.327
  iterations_since_restore: 161
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.001903041101457
    mean_inference_ms: 1.3631698878657925
    mean_processing_ms: 3.305188434282303
  time_since_restore: 1353.3979518413544
  time_this_iter_s: 8.628516912460327
  time_total_s: 1353.3979518413544
  timestamp: 1563925590
  timesteps_since_restore: 5844300
  timesteps_this_iter: 36300
  timesteps_total: 5844300
  training_iteration: 161
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1353 s, 161 iter, 5844300 ts, 19.1 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.197504706977384
  episode_reward_mean: 19.974013593260114
  episode_reward_min: -0.7332501608815348
  episodes_this_iter: 242
  episodes_total: 39204
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6530.539
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7937116026878357
        kl: 0.017501557245850563
        policy_loss: -0.021726153790950775
        total_loss: -9.641764336265624e-05
        vf_explained_var: 0.9995821714401245
        vf_loss: 0.01670742593705654
    load_time_ms: 1.668
    num_steps_sampled: 5880600
    num_steps_trained: 5832000
    sample_time_ms: 1870.714
    update_time_ms: 4.295
  iterations_since_restore: 162
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.001759728839002
    mean_inference_ms: 1.3632907136445738
    mean_processing_ms: 3.305289549139471
  time_since_restore: 1360.4710776805878
  time_this_iter_s: 7.073125839233398
  time_total_s: 1360.4710776805878
  timestamp: 1563925597
  timesteps_since_restore: 5880600
  timesteps_this_iter: 36300
  timesteps_total: 5880600
  training_iteration: 162
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1360 s, 162 iter, 5880600 ts, 20 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.69790521062435
  episode_reward_mean: 18.75301277409451
  episode_reward_min: -1.035415477178055
  episodes_this_iter: 242
  episodes_total: 39446
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6528.966
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7884232401847839
        kl: 0.01945504918694496
        policy_loss: -0.023280074819922447
        total_loss: -0.0030239324551075697
        vf_explained_var: 0.999586820602417
        vf_loss: 0.01478441059589386
    load_time_ms: 1.671
    num_steps_sampled: 5916900
    num_steps_trained: 5868000
    sample_time_ms: 1872.373
    update_time_ms: 4.223
  iterations_since_restore: 163
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002003842602369
    mean_inference_ms: 1.3634793417603541
    mean_processing_ms: 3.3052124333474575
  time_since_restore: 1368.6917097568512
  time_this_iter_s: 8.220632076263428
  time_total_s: 1368.6917097568512
  timestamp: 1563925606
  timesteps_since_restore: 5916900
  timesteps_this_iter: 36300
  timesteps_total: 5916900
  training_iteration: 163
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1368 s, 163 iter, 5916900 ts, 18.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-46-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.91441806522211
  episode_reward_mean: 19.925664895628064
  episode_reward_min: 0.3371817670726309
  episodes_this_iter: 242
  episodes_total: 39688
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6726.46
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7837547063827515
        kl: 0.018204396590590477
        policy_loss: -0.020099546760320663
        total_loss: 2.4453529476886615e-05
        vf_explained_var: 0.9996283650398254
        vf_loss: 0.015004013665020466
    load_time_ms: 1.68
    num_steps_sampled: 5953200
    num_steps_trained: 5904000
    sample_time_ms: 1872.208
    update_time_ms: 4.231
  iterations_since_restore: 164
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00213664518848
    mean_inference_ms: 1.3634723707869372
    mean_processing_ms: 3.3052192049932754
  time_since_restore: 1377.5848100185394
  time_this_iter_s: 8.893100261688232
  time_total_s: 1377.5848100185394
  timestamp: 1563925615
  timesteps_since_restore: 5953200
  timesteps_this_iter: 36300
  timesteps_total: 5953200
  training_iteration: 164
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1377 s, 164 iter, 5953200 ts, 19.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.541403599198325
  episode_reward_mean: 19.384870166264967
  episode_reward_min: 0.9739966034585704
  episodes_this_iter: 242
  episodes_total: 39930
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6758.923
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7717959880828857
        kl: 0.019485116004943848
        policy_loss: -0.02089618518948555
        total_loss: -0.0012895483523607254
        vf_explained_var: 0.9996379017829895
        vf_loss: 0.014126449823379517
    load_time_ms: 1.686
    num_steps_sampled: 5989500
    num_steps_trained: 5940000
    sample_time_ms: 1735.816
    update_time_ms: 4.369
  iterations_since_restore: 165
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.001984244927154
    mean_inference_ms: 1.363293420431397
    mean_processing_ms: 3.304816386295487
  time_since_restore: 1386.438217639923
  time_this_iter_s: 8.853407621383667
  time_total_s: 1386.438217639923
  timestamp: 1563925623
  timesteps_since_restore: 5989500
  timesteps_this_iter: 36300
  timesteps_total: 5989500
  training_iteration: 165
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1386 s, 165 iter, 5989500 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.396209054724274
  episode_reward_mean: 19.535048235643114
  episode_reward_min: 0.9974860507936735
  episodes_this_iter: 242
  episodes_total: 40172
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6562.661
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7603645324707031
        kl: 0.01648215763270855
        policy_loss: -0.024099640548229218
        total_loss: -0.004011413548141718
        vf_explained_var: 0.9995866417884827
        vf_loss: 0.015452618710696697
    load_time_ms: 1.681
    num_steps_sampled: 6025800
    num_steps_trained: 5976000
    sample_time_ms: 1869.169
    update_time_ms: 4.397
  iterations_since_restore: 166
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.001936055213796
    mean_inference_ms: 1.3632339413472996
    mean_processing_ms: 3.3048277676251727
  time_since_restore: 1394.6638894081116
  time_this_iter_s: 8.225671768188477
  time_total_s: 1394.6638894081116
  timestamp: 1563925632
  timesteps_since_restore: 6025800
  timesteps_this_iter: 36300
  timesteps_total: 6025800
  training_iteration: 166
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1394 s, 166 iter, 6025800 ts, 19.5 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.82809013060406
  episode_reward_mean: 18.718012251607718
  episode_reward_min: -9.478483505103542
  episodes_this_iter: 242
  episodes_total: 40414
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6369.144
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7631553411483765
        kl: 0.019015057012438774
        policy_loss: -0.01831989549100399
        total_loss: 0.020013323053717613
        vf_explained_var: 0.9989922046661377
        vf_loss: 0.03298523277044296
    load_time_ms: 1.678
    num_steps_sampled: 6062100
    num_steps_trained: 6012000
    sample_time_ms: 1873.058
    update_time_ms: 4.233
  iterations_since_restore: 167
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.00202812793639
    mean_inference_ms: 1.3632209753989608
    mean_processing_ms: 3.304849982732513
  time_since_restore: 1401.5683817863464
  time_this_iter_s: 6.904492378234863
  time_total_s: 1401.5683817863464
  timestamp: 1563925639
  timesteps_since_restore: 6062100
  timesteps_this_iter: 36300
  timesteps_total: 6062100
  training_iteration: 167
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1401 s, 167 iter, 6062100 ts, 18.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.58638180757106
  episode_reward_mean: 19.673071027661926
  episode_reward_min: -2.09636978700566
  episodes_this_iter: 242
  episodes_total: 40656
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6369.666
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7528489232063293
        kl: 0.01649230532348156
        policy_loss: -0.01687687449157238
        total_loss: 0.0024555313866585493
        vf_explained_var: 0.999624490737915
        vf_loss: 0.014693941920995712
    load_time_ms: 1.66
    num_steps_sampled: 6098400
    num_steps_trained: 6048000
    sample_time_ms: 1743.074
    update_time_ms: 4.16
  iterations_since_restore: 168
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002135083705665
    mean_inference_ms: 1.363312351685848
    mean_processing_ms: 3.3048876306523693
  time_since_restore: 1410.4419212341309
  time_this_iter_s: 8.873539447784424
  time_total_s: 1410.4419212341309
  timestamp: 1563925647
  timesteps_since_restore: 6098400
  timesteps_this_iter: 36300
  timesteps_total: 6098400
  training_iteration: 168
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1410 s, 168 iter, 6098400 ts, 19.7 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.01430361985557
  episode_reward_mean: 19.43433076347634
  episode_reward_min: -10.34593262765093
  episodes_this_iter: 242
  episodes_total: 40898
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6173.637
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7495848536491394
        kl: 0.01462357398122549
        policy_loss: -0.016932565718889236
        total_loss: 0.022395649924874306
        vf_explained_var: 0.9990861415863037
        vf_loss: 0.035215333104133606
    load_time_ms: 1.658
    num_steps_sampled: 6134700
    num_steps_trained: 6084000
    sample_time_ms: 1877.771
    update_time_ms: 4.263
  iterations_since_restore: 169
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002311198382679
    mean_inference_ms: 1.363172607925724
    mean_processing_ms: 3.304910197911832
  time_since_restore: 1418.6671636104584
  time_this_iter_s: 8.225242376327515
  time_total_s: 1418.6671636104584
  timestamp: 1563925656
  timesteps_since_restore: 6134700
  timesteps_this_iter: 36300
  timesteps_total: 6134700
  training_iteration: 169
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1418 s, 169 iter, 6134700 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.74653902259208
  episode_reward_mean: 20.854132277887462
  episode_reward_min: -3.0666365838579646
  episodes_this_iter: 242
  episodes_total: 41140
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6267.64
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7413671612739563
        kl: 0.018451055511832237
        policy_loss: -0.022454939782619476
        total_loss: -0.00042890349868685007
        vf_explained_var: 0.9996257424354553
        vf_loss: 0.016836680471897125
    load_time_ms: 1.662
    num_steps_sampled: 6171000
    num_steps_trained: 6120000
    sample_time_ms: 1879.461
    update_time_ms: 4.233
  iterations_since_restore: 170
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002395838405752
    mean_inference_ms: 1.363207644435641
    mean_processing_ms: 3.305154427783777
  time_since_restore: 1426.5160160064697
  time_this_iter_s: 7.8488523960113525
  time_total_s: 1426.5160160064697
  timestamp: 1563925664
  timesteps_since_restore: 6171000
  timesteps_this_iter: 36300
  timesteps_total: 6171000
  training_iteration: 170
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1426 s, 170 iter, 6171000 ts, 20.9 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-47-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.17709810287365
  episode_reward_mean: 20.23453054699581
  episode_reward_min: -0.4781123495143272
  episodes_this_iter: 242
  episodes_total: 41382
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6424.458
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.728701651096344
        kl: 0.014964547008275986
        policy_loss: -0.01763836294412613
        total_loss: -0.0008351144497282803
        vf_explained_var: 0.9996915459632874
        vf_loss: 0.012594467028975487
    load_time_ms: 1.668
    num_steps_sampled: 6207300
    num_steps_trained: 6156000
    sample_time_ms: 1877.245
    update_time_ms: 4.295
  iterations_since_restore: 171
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.0024650641156825
    mean_inference_ms: 1.3631787619424998
    mean_processing_ms: 3.3054318604087163
  time_since_restore: 1436.6951048374176
  time_this_iter_s: 10.179088830947876
  time_total_s: 1436.6951048374176
  timestamp: 1563925674
  timesteps_since_restore: 6207300
  timesteps_this_iter: 36300
  timesteps_total: 6207300
  training_iteration: 171
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1436 s, 171 iter, 6207300 ts, 20.2 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-48-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.88912366880778
  episode_reward_mean: 19.57097999376295
  episode_reward_min: -0.08395759149755526
  episodes_this_iter: 242
  episodes_total: 41624
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6466.036
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7236570119857788
        kl: 0.016455544158816338
        policy_loss: -0.020702339708805084
        total_loss: -0.0017983638681471348
        vf_explained_var: 0.9996265769004822
        vf_loss: 0.014275855384767056
    load_time_ms: 1.672
    num_steps_sampled: 6243600
    num_steps_trained: 6192000
    sample_time_ms: 1876.093
    update_time_ms: 4.401
  iterations_since_restore: 172
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002350627558895
    mean_inference_ms: 1.3630540438962162
    mean_processing_ms: 3.305213453659571
  time_since_restore: 1444.173835515976
  time_this_iter_s: 7.47873067855835
  time_total_s: 1444.173835515976
  timestamp: 1563925681
  timesteps_since_restore: 6243600
  timesteps_this_iter: 36300
  timesteps_total: 6243600
  training_iteration: 172
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1444 s, 172 iter, 6243600 ts, 19.6 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-48-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.52158989917362
  episode_reward_mean: 19.35226165845208
  episode_reward_min: -0.4282530633411702
  episodes_this_iter: 242
  episodes_total: 41866
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6662.881
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7138742804527283
        kl: 0.0219099298119545
        policy_loss: -0.02046789973974228
        total_loss: -0.0014008517609909177
        vf_explained_var: 0.9996566772460938
        vf_loss: 0.012904882431030273
    load_time_ms: 1.674
    num_steps_sampled: 6279900
    num_steps_trained: 6228000
    sample_time_ms: 1740.655
    update_time_ms: 4.401
  iterations_since_restore: 173
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002145075364497
    mean_inference_ms: 1.3630563248083865
    mean_processing_ms: 3.3048454660648443
  time_since_restore: 1453.0121092796326
  time_this_iter_s: 8.838273763656616
  time_total_s: 1453.0121092796326
  timestamp: 1563925690
  timesteps_since_restore: 6279900
  timesteps_this_iter: 36300
  timesteps_total: 6279900
  training_iteration: 173
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1453 s, 173 iter, 6279900 ts, 19.4 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-48-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.569907341693366
  episode_reward_mean: 19.028594612778758
  episode_reward_min: -0.5338148843071568
  episodes_this_iter: 242
  episodes_total: 42108
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6659.361
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7091658115386963
        kl: 0.013096747919917107
        policy_loss: -0.021951446309685707
        total_loss: -0.004149876069277525
        vf_explained_var: 0.9996740818023682
        vf_loss: 0.01227638404816389
    load_time_ms: 1.669
    num_steps_sampled: 6316200
    num_steps_trained: 6264000
    sample_time_ms: 1872.752
    update_time_ms: 4.492
  iterations_since_restore: 174
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002138036658467
    mean_inference_ms: 1.363014686663057
    mean_processing_ms: 3.304947719724584
  time_since_restore: 1463.1919236183167
  time_this_iter_s: 10.179814338684082
  time_total_s: 1463.1919236183167
  timestamp: 1563925700
  timesteps_since_restore: 6316200
  timesteps_this_iter: 36300
  timesteps_total: 6316200
  training_iteration: 174
  2019-07-24 01:48:37,073	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-24 01:48:37,081	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1463 s, 174 iter, 6316200 ts, 19 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-48-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.95438680530071
  episode_reward_mean: 19.840476012384983
  episode_reward_min: -0.05930230379949871
  episodes_this_iter: 242
  episodes_total: 42350
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6515.437
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7030126452445984
        kl: 0.016112806275486946
        policy_loss: -0.021571330726146698
        total_loss: -0.0007940922514535487
        vf_explained_var: 0.9996426105499268
        vf_loss: 0.013979647308588028
    load_time_ms: 1.671
    num_steps_sampled: 6352500
    num_steps_trained: 6300000
    sample_time_ms: 1873.082
    update_time_ms: 4.584
  iterations_since_restore: 175
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002039864221298
    mean_inference_ms: 1.3629942661377212
    mean_processing_ms: 3.3047091953382606
  time_since_restore: 1470.6065864562988
  time_this_iter_s: 7.414662837982178
  time_total_s: 1470.6065864562988
  timestamp: 1563925708
  timesteps_since_restore: 6352500
  timesteps_this_iter: 36300
  timesteps_total: 6352500
  training_iteration: 175
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=30530], 1470 s, 175 iter, 6352500 ts, 19.8 rew

Result for PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_01-48-37
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.76825729211206
  episode_reward_mean: 21.2797415909532
  episode_reward_min: 1.4560066991504834
  episodes_this_iter: 242
  episodes_total: 42592
  experiment_id: 39f68b4f9fc040a6b441fbeee39166d1
  hostname: navel-notebook-1
  info:
    grad_time_ms: 6702.683
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6990776658058167
        kl: 0.015341522172093391
        policy_loss: -0.02001659758388996
        total_loss: -0.0001791672984836623
        vf_explained_var: 0.9996951222419739
        vf_loss: 0.013365227729082108
    load_time_ms: 1.66
    num_steps_sampled: 6388800
    num_steps_trained: 6336000
    sample_time_ms: 1741.847
    update_time_ms: 4.465
  iterations_since_restore: 176
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30530
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 4.002141302777733
    mean_inference_ms: 1.3629026663090487
    mean_processing_ms: 3.304759570700735
  time_since_restore: 1479.3923859596252
  time_this_iter_s: 8.785799503326416
  time_total_s: 1479.3923859596252
  timestamp: 1563925717
  timesteps_since_restore: 6388800
  timesteps_this_iter: 36300
  timesteps_total: 6388800
  training_iteration: 176
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 5.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'PENDING': 2})
PENDING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

[2m[36m(pid=30529)[0m [32m [     0.03305s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m 2019-07-24 01:48:37,223	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=30529)[0m 2019-07-24 01:48:37.229300: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30529)[0m 2019-07-24 01:48:37,412	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=30529)[0m 
[2m[36m(pid=30529)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30529)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30529)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=30529)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30529)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30529)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30529)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=30529)[0m 
[2m[36m(pid=30529)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30529)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30529)[0m [32m [     1.00064s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m [32m [     1.00103s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m [32m [     1.00140s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m [32m [     1.00177s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m [32m [     1.00214s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m [32m [     1.00250s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m [32m [     1.00287s,  INFO] TimeLimit:
[2m[36m(pid=30529)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30529)[0m - action_space = Box(2,)
[2m[36m(pid=30529)[0m - observation_space = Box(9,)
[2m[36m(pid=30529)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30529)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30529)[0m - _max_episode_steps = 150
[2m[36m(pid=30529)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30529)[0m 2019-07-24 01:48:38,194	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fc3a194c278>}
[2m[36m(pid=30529)[0m 2019-07-24 01:48:38,194	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fc3a4fd8e48>}
[2m[36m(pid=30529)[0m 2019-07-24 01:48:38,194	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=30529)[0m 2019-07-24 01:48:38,416	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=30890)[0m 2019-07-24 01:48:44,021	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30890)[0m 2019-07-24 01:48:44.022024: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30890)[0m [32m [     0.04183s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m 2019-07-24 01:48:44,248	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30893)[0m 2019-07-24 01:48:44.249780: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30893)[0m [32m [     0.04032s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m 2019-07-24 01:48:44,270	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30894)[0m 2019-07-24 01:48:44.271260: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30894)[0m [32m [     0.08040s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     0.08911s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m 2019-07-24 01:48:44,265	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30966)[0m 2019-07-24 01:48:44.265725: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30907)[0m [32m [     0.06560s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m 2019-07-24 01:48:44,622	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30907)[0m 2019-07-24 01:48:44.623538: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30892)[0m [32m [     0.07685s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m 2019-07-24 01:48:44,752	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30892)[0m 2019-07-24 01:48:44.752889: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30895)[0m 2019-07-24 01:48:44,774	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30895)[0m 2019-07-24 01:48:44.785000: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30895)[0m [32m [     0.07279s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m 2019-07-24 01:48:44,793	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30954)[0m 2019-07-24 01:48:44.794173: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30954)[0m [32m [     0.07195s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m 2019-07-24 01:48:44,834	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30942)[0m 2019-07-24 01:48:44.835062: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30942)[0m [32m [     0.08073s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30890)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30891)[0m 2019-07-24 01:48:45,217	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30891)[0m 2019-07-24 01:48:45.218274: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30891)[0m [32m [     0.07737s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30893)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30966)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30966)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30894)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30894)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30919)[0m [32m [     0.07808s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m 2019-07-24 01:48:45,602	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=30919)[0m 2019-07-24 01:48:45.602950: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=30895)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30895)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30907)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30907)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30954)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30954)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30891)[0m 2019-07-24 01:48:45,644	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30891)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30891)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30891)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30891)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=30891)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30891)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30891)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30891)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30891)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30891)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30891)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30892)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30892)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30942)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30942)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30891)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30891)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30919)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=30919)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=30890)[0m [32m [     2.36058s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m [32m [     2.36147s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m [32m [     2.36235s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m [32m [     2.36322s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m [32m [     2.36409s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m [32m [     2.36495s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30890)[0m [32m [     2.36581s,  INFO] TimeLimit:
[2m[36m(pid=30890)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30890)[0m - action_space = Box(2,)
[2m[36m(pid=30890)[0m - observation_space = Box(9,)
[2m[36m(pid=30890)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30890)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30890)[0m - _max_episode_steps = 150
[2m[36m(pid=30890)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.25994s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.26055s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.26113s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.26169s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.26233s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.26295s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30894)[0m [32m [     2.26355s,  INFO] TimeLimit:
[2m[36m(pid=30894)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30894)[0m - action_space = Box(2,)
[2m[36m(pid=30894)[0m - observation_space = Box(9,)
[2m[36m(pid=30894)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30894)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30894)[0m - _max_episode_steps = 150
[2m[36m(pid=30894)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.38816s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.38905s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.38986s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.39071s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.39158s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.39250s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30966)[0m [32m [     2.39331s,  INFO] TimeLimit:
[2m[36m(pid=30966)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30966)[0m - action_space = Box(2,)
[2m[36m(pid=30966)[0m - observation_space = Box(9,)
[2m[36m(pid=30966)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30966)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30966)[0m - _max_episode_steps = 150
[2m[36m(pid=30966)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39344s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39432s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39514s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39598s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39688s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39776s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30893)[0m [32m [     2.39862s,  INFO] TimeLimit:
[2m[36m(pid=30893)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30893)[0m - action_space = Box(2,)
[2m[36m(pid=30893)[0m - observation_space = Box(9,)
[2m[36m(pid=30893)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30893)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30893)[0m - _max_episode_steps = 150
[2m[36m(pid=30893)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.12876s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.12968s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.13056s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.13145s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.13235s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.13326s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30907)[0m [32m [     2.13417s,  INFO] TimeLimit:
[2m[36m(pid=30907)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30907)[0m - action_space = Box(2,)
[2m[36m(pid=30907)[0m - observation_space = Box(9,)
[2m[36m(pid=30907)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30907)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30907)[0m - _max_episode_steps = 150
[2m[36m(pid=30907)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.05728s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.05797s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.05862s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.05924s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.05983s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.06040s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30892)[0m [32m [     2.06104s,  INFO] TimeLimit:
[2m[36m(pid=30892)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30892)[0m - action_space = Box(2,)
[2m[36m(pid=30892)[0m - observation_space = Box(9,)
[2m[36m(pid=30892)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30892)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30892)[0m - _max_episode_steps = 150
[2m[36m(pid=30892)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.03930s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.04025s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.04117s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.04211s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.04298s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.04385s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.98577s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.98667s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.98753s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.98836s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.98919s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.99013s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30954)[0m [32m [     1.99101s,  INFO] TimeLimit:
[2m[36m(pid=30954)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30954)[0m - action_space = Box(2,)
[2m[36m(pid=30954)[0m - observation_space = Box(9,)
[2m[36m(pid=30954)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30954)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30954)[0m - _max_episode_steps = 150
[2m[36m(pid=30954)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30895)[0m [32m [     2.04478s,  INFO] TimeLimit:
[2m[36m(pid=30895)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30895)[0m - action_space = Box(2,)
[2m[36m(pid=30895)[0m - observation_space = Box(9,)
[2m[36m(pid=30895)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30895)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30895)[0m - _max_episode_steps = 150
[2m[36m(pid=30895)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11309s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11406s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11495s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11584s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11675s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11766s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30942)[0m [32m [     2.11854s,  INFO] TimeLimit:
[2m[36m(pid=30942)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30942)[0m - action_space = Box(2,)
[2m[36m(pid=30942)[0m - observation_space = Box(9,)
[2m[36m(pid=30942)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30942)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30942)[0m - _max_episode_steps = 150
[2m[36m(pid=30942)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.86646s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.86784s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.86877s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.86967s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.87048s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.87141s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m [32m [     1.87231s,  INFO] TimeLimit:
[2m[36m(pid=30891)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30891)[0m - action_space = Box(2,)
[2m[36m(pid=30891)[0m - observation_space = Box(9,)
[2m[36m(pid=30891)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30891)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30891)[0m - _max_episode_steps = 150
[2m[36m(pid=30891)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,049	INFO rollout_worker.py:428 -- Generating sample batch of size 1600
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,126	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.866, max=0.868, mean=0.064)},
[2m[36m(pid=30891)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.668, max=0.744, mean=0.102)},
[2m[36m(pid=30891)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.89, max=0.915, mean=-0.049)},
[2m[36m(pid=30891)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.258, max=0.975, mean=0.147)},
[2m[36m(pid=30891)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.254, max=0.999, mean=0.117)},
[2m[36m(pid=30891)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.864, max=0.704, mean=-0.106)},
[2m[36m(pid=30891)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.987, max=0.142, mean=-0.142)},
[2m[36m(pid=30891)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.976, max=0.537, mean=-0.018)}}
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,126	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=30891)[0m   1: {'agent0': None},
[2m[36m(pid=30891)[0m   2: {'agent0': None},
[2m[36m(pid=30891)[0m   3: {'agent0': None},
[2m[36m(pid=30891)[0m   4: {'agent0': None},
[2m[36m(pid=30891)[0m   5: {'agent0': None},
[2m[36m(pid=30891)[0m   6: {'agent0': None},
[2m[36m(pid=30891)[0m   7: {'agent0': None}}
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,127	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.866, max=0.868, mean=0.064)
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,127	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,133	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 0,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 1,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=0.079),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 2,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.151, max=1.084, mean=0.072),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 3,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.257, max=1.167, mean=0.235),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 4,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.558, max=1.685, mean=0.013),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 5,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.162, max=1.093, mean=-0.231),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 6,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-2.03, max=1.281, mean=-0.288),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=30891)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=30891)[0m                                   'env_id': 7,
[2m[36m(pid=30891)[0m                                   'info': None,
[2m[36m(pid=30891)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.063, max=0.683, mean=-0.08),
[2m[36m(pid=30891)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=30891)[0m                                   'rnn_state': []},
[2m[36m(pid=30891)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,133	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,171	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-1.764, max=2.33, mean=0.088),
[2m[36m(pid=30891)[0m                       [],
[2m[36m(pid=30891)[0m                       { 'action_prob': np.ndarray((8,), dtype=float32, min=0.007, max=0.157, mean=0.087),
[2m[36m(pid=30891)[0m                         'behaviour_logits': np.ndarray((8, 4), dtype=float32, min=-0.008, max=0.004, mean=-0.0),
[2m[36m(pid=30891)[0m                         'vf_preds': np.ndarray((8,), dtype=float32, min=-0.003, max=0.005, mean=0.0)})}
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30919)[0m [32m [     1.72977s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m [32m [     1.73069s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m [32m [     1.73160s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m [32m [     1.73253s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m [32m [     1.73363s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m [32m [     1.73451s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30919)[0m [32m [     1.73540s,  INFO] TimeLimit:
[2m[36m(pid=30919)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=30919)[0m - action_space = Box(2,)
[2m[36m(pid=30919)[0m - observation_space = Box(9,)
[2m[36m(pid=30919)[0m - reward_range = (-inf, inf)
[2m[36m(pid=30919)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=30919)[0m - _max_episode_steps = 150
[2m[36m(pid=30919)[0m - _elapsed_steps = None [0m
[2m[36m(pid=30891)[0m 2019-07-24 01:48:47,876	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.0, max=0.159, mean=0.079),
[2m[36m(pid=30891)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-3.367, max=2.94, mean=-0.068),
[2m[36m(pid=30891)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-12.333, max=15.18, mean=-2.271),
[2m[36m(pid=30891)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.008, max=0.009, mean=-0.0),
[2m[36m(pid=30891)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=30891)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=710000134.0, max=710000134.0, mean=710000134.0),
[2m[36m(pid=30891)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=30891)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-3.01, max=2.667, mean=0.009),
[2m[36m(pid=30891)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-3.01, max=2.667, mean=0.009),
[2m[36m(pid=30891)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-3.367, max=2.94, mean=-0.062),
[2m[36m(pid=30891)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-1.849, max=2.499, mean=-0.068),
[2m[36m(pid=30891)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-1.849, max=2.499, mean=-0.069),
[2m[36m(pid=30891)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=30891)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-12.332, max=15.184, mean=-2.271),
[2m[36m(pid=30891)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.005, max=0.007, mean=-0.0)},
[2m[36m(pid=30891)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m 2019-07-24 01:48:48,654	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30891)[0m { 'data': { 'action_prob': np.ndarray((1650,), dtype=float32, min=0.0, max=0.159, mean=0.08),
[2m[36m(pid=30891)[0m             'actions': np.ndarray((1650, 2), dtype=float32, min=-3.773, max=3.523, mean=0.004),
[2m[36m(pid=30891)[0m             'advantages': np.ndarray((1650,), dtype=float32, min=-36.067, max=16.29, mean=-7.438),
[2m[36m(pid=30891)[0m             'agent_index': np.ndarray((1650,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=30891)[0m             'behaviour_logits': np.ndarray((1650, 4), dtype=float32, min=-0.011, max=0.01, mean=-0.0),
[2m[36m(pid=30891)[0m             'dones': np.ndarray((1650,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=30891)[0m             'eps_id': np.ndarray((1650,), dtype=int64, min=85923497.0, max=1826555411.0, mean=896330378.091),
[2m[36m(pid=30891)[0m             'infos': np.ndarray((1650,), dtype=object, head={}),
[2m[36m(pid=30891)[0m             'new_obs': np.ndarray((1650, 9), dtype=float32, min=-4.131, max=4.426, mean=-0.0),
[2m[36m(pid=30891)[0m             'obs': np.ndarray((1650, 9), dtype=float32, min=-4.131, max=4.426, mean=0.001),
[2m[36m(pid=30891)[0m             'prev_actions': np.ndarray((1650, 2), dtype=float32, min=-3.773, max=3.523, mean=0.003),
[2m[36m(pid=30891)[0m             'prev_rewards': np.ndarray((1650,), dtype=float32, min=-5.072, max=5.789, mean=-0.084),
[2m[36m(pid=30891)[0m             'rewards': np.ndarray((1650,), dtype=float32, min=-5.072, max=5.789, mean=-0.086),
[2m[36m(pid=30891)[0m             't': np.ndarray((1650,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=30891)[0m             'unroll_id': np.ndarray((1650,), dtype=int64, min=0.0, max=1.0, mean=0.273),
[2m[36m(pid=30891)[0m             'value_targets': np.ndarray((1650,), dtype=float32, min=-36.059, max=16.294, mean=-7.437),
[2m[36m(pid=30891)[0m             'vf_preds': np.ndarray((1650,), dtype=float32, min=-0.01, max=0.01, mean=0.001)},
[2m[36m(pid=30891)[0m   'type': 'SampleBatch'}
[2m[36m(pid=30891)[0m 
[2m[36m(pid=30529)[0m 2019-07-24 01:48:48,937	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=30529)[0m 
[2m[36m(pid=30529)[0m { 'inputs': [ np.ndarray((26400, 2), dtype=float32, min=-4.177, max=3.848, mean=0.001),
[2m[36m(pid=30529)[0m               np.ndarray((26400,), dtype=float32, min=-17.347, max=14.782, mean=-0.104),
[2m[36m(pid=30529)[0m               np.ndarray((26400, 9), dtype=float32, min=-7.294, max=5.348, mean=-0.003),
[2m[36m(pid=30529)[0m               np.ndarray((26400, 2), dtype=float32, min=-4.177, max=3.848, mean=0.002),
[2m[36m(pid=30529)[0m               np.ndarray((26400,), dtype=float32, min=-3.786, max=3.706, mean=-0.0),
[2m[36m(pid=30529)[0m               np.ndarray((26400, 4), dtype=float32, min=-0.013, max=0.013, mean=0.0),
[2m[36m(pid=30529)[0m               np.ndarray((26400,), dtype=float32, min=-46.155, max=31.351, mean=-6.987),
[2m[36m(pid=30529)[0m               np.ndarray((26400,), dtype=float32, min=-0.012, max=0.011, mean=0.0)],
[2m[36m(pid=30529)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=30529)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=30529)[0m   'state_inputs': []}
[2m[36m(pid=30529)[0m 
[2m[36m(pid=30529)[0m 2019-07-24 01:48:48,937	INFO multi_gpu_impl.py:191 -- Divided 26400 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-48-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 13.613641036570225
  episode_reward_mean: -15.771245726603393
  episode_reward_min: -48.7023888825606
  episodes_this_iter: 176
  episodes_total: 176
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4922.449
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.8370184898376465
        kl: 0.0011099276598542929
        policy_loss: -0.0033405323047190905
        total_loss: 97.04024505615234
        vf_explained_var: 0.10657519847154617
        vf_loss: 97.04248046875
    load_time_ms: 19.363
    num_steps_sampled: 26400
    num_steps_trained: 26000
    sample_time_ms: 2762.066
    update_time_ms: 446.445
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.1878167524854892
    mean_inference_ms: 1.2746158114872423
    mean_processing_ms: 1.6090241562174972
  time_since_restore: 8.190369606018066
  time_this_iter_s: 8.190369606018066
  time_total_s: 8.190369606018066
  timestamp: 1563925733
  timesteps_since_restore: 26400
  timesteps_this_iter: 26400
  timesteps_total: 26400
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 8 s, 1 iter, 26400 ts, -15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.24621862358398
  episode_reward_mean: -14.633657599325318
  episode_reward_min: -66.26764935154463
  episodes_this_iter: 176
  episodes_total: 352
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4844.265
    learner:
      default_policy:
        cur_kl_coeff: 0.5
        cur_lr: 9.999999747378752e-05
        entropy: 2.8394269943237305
        kl: 0.0034166460391134024
        policy_loss: -0.005690299905836582
        total_loss: 79.32588958740234
        vf_explained_var: 0.32225602865219116
        vf_loss: 79.32987213134766
    load_time_ms: 10.065
    num_steps_sampled: 52800
    num_steps_trained: 52000
    sample_time_ms: 2370.663
    update_time_ms: 225.514
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.0473906427269792
    mean_inference_ms: 1.1994290799156362
    mean_processing_ms: 1.5812773855287465
  time_since_restore: 14.959805250167847
  time_this_iter_s: 6.76943564414978
  time_total_s: 14.959805250167847
  timestamp: 1563925740
  timesteps_since_restore: 52800
  timesteps_this_iter: 26400
  timesteps_total: 52800
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 14 s, 2 iter, 52800 ts, -14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.867280965546243
  episode_reward_mean: -13.696069419834197
  episode_reward_min: -48.91167199226575
  episodes_this_iter: 176
  episodes_total: 528
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4709.182
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.834719657897949
        kl: 0.004712282679975033
        policy_loss: -0.005085344426333904
        total_loss: 61.724098205566406
        vf_explained_var: 0.37542420625686646
        vf_loss: 61.72800827026367
    load_time_ms: 6.96
    num_steps_sampled: 79200
    num_steps_trained: 78000
    sample_time_ms: 2009.254
    update_time_ms: 151.666
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9964214231805852
    mean_inference_ms: 1.173190095930801
    mean_processing_ms: 1.5598862654365133
  time_since_restore: 20.708380699157715
  time_this_iter_s: 5.748575448989868
  time_total_s: 20.708380699157715
  timestamp: 1563925746
  timesteps_since_restore: 79200
  timesteps_this_iter: 26400
  timesteps_total: 79200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 20 s, 3 iter, 79200 ts, -13.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.729557930467585
  episode_reward_mean: -11.64668203995286
  episode_reward_min: -40.43657264637054
  episodes_this_iter: 176
  episodes_total: 704
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4827.03
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.826019763946533
        kl: 0.007555891759693623
        policy_loss: -0.006013401784002781
        total_loss: 55.08816146850586
        vf_explained_var: 0.3901779055595398
        vf_loss: 55.09322738647461
    load_time_ms: 5.399
    num_steps_sampled: 105600
    num_steps_trained: 104000
    sample_time_ms: 1993.724
    update_time_ms: 114.811
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9848859677201829
    mean_inference_ms: 1.1689318212402442
    mean_processing_ms: 1.5717143595381828
  time_since_restore: 27.86068892478943
  time_this_iter_s: 7.152308225631714
  time_total_s: 27.86068892478943
  timestamp: 1563925753
  timesteps_since_restore: 105600
  timesteps_this_iter: 26400
  timesteps_total: 105600
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 27 s, 4 iter, 105600 ts, -11.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.5773928164185
  episode_reward_mean: -11.655263849846584
  episode_reward_min: -41.65776009041679
  episodes_this_iter: 176
  episodes_total: 880
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4659.174
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8194761276245117
        kl: 0.0067661660723388195
        policy_loss: -0.007061499636620283
        total_loss: 48.084266662597656
        vf_explained_var: 0.35070863366127014
        vf_loss: 48.09048080444336
    load_time_ms: 4.458
    num_steps_sampled: 132000
    num_steps_trained: 130000
    sample_time_ms: 1868.024
    update_time_ms: 92.792
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9648787917546011
    mean_inference_ms: 1.1493089609830236
    mean_processing_ms: 1.5628766189419927
  time_since_restore: 33.23485851287842
  time_this_iter_s: 5.374169588088989
  time_total_s: 33.23485851287842
  timestamp: 1563925759
  timesteps_since_restore: 132000
  timesteps_this_iter: 26400
  timesteps_total: 132000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 33 s, 5 iter, 132000 ts, -11.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.363633687141697
  episode_reward_mean: -9.141278328113144
  episode_reward_min: -42.26604231461884
  episodes_this_iter: 176
  episodes_total: 1056
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4619.186
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.813978433609009
        kl: 0.008072929456830025
        policy_loss: -0.007940332405269146
        total_loss: 43.88271713256836
        vf_explained_var: 0.3725431263446808
        vf_loss: 43.889652252197266
    load_time_ms: 3.839
    num_steps_sampled: 158400
    num_steps_trained: 156000
    sample_time_ms: 1875.271
    update_time_ms: 77.88
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9679230696625105
    mean_inference_ms: 1.1586049595993844
    mean_processing_ms: 1.5792587992954097
  time_since_restore: 39.588234424591064
  time_this_iter_s: 6.3533759117126465
  time_total_s: 39.588234424591064
  timestamp: 1563925765
  timesteps_since_restore: 158400
  timesteps_this_iter: 26400
  timesteps_total: 158400
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 39 s, 6 iter, 158400 ts, -9.14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.73677241727881
  episode_reward_mean: -10.005352763737825
  episode_reward_min: -41.81268556565034
  episodes_this_iter: 176
  episodes_total: 1232
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4710.078
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8084092140197754
        kl: 0.007768497336655855
        policy_loss: -0.006667341571301222
        total_loss: 33.358665466308594
        vf_explained_var: 0.44390037655830383
        vf_loss: 33.36436462402344
    load_time_ms: 3.388
    num_steps_sampled: 184800
    num_steps_trained: 182000
    sample_time_ms: 1876.409
    update_time_ms: 67.348
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9375389967698844
    mean_inference_ms: 1.1319443054206189
    mean_processing_ms: 1.5568170240223786
  time_since_restore: 46.74922299385071
  time_this_iter_s: 7.1609885692596436
  time_total_s: 46.74922299385071
  timestamp: 1563925772
  timesteps_since_restore: 184800
  timesteps_this_iter: 26400
  timesteps_total: 184800
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 46 s, 7 iter, 184800 ts, -10 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.650614865331796
  episode_reward_mean: -6.410061587322713
  episode_reward_min: -41.26292831525264
  episodes_this_iter: 176
  episodes_total: 1408
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4620.494
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.799787998199463
        kl: 0.007148786447942257
        policy_loss: -0.006341854110360146
        total_loss: 33.15210723876953
        vf_explained_var: 0.4734494388103485
        vf_loss: 33.157554626464844
    load_time_ms: 3.058
    num_steps_sampled: 211200
    num_steps_trained: 208000
    sample_time_ms: 1856.544
    update_time_ms: 59.497
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9275062193302093
    mean_inference_ms: 1.1299931038455768
    mean_processing_ms: 1.5541221917827237
  time_since_restore: 52.48161482810974
  time_this_iter_s: 5.732391834259033
  time_total_s: 52.48161482810974
  timestamp: 1563925778
  timesteps_since_restore: 211200
  timesteps_this_iter: 26400
  timesteps_total: 211200
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 52 s, 8 iter, 211200 ts, -6.41 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.617910988001224
  episode_reward_mean: -5.709050052362859
  episode_reward_min: -32.35472268811746
  episodes_this_iter: 176
  episodes_total: 1584
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4706.706
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7974307537078857
        kl: 0.00761996116489172
        policy_loss: -0.007787955924868584
        total_loss: 30.757856369018555
        vf_explained_var: 0.4217432141304016
        vf_loss: 30.764692306518555
    load_time_ms: 2.799
    num_steps_sampled: 237600
    num_steps_trained: 234000
    sample_time_ms: 1850.199
    update_time_ms: 53.308
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.927464829535648
    mean_inference_ms: 1.1283582164741996
    mean_processing_ms: 1.5583736486798898
  time_since_restore: 59.708167552948
  time_this_iter_s: 7.226552724838257
  time_total_s: 59.708167552948
  timestamp: 1563925785
  timesteps_since_restore: 237600
  timesteps_this_iter: 26400
  timesteps_total: 237600
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 59 s, 9 iter, 237600 ts, -5.71 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.950334002330663
  episode_reward_mean: -4.20148539618867
  episode_reward_min: -30.881466346277442
  episodes_this_iter: 176
  episodes_total: 1760
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4713.149
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.786892890930176
        kl: 0.008764252997934818
        policy_loss: -0.007119801826775074
        total_loss: 25.391252517700195
        vf_explained_var: 0.4935382306575775
        vf_loss: 25.397275924682617
    load_time_ms: 2.585
    num_steps_sampled: 264000
    num_steps_trained: 260000
    sample_time_ms: 1804.704
    update_time_ms: 48.508
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9268921443592397
    mean_inference_ms: 1.1269432335080725
    mean_processing_ms: 1.5612168826794046
  time_since_restore: 65.89942979812622
  time_this_iter_s: 6.191262245178223
  time_total_s: 65.89942979812622
  timestamp: 1563925791
  timesteps_since_restore: 264000
  timesteps_this_iter: 26400
  timesteps_total: 264000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 65 s, 10 iter, 264000 ts, -4.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-49-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.937482863385256
  episode_reward_mean: -2.7005781464489913
  episode_reward_min: -30.094817482377632
  episodes_this_iter: 176
  episodes_total: 1936
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4619.493
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7795259952545166
        kl: 0.0076994383707642555
        policy_loss: -0.00719510717317462
        total_loss: 18.131929397583008
        vf_explained_var: 0.5878438353538513
        vf_loss: 18.138160705566406
    load_time_ms: 0.725
    num_steps_sampled: 290400
    num_steps_trained: 286000
    sample_time_ms: 1743.125
    update_time_ms: 4.273
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9240677867230678
    mean_inference_ms: 1.1316175441193068
    mean_processing_ms: 1.565756142108877
  time_since_restore: 72.05358171463013
  time_this_iter_s: 6.154151916503906
  time_total_s: 72.05358171463013
  timestamp: 1563925797
  timesteps_since_restore: 290400
  timesteps_this_iter: 26400
  timesteps_total: 290400
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 72 s, 11 iter, 290400 ts, -2.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.47923241680578
  episode_reward_mean: -3.1632792587656215
  episode_reward_min: -29.917618146409428
  episodes_this_iter: 176
  episodes_total: 2112
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4687.188
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.771071195602417
        kl: 0.008148803375661373
        policy_loss: -0.00818970799446106
        total_loss: 19.271509170532227
        vf_explained_var: 0.6033068895339966
        vf_loss: 19.27867889404297
    load_time_ms: 0.719
    num_steps_sampled: 316800
    num_steps_trained: 312000
    sample_time_ms: 1723.065
    update_time_ms: 4.147
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9105001929215495
    mean_inference_ms: 1.1200676883716645
    mean_processing_ms: 1.5521703191130114
  time_since_restore: 79.30057835578918
  time_this_iter_s: 7.246996641159058
  time_total_s: 79.30057835578918
  timestamp: 1563925805
  timesteps_since_restore: 316800
  timesteps_this_iter: 26400
  timesteps_total: 316800
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 79 s, 12 iter, 316800 ts, -3.16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.363107157489786
  episode_reward_mean: -0.6001944976045147
  episode_reward_min: -31.85865971025479
  episodes_this_iter: 176
  episodes_total: 2288
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4709.826
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7552084922790527
        kl: 0.009993690997362137
        policy_loss: -0.011220772750675678
        total_loss: 22.158939361572266
        vf_explained_var: 0.6116800904273987
        vf_loss: 22.168912887573242
    load_time_ms: 0.71
    num_steps_sampled: 343200
    num_steps_trained: 338000
    sample_time_ms: 1776.584
    update_time_ms: 4.228
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9162345733870205
    mean_inference_ms: 1.1244949061475138
    mean_processing_ms: 1.560423452772575
  time_since_restore: 85.81164813041687
  time_this_iter_s: 6.5110697746276855
  time_total_s: 85.81164813041687
  timestamp: 1563925811
  timesteps_since_restore: 343200
  timesteps_this_iter: 26400
  timesteps_total: 343200
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 85 s, 13 iter, 343200 ts, -0.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.731449737300913
  episode_reward_mean: -0.5947969687469261
  episode_reward_min: -35.02886367065394
  episodes_this_iter: 176
  episodes_total: 2464
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4730.511
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7403314113616943
        kl: 0.01067473366856575
        policy_loss: -0.009652248583734035
        total_loss: 18.716781616210938
        vf_explained_var: 0.6124874949455261
        vf_loss: 18.72509765625
    load_time_ms: 0.713
    num_steps_sampled: 369600
    num_steps_trained: 364000
    sample_time_ms: 1755.093
    update_time_ms: 4.241
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889577854129108
    mean_inference_ms: 1.1100549170402536
    mean_processing_ms: 1.5395912657727486
  time_since_restore: 92.95719313621521
  time_this_iter_s: 7.14554500579834
  time_total_s: 92.95719313621521
  timestamp: 1563925818
  timesteps_since_restore: 369600
  timesteps_this_iter: 26400
  timesteps_total: 369600
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 92 s, 14 iter, 369600 ts, -0.595 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.04180630743647
  episode_reward_mean: 0.48117304293127855
  episode_reward_min: -25.60502211336616
  episodes_this_iter: 176
  episodes_total: 2640
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4726.827
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7335915565490723
        kl: 0.00926126167178154
        policy_loss: -0.010279865935444832
        total_loss: 21.532930374145508
        vf_explained_var: 0.632828950881958
        vf_loss: 21.542051315307617
    load_time_ms: 0.714
    num_steps_sampled: 396000
    num_steps_trained: 390000
    sample_time_ms: 1788.206
    update_time_ms: 4.262
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890880539676664
    mean_inference_ms: 1.107204809501465
    mean_processing_ms: 1.5387724573483827
  time_since_restore: 98.6272840499878
  time_this_iter_s: 5.670090913772583
  time_total_s: 98.6272840499878
  timestamp: 1563925824
  timesteps_since_restore: 396000
  timesteps_this_iter: 26400
  timesteps_total: 396000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 98 s, 15 iter, 396000 ts, 0.481 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.949799229812502
  episode_reward_mean: 2.7748607547767223
  episode_reward_min: -43.26291017189867
  episodes_this_iter: 176
  episodes_total: 2816
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4680.376
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.725471019744873
        kl: 0.00864814780652523
        policy_loss: -0.01098095066845417
        total_loss: 18.045345306396484
        vf_explained_var: 0.677071213722229
        vf_loss: 18.055246353149414
    load_time_ms: 0.721
    num_steps_sampled: 422400
    num_steps_trained: 416000
    sample_time_ms: 1783.513
    update_time_ms: 4.288
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9075565722588776
    mean_inference_ms: 1.1227047318383323
    mean_processing_ms: 1.5616799595454116
  time_since_restore: 104.46747040748596
  time_this_iter_s: 5.840186357498169
  time_total_s: 104.46747040748596
  timestamp: 1563925830
  timesteps_since_restore: 422400
  timesteps_this_iter: 26400
  timesteps_total: 422400
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 104 s, 16 iter, 422400 ts, 2.77 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.32943772310673
  episode_reward_mean: 3.917871654919969
  episode_reward_min: -19.775206757494658
  episodes_this_iter: 176
  episodes_total: 2992
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4632.344
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7135941982269287
        kl: 0.008729282766580582
        policy_loss: -0.010911826975643635
        total_loss: 14.056398391723633
        vf_explained_var: 0.7211965918540955
        vf_loss: 14.066217422485352
    load_time_ms: 0.722
    num_steps_sampled: 448800
    num_steps_trained: 442000
    sample_time_ms: 1778.869
    update_time_ms: 4.241
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.90126441856274
    mean_inference_ms: 1.1144636340738059
    mean_processing_ms: 1.5496086916118548
  time_since_restore: 111.10349917411804
  time_this_iter_s: 6.63602876663208
  time_total_s: 111.10349917411804
  timestamp: 1563925837
  timesteps_since_restore: 448800
  timesteps_this_iter: 26400
  timesteps_total: 448800
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 111 s, 17 iter, 448800 ts, 3.92 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.307572987081205
  episode_reward_mean: 5.763057350278193
  episode_reward_min: -45.424459202228114
  episodes_this_iter: 176
  episodes_total: 3168
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4647.563
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7086877822875977
        kl: 0.009570506401360035
        policy_loss: -0.009268487803637981
        total_loss: 14.905162811279297
        vf_explained_var: 0.7030445337295532
        vf_loss: 14.913233757019043
    load_time_ms: 0.724
    num_steps_sampled: 475200
    num_steps_trained: 468000
    sample_time_ms: 1777.668
    update_time_ms: 4.184
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8974835910284389
    mean_inference_ms: 1.1120135734971397
    mean_processing_ms: 1.548757429122714
  time_since_restore: 116.97569370269775
  time_this_iter_s: 5.872194528579712
  time_total_s: 116.97569370269775
  timestamp: 1563925842
  timesteps_since_restore: 475200
  timesteps_this_iter: 26400
  timesteps_total: 475200
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 116 s, 18 iter, 475200 ts, 5.76 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.831759660813447
  episode_reward_mean: 5.760576611031389
  episode_reward_min: -20.45515491823574
  episodes_this_iter: 176
  episodes_total: 3344
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4650.69
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.703986167907715
        kl: 0.008382761850953102
        policy_loss: -0.010163581930100918
        total_loss: 9.12789249420166
        vf_explained_var: 0.7935975193977356
        vf_loss: 9.137007713317871
    load_time_ms: 0.723
    num_steps_sampled: 501600
    num_steps_trained: 494000
    sample_time_ms: 1786.502
    update_time_ms: 4.191
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8946247698964498
    mean_inference_ms: 1.1127878232709696
    mean_processing_ms: 1.5487025968292203
  time_since_restore: 124.31611108779907
  time_this_iter_s: 7.340417385101318
  time_total_s: 124.31611108779907
  timestamp: 1563925850
  timesteps_since_restore: 501600
  timesteps_this_iter: 26400
  timesteps_total: 501600
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 124 s, 19 iter, 501600 ts, 5.76 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-50-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.3436613068206
  episode_reward_mean: 6.362961871591585
  episode_reward_min: -19.399538470163066
  episodes_this_iter: 176
  episodes_total: 3520
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4586.22
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.693000316619873
        kl: 0.009461707435548306
        policy_loss: -0.010297322645783424
        total_loss: 8.25497055053711
        vf_explained_var: 0.8138648271560669
        vf_loss: 8.264084815979004
    load_time_ms: 0.729
    num_steps_sampled: 528000
    num_steps_trained: 520000
    sample_time_ms: 1787.907
    update_time_ms: 4.123
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.892099422497474
    mean_inference_ms: 1.1069928023342586
    mean_processing_ms: 1.5476101141212417
  time_since_restore: 129.87410163879395
  time_this_iter_s: 5.557990550994873
  time_total_s: 129.87410163879395
  timestamp: 1563925855
  timesteps_since_restore: 528000
  timesteps_this_iter: 26400
  timesteps_total: 528000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 129 s, 20 iter, 528000 ts, 6.36 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.36723744713733
  episode_reward_mean: 8.493054413713974
  episode_reward_min: -17.190796905202454
  episodes_this_iter: 176
  episodes_total: 3696
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4731.549
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.685798168182373
        kl: 0.00911431573331356
        policy_loss: -0.010348917916417122
        total_loss: 5.413725852966309
        vf_explained_var: 0.8572518825531006
        vf_loss: 5.422935962677002
    load_time_ms: 0.724
    num_steps_sampled: 554400
    num_steps_trained: 546000
    sample_time_ms: 1761.205
    update_time_ms: 4.079
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9070059866564792
    mean_inference_ms: 1.1216200601779016
    mean_processing_ms: 1.5650586417869072
  time_since_restore: 137.21767950057983
  time_this_iter_s: 7.343577861785889
  time_total_s: 137.21767950057983
  timestamp: 1563925863
  timesteps_since_restore: 554400
  timesteps_this_iter: 26400
  timesteps_total: 554400
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 137 s, 21 iter, 554400 ts, 8.49 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.0773377681055
  episode_reward_mean: 5.912209215967359
  episode_reward_min: -18.19350349725126
  episodes_this_iter: 176
  episodes_total: 3872
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4586.198
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.668405771255493
        kl: 0.009462523274123669
        policy_loss: -0.010802769102156162
        total_loss: 5.747096538543701
        vf_explained_var: 0.8446493744850159
        vf_loss: 5.756716728210449
    load_time_ms: 0.721
    num_steps_sampled: 580800
    num_steps_trained: 572000
    sample_time_ms: 1760.126
    update_time_ms: 4.23
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8968545788177302
    mean_inference_ms: 1.1118269280022695
    mean_processing_ms: 1.5492357574431999
  time_since_restore: 142.99733781814575
  time_this_iter_s: 5.779658317565918
  time_total_s: 142.99733781814575
  timestamp: 1563925869
  timesteps_since_restore: 580800
  timesteps_this_iter: 26400
  timesteps_total: 580800
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 142 s, 22 iter, 580800 ts, 5.91 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.109500514669868
  episode_reward_mean: 8.291643628575804
  episode_reward_min: -19.10961570111965
  episodes_this_iter: 176
  episodes_total: 4048
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4658.926
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6674981117248535
        kl: 0.008345517329871655
        policy_loss: -0.01129932515323162
        total_loss: 4.7797017097473145
        vf_explained_var: 0.869605302810669
        vf_loss: 4.7899580001831055
    load_time_ms: 0.726
    num_steps_sampled: 607200
    num_steps_trained: 598000
    sample_time_ms: 1745.796
    update_time_ms: 4.089
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8940987414915158
    mean_inference_ms: 1.1106572535793597
    mean_processing_ms: 1.5565635127937967
  time_since_restore: 150.0895812511444
  time_this_iter_s: 7.092243432998657
  time_total_s: 150.0895812511444
  timestamp: 1563925876
  timesteps_since_restore: 607200
  timesteps_this_iter: 26400
  timesteps_total: 607200
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 150 s, 23 iter, 607200 ts, 8.29 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.574357670870725
  episode_reward_mean: 8.468467550621083
  episode_reward_min: -24.438528443775898
  episodes_this_iter: 176
  episodes_total: 4224
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4516.745
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.657927989959717
        kl: 0.009773382917046547
        policy_loss: -0.012028096243739128
        total_loss: 4.378087997436523
        vf_explained_var: 0.8875213265419006
        vf_loss: 4.388894557952881
    load_time_ms: 0.729
    num_steps_sampled: 633600
    num_steps_trained: 624000
    sample_time_ms: 1766.552
    update_time_ms: 3.965
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8927674720238867
    mean_inference_ms: 1.1123964650339275
    mean_processing_ms: 1.5538497006581637
  time_since_restore: 156.01651120185852
  time_this_iter_s: 5.926929950714111
  time_total_s: 156.01651120185852
  timestamp: 1563925882
  timesteps_since_restore: 633600
  timesteps_this_iter: 26400
  timesteps_total: 633600
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 156 s, 24 iter, 633600 ts, 8.47 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.807483072291586
  episode_reward_mean: 8.26764606211005
  episode_reward_min: -13.66325769364296
  episodes_this_iter: 176
  episodes_total: 4400
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4517.1
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.644362449645996
        kl: 0.011023832485079765
        policy_loss: -0.012845369055867195
        total_loss: 3.6615312099456787
        vf_explained_var: 0.9073563814163208
        vf_loss: 3.6729981899261475
    load_time_ms: 0.73
    num_steps_sampled: 660000
    num_steps_trained: 650000
    sample_time_ms: 1760.36
    update_time_ms: 3.813
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888954701510101
    mean_inference_ms: 1.1089954419729677
    mean_processing_ms: 1.5483595230587355
  time_since_restore: 161.62593722343445
  time_this_iter_s: 5.609426021575928
  time_total_s: 161.62593722343445
  timestamp: 1563925887
  timesteps_since_restore: 660000
  timesteps_this_iter: 26400
  timesteps_total: 660000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 161 s, 25 iter, 660000 ts, 8.27 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.953297235473457
  episode_reward_mean: 8.894554636832797
  episode_reward_min: -16.935665888600646
  episodes_this_iter: 176
  episodes_total: 4576
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4656.141
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6378016471862793
        kl: 0.008868379518389702
        policy_loss: -0.013094812631607056
        total_loss: 3.459592580795288
        vf_explained_var: 0.9012272357940674
        vf_loss: 3.4715793132781982
    load_time_ms: 0.721
    num_steps_sampled: 686400
    num_steps_trained: 676000
    sample_time_ms: 1797.864
    update_time_ms: 3.854
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8993770397941474
    mean_inference_ms: 1.1168111597018695
    mean_processing_ms: 1.5601257704329412
  time_since_restore: 169.23630285263062
  time_this_iter_s: 7.610365629196167
  time_total_s: 169.23630285263062
  timestamp: 1563925895
  timesteps_since_restore: 686400
  timesteps_this_iter: 26400
  timesteps_total: 686400
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 169 s, 26 iter, 686400 ts, 8.89 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.862493651910597
  episode_reward_mean: 10.109799564239477
  episode_reward_min: -15.753456292760665
  episodes_this_iter: 176
  episodes_total: 4752
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4618.193
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.624455690383911
        kl: 0.009970223531126976
        policy_loss: -0.013026166707277298
        total_loss: 2.6652207374572754
        vf_explained_var: 0.9278180599212646
        vf_loss: 2.6770005226135254
    load_time_ms: 0.728
    num_steps_sampled: 712800
    num_steps_trained: 702000
    sample_time_ms: 1775.461
    update_time_ms: 3.929
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8940493526217776
    mean_inference_ms: 1.1134696442450045
    mean_processing_ms: 1.556255697333362
  time_since_restore: 175.26768708229065
  time_this_iter_s: 6.031384229660034
  time_total_s: 175.26768708229065
  timestamp: 1563925901
  timesteps_since_restore: 712800
  timesteps_this_iter: 26400
  timesteps_total: 712800
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 175 s, 27 iter, 712800 ts, 10.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.0716616005308
  episode_reward_mean: 9.975030598891884
  episode_reward_min: -8.735512004349884
  episodes_this_iter: 176
  episodes_total: 4928
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4646.196
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.609888792037964
        kl: 0.009235345758497715
        policy_loss: -0.013266782276332378
        total_loss: 1.9418387413024902
        vf_explained_var: 0.947033166885376
        vf_loss: 1.9539508819580078
    load_time_ms: 0.727
    num_steps_sampled: 739200
    num_steps_trained: 728000
    sample_time_ms: 1795.731
    update_time_ms: 3.945
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9001321607107136
    mean_inference_ms: 1.1201515273429887
    mean_processing_ms: 1.5649422160771964
  time_since_restore: 181.62394452095032
  time_this_iter_s: 6.356257438659668
  time_total_s: 181.62394452095032
  timestamp: 1563925907
  timesteps_since_restore: 739200
  timesteps_this_iter: 26400
  timesteps_total: 739200
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 181 s, 28 iter, 739200 ts, 9.98 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.48007534573499
  episode_reward_mean: 10.893603810953428
  episode_reward_min: -12.802005908330743
  episodes_this_iter: 176
  episodes_total: 5104
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4505.884
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6069066524505615
        kl: 0.01023179292678833
        policy_loss: -0.012901908718049526
        total_loss: 3.124300718307495
        vf_explained_var: 0.9209086298942566
        vf_loss: 3.135923147201538
    load_time_ms: 0.73
    num_steps_sampled: 765600
    num_steps_trained: 754000
    sample_time_ms: 1768.908
    update_time_ms: 3.963
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8973918215179786
    mean_inference_ms: 1.1171247327166414
    mean_processing_ms: 1.5642361174028325
  time_since_restore: 187.28963470458984
  time_this_iter_s: 5.665690183639526
  time_total_s: 187.28963470458984
  timestamp: 1563925913
  timesteps_since_restore: 765600
  timesteps_this_iter: 26400
  timesteps_total: 765600
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 187 s, 29 iter, 765600 ts, 10.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-51-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.55711024140488
  episode_reward_mean: 11.193579942447258
  episode_reward_min: -10.416888508830349
  episodes_this_iter: 176
  episodes_total: 5280
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4492.504
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.58811092376709
        kl: 0.009991548024117947
        policy_loss: -0.010449514724314213
        total_loss: 1.699270248413086
        vf_explained_var: 0.9504935145378113
        vf_loss: 1.7084707021713257
    load_time_ms: 0.732
    num_steps_sampled: 792000
    num_steps_trained: 780000
    sample_time_ms: 1817.245
    update_time_ms: 3.821
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8995995305520879
    mean_inference_ms: 1.1196723640428294
    mean_processing_ms: 1.5618557603711976
  time_since_restore: 193.19586753845215
  time_this_iter_s: 5.906232833862305
  time_total_s: 193.19586753845215
  timestamp: 1563925919
  timesteps_since_restore: 792000
  timesteps_this_iter: 26400
  timesteps_total: 792000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 193 s, 30 iter, 792000 ts, 11.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.064092559088678
  episode_reward_mean: 10.673247130322057
  episode_reward_min: -9.182635300896552
  episodes_this_iter: 176
  episodes_total: 5456
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4390.877
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5791831016540527
        kl: 0.011404131539165974
        policy_loss: -0.013465077616274357
        total_loss: 1.5538043975830078
        vf_explained_var: 0.9552328586578369
        vf_loss: 1.5658440589904785
    load_time_ms: 0.755
    num_steps_sampled: 818400
    num_steps_trained: 806000
    sample_time_ms: 1803.591
    update_time_ms: 3.792
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.902484031781276
    mean_inference_ms: 1.1190484742111133
    mean_processing_ms: 1.5685744961177974
  time_since_restore: 199.38736510276794
  time_this_iter_s: 6.191497564315796
  time_total_s: 199.38736510276794
  timestamp: 1563925925
  timesteps_since_restore: 818400
  timesteps_this_iter: 26400
  timesteps_total: 818400
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 199 s, 31 iter, 818400 ts, 10.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.91343543210168
  episode_reward_mean: 13.249678905947059
  episode_reward_min: -6.52245505713528
  episodes_this_iter: 176
  episodes_total: 5632
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4533.741
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5686800479888916
        kl: 0.010596786625683308
        policy_loss: -0.01205496583133936
        total_loss: 1.4249260425567627
        vf_explained_var: 0.9625743627548218
        vf_loss: 1.4356564283370972
    load_time_ms: 0.76
    num_steps_sampled: 844800
    num_steps_trained: 832000
    sample_time_ms: 1814.947
    update_time_ms: 3.748
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.9004997761123748
    mean_inference_ms: 1.1177609945135332
    mean_processing_ms: 1.561385147306958
  time_since_restore: 206.71321773529053
  time_this_iter_s: 7.325852632522583
  time_total_s: 206.71321773529053
  timestamp: 1563925932
  timesteps_since_restore: 844800
  timesteps_this_iter: 26400
  timesteps_total: 844800
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 206 s, 32 iter, 844800 ts, 13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.90456886079289
  episode_reward_mean: 9.962433698272376
  episode_reward_min: -9.518852291630708
  episodes_this_iter: 176
  episodes_total: 5808
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4450.211
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5575826168060303
        kl: 0.00997044239193201
        policy_loss: -0.011632748879492283
        total_loss: 1.5965055227279663
        vf_explained_var: 0.9503101706504822
        vf_loss: 1.6068918704986572
    load_time_ms: 0.768
    num_steps_sampled: 871200
    num_steps_trained: 858000
    sample_time_ms: 1806.011
    update_time_ms: 3.944
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8934062873879527
    mean_inference_ms: 1.1151461083065184
    mean_processing_ms: 1.55728294805794
  time_since_restore: 212.88277864456177
  time_this_iter_s: 6.16956090927124
  time_total_s: 212.88277864456177
  timestamp: 1563925939
  timesteps_since_restore: 871200
  timesteps_this_iter: 26400
  timesteps_total: 871200
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 212 s, 33 iter, 871200 ts, 9.96 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.45624438285322
  episode_reward_mean: 11.295746995487066
  episode_reward_min: -9.541852585076295
  episodes_this_iter: 176
  episodes_total: 5984
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4589.786
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.541651725769043
        kl: 0.011245885863900185
        policy_loss: -0.013888344168663025
        total_loss: 1.7665959596633911
        vf_explained_var: 0.9500145316123962
        vf_loss: 1.7790783643722534
    load_time_ms: 0.765
    num_steps_sampled: 897600
    num_steps_trained: 884000
    sample_time_ms: 1806.977
    update_time_ms: 4.046
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.894200356708023
    mean_inference_ms: 1.1149944368559435
    mean_processing_ms: 1.557914002467375
  time_since_restore: 220.21882343292236
  time_this_iter_s: 7.336044788360596
  time_total_s: 220.21882343292236
  timestamp: 1563925946
  timesteps_since_restore: 897600
  timesteps_this_iter: 26400
  timesteps_total: 897600
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 220 s, 34 iter, 897600 ts, 11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.79181617134751
  episode_reward_mean: 11.633958022186848
  episode_reward_min: -9.47474509897738
  episodes_this_iter: 176
  episodes_total: 6160
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4596.905
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5263876914978027
        kl: 0.01049124076962471
        policy_loss: -0.010303313843905926
        total_loss: 1.1978784799575806
        vf_explained_var: 0.965537965297699
        vf_loss: 1.2068703174591064
    load_time_ms: 0.764
    num_steps_sampled: 924000
    num_steps_trained: 910000
    sample_time_ms: 1806.856
    update_time_ms: 4.234
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8882482751004426
    mean_inference_ms: 1.1122848817253905
    mean_processing_ms: 1.55279313238509
  time_since_restore: 225.9002332687378
  time_this_iter_s: 5.68140983581543
  time_total_s: 225.9002332687378
  timestamp: 1563925952
  timesteps_since_restore: 924000
  timesteps_this_iter: 26400
  timesteps_total: 924000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 225 s, 35 iter, 924000 ts, 11.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.31532670926356
  episode_reward_mean: 11.720546275113158
  episode_reward_min: -10.765578767012638
  episodes_this_iter: 176
  episodes_total: 6336
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4464.836
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5179622173309326
        kl: 0.01107530202716589
        policy_loss: -0.01180069800466299
        total_loss: 1.1624019145965576
        vf_explained_var: 0.9640505909919739
        vf_loss: 1.1728183031082153
    load_time_ms: 0.764
    num_steps_sampled: 950400
    num_steps_trained: 936000
    sample_time_ms: 1773.689
    update_time_ms: 4.177
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8988222378697763
    mean_inference_ms: 1.1179016530729404
    mean_processing_ms: 1.5627157247472903
  time_since_restore: 231.8530044555664
  time_this_iter_s: 5.952771186828613
  time_total_s: 231.8530044555664
  timestamp: 1563925958
  timesteps_since_restore: 950400
  timesteps_this_iter: 26400
  timesteps_total: 950400
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 231 s, 36 iter, 950400 ts, 11.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.736107356986167
  episode_reward_mean: 12.307087453066437
  episode_reward_min: -10.110811596858946
  episodes_this_iter: 176
  episodes_total: 6512
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4532.704
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5079736709594727
        kl: 0.010546045377850533
        policy_loss: -0.013762355782091618
        total_loss: 1.0014150142669678
        vf_explained_var: 0.9703705310821533
        vf_loss: 1.0138590335845947
    load_time_ms: 0.767
    num_steps_sampled: 976800
    num_steps_trained: 962000
    sample_time_ms: 1783.817
    update_time_ms: 4.15
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8967350521673974
    mean_inference_ms: 1.1173421025373855
    mean_processing_ms: 1.563728472657349
  time_since_restore: 238.6654555797577
  time_this_iter_s: 6.812451124191284
  time_total_s: 238.6654555797577
  timestamp: 1563925964
  timesteps_since_restore: 976800
  timesteps_this_iter: 26400
  timesteps_total: 976800
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 238 s, 37 iter, 976800 ts, 12.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.97131168735347
  episode_reward_mean: 12.78111155173013
  episode_reward_min: -7.80154127007503
  episodes_this_iter: 176
  episodes_total: 6688
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4487.889
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.488306999206543
        kl: 0.010109344497323036
        policy_loss: -0.012882588431239128
        total_loss: 0.9264245629310608
        vf_explained_var: 0.9724797606468201
        vf_loss: 0.9380435347557068
    load_time_ms: 0.758
    num_steps_sampled: 1003200
    num_steps_trained: 988000
    sample_time_ms: 1784.791
    update_time_ms: 4.139
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.900308902843503
    mean_inference_ms: 1.119406090222615
    mean_processing_ms: 1.564295724006714
  time_since_restore: 244.58189225196838
  time_this_iter_s: 5.916436672210693
  time_total_s: 244.58189225196838
  timestamp: 1563925970
  timesteps_since_restore: 1003200
  timesteps_this_iter: 26400
  timesteps_total: 1003200
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 244 s, 38 iter, 1003200 ts, 12.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-52-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.43939919450877
  episode_reward_mean: 12.313228266148338
  episode_reward_min: -11.843609507378403
  episodes_this_iter: 176
  episodes_total: 6864
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4570.397
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.47996187210083
        kl: 0.012661272659897804
        policy_loss: -0.01266561821103096
        total_loss: 0.804831862449646
        vf_explained_var: 0.9759101271629333
        vf_loss: 0.8159148693084717
    load_time_ms: 0.757
    num_steps_sampled: 1029600
    num_steps_trained: 1014000
    sample_time_ms: 1791.911
    update_time_ms: 4.09
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8980385951369243
    mean_inference_ms: 1.1180061467222087
    mean_processing_ms: 1.5656816398307678
  time_since_restore: 251.1443772315979
  time_this_iter_s: 6.562484979629517
  time_total_s: 251.1443772315979
  timestamp: 1563925977
  timesteps_since_restore: 1029600
  timesteps_this_iter: 26400
  timesteps_total: 1029600
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 251 s, 39 iter, 1029600 ts, 12.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.502601079439074
  episode_reward_mean: 14.629369102040373
  episode_reward_min: -10.048065001845183
  episodes_this_iter: 176
  episodes_total: 7040
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4714.046
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4606335163116455
        kl: 0.01121061947196722
        policy_loss: -0.012879790738224983
        total_loss: 0.8093307018280029
        vf_explained_var: 0.9790589213371277
        vf_loss: 0.8208092451095581
    load_time_ms: 0.751
    num_steps_sampled: 1056000
    num_steps_trained: 1040000
    sample_time_ms: 1784.055
    update_time_ms: 4.171
  iterations_since_restore: 40
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8916750698073244
    mean_inference_ms: 1.1124459333052739
    mean_processing_ms: 1.5600256372589867
  time_since_restore: 258.4138560295105
  time_this_iter_s: 7.269478797912598
  time_total_s: 258.4138560295105
  timestamp: 1563925984
  timesteps_since_restore: 1056000
  timesteps_this_iter: 26400
  timesteps_total: 1056000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 258 s, 40 iter, 1056000 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.00136631776029
  episode_reward_mean: 12.994936734600003
  episode_reward_min: -6.787886463051401
  episodes_this_iter: 176
  episodes_total: 7216
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4673.546
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4462742805480957
        kl: 0.010271867737174034
        policy_loss: -0.009668650105595589
        total_loss: 0.836294412612915
        vf_explained_var: 0.9750419855117798
        vf_loss: 0.8446789383888245
    load_time_ms: 0.729
    num_steps_sampled: 1082400
    num_steps_trained: 1066000
    sample_time_ms: 1780.382
    update_time_ms: 4.333
  iterations_since_restore: 41
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8937148565569502
    mean_inference_ms: 1.1149379985373384
    mean_processing_ms: 1.5619641685936427
  time_since_restore: 264.1592524051666
  time_this_iter_s: 5.745396375656128
  time_total_s: 264.1592524051666
  timestamp: 1563925990
  timesteps_since_restore: 1082400
  timesteps_this_iter: 26400
  timesteps_total: 1082400
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 264 s, 41 iter, 1082400 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.753689368219714
  episode_reward_mean: 13.991976908892783
  episode_reward_min: -10.133078519322066
  episodes_this_iter: 176
  episodes_total: 7392
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4529.673
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.440674066543579
        kl: 0.01133816596120596
        policy_loss: -0.01239315327256918
        total_loss: 0.7827535271644592
        vf_explained_var: 0.9767216444015503
        vf_loss: 0.7937294840812683
    load_time_ms: 0.73
    num_steps_sampled: 1108800
    num_steps_trained: 1092000
    sample_time_ms: 1771.311
    update_time_ms: 4.318
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8915027539964144
    mean_inference_ms: 1.1120249119422092
    mean_processing_ms: 1.5581581667189115
  time_since_restore: 269.95169281959534
  time_this_iter_s: 5.792440414428711
  time_total_s: 269.95169281959534
  timestamp: 1563925996
  timesteps_since_restore: 1108800
  timesteps_this_iter: 26400
  timesteps_total: 1108800
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 269 s, 42 iter, 1108800 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.20074614026814
  episode_reward_mean: 13.238708947983179
  episode_reward_min: -7.611337810673856
  episodes_this_iter: 176
  episodes_total: 7568
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4567.524
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4270384311676025
        kl: 0.010841703042387962
        policy_loss: -0.014158742502331734
        total_loss: 0.698706328868866
        vf_explained_var: 0.9783545136451721
        vf_loss: 0.7115098237991333
    load_time_ms: 0.727
    num_steps_sampled: 1135200
    num_steps_trained: 1118000
    sample_time_ms: 1804.142
    update_time_ms: 4.156
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8913706024350034
    mean_inference_ms: 1.1118697355741993
    mean_processing_ms: 1.5577476088449822
  time_since_restore: 276.82909297943115
  time_this_iter_s: 6.877400159835815
  time_total_s: 276.82909297943115
  timestamp: 1563926003
  timesteps_since_restore: 1135200
  timesteps_this_iter: 26400
  timesteps_total: 1135200
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 276 s, 43 iter, 1135200 ts, 13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.15098796048721
  episode_reward_mean: 12.461539580359624
  episode_reward_min: -6.6311286372252365
  episodes_this_iter: 176
  episodes_total: 7744
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4510.005
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.410550832748413
        kl: 0.011510484851896763
        policy_loss: -0.012267816811800003
        total_loss: 0.7412630915641785
        vf_explained_var: 0.9720922708511353
        vf_loss: 0.7520921230316162
    load_time_ms: 0.726
    num_steps_sampled: 1161600
    num_steps_trained: 1144000
    sample_time_ms: 1783.805
    update_time_ms: 4.144
  iterations_since_restore: 44
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8902710078802711
    mean_inference_ms: 1.1118613695283743
    mean_processing_ms: 1.558855259215008
  time_since_restore: 283.3839375972748
  time_this_iter_s: 6.554844617843628
  time_total_s: 283.3839375972748
  timestamp: 1563926009
  timesteps_since_restore: 1161600
  timesteps_this_iter: 26400
  timesteps_total: 1161600
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 283 s, 44 iter, 1161600 ts, 12.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.698856746725898
  episode_reward_mean: 12.788727973595417
  episode_reward_min: -6.04893943910009
  episodes_this_iter: 176
  episodes_total: 7920
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4651.939
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3931875228881836
        kl: 0.011980569921433926
        policy_loss: -0.014019071124494076
        total_loss: 0.7287145256996155
        vf_explained_var: 0.9785661101341248
        vf_loss: 0.7412360906600952
    load_time_ms: 0.729
    num_steps_sampled: 1188000
    num_steps_trained: 1170000
    sample_time_ms: 1797.112
    update_time_ms: 4.01
  iterations_since_restore: 45
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8872086155526342
    mean_inference_ms: 1.1105305177795037
    mean_processing_ms: 1.5556258050950682
  time_since_restore: 290.6210927963257
  time_this_iter_s: 7.237155199050903
  time_total_s: 290.6210927963257
  timestamp: 1563926017
  timesteps_since_restore: 1188000
  timesteps_this_iter: 26400
  timesteps_total: 1188000
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 290 s, 45 iter, 1188000 ts, 12.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.269574835166566
  episode_reward_mean: 13.793345724459744
  episode_reward_min: -6.759730972226673
  episodes_this_iter: 176
  episodes_total: 8096
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4649.069
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3873815536499023
        kl: 0.009537654928863049
        policy_loss: -0.010922908782958984
        total_loss: 0.6943023204803467
        vf_explained_var: 0.9798254370689392
        vf_loss: 0.7040331363677979
    load_time_ms: 0.725
    num_steps_sampled: 1214400
    num_steps_trained: 1196000
    sample_time_ms: 1779.37
    update_time_ms: 4.224
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8868129874443467
    mean_inference_ms: 1.1089798714056898
    mean_processing_ms: 1.556951200217239
  time_since_restore: 296.37084794044495
  time_this_iter_s: 5.749755144119263
  time_total_s: 296.37084794044495
  timestamp: 1563926022
  timesteps_since_restore: 1214400
  timesteps_this_iter: 26400
  timesteps_total: 1214400
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 296 s, 46 iter, 1214400 ts, 13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.11040023859771
  episode_reward_mean: 12.126141061708212
  episode_reward_min: -6.947047868561459
  episodes_this_iter: 176
  episodes_total: 8272
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4681.724
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3724145889282227
        kl: 0.010106025263667107
        policy_loss: -0.013229654170572758
        total_loss: 0.6771242618560791
        vf_explained_var: 0.9773677587509155
        vf_loss: 0.6890906691551208
    load_time_ms: 0.714
    num_steps_sampled: 1240800
    num_steps_trained: 1222000
    sample_time_ms: 1786.052
    update_time_ms: 4.243
  iterations_since_restore: 47
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.883509003151521
    mean_inference_ms: 1.105431525937008
    mean_processing_ms: 1.5495820359238877
  time_since_restore: 303.5783643722534
  time_this_iter_s: 7.207516431808472
  time_total_s: 303.5783643722534
  timestamp: 1563926030
  timesteps_since_restore: 1240800
  timesteps_this_iter: 26400
  timesteps_total: 1240800
  training_iteration: 47
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 303 s, 47 iter, 1240800 ts, 12.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-53-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.72919741519081
  episode_reward_mean: 13.970556784715274
  episode_reward_min: -4.546012811624689
  episodes_this_iter: 176
  episodes_total: 8448
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4682.043
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3607046604156494
        kl: 0.011224220506846905
        policy_loss: -0.011629343032836914
        total_loss: 0.668857991695404
        vf_explained_var: 0.979988157749176
        vf_loss: 0.6790843605995178
    load_time_ms: 0.715
    num_steps_sampled: 1267200
    num_steps_trained: 1248000
    sample_time_ms: 1776.057
    update_time_ms: 4.366
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8880869457507274
    mean_inference_ms: 1.1102897352202732
    mean_processing_ms: 1.5600628268035501
  time_since_restore: 309.399863243103
  time_this_iter_s: 5.821498870849609
  time_total_s: 309.399863243103
  timestamp: 1563926035
  timesteps_since_restore: 1267200
  timesteps_this_iter: 26400
  timesteps_total: 1267200
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 309 s, 48 iter, 1267200 ts, 14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.76347414153421
  episode_reward_mean: 13.33721644247481
  episode_reward_min: -7.334653756735646
  episodes_this_iter: 176
  episodes_total: 8624
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4693.284
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.341484308242798
        kl: 0.011473581194877625
        policy_loss: -0.012622903101146221
        total_loss: 0.6347905397415161
        vf_explained_var: 0.9801729917526245
        vf_loss: 0.6459792852401733
    load_time_ms: 0.712
    num_steps_sampled: 1293600
    num_steps_trained: 1274000
    sample_time_ms: 1749.654
    update_time_ms: 4.429
  iterations_since_restore: 49
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8858077351766582
    mean_inference_ms: 1.108691164317954
    mean_processing_ms: 1.5558452374967933
  time_since_restore: 315.8128879070282
  time_this_iter_s: 6.413024663925171
  time_total_s: 315.8128879070282
  timestamp: 1563926042
  timesteps_since_restore: 1293600
  timesteps_this_iter: 26400
  timesteps_total: 1293600
  training_iteration: 49
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 315 s, 49 iter, 1293600 ts, 13.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.054524294837414
  episode_reward_mean: 14.128854348997447
  episode_reward_min: -7.225641851322981
  episodes_this_iter: 176
  episodes_total: 8800
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4590.775
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3225724697113037
        kl: 0.013468062505126
        policy_loss: -0.014660477638244629
        total_loss: 0.6997157335281372
        vf_explained_var: 0.9812731146812439
        vf_loss: 0.7126926779747009
    load_time_ms: 0.715
    num_steps_sampled: 1320000
    num_steps_trained: 1300000
    sample_time_ms: 1795.442
    update_time_ms: 4.462
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8901216263842757
    mean_inference_ms: 1.1124484405746495
    mean_processing_ms: 1.5601297350589272
  time_since_restore: 322.51182770729065
  time_this_iter_s: 6.698939800262451
  time_total_s: 322.51182770729065
  timestamp: 1563926049
  timesteps_since_restore: 1320000
  timesteps_this_iter: 26400
  timesteps_total: 1320000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 322 s, 50 iter, 1320000 ts, 14.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.24277526658197
  episode_reward_mean: 14.671703035027178
  episode_reward_min: -8.330267767763244
  episodes_this_iter: 176
  episodes_total: 8976
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4721.943
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3103106021881104
        kl: 0.01374385878443718
        policy_loss: -0.014148363843560219
        total_loss: 0.5188604593276978
        vf_explained_var: 0.9838875532150269
        vf_loss: 0.5312908291816711
    load_time_ms: 0.719
    num_steps_sampled: 1346400
    num_steps_trained: 1326000
    sample_time_ms: 1778.593
    update_time_ms: 4.409
  iterations_since_restore: 51
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8841923233182165
    mean_inference_ms: 1.1079746625105629
    mean_processing_ms: 1.5562637638772727
  time_since_restore: 329.403568983078
  time_this_iter_s: 6.8917412757873535
  time_total_s: 329.403568983078
  timestamp: 1563926055
  timesteps_since_restore: 1346400
  timesteps_this_iter: 26400
  timesteps_total: 1346400
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 329 s, 51 iter, 1346400 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.62850504273497
  episode_reward_mean: 13.207029510946871
  episode_reward_min: -8.014661931884703
  episodes_this_iter: 176
  episodes_total: 9152
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4863.465
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2972564697265625
        kl: 0.010703539475798607
        policy_loss: -0.011725707910954952
        total_loss: 0.6098294258117676
        vf_explained_var: 0.9802283644676208
        vf_loss: 0.6202172040939331
    load_time_ms: 0.721
    num_steps_sampled: 1372800
    num_steps_trained: 1352000
    sample_time_ms: 1796.581
    update_time_ms: 4.482
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8881829050361558
    mean_inference_ms: 1.1132160428007682
    mean_processing_ms: 1.5606434326647984
  time_since_restore: 336.7966833114624
  time_this_iter_s: 7.393114328384399
  time_total_s: 336.7966833114624
  timestamp: 1563926063
  timesteps_since_restore: 1372800
  timesteps_this_iter: 26400
  timesteps_total: 1372800
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 336 s, 52 iter, 1372800 ts, 13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.46260029469316
  episode_reward_mean: 14.34577230654058
  episode_reward_min: -6.082080355369263
  episodes_this_iter: 176
  episodes_total: 9328
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4770.13
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2789864540100098
        kl: 0.012473740614950657
        policy_loss: -0.015547724440693855
        total_loss: 0.5211619734764099
        vf_explained_var: 0.9840294122695923
        vf_loss: 0.5351504683494568
    load_time_ms: 0.712
    num_steps_sampled: 1399200
    num_steps_trained: 1378000
    sample_time_ms: 1768.053
    update_time_ms: 4.538
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8855190688121375
    mean_inference_ms: 1.1092150143441017
    mean_processing_ms: 1.554985750889866
  time_since_restore: 342.45272946357727
  time_this_iter_s: 5.656046152114868
  time_total_s: 342.45272946357727
  timestamp: 1563926069
  timesteps_since_restore: 1399200
  timesteps_this_iter: 26400
  timesteps_total: 1399200
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 342 s, 53 iter, 1399200 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.80883875378026
  episode_reward_mean: 14.975570949842938
  episode_reward_min: -8.874254517762889
  episodes_this_iter: 176
  episodes_total: 9504
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4689.072
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2656965255737305
        kl: 0.01220481563359499
        policy_loss: -0.01394924707710743
        total_loss: 0.5083234906196594
        vf_explained_var: 0.9853295683860779
        vf_loss: 0.5207471251487732
    load_time_ms: 0.71
    num_steps_sampled: 1425600
    num_steps_trained: 1404000
    sample_time_ms: 1771.949
    update_time_ms: 4.49
  iterations_since_restore: 54
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8825844296387564
    mean_inference_ms: 1.1073133599942329
    mean_processing_ms: 1.5524794607912187
  time_since_restore: 348.2353193759918
  time_this_iter_s: 5.782589912414551
  time_total_s: 348.2353193759918
  timestamp: 1563926074
  timesteps_since_restore: 1425600
  timesteps_this_iter: 26400
  timesteps_total: 1425600
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 348 s, 54 iter, 1425600 ts, 15 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.515213867782926
  episode_reward_mean: 13.192715126251159
  episode_reward_min: -6.021736321278653
  episodes_this_iter: 176
  episodes_total: 9680
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4558.318
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2489724159240723
        kl: 0.012577892281115055
        policy_loss: -0.013226007111370564
        total_loss: 0.5345652103424072
        vf_explained_var: 0.9814673066139221
        vf_loss: 0.546218991279602
    load_time_ms: 0.713
    num_steps_sampled: 1452000
    num_steps_trained: 1430000
    sample_time_ms: 1778.751
    update_time_ms: 4.537
  iterations_since_restore: 55
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8848320252859956
    mean_inference_ms: 1.1078089915884568
    mean_processing_ms: 1.553572928088459
  time_since_restore: 354.228533744812
  time_this_iter_s: 5.99321436882019
  time_total_s: 354.228533744812
  timestamp: 1563926080
  timesteps_since_restore: 1452000
  timesteps_this_iter: 26400
  timesteps_total: 1452000
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 354 s, 55 iter, 1452000 ts, 13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.82383914669026
  episode_reward_mean: 14.579703776902551
  episode_reward_min: -5.976423404633969
  episodes_this_iter: 176
  episodes_total: 9856
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4571.333
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2340474128723145
        kl: 0.012893157079815865
        policy_loss: -0.016476379707455635
        total_loss: 0.5343751311302185
        vf_explained_var: 0.9841853380203247
        vf_loss: 0.549239993095398
    load_time_ms: 0.711
    num_steps_sampled: 1478400
    num_steps_trained: 1456000
    sample_time_ms: 1777.985
    update_time_ms: 4.343
  iterations_since_restore: 56
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8845204372673274
    mean_inference_ms: 1.1090520144885379
    mean_processing_ms: 1.5566800709935178
  time_since_restore: 360.0991611480713
  time_this_iter_s: 5.870627403259277
  time_total_s: 360.0991611480713
  timestamp: 1563926086
  timesteps_since_restore: 1478400
  timesteps_this_iter: 26400
  timesteps_total: 1478400
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 360 s, 56 iter, 1478400 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.55911039912034
  episode_reward_mean: 14.557861352856321
  episode_reward_min: -6.536395656681224
  episodes_this_iter: 176
  episodes_total: 10032
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4432.161
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.21913743019104
        kl: 0.012405778281390667
        policy_loss: -0.01522862259298563
        total_loss: 0.47368142008781433
        vf_explained_var: 0.9850348830223083
        vf_loss: 0.48735925555229187
    load_time_ms: 0.718
    num_steps_sampled: 1504800
    num_steps_trained: 1482000
    sample_time_ms: 1780.683
    update_time_ms: 4.322
  iterations_since_restore: 57
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.882599985669392
    mean_inference_ms: 1.107408441713224
    mean_processing_ms: 1.5519615841435945
  time_since_restore: 365.9377164840698
  time_this_iter_s: 5.838555335998535
  time_total_s: 365.9377164840698
  timestamp: 1563926092
  timesteps_since_restore: 1504800
  timesteps_this_iter: 26400
  timesteps_total: 1504800
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 365 s, 57 iter, 1504800 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-54-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.82780452894625
  episode_reward_mean: 13.815498799129307
  episode_reward_min: -5.060383763969214
  episodes_this_iter: 176
  episodes_total: 10208
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4478.822
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.209664821624756
        kl: 0.01261480525135994
        policy_loss: -0.013826669193804264
        total_loss: 0.47985774278640747
        vf_explained_var: 0.9845489263534546
        vf_loss: 0.4921075403690338
    load_time_ms: 0.717
    num_steps_sampled: 1531200
    num_steps_trained: 1508000
    sample_time_ms: 1779.983
    update_time_ms: 4.148
  iterations_since_restore: 58
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8847046811060353
    mean_inference_ms: 1.10818286280755
    mean_processing_ms: 1.5575978992763828
  time_since_restore: 372.21802401542664
  time_this_iter_s: 6.2803075313568115
  time_total_s: 372.21802401542664
  timestamp: 1563926098
  timesteps_since_restore: 1531200
  timesteps_this_iter: 26400
  timesteps_total: 1531200
  training_iteration: 58
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 372 s, 58 iter, 1531200 ts, 13.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.74038738782443
  episode_reward_mean: 13.887518139877194
  episode_reward_min: -5.704312596575464
  episodes_this_iter: 176
  episodes_total: 10384
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4382.525
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1949727535247803
        kl: 0.012078344821929932
        policy_loss: -0.012707842513918877
        total_loss: 0.42799073457717896
        vf_explained_var: 0.9858221411705017
        vf_loss: 0.43918877840042114
    load_time_ms: 0.714
    num_steps_sampled: 1557600
    num_steps_trained: 1534000
    sample_time_ms: 1803.728
    update_time_ms: 4.166
  iterations_since_restore: 59
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8810370489126653
    mean_inference_ms: 1.1064915016521604
    mean_processing_ms: 1.552769745080209
  time_since_restore: 377.90260124206543
  time_this_iter_s: 5.684577226638794
  time_total_s: 377.90260124206543
  timestamp: 1563926104
  timesteps_since_restore: 1557600
  timesteps_this_iter: 26400
  timesteps_total: 1557600
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 377 s, 59 iter, 1557600 ts, 13.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.60217174479487
  episode_reward_mean: 15.33297665632969
  episode_reward_min: -5.0321552229468836
  episodes_this_iter: 176
  episodes_total: 10560
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4482.004
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1833481788635254
        kl: 0.012911350466310978
        policy_loss: -0.01420050673186779
        total_loss: 0.463817298412323
        vf_explained_var: 0.9866284728050232
        vf_loss: 0.4764038920402527
    load_time_ms: 0.712
    num_steps_sampled: 1584000
    num_steps_trained: 1560000
    sample_time_ms: 1748.101
    update_time_ms: 4.083
  iterations_since_restore: 60
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8827349801043263
    mean_inference_ms: 1.1066672097350314
    mean_processing_ms: 1.5550398198016844
  time_since_restore: 385.0420835018158
  time_this_iter_s: 7.139482259750366
  time_total_s: 385.0420835018158
  timestamp: 1563926111
  timesteps_since_restore: 1584000
  timesteps_this_iter: 26400
  timesteps_total: 1584000
  training_iteration: 60
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 385 s, 60 iter, 1584000 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.602501892779244
  episode_reward_mean: 15.302644797371386
  episode_reward_min: -5.668957010542688
  episodes_this_iter: 176
  episodes_total: 10736
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4347.413
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1705236434936523
        kl: 0.012247337959706783
        policy_loss: -0.013556724414229393
        total_loss: 0.4424087107181549
        vf_explained_var: 0.9859161972999573
        vf_loss: 0.4544345736503601
    load_time_ms: 0.702
    num_steps_sampled: 1610400
    num_steps_trained: 1586000
    sample_time_ms: 1785.612
    update_time_ms: 4.086
  iterations_since_restore: 61
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.879807270010337
    mean_inference_ms: 1.1055380225695435
    mean_processing_ms: 1.550422486699817
  time_since_restore: 390.959148645401
  time_this_iter_s: 5.917065143585205
  time_total_s: 390.959148645401
  timestamp: 1563926117
  timesteps_since_restore: 1610400
  timesteps_this_iter: 26400
  timesteps_total: 1610400
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 390 s, 61 iter, 1610400 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.88693590066019
  episode_reward_mean: 15.417530345204892
  episode_reward_min: -6.06586030999849
  episodes_this_iter: 176
  episodes_total: 10912
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4345.432
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.157827377319336
        kl: 0.01383308693766594
        policy_loss: -0.013583357445895672
        total_loss: 0.41925349831581116
        vf_explained_var: 0.9878442287445068
        vf_loss: 0.43110767006874084
    load_time_ms: 0.702
    num_steps_sampled: 1636800
    num_steps_trained: 1612000
    sample_time_ms: 1756.012
    update_time_ms: 3.934
  iterations_since_restore: 62
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8859744019150784
    mean_inference_ms: 1.1097981268552808
    mean_processing_ms: 1.5582015150587536
  time_since_restore: 398.0348370075226
  time_this_iter_s: 7.075688362121582
  time_total_s: 398.0348370075226
  timestamp: 1563926124
  timesteps_since_restore: 1636800
  timesteps_this_iter: 26400
  timesteps_total: 1636800
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 398 s, 62 iter, 1636800 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.40201292987853
  episode_reward_mean: 14.528158863534577
  episode_reward_min: -4.746548363196309
  episodes_this_iter: 176
  episodes_total: 11088
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4405.397
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.142275333404541
        kl: 0.013096733950078487
        policy_loss: -0.01645214483141899
        total_loss: 0.36190032958984375
        vf_explained_var: 0.9874414205551147
        vf_loss: 0.37671539187431335
    load_time_ms: 0.709
    num_steps_sampled: 1663200
    num_steps_trained: 1638000
    sample_time_ms: 1785.391
    update_time_ms: 4.024
  iterations_since_restore: 63
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.879401847381988
    mean_inference_ms: 1.105307789516013
    mean_processing_ms: 1.549370452950958
  time_since_restore: 404.5878541469574
  time_this_iter_s: 6.5530171394348145
  time_total_s: 404.5878541469574
  timestamp: 1563926131
  timesteps_since_restore: 1663200
  timesteps_this_iter: 26400
  timesteps_total: 1663200
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 404 s, 63 iter, 1663200 ts, 14.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.22085206001101
  episode_reward_mean: 15.073505798058223
  episode_reward_min: -5.9513728111055375
  episodes_this_iter: 176
  episodes_total: 11264
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4434.606
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.132136344909668
        kl: 0.013282173313200474
        policy_loss: -0.016789447516202927
        total_loss: 0.37215009331703186
        vf_explained_var: 0.9887205362319946
        vf_loss: 0.3872792422771454
    load_time_ms: 0.709
    num_steps_sampled: 1689600
    num_steps_trained: 1664000
    sample_time_ms: 1766.65
    update_time_ms: 4.096
  iterations_since_restore: 64
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8792338642978075
    mean_inference_ms: 1.1062504608430757
    mean_processing_ms: 1.5540825462445709
  time_since_restore: 410.47613978385925
  time_this_iter_s: 5.8882856369018555
  time_total_s: 410.47613978385925
  timestamp: 1563926137
  timesteps_since_restore: 1689600
  timesteps_this_iter: 26400
  timesteps_total: 1689600
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 410 s, 64 iter, 1689600 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.00118358454192
  episode_reward_mean: 16.023203924715293
  episode_reward_min: -1.5498910618692254
  episodes_this_iter: 176
  episodes_total: 11440
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4515.372
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.115903854370117
        kl: 0.012624721974134445
        policy_loss: -0.0144993020221591
        total_loss: 0.32876819372177124
        vf_explained_var: 0.9900360107421875
        vf_loss: 0.34168940782546997
    load_time_ms: 0.71
    num_steps_sampled: 1716000
    num_steps_trained: 1690000
    sample_time_ms: 1770.254
    update_time_ms: 4.056
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8811637280875873
    mean_inference_ms: 1.1061995906237068
    mean_processing_ms: 1.5511700055164186
  time_since_restore: 417.31629395484924
  time_this_iter_s: 6.84015417098999
  time_total_s: 417.31629395484924
  timestamp: 1563926144
  timesteps_since_restore: 1716000
  timesteps_this_iter: 26400
  timesteps_total: 1716000
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 417 s, 65 iter, 1716000 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.77834205975401
  episode_reward_mean: 16.253058334768085
  episode_reward_min: -0.5167583757334087
  episodes_this_iter: 176
  episodes_total: 11616
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4641.993
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1031956672668457
        kl: 0.01344594918191433
        policy_loss: -0.016307197511196136
        total_loss: 0.35097536444664
        vf_explained_var: 0.9901103377342224
        vf_loss: 0.3656017482280731
    load_time_ms: 0.714
    num_steps_sampled: 1742400
    num_steps_trained: 1716000
    sample_time_ms: 1769.899
    update_time_ms: 4.166
  iterations_since_restore: 66
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8836309857066724
    mean_inference_ms: 1.1085758581820306
    mean_processing_ms: 1.553608023001339
  time_since_restore: 424.45452547073364
  time_this_iter_s: 7.138231515884399
  time_total_s: 424.45452547073364
  timestamp: 1563926151
  timesteps_since_restore: 1742400
  timesteps_this_iter: 26400
  timesteps_total: 1742400
  training_iteration: 66
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 424 s, 66 iter, 1742400 ts, 16.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-55-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.78103631956036
  episode_reward_mean: 15.007957237647263
  episode_reward_min: -4.35765802051013
  episodes_this_iter: 176
  episodes_total: 11792
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4640.763
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0931074619293213
        kl: 0.012630664743483067
        policy_loss: -0.015538647770881653
        total_loss: 0.36147409677505493
        vf_explained_var: 0.9873198866844177
        vf_loss: 0.37543395161628723
    load_time_ms: 0.717
    num_steps_sampled: 1768800
    num_steps_trained: 1742000
    sample_time_ms: 1770.814
    update_time_ms: 4.245
  iterations_since_restore: 67
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8803498037667965
    mean_inference_ms: 1.1065605854564924
    mean_processing_ms: 1.5521513483369445
  time_since_restore: 430.2904405593872
  time_this_iter_s: 5.8359150886535645
  time_total_s: 430.2904405593872
  timestamp: 1563926157
  timesteps_since_restore: 1768800
  timesteps_this_iter: 26400
  timesteps_total: 1768800
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 430 s, 67 iter, 1768800 ts, 15 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.129614011828195
  episode_reward_mean: 16.7034810733454
  episode_reward_min: -5.500281340193877
  episodes_this_iter: 176
  episodes_total: 11968
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4595.275
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0745205879211426
        kl: 0.012445519678294659
        policy_loss: -0.015657516196370125
        total_loss: 0.3354443311691284
        vf_explained_var: 0.9900599122047424
        vf_loss: 0.3495462238788605
    load_time_ms: 0.718
    num_steps_sampled: 1795200
    num_steps_trained: 1768000
    sample_time_ms: 1763.748
    update_time_ms: 4.232
  iterations_since_restore: 68
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8841768956768044
    mean_inference_ms: 1.1113437250651328
    mean_processing_ms: 1.5571700270768865
  time_since_restore: 436.04397916793823
  time_this_iter_s: 5.753538608551025
  time_total_s: 436.04397916793823
  timestamp: 1563926162
  timesteps_since_restore: 1795200
  timesteps_this_iter: 26400
  timesteps_total: 1795200
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 436 s, 68 iter, 1795200 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.34084500212909
  episode_reward_mean: 15.860345757151533
  episode_reward_min: -5.134654654523845
  episodes_this_iter: 176
  episodes_total: 12144
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4594.01
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0575690269470215
        kl: 0.0140126533806324
        policy_loss: -0.016211433336138725
        total_loss: 0.31758934259414673
        vf_explained_var: 0.9900575876235962
        vf_loss: 0.3320492208003998
    load_time_ms: 0.724
    num_steps_sampled: 1821600
    num_steps_trained: 1794000
    sample_time_ms: 1776.834
    update_time_ms: 4.145
  iterations_since_restore: 69
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8796154115833652
    mean_inference_ms: 1.1067906403625594
    mean_processing_ms: 1.5531380694646828
  time_since_restore: 441.8459668159485
  time_this_iter_s: 5.801987648010254
  time_total_s: 441.8459668159485
  timestamp: 1563926168
  timesteps_since_restore: 1821600
  timesteps_this_iter: 26400
  timesteps_total: 1821600
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 441 s, 69 iter, 1821600 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.47655126944694
  episode_reward_mean: 14.58845994091438
  episode_reward_min: -4.15429104167919
  episodes_this_iter: 176
  episodes_total: 12320
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4513.482
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.037522792816162
        kl: 0.014258732087910175
        policy_loss: -0.016412027180194855
        total_loss: 0.3240755498409271
        vf_explained_var: 0.9894803166389465
        vf_loss: 0.33870524168014526
    load_time_ms: 0.73
    num_steps_sampled: 1848000
    num_steps_trained: 1820000
    sample_time_ms: 1761.744
    update_time_ms: 4.166
  iterations_since_restore: 70
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8847795784152341
    mean_inference_ms: 1.1101535007540362
    mean_processing_ms: 1.556016345276577
  time_since_restore: 448.02739238739014
  time_this_iter_s: 6.18142557144165
  time_total_s: 448.02739238739014
  timestamp: 1563926174
  timesteps_since_restore: 1848000
  timesteps_this_iter: 26400
  timesteps_total: 1848000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 448 s, 70 iter, 1848000 ts, 14.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.951633923395825
  episode_reward_mean: 15.508968315155625
  episode_reward_min: -7.17399076277523
  episodes_this_iter: 176
  episodes_total: 12496
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4660.695
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0254900455474854
        kl: 0.012240679003298283
        policy_loss: -0.013475646264851093
        total_loss: 0.31262505054473877
        vf_explained_var: 0.9899314641952515
        vf_loss: 0.32457059621810913
    load_time_ms: 0.734
    num_steps_sampled: 1874400
    num_steps_trained: 1846000
    sample_time_ms: 1763.544
    update_time_ms: 4.209
  iterations_since_restore: 71
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.886949058922445
    mean_inference_ms: 1.1137393159444438
    mean_processing_ms: 1.5583569226668763
  time_since_restore: 455.44065833091736
  time_this_iter_s: 7.413265943527222
  time_total_s: 455.44065833091736
  timestamp: 1563926182
  timesteps_since_restore: 1874400
  timesteps_this_iter: 26400
  timesteps_total: 1874400
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 455 s, 71 iter, 1874400 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.883685408653484
  episode_reward_mean: 15.019028362484478
  episode_reward_min: -6.936468541665822
  episodes_this_iter: 176
  episodes_total: 12672
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4522.321
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0183236598968506
        kl: 0.012949885800480843
        policy_loss: -0.015791701152920723
        total_loss: 0.2927434742450714
        vf_explained_var: 0.9899283051490784
        vf_loss: 0.3069164454936981
    load_time_ms: 0.722
    num_steps_sampled: 1900800
    num_steps_trained: 1872000
    sample_time_ms: 1733.021
    update_time_ms: 4.373
  iterations_since_restore: 72
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.886998038826902
    mean_inference_ms: 1.1125681870794755
    mean_processing_ms: 1.5573405603697823
  time_since_restore: 460.823454618454
  time_this_iter_s: 5.382796287536621
  time_total_s: 460.823454618454
  timestamp: 1563926187
  timesteps_since_restore: 1900800
  timesteps_this_iter: 26400
  timesteps_total: 1900800
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 460 s, 72 iter, 1900800 ts, 15 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.02642790376639
  episode_reward_mean: 15.796619266692234
  episode_reward_min: -3.9139469727321834
  episodes_this_iter: 176
  episodes_total: 12848
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4602.161
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.002666711807251
        kl: 0.01313681062310934
        policy_loss: -0.016695113852620125
        total_loss: 0.2920381426811218
        vf_explained_var: 0.990266740322113
        vf_loss: 0.3070911765098572
    load_time_ms: 0.716
    num_steps_sampled: 1927200
    num_steps_trained: 1898000
    sample_time_ms: 1748.361
    update_time_ms: 4.3
  iterations_since_restore: 73
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888862436341021
    mean_inference_ms: 1.1145553845125606
    mean_processing_ms: 1.5611498295802555
  time_since_restore: 468.32949781417847
  time_this_iter_s: 7.506043195724487
  time_total_s: 468.32949781417847
  timestamp: 1563926195
  timesteps_since_restore: 1927200
  timesteps_this_iter: 26400
  timesteps_total: 1927200
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 468 s, 73 iter, 1927200 ts, 15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.440700942554464
  episode_reward_mean: 15.606226278546625
  episode_reward_min: -3.9141630301870625
  episodes_this_iter: 176
  episodes_total: 13024
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4631.185
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9905979633331299
        kl: 0.014630245044827461
        policy_loss: -0.016230447217822075
        total_loss: 0.2722083330154419
        vf_explained_var: 0.9908813834190369
        vf_loss: 0.2866099774837494
    load_time_ms: 0.721
    num_steps_sampled: 1953600
    num_steps_trained: 1924000
    sample_time_ms: 1747.051
    update_time_ms: 4.386
  iterations_since_restore: 74
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8865120023204867
    mean_inference_ms: 1.1136907665446811
    mean_processing_ms: 1.5578767425119955
  time_since_restore: 474.49587082862854
  time_this_iter_s: 6.166373014450073
  time_total_s: 474.49587082862854
  timestamp: 1563926201
  timesteps_since_restore: 1953600
  timesteps_this_iter: 26400
  timesteps_total: 1953600
  training_iteration: 74
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 474 s, 74 iter, 1953600 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.63653443835705
  episode_reward_mean: 17.499142355824873
  episode_reward_min: -4.758624712901529
  episodes_this_iter: 176
  episodes_total: 13200
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4678.674
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.977602481842041
        kl: 0.0132595831528306
        policy_loss: -0.016750391572713852
        total_loss: 0.2726729214191437
        vf_explained_var: 0.9924529194831848
        vf_loss: 0.28776586055755615
    load_time_ms: 0.716
    num_steps_sampled: 1980000
    num_steps_trained: 1950000
    sample_time_ms: 1747.731
    update_time_ms: 4.395
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.887971552804173
    mean_inference_ms: 1.1141437039641593
    mean_processing_ms: 1.5596499190840765
  time_since_restore: 481.8181302547455
  time_this_iter_s: 7.322259426116943
  time_total_s: 481.8181302547455
  timestamp: 1563926208
  timesteps_since_restore: 1980000
  timesteps_this_iter: 26400
  timesteps_total: 1980000
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 481 s, 75 iter, 1980000 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-56-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.47097465213047
  episode_reward_mean: 14.737020684015233
  episode_reward_min: -3.929203956383452
  episodes_this_iter: 176
  episodes_total: 13376
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4550.637
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9641810655593872
        kl: 0.01406015269458294
        policy_loss: -0.01656252332031727
        total_loss: 0.24505659937858582
        vf_explained_var: 0.991014838218689
        vf_loss: 0.2598615884780884
    load_time_ms: 0.718
    num_steps_sampled: 2006400
    num_steps_trained: 1976000
    sample_time_ms: 1745.381
    update_time_ms: 4.474
  iterations_since_restore: 76
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.89085787637777
    mean_inference_ms: 1.1169861119326272
    mean_processing_ms: 1.562573484574689
  time_since_restore: 487.6485483646393
  time_this_iter_s: 5.830418109893799
  time_total_s: 487.6485483646393
  timestamp: 1563926214
  timesteps_since_restore: 2006400
  timesteps_this_iter: 26400
  timesteps_total: 2006400
  training_iteration: 76
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 487 s, 76 iter, 2006400 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.14936175664224
  episode_reward_mean: 17.906488937634574
  episode_reward_min: -2.356918468973074
  episodes_this_iter: 176
  episodes_total: 13552
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4549.102
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9493966102600098
        kl: 0.013981923460960388
        policy_loss: -0.016398143023252487
        total_loss: 0.2354557365179062
        vf_explained_var: 0.9934821724891663
        vf_loss: 0.25010615587234497
    load_time_ms: 0.716
    num_steps_sampled: 2032800
    num_steps_trained: 2002000
    sample_time_ms: 1751.809
    update_time_ms: 4.397
  iterations_since_restore: 77
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888827870898794
    mean_inference_ms: 1.1134130570618057
    mean_processing_ms: 1.55852466586992
  time_since_restore: 493.53377866744995
  time_this_iter_s: 5.885230302810669
  time_total_s: 493.53377866744995
  timestamp: 1563926220
  timesteps_since_restore: 2032800
  timesteps_this_iter: 26400
  timesteps_total: 2032800
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 493 s, 77 iter, 2032800 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.758475470756906
  episode_reward_mean: 16.031188485404982
  episode_reward_min: -4.078795601432354
  episodes_this_iter: 176
  episodes_total: 13728
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4691.088
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9435908794403076
        kl: 0.013401131145656109
        policy_loss: -0.015451891347765923
        total_loss: 0.23207011818885803
        vf_explained_var: 0.9925235509872437
        vf_loss: 0.24584683775901794
    load_time_ms: 0.713
    num_steps_sampled: 2059200
    num_steps_trained: 2028000
    sample_time_ms: 1756.456
    update_time_ms: 4.449
  iterations_since_restore: 78
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8916705180831863
    mean_inference_ms: 1.1173547465763956
    mean_processing_ms: 1.5625411407469352
  time_since_restore: 500.7597918510437
  time_this_iter_s: 7.22601318359375
  time_total_s: 500.7597918510437
  timestamp: 1563926227
  timesteps_since_restore: 2059200
  timesteps_this_iter: 26400
  timesteps_total: 2059200
  training_iteration: 78
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 500 s, 78 iter, 2059200 ts, 16 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.80296401283264
  episode_reward_mean: 16.54162061563575
  episode_reward_min: -2.4213368458470814
  episodes_this_iter: 176
  episodes_total: 13904
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4750.488
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9256503582000732
        kl: 0.013167674653232098
        policy_loss: -0.01581186056137085
        total_loss: 0.22812803089618683
        vf_explained_var: 0.9926213026046753
        vf_loss: 0.24229392409324646
    load_time_ms: 0.707
    num_steps_sampled: 2085600
    num_steps_trained: 2054000
    sample_time_ms: 1747.059
    update_time_ms: 4.591
  iterations_since_restore: 79
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8859905247351352
    mean_inference_ms: 1.1132633127444607
    mean_processing_ms: 1.5584220351053908
  time_since_restore: 507.0658006668091
  time_this_iter_s: 6.306008815765381
  time_total_s: 507.0658006668091
  timestamp: 1563926234
  timesteps_since_restore: 2085600
  timesteps_this_iter: 26400
  timesteps_total: 2085600
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 507 s, 79 iter, 2085600 ts, 16.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.581163235491054
  episode_reward_mean: 16.3677049545509
  episode_reward_min: -4.264426022647216
  episodes_this_iter: 176
  episodes_total: 14080
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4815.226
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.906511902809143
        kl: 0.014654259197413921
        policy_loss: -0.01789424568414688
        total_loss: 0.2175300121307373
        vf_explained_var: 0.9926902055740356
        vf_loss: 0.2335924506187439
    load_time_ms: 0.704
    num_steps_sampled: 2112000
    num_steps_trained: 2080000
    sample_time_ms: 1772.149
    update_time_ms: 4.686
  iterations_since_restore: 80
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8891857249151813
    mean_inference_ms: 1.1138108777844713
    mean_processing_ms: 1.5602052289540276
  time_since_restore: 514.1475548744202
  time_this_iter_s: 7.081754207611084
  time_total_s: 514.1475548744202
  timestamp: 1563926241
  timesteps_since_restore: 2112000
  timesteps_this_iter: 26400
  timesteps_total: 2112000
  training_iteration: 80
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 514 s, 80 iter, 2112000 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.418681390494555
  episode_reward_mean: 16.58597612356266
  episode_reward_min: -2.4734266110594305
  episodes_this_iter: 176
  episodes_total: 14256
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4669.51
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8962304592132568
        kl: 0.014958038926124573
        policy_loss: -0.018427228555083275
        total_loss: 0.19403265416622162
        vf_explained_var: 0.9935818910598755
        vf_loss: 0.21059013903141022
    load_time_ms: 0.706
    num_steps_sampled: 2138400
    num_steps_trained: 2106000
    sample_time_ms: 1749.995
    update_time_ms: 4.596
  iterations_since_restore: 81
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8922435608409174
    mean_inference_ms: 1.1180570647948789
    mean_processing_ms: 1.5641173059780655
  time_since_restore: 519.8761873245239
  time_this_iter_s: 5.72863245010376
  time_total_s: 519.8761873245239
  timestamp: 1563926246
  timesteps_since_restore: 2138400
  timesteps_this_iter: 26400
  timesteps_total: 2138400
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 519 s, 81 iter, 2138400 ts, 16.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.91261354219973
  episode_reward_mean: 16.588914573601865
  episode_reward_min: -4.306757369543592
  episodes_this_iter: 176
  episodes_total: 14432
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4671.62
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.879032015800476
        kl: 0.015626830980181694
        policy_loss: -0.019901039078831673
        total_loss: 0.20685912668704987
        vf_explained_var: 0.993242621421814
        vf_loss: 0.22480681538581848
    load_time_ms: 0.715
    num_steps_sampled: 2164800
    num_steps_trained: 2132000
    sample_time_ms: 1794.192
    update_time_ms: 4.439
  iterations_since_restore: 82
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.886219938203009
    mean_inference_ms: 1.1136399909678205
    mean_processing_ms: 1.556782331726879
  time_since_restore: 525.7199411392212
  time_this_iter_s: 5.843753814697266
  time_total_s: 525.7199411392212
  timestamp: 1563926252
  timesteps_since_restore: 2164800
  timesteps_this_iter: 26400
  timesteps_total: 2164800
  training_iteration: 82
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 525 s, 82 iter, 2164800 ts, 16.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.26393354758715
  episode_reward_mean: 17.360746721309024
  episode_reward_min: -2.7834546372602826
  episodes_this_iter: 176
  episodes_total: 14608
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4532.175
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8670378923416138
        kl: 0.014809491112828255
        policy_loss: -0.0159018412232399
        total_loss: 0.20518429577350616
        vf_explained_var: 0.9937876462936401
        vf_loss: 0.21923494338989258
    load_time_ms: 0.716
    num_steps_sampled: 2191200
    num_steps_trained: 2158000
    sample_time_ms: 1769.955
    update_time_ms: 4.351
  iterations_since_restore: 83
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8855089263707276
    mean_inference_ms: 1.1129946100012127
    mean_processing_ms: 1.5573814342835006
  time_since_restore: 531.5830218791962
  time_this_iter_s: 5.863080739974976
  time_total_s: 531.5830218791962
  timestamp: 1563926258
  timesteps_since_restore: 2191200
  timesteps_this_iter: 26400
  timesteps_total: 2191200
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 531 s, 83 iter, 2191200 ts, 17.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.00693859101855
  episode_reward_mean: 18.053642346568303
  episode_reward_min: 0.13125159034511757
  episodes_this_iter: 176
  episodes_total: 14784
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4566.511
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8583695888519287
        kl: 0.015243872068822384
        policy_loss: -0.018568353727459908
        total_loss: 0.18318048119544983
        vf_explained_var: 0.9946119785308838
        vf_loss: 0.19984334707260132
    load_time_ms: 0.713
    num_steps_sampled: 2217600
    num_steps_trained: 2184000
    sample_time_ms: 1784.158
    update_time_ms: 4.224
  iterations_since_restore: 84
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.886705090650375
    mean_inference_ms: 1.1144672548180816
    mean_processing_ms: 1.559908702083215
  time_since_restore: 538.2355592250824
  time_this_iter_s: 6.6525373458862305
  time_total_s: 538.2355592250824
  timestamp: 1563926265
  timesteps_since_restore: 2217600
  timesteps_this_iter: 26400
  timesteps_total: 2217600
  training_iteration: 84
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 538 s, 84 iter, 2217600 ts, 18.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.18311472967869
  episode_reward_mean: 16.834748089923252
  episode_reward_min: -2.8997840425506585
  episodes_this_iter: 176
  episodes_total: 14960
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4424.713
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8404922485351562
        kl: 0.014939744956791401
        policy_loss: -0.017027951776981354
        total_loss: 0.18010470271110535
        vf_explained_var: 0.994225800037384
        vf_loss: 0.19526518881320953
    load_time_ms: 0.715
    num_steps_sampled: 2244000
    num_steps_trained: 2210000
    sample_time_ms: 1783.836
    update_time_ms: 4.254
  iterations_since_restore: 85
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8878118916825357
    mean_inference_ms: 1.113162706454593
    mean_processing_ms: 1.5572277551957547
  time_since_restore: 544.1335155963898
  time_this_iter_s: 5.897956371307373
  time_total_s: 544.1335155963898
  timestamp: 1563926271
  timesteps_since_restore: 2244000
  timesteps_this_iter: 26400
  timesteps_total: 2244000
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 544 s, 85 iter, 2244000 ts, 16.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-57-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.45098223362087
  episode_reward_mean: 16.73740354336018
  episode_reward_min: -3.267745940199826
  episodes_this_iter: 176
  episodes_total: 15136
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4414.955
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8312866687774658
        kl: 0.014909271150827408
        policy_loss: -0.017804283648729324
        total_loss: 0.16067229211330414
        vf_explained_var: 0.994678258895874
        vf_loss: 0.17661291360855103
    load_time_ms: 0.72
    num_steps_sampled: 2270400
    num_steps_trained: 2236000
    sample_time_ms: 1782.805
    update_time_ms: 4.071
  iterations_since_restore: 86
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.887177891561312
    mean_inference_ms: 1.113172388755739
    mean_processing_ms: 1.5588606259379725
  time_since_restore: 549.8554782867432
  time_this_iter_s: 5.7219626903533936
  time_total_s: 549.8554782867432
  timestamp: 1563926276
  timesteps_since_restore: 2270400
  timesteps_this_iter: 26400
  timesteps_total: 2270400
  training_iteration: 86
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 549 s, 86 iter, 2270400 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.309714016208616
  episode_reward_mean: 17.750198078066326
  episode_reward_min: -2.3890545598075286
  episodes_this_iter: 176
  episodes_total: 15312
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4492.5
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8175933361053467
        kl: 0.014385748654603958
        policy_loss: -0.017239142209291458
        total_loss: 0.1450437605381012
        vf_explained_var: 0.9952585697174072
        vf_loss: 0.1604847013950348
    load_time_ms: 0.718
    num_steps_sampled: 2296800
    num_steps_trained: 2262000
    sample_time_ms: 1769.643
    update_time_ms: 3.997
  iterations_since_restore: 87
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8851659127446583
    mean_inference_ms: 1.1115308845726435
    mean_processing_ms: 1.5582011275349033
  time_since_restore: 556.3841054439545
  time_this_iter_s: 6.528627157211304
  time_total_s: 556.3841054439545
  timestamp: 1563926283
  timesteps_since_restore: 2296800
  timesteps_this_iter: 26400
  timesteps_total: 2296800
  training_iteration: 87
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 556 s, 87 iter, 2296800 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.81929371697259
  episode_reward_mean: 17.574010244654062
  episode_reward_min: -2.05314769932837
  episodes_this_iter: 176
  episodes_total: 15488
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4351.239
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8063098192214966
        kl: 0.014395464211702347
        policy_loss: -0.01662491261959076
        total_loss: 0.1677684783935547
        vf_explained_var: 0.995215117931366
        vf_loss: 0.18259397149085999
    load_time_ms: 0.724
    num_steps_sampled: 2323200
    num_steps_trained: 2288000
    sample_time_ms: 1781.816
    update_time_ms: 4.039
  iterations_since_restore: 88
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.88328766846248
    mean_inference_ms: 1.110275263083019
    mean_processing_ms: 1.5546158404015897
  time_since_restore: 562.3138103485107
  time_this_iter_s: 5.929704904556274
  time_total_s: 562.3138103485107
  timestamp: 1563926289
  timesteps_since_restore: 2323200
  timesteps_this_iter: 26400
  timesteps_total: 2323200
  training_iteration: 88
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 562 s, 88 iter, 2323200 ts, 17.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.78513678185427
  episode_reward_mean: 16.39125474783425
  episode_reward_min: -1.5055035018861944
  episodes_this_iter: 176
  episodes_total: 15664
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4433.374
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7980417013168335
        kl: 0.01466856524348259
        policy_loss: -0.014675215817987919
        total_loss: 0.16821743547916412
        vf_explained_var: 0.9940751791000366
        vf_loss: 0.1810591220855713
    load_time_ms: 0.732
    num_steps_sampled: 2349600
    num_steps_trained: 2314000
    sample_time_ms: 1775.758
    update_time_ms: 3.924
  iterations_since_restore: 89
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8901120376527205
    mean_inference_ms: 1.1147552403618595
    mean_processing_ms: 1.5611007178877878
  time_since_restore: 569.3821976184845
  time_this_iter_s: 7.068387269973755
  time_total_s: 569.3821976184845
  timestamp: 1563926296
  timesteps_since_restore: 2349600
  timesteps_this_iter: 26400
  timesteps_total: 2349600
  training_iteration: 89
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 569 s, 89 iter, 2349600 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.7966432488447
  episode_reward_mean: 18.03798746567889
  episode_reward_min: -3.642011724859237
  episodes_this_iter: 176
  episodes_total: 15840
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4308.231
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7815208435058594
        kl: 0.013832557946443558
        policy_loss: -0.016914548352360725
        total_loss: 0.15018658339977264
        vf_explained_var: 0.9955089092254639
        vf_loss: 0.1653720587491989
    load_time_ms: 0.728
    num_steps_sampled: 2376000
    num_steps_trained: 2340000
    sample_time_ms: 1786.397
    update_time_ms: 3.937
  iterations_since_restore: 90
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8838267094959145
    mean_inference_ms: 1.1112870958704013
    mean_processing_ms: 1.555272767244864
  time_since_restore: 575.3161776065826
  time_this_iter_s: 5.9339799880981445
  time_total_s: 575.3161776065826
  timestamp: 1563926302
  timesteps_since_restore: 2376000
  timesteps_this_iter: 26400
  timesteps_total: 2376000
  training_iteration: 90
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 575 s, 90 iter, 2376000 ts, 18 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.699280311627156
  episode_reward_mean: 18.364145901069126
  episode_reward_min: -3.869123282400077
  episodes_this_iter: 176
  episodes_total: 16016
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4308.727
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7689505815505981
        kl: 0.015578720718622208
        policy_loss: -0.017613830044865608
        total_loss: 0.13977693021297455
        vf_explained_var: 0.995897650718689
        vf_loss: 0.15544341504573822
    load_time_ms: 0.727
    num_steps_sampled: 2402400
    num_steps_trained: 2366000
    sample_time_ms: 1780.706
    update_time_ms: 3.902
  iterations_since_restore: 91
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8887688278295114
    mean_inference_ms: 1.1141595557362634
    mean_processing_ms: 1.5615219105944682
  time_since_restore: 580.9924325942993
  time_this_iter_s: 5.676254987716675
  time_total_s: 580.9924325942993
  timestamp: 1563926308
  timesteps_since_restore: 2402400
  timesteps_this_iter: 26400
  timesteps_total: 2402400
  training_iteration: 91
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 580 s, 91 iter, 2402400 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.1857364101875
  episode_reward_mean: 17.88164231071625
  episode_reward_min: -3.1978741354734552
  episodes_this_iter: 176
  episodes_total: 16192
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4448.024
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7505618333816528
        kl: 0.015435311943292618
        policy_loss: -0.016937894746661186
        total_loss: 0.13803938031196594
        vf_explained_var: 0.9959201812744141
        vf_loss: 0.1530478596687317
    load_time_ms: 0.729
    num_steps_sampled: 2428800
    num_steps_trained: 2392000
    sample_time_ms: 1788.961
    update_time_ms: 3.895
  iterations_since_restore: 92
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8842651271130313
    mean_inference_ms: 1.1109172467044084
    mean_processing_ms: 1.5560687024496263
  time_since_restore: 588.316656589508
  time_this_iter_s: 7.32422399520874
  time_total_s: 588.316656589508
  timestamp: 1563926315
  timesteps_since_restore: 2428800
  timesteps_this_iter: 26400
  timesteps_total: 2428800
  training_iteration: 92
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 588 s, 92 iter, 2428800 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.885939590874294
  episode_reward_mean: 17.52315574984072
  episode_reward_min: -2.343525752471416
  episodes_this_iter: 176
  episodes_total: 16368
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4446.127
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7376703023910522
        kl: 0.015331311151385307
        policy_loss: -0.020726673305034637
        total_loss: 0.18928852677345276
        vf_explained_var: 0.9938802123069763
        vf_loss: 0.20809879899024963
    load_time_ms: 0.73
    num_steps_sampled: 2455200
    num_steps_trained: 2418000
    sample_time_ms: 1773.65
    update_time_ms: 3.998
  iterations_since_restore: 93
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8910301764656234
    mean_inference_ms: 1.1148009231115592
    mean_processing_ms: 1.5609316675789173
  time_since_restore: 594.008974313736
  time_this_iter_s: 5.692317724227905
  time_total_s: 594.008974313736
  timestamp: 1563926321
  timesteps_since_restore: 2455200
  timesteps_this_iter: 26400
  timesteps_total: 2455200
  training_iteration: 93
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 594 s, 93 iter, 2455200 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.063164002183356
  episode_reward_mean: 16.770100784096073
  episode_reward_min: -2.6699671877661655
  episodes_this_iter: 176
  episodes_total: 16544
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4354.142
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7181305885314941
        kl: 0.017183084040880203
        policy_loss: -0.01991422101855278
        total_loss: 0.12427009642124176
        vf_explained_var: 0.9956165552139282
        vf_loss: 0.14203642308712006
    load_time_ms: 0.725
    num_steps_sampled: 2481600
    num_steps_trained: 2444000
    sample_time_ms: 1818.629
    update_time_ms: 3.958
  iterations_since_restore: 94
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8855359093856585
    mean_inference_ms: 1.1110052522607208
    mean_processing_ms: 1.5570673818836311
  time_since_restore: 600.1876256465912
  time_this_iter_s: 6.178651332855225
  time_total_s: 600.1876256465912
  timestamp: 1563926327
  timesteps_since_restore: 2481600
  timesteps_this_iter: 26400
  timesteps_total: 2481600
  training_iteration: 94
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 600 s, 94 iter, 2481600 ts, 16.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.00967861796759
  episode_reward_mean: 15.720239045954992
  episode_reward_min: -2.8299918986623127
  episodes_this_iter: 176
  episodes_total: 16720
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4352.555
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.705276370048523
        kl: 0.016780756413936615
        policy_loss: -0.01956462487578392
        total_loss: 0.11331412196159363
        vf_explained_var: 0.9953828454017639
        vf_loss: 0.1307811439037323
    load_time_ms: 0.735
    num_steps_sampled: 2508000
    num_steps_trained: 2470000
    sample_time_ms: 1800.622
    update_time_ms: 3.858
  iterations_since_restore: 95
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8907820134100772
    mean_inference_ms: 1.1159958490709552
    mean_processing_ms: 1.5628246181454521
  time_since_restore: 605.8879752159119
  time_this_iter_s: 5.700349569320679
  time_total_s: 605.8879752159119
  timestamp: 1563926333
  timesteps_since_restore: 2508000
  timesteps_this_iter: 26400
  timesteps_total: 2508000
  training_iteration: 95
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 605 s, 95 iter, 2508000 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-58-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.265281023184244
  episode_reward_mean: 18.885873671575993
  episode_reward_min: -1.5542027234331288
  episodes_this_iter: 176
  episodes_total: 16896
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4362.979
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6895897388458252
        kl: 0.016438283026218414
        policy_loss: -0.016549596562981606
        total_loss: 0.12235552817583084
        vf_explained_var: 0.996303915977478
        vf_loss: 0.13685034215450287
    load_time_ms: 0.724
    num_steps_sampled: 2534400
    num_steps_trained: 2496000
    sample_time_ms: 1820.742
    update_time_ms: 3.849
  iterations_since_restore: 96
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8889679196642855
    mean_inference_ms: 1.1127350481908531
    mean_processing_ms: 1.5599645842811896
  time_since_restore: 611.9149398803711
  time_this_iter_s: 6.0269646644592285
  time_total_s: 611.9149398803711
  timestamp: 1563926339
  timesteps_since_restore: 2534400
  timesteps_this_iter: 26400
  timesteps_total: 2534400
  training_iteration: 96
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 611 s, 96 iter, 2534400 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.62399390356063
  episode_reward_mean: 16.857401288817798
  episode_reward_min: -7.257832402388379
  episodes_this_iter: 176
  episodes_total: 17072
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4344.967
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6783727407455444
        kl: 0.01556221954524517
        policy_loss: -0.015983158722519875
        total_loss: 0.13835549354553223
        vf_explained_var: 0.995166540145874
        vf_loss: 0.1523933708667755
    load_time_ms: 0.723
    num_steps_sampled: 2560800
    num_steps_trained: 2522000
    sample_time_ms: 1803.301
    update_time_ms: 3.884
  iterations_since_restore: 97
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8874122869526744
    mean_inference_ms: 1.1130317193639163
    mean_processing_ms: 1.5600585282318498
  time_since_restore: 618.0893771648407
  time_this_iter_s: 6.1744372844696045
  time_total_s: 618.0893771648407
  timestamp: 1563926345
  timesteps_since_restore: 2560800
  timesteps_this_iter: 26400
  timesteps_total: 2560800
  training_iteration: 97
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 618 s, 97 iter, 2560800 ts, 16.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.70209354907285
  episode_reward_mean: 18.255987715520995
  episode_reward_min: -4.193276618569011
  episodes_this_iter: 176
  episodes_total: 17248
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4388.454
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6638528108596802
        kl: 0.015445804223418236
        policy_loss: -0.017354417592287064
        total_loss: 0.11737316101789474
        vf_explained_var: 0.996330201625824
        vf_loss: 0.13279685378074646
    load_time_ms: 0.724
    num_steps_sampled: 2587200
    num_steps_trained: 2548000
    sample_time_ms: 1802.923
    update_time_ms: 3.87
  iterations_since_restore: 98
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8905917798874434
    mean_inference_ms: 1.115032233611522
    mean_processing_ms: 1.5626537180293005
  time_since_restore: 624.4512324333191
  time_this_iter_s: 6.3618552684783936
  time_total_s: 624.4512324333191
  timestamp: 1563926351
  timesteps_since_restore: 2587200
  timesteps_this_iter: 26400
  timesteps_total: 2587200
  training_iteration: 98
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 624 s, 98 iter, 2587200 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.26333031127501
  episode_reward_mean: 18.20200828125214
  episode_reward_min: -2.5255818479855505
  episodes_this_iter: 176
  episodes_total: 17424
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4245.32
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6467665433883667
        kl: 0.01684209518134594
        policy_loss: -0.020313937216997147
        total_loss: 0.08944177627563477
        vf_explained_var: 0.9969946146011353
        vf_loss: 0.10765045136213303
    load_time_ms: 0.719
    num_steps_sampled: 2613600
    num_steps_trained: 2574000
    sample_time_ms: 1811.144
    update_time_ms: 3.915
  iterations_since_restore: 99
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8884137188445842
    mean_inference_ms: 1.1142727779162234
    mean_processing_ms: 1.561989842282133
  time_since_restore: 630.1656756401062
  time_this_iter_s: 5.714443206787109
  time_total_s: 630.1656756401062
  timestamp: 1563926357
  timesteps_since_restore: 2613600
  timesteps_this_iter: 26400
  timesteps_total: 2613600
  training_iteration: 99
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 630 s, 99 iter, 2613600 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.661659043366434
  episode_reward_mean: 17.258808998033285
  episode_reward_min: -3.408139283634689
  episodes_this_iter: 176
  episodes_total: 17600
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4301.816
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6339110136032104
        kl: 0.015777623280882835
        policy_loss: -0.019868183881044388
        total_loss: 0.08627431839704514
        vf_explained_var: 0.9968335032463074
        vf_loss: 0.1041703075170517
    load_time_ms: 0.727
    num_steps_sampled: 2640000
    num_steps_trained: 2600000
    sample_time_ms: 1803.815
    update_time_ms: 3.783
  iterations_since_restore: 100
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8899708955835444
    mean_inference_ms: 1.1151216588064472
    mean_processing_ms: 1.5617115181312824
  time_since_restore: 636.591082572937
  time_this_iter_s: 6.4254069328308105
  time_total_s: 636.591082572937
  timestamp: 1563926363
  timesteps_since_restore: 2640000
  timesteps_this_iter: 26400
  timesteps_total: 2640000
  training_iteration: 100
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 636 s, 100 iter, 2640000 ts, 17.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.495582914731514
  episode_reward_mean: 16.94171087981874
  episode_reward_min: -10.053337965269826
  episodes_this_iter: 176
  episodes_total: 17776
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4298.369
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.623565912246704
        kl: 0.016413230448961258
        policy_loss: -0.016339417546987534
        total_loss: 0.1147250235080719
        vf_explained_var: 0.9962002038955688
        vf_loss: 0.129012793302536
    load_time_ms: 0.73
    num_steps_sampled: 2666400
    num_steps_trained: 2626000
    sample_time_ms: 1812.811
    update_time_ms: 3.822
  iterations_since_restore: 101
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8871068364417551
    mean_inference_ms: 1.1119018028969743
    mean_processing_ms: 1.557670074516553
  time_since_restore: 642.323787689209
  time_this_iter_s: 5.732705116271973
  time_total_s: 642.323787689209
  timestamp: 1563926369
  timesteps_since_restore: 2666400
  timesteps_this_iter: 26400
  timesteps_total: 2666400
  training_iteration: 101
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 642 s, 101 iter, 2666400 ts, 16.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.70369666369581
  episode_reward_mean: 17.57535728738683
  episode_reward_min: -0.9782604415640914
  episodes_this_iter: 176
  episodes_total: 17952
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4250.762
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6018180847167969
        kl: 0.015790438279509544
        policy_loss: -0.02046639658510685
        total_loss: 0.07954919338226318
        vf_explained_var: 0.9970963001251221
        vf_loss: 0.09804177284240723
    load_time_ms: 0.731
    num_steps_sampled: 2692800
    num_steps_trained: 2652000
    sample_time_ms: 1802.096
    update_time_ms: 3.807
  iterations_since_restore: 102
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8902884390202408
    mean_inference_ms: 1.114909811922096
    mean_processing_ms: 1.5620934679119378
  time_since_restore: 649.0619342327118
  time_this_iter_s: 6.738146543502808
  time_total_s: 649.0619342327118
  timestamp: 1563926376
  timesteps_since_restore: 2692800
  timesteps_this_iter: 26400
  timesteps_total: 2692800
  training_iteration: 102
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 649 s, 102 iter, 2692800 ts, 17.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.99942158948871
  episode_reward_mean: 16.606315972470025
  episode_reward_min: -3.2090033038521066
  episodes_this_iter: 176
  episodes_total: 18128
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4380.326
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5887812376022339
        kl: 0.017228150740265846
        policy_loss: -0.018866313621401787
        total_loss: 0.08153624087572098
        vf_explained_var: 0.9970186948776245
        vf_loss: 0.09824903309345245
    load_time_ms: 0.733
    num_steps_sampled: 2719200
    num_steps_trained: 2678000
    sample_time_ms: 1811.308
    update_time_ms: 3.727
  iterations_since_restore: 103
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8923972474095407
    mean_inference_ms: 1.1158976214331353
    mean_processing_ms: 1.5637970735461748
  time_since_restore: 656.1430943012238
  time_this_iter_s: 7.081160068511963
  time_total_s: 656.1430943012238
  timestamp: 1563926383
  timesteps_since_restore: 2719200
  timesteps_this_iter: 26400
  timesteps_total: 2719200
  training_iteration: 103
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 656 s, 103 iter, 2719200 ts, 16.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.94391331058899
  episode_reward_mean: 19.751670267684514
  episode_reward_min: 0.09749073807698237
  episodes_this_iter: 176
  episodes_total: 18304
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4522.168
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.565680980682373
        kl: 0.018113113939762115
        policy_loss: -0.020041147246956825
        total_loss: 0.06496764719486237
        vf_explained_var: 0.9980016350746155
        vf_loss: 0.08274465054273605
    load_time_ms: 0.733
    num_steps_sampled: 2745600
    num_steps_trained: 2704000
    sample_time_ms: 1774.033
    update_time_ms: 3.815
  iterations_since_restore: 104
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8886144542861338
    mean_inference_ms: 1.1148965591962936
    mean_processing_ms: 1.5613936547649745
  time_since_restore: 663.3680901527405
  time_this_iter_s: 7.224995851516724
  time_total_s: 663.3680901527405
  timestamp: 1563926390
  timesteps_since_restore: 2745600
  timesteps_this_iter: 26400
  timesteps_total: 2745600
  training_iteration: 104
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 663 s, 104 iter, 2745600 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_01-59-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.58412564543037
  episode_reward_mean: 17.480160747714702
  episode_reward_min: -6.44978569977199
  episodes_this_iter: 176
  episodes_total: 18480
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4523.721
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5494112968444824
        kl: 0.015824750065803528
        policy_loss: -0.01805034838616848
        total_loss: 0.07276301831007004
        vf_explained_var: 0.9972403049468994
        vf_loss: 0.08883526921272278
    load_time_ms: 0.717
    num_steps_sampled: 2772000
    num_steps_trained: 2730000
    sample_time_ms: 1778.486
    update_time_ms: 3.908
  iterations_since_restore: 105
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8859625727907117
    mean_inference_ms: 1.1134377984504122
    mean_processing_ms: 1.558957483336404
  time_since_restore: 669.1296188831329
  time_this_iter_s: 5.761528730392456
  time_total_s: 669.1296188831329
  timestamp: 1563926396
  timesteps_since_restore: 2772000
  timesteps_this_iter: 26400
  timesteps_total: 2772000
  training_iteration: 105
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 669 s, 105 iter, 2772000 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.35958737071948
  episode_reward_mean: 18.980734953213993
  episode_reward_min: -2.605708612912126
  episodes_this_iter: 176
  episodes_total: 18656
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4507.408
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.533044695854187
        kl: 0.016681354492902756
        policy_loss: -0.021633241325616837
        total_loss: 0.057331763207912445
        vf_explained_var: 0.9979985356330872
        vf_loss: 0.0768798291683197
    load_time_ms: 0.731
    num_steps_sampled: 2798400
    num_steps_trained: 2756000
    sample_time_ms: 1760.604
    update_time_ms: 3.942
  iterations_since_restore: 106
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8892369640429794
    mean_inference_ms: 1.114513708864856
    mean_processing_ms: 1.5640579245335295
  time_since_restore: 674.8141975402832
  time_this_iter_s: 5.6845786571502686
  time_total_s: 674.8141975402832
  timestamp: 1563926402
  timesteps_since_restore: 2798400
  timesteps_this_iter: 26400
  timesteps_total: 2798400
  training_iteration: 106
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 674 s, 106 iter, 2798400 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.247506713543274
  episode_reward_mean: 18.759066691742895
  episode_reward_min: -2.2509185119210877
  episodes_this_iter: 176
  episodes_total: 18832
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4567.979
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5169970989227295
        kl: 0.019745836034417152
        policy_loss: -0.01692976802587509
        total_loss: 0.0718885213136673
        vf_explained_var: 0.9977350831031799
        vf_loss: 0.08635005354881287
    load_time_ms: 0.732
    num_steps_sampled: 2824800
    num_steps_trained: 2782000
    sample_time_ms: 1792.958
    update_time_ms: 3.961
  iterations_since_restore: 107
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8872118246877592
    mean_inference_ms: 1.11198451124739
    mean_processing_ms: 1.5586994990227618
  time_since_restore: 681.9203515052795
  time_this_iter_s: 7.106153964996338
  time_total_s: 681.9203515052795
  timestamp: 1563926409
  timesteps_since_restore: 2824800
  timesteps_this_iter: 26400
  timesteps_total: 2824800
  training_iteration: 107
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 681 s, 107 iter, 2824800 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.3208869426327
  episode_reward_mean: 19.243009708508826
  episode_reward_min: -1.6420980541771182
  episodes_this_iter: 176
  episodes_total: 19008
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4634.419
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.510181188583374
        kl: 0.016673104837536812
        policy_loss: -0.015277588739991188
        total_loss: 0.10142212361097336
        vf_explained_var: 0.9971411824226379
        vf_loss: 0.11461557447910309
    load_time_ms: 0.731
    num_steps_sampled: 2851200
    num_steps_trained: 2808000
    sample_time_ms: 1770.258
    update_time_ms: 4.022
  iterations_since_restore: 108
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8898368970355308
    mean_inference_ms: 1.1146271775403394
    mean_processing_ms: 1.5613374737695893
  time_since_restore: 688.7206709384918
  time_this_iter_s: 6.80031943321228
  time_total_s: 688.7206709384918
  timestamp: 1563926416
  timesteps_since_restore: 2851200
  timesteps_this_iter: 26400
  timesteps_total: 2851200
  training_iteration: 108
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 688 s, 108 iter, 2851200 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.10048784000978
  episode_reward_mean: 17.51794011196995
  episode_reward_min: -0.44609473898301566
  episodes_this_iter: 176
  episodes_total: 19184
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4731.823
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4879697561264038
        kl: 0.018075324594974518
        policy_loss: -0.022145237773656845
        total_loss: 0.054622773081064224
        vf_explained_var: 0.9977718591690063
        vf_loss: 0.07450859248638153
    load_time_ms: 0.736
    num_steps_sampled: 2877600
    num_steps_trained: 2834000
    sample_time_ms: 1784.709
    update_time_ms: 4.034
  iterations_since_restore: 109
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890612638367371
    mean_inference_ms: 1.115027660159669
    mean_processing_ms: 1.5630793421259797
  time_since_restore: 695.5563316345215
  time_this_iter_s: 6.835660696029663
  time_total_s: 695.5563316345215
  timestamp: 1563926422
  timesteps_since_restore: 2877600
  timesteps_this_iter: 26400
  timesteps_total: 2877600
  training_iteration: 109
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 695 s, 109 iter, 2877600 ts, 17.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.25549992816329
  episode_reward_mean: 17.886918388380813
  episode_reward_min: -3.8984889377790397
  episodes_this_iter: 176
  episodes_total: 19360
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4735.175
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4761039018630981
        kl: 0.01745269075036049
        policy_loss: -0.01830190233886242
        total_loss: 0.05249807983636856
        vf_explained_var: 0.9980127811431885
        vf_loss: 0.06861840188503265
    load_time_ms: 0.734
    num_steps_sampled: 2904000
    num_steps_trained: 2860000
    sample_time_ms: 1770.608
    update_time_ms: 4.161
  iterations_since_restore: 110
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8899005344560142
    mean_inference_ms: 1.1138647438344114
    mean_processing_ms: 1.5622979327602886
  time_since_restore: 701.8774318695068
  time_this_iter_s: 6.321100234985352
  time_total_s: 701.8774318695068
  timestamp: 1563926429
  timesteps_since_restore: 2904000
  timesteps_this_iter: 26400
  timesteps_total: 2904000
  training_iteration: 110
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 701 s, 110 iter, 2904000 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.98363489221741
  episode_reward_mean: 19.712789727183722
  episode_reward_min: -8.304057958838841
  episodes_this_iter: 176
  episodes_total: 19536
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4738.344
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4642302989959717
        kl: 0.017038656398653984
        policy_loss: -0.016980335116386414
        total_loss: 0.06533817946910858
        vf_explained_var: 0.9979724287986755
        vf_loss: 0.08018868416547775
    load_time_ms: 0.73
    num_steps_sampled: 2930400
    num_steps_trained: 2886000
    sample_time_ms: 1783.622
    update_time_ms: 4.133
  iterations_since_restore: 111
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889853746665918
    mean_inference_ms: 1.1152212586843169
    mean_processing_ms: 1.5642046444687894
  time_since_restore: 707.7712819576263
  time_this_iter_s: 5.893850088119507
  time_total_s: 707.7712819576263
  timestamp: 1563926435
  timesteps_since_restore: 2930400
  timesteps_this_iter: 26400
  timesteps_total: 2930400
  training_iteration: 111
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 707 s, 111 iter, 2930400 ts, 19.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.85289688631032
  episode_reward_mean: 18.490984695978984
  episode_reward_min: -1.7551651684527365
  episodes_this_iter: 176
  episodes_total: 19712
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4779.78
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4488755464553833
        kl: 0.018842056393623352
        policy_loss: -0.0155341736972332
        total_loss: 0.06536023318767548
        vf_explained_var: 0.9977506995201111
        vf_loss: 0.07853914797306061
    load_time_ms: 0.725
    num_steps_sampled: 2956800
    num_steps_trained: 2912000
    sample_time_ms: 1770.756
    update_time_ms: 4.213
  iterations_since_restore: 112
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8896714554041016
    mean_inference_ms: 1.1133959489988738
    mean_processing_ms: 1.5625041417075254
  time_since_restore: 714.7989499568939
  time_this_iter_s: 7.027667999267578
  time_total_s: 714.7989499568939
  timestamp: 1563926442
  timesteps_since_restore: 2956800
  timesteps_this_iter: 26400
  timesteps_total: 2956800
  training_iteration: 112
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 714 s, 112 iter, 2956800 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.49401473119207
  episode_reward_mean: 17.18766632691614
  episode_reward_min: -1.5497305112200868
  episodes_this_iter: 176
  episodes_total: 19888
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4792.775
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.437347412109375
        kl: 0.015917906537652016
        policy_loss: -0.020875124260783195
        total_loss: 0.07365215569734573
        vf_explained_var: 0.9971871972084045
        vf_loss: 0.0925375372171402
    load_time_ms: 0.725
    num_steps_sampled: 2983200
    num_steps_trained: 2938000
    sample_time_ms: 1779.568
    update_time_ms: 4.326
  iterations_since_restore: 113
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8921501469713762
    mean_inference_ms: 1.1152512902180935
    mean_processing_ms: 1.5639872027381807
  time_since_restore: 722.1013708114624
  time_this_iter_s: 7.3024208545684814
  time_total_s: 722.1013708114624
  timestamp: 1563926449
  timesteps_since_restore: 2983200
  timesteps_this_iter: 26400
  timesteps_total: 2983200
  training_iteration: 113
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 722 s, 113 iter, 2983200 ts, 17.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-00-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.76359033427792
  episode_reward_mean: 18.906539325302045
  episode_reward_min: -1.2887279873938267
  episodes_this_iter: 176
  episodes_total: 20064
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4722.258
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4215881824493408
        kl: 0.018241479992866516
        policy_loss: -0.018483983352780342
        total_loss: 0.07573254406452179
        vf_explained_var: 0.9976614713668823
        vf_loss: 0.09193634241819382
    load_time_ms: 0.735
    num_steps_sampled: 3009600
    num_steps_trained: 2964000
    sample_time_ms: 1771.281
    update_time_ms: 4.438
  iterations_since_restore: 114
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8929426346299147
    mean_inference_ms: 1.1148847062462972
    mean_processing_ms: 1.5641940982881886
  time_since_restore: 728.5415418148041
  time_this_iter_s: 6.440171003341675
  time_total_s: 728.5415418148041
  timestamp: 1563926455
  timesteps_since_restore: 3009600
  timesteps_this_iter: 26400
  timesteps_total: 3009600
  training_iteration: 114
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 728 s, 114 iter, 3009600 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.774719172322605
  episode_reward_mean: 18.455705218267234
  episode_reward_min: 0.08760321934746122
  episodes_this_iter: 176
  episodes_total: 20240
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4783.057
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4080411195755005
        kl: 0.01707100309431553
        policy_loss: -0.019148046150803566
        total_loss: 0.04339807853102684
        vf_explained_var: 0.9982739090919495
        vf_loss: 0.06041225045919418
    load_time_ms: 0.739
    num_steps_sampled: 3036000
    num_steps_trained: 2990000
    sample_time_ms: 1772.377
    update_time_ms: 4.46
  iterations_since_restore: 115
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8892577077840707
    mean_inference_ms: 1.113065252490836
    mean_processing_ms: 1.562334436586974
  time_since_restore: 734.9240465164185
  time_this_iter_s: 6.38250470161438
  time_total_s: 734.9240465164185
  timestamp: 1563926462
  timesteps_since_restore: 3036000
  timesteps_this_iter: 26400
  timesteps_total: 3036000
  training_iteration: 115
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 734 s, 115 iter, 3036000 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.71824427145631
  episode_reward_mean: 18.936508527893583
  episode_reward_min: -1.8789199309756859
  episodes_this_iter: 176
  episodes_total: 20416
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4928.051
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4031411409378052
        kl: 0.01471027359366417
        policy_loss: -0.01704973727464676
        total_loss: 0.06506801396608353
        vf_explained_var: 0.9977652430534363
        vf_loss: 0.08027896285057068
    load_time_ms: 0.728
    num_steps_sampled: 3062400
    num_steps_trained: 3016000
    sample_time_ms: 1777.486
    update_time_ms: 4.497
  iterations_since_restore: 116
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8884516659572042
    mean_inference_ms: 1.1144609032359847
    mean_processing_ms: 1.5638415685227154
  time_since_restore: 742.1137039661407
  time_this_iter_s: 7.18965744972229
  time_total_s: 742.1137039661407
  timestamp: 1563926469
  timesteps_since_restore: 3062400
  timesteps_this_iter: 26400
  timesteps_total: 3062400
  training_iteration: 116
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 742 s, 116 iter, 3062400 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.89883201534022
  episode_reward_mean: 19.367600112728947
  episode_reward_min: -0.7858314613222709
  episodes_this_iter: 176
  episodes_total: 20592
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4868.548
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3892544507980347
        kl: 0.016337547451257706
        policy_loss: -0.01667414978146553
        total_loss: 0.05548439919948578
        vf_explained_var: 0.9982502460479736
        vf_loss: 0.07011635601520538
    load_time_ms: 0.722
    num_steps_sampled: 3088800
    num_steps_trained: 3042000
    sample_time_ms: 1769.959
    update_time_ms: 4.614
  iterations_since_restore: 117
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8910867856227844
    mean_inference_ms: 1.113755492548673
    mean_processing_ms: 1.564395990799856
  time_since_restore: 748.5480573177338
  time_this_iter_s: 6.434353351593018
  time_total_s: 748.5480573177338
  timestamp: 1563926476
  timesteps_since_restore: 3088800
  timesteps_this_iter: 26400
  timesteps_total: 3088800
  training_iteration: 117
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 748 s, 117 iter, 3088800 ts, 19.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.6295849047398
  episode_reward_mean: 18.531862046930602
  episode_reward_min: 0.7288057494110787
  episodes_this_iter: 176
  episodes_total: 20768
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4779.632
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3752741813659668
        kl: 0.017350850626826286
        policy_loss: -0.018795469775795937
        total_loss: 0.03903663903474808
        vf_explained_var: 0.9984433054924011
        vf_loss: 0.05566326156258583
    load_time_ms: 0.736
    num_steps_sampled: 3115200
    num_steps_trained: 3068000
    sample_time_ms: 1777.21
    update_time_ms: 4.556
  iterations_since_restore: 118
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8875363701613694
    mean_inference_ms: 1.1125460284148452
    mean_processing_ms: 1.5611303091308941
  time_since_restore: 754.5305080413818
  time_this_iter_s: 5.982450723648071
  time_total_s: 754.5305080413818
  timestamp: 1563926482
  timesteps_since_restore: 3115200
  timesteps_this_iter: 26400
  timesteps_total: 3115200
  training_iteration: 118
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 754 s, 118 iter, 3115200 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.90393714608541
  episode_reward_mean: 19.250131785346387
  episode_reward_min: -0.5098903945752733
  episodes_this_iter: 176
  episodes_total: 20944
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4699.424
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3627655506134033
        kl: 0.01802866905927658
        policy_loss: -0.023510010913014412
        total_loss: 0.03309338167309761
        vf_explained_var: 0.9985681772232056
        vf_loss: 0.05434981361031532
    load_time_ms: 0.73
    num_steps_sampled: 3141600
    num_steps_trained: 3094000
    sample_time_ms: 1772.504
    update_time_ms: 4.536
  iterations_since_restore: 119
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8947358516042798
    mean_inference_ms: 1.1164315675505505
    mean_processing_ms: 1.5658067139714853
  time_since_restore: 760.5146996974945
  time_this_iter_s: 5.984191656112671
  time_total_s: 760.5146996974945
  timestamp: 1563926488
  timesteps_since_restore: 3141600
  timesteps_this_iter: 26400
  timesteps_total: 3141600
  training_iteration: 119
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 760 s, 119 iter, 3141600 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.35751655766935
  episode_reward_mean: 19.575053997751194
  episode_reward_min: 0.1450053423698606
  episodes_this_iter: 176
  episodes_total: 21120
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4639.449
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3597023487091064
        kl: 0.016061490401625633
        policy_loss: -0.012640214525163174
        total_loss: 0.09958796948194504
        vf_explained_var: 0.9971367120742798
        vf_loss: 0.11022049933671951
    load_time_ms: 0.732
    num_steps_sampled: 3168000
    num_steps_trained: 3120000
    sample_time_ms: 1771.025
    update_time_ms: 4.46
  iterations_since_restore: 120
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8899718462636763
    mean_inference_ms: 1.1124619235763336
    mean_processing_ms: 1.5614666266205177
  time_since_restore: 766.2174098491669
  time_this_iter_s: 5.702710151672363
  time_total_s: 766.2174098491669
  timestamp: 1563926493
  timesteps_since_restore: 3168000
  timesteps_this_iter: 26400
  timesteps_total: 3168000
  training_iteration: 120
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 766 s, 120 iter, 3168000 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.26378104260394
  episode_reward_mean: 18.93368006515184
  episode_reward_min: 0.13273418782019503
  episodes_this_iter: 176
  episodes_total: 21296
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4738.734
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3483161926269531
        kl: 0.01857571490108967
        policy_loss: -0.021323205903172493
        total_loss: 0.03751339018344879
        vf_explained_var: 0.9983407258987427
        vf_loss: 0.05651463568210602
    load_time_ms: 0.735
    num_steps_sampled: 3194400
    num_steps_trained: 3146000
    sample_time_ms: 1773.459
    update_time_ms: 4.491
  iterations_since_restore: 121
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.892165959263892
    mean_inference_ms: 1.1163619369798812
    mean_processing_ms: 1.5660210679674302
  time_since_restore: 773.1306886672974
  time_this_iter_s: 6.913278818130493
  time_total_s: 773.1306886672974
  timestamp: 1563926500
  timesteps_since_restore: 3194400
  timesteps_this_iter: 26400
  timesteps_total: 3194400
  training_iteration: 121
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 773 s, 121 iter, 3194400 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.31352000084116
  episode_reward_mean: 18.880668637721357
  episode_reward_min: 0.4135475205716993
  episodes_this_iter: 176
  episodes_total: 21472
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4746.471
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3366897106170654
        kl: 0.018466029316186905
        policy_loss: -0.020372046157717705
        total_loss: 0.031473081558942795
        vf_explained_var: 0.9986801743507385
        vf_loss: 0.04953687638044357
    load_time_ms: 0.739
    num_steps_sampled: 3220800
    num_steps_trained: 3172000
    sample_time_ms: 1783.34
    update_time_ms: 4.575
  iterations_since_restore: 122
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8881303482199598
    mean_inference_ms: 1.112048384335117
    mean_processing_ms: 1.5612636098203614
  time_since_restore: 780.3337986469269
  time_this_iter_s: 7.203109979629517
  time_total_s: 780.3337986469269
  timestamp: 1563926507
  timesteps_since_restore: 3220800
  timesteps_this_iter: 26400
  timesteps_total: 3220800
  training_iteration: 122
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 780 s, 122 iter, 3220800 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.720105114414146
  episode_reward_mean: 17.927634348104935
  episode_reward_min: -1.3208954782162081
  episodes_this_iter: 176
  episodes_total: 21648
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4605.983
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3238061666488647
        kl: 0.01951725222170353
        policy_loss: -0.02092261053621769
        total_loss: 0.03261461481451988
        vf_explained_var: 0.9985863566398621
        vf_loss: 0.0510975643992424
    load_time_ms: 0.737
    num_steps_sampled: 3247200
    num_steps_trained: 3198000
    sample_time_ms: 1767.571
    update_time_ms: 4.582
  iterations_since_restore: 123
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8901641828136249
    mean_inference_ms: 1.1125269712928392
    mean_processing_ms: 1.5602833739367659
  time_since_restore: 786.0696315765381
  time_this_iter_s: 5.735832929611206
  time_total_s: 786.0696315765381
  timestamp: 1563926513
  timesteps_since_restore: 3247200
  timesteps_this_iter: 26400
  timesteps_total: 3247200
  training_iteration: 123
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 786 s, 123 iter, 3247200 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-01-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.21486496920275
  episode_reward_mean: 18.399042911007612
  episode_reward_min: -1.569055617455034
  episodes_this_iter: 176
  episodes_total: 21824
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4565.556
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3111159801483154
        kl: 0.020222855731844902
        policy_loss: -0.020784446969628334
        total_loss: 0.026264896616339684
        vf_explained_var: 0.9987040758132935
        vf_loss: 0.04452149197459221
    load_time_ms: 0.732
    num_steps_sampled: 3273600
    num_steps_trained: 3224000
    sample_time_ms: 1780.151
    update_time_ms: 4.444
  iterations_since_restore: 124
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8917293705685128
    mean_inference_ms: 1.1149709598790074
    mean_processing_ms: 1.5642462141329323
  time_since_restore: 792.2287940979004
  time_this_iter_s: 6.159162521362305
  time_total_s: 792.2287940979004
  timestamp: 1563926519
  timesteps_since_restore: 3273600
  timesteps_this_iter: 26400
  timesteps_total: 3273600
  training_iteration: 124
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 792 s, 124 iter, 3273600 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.806079668683815
  episode_reward_mean: 18.45179228671
  episode_reward_min: -0.15521145065609318
  episodes_this_iter: 176
  episodes_total: 22000
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4583.307
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.3028373718261719
        kl: 0.016802344471216202
        policy_loss: -0.02021397277712822
        total_loss: 0.027457542717456818
        vf_explained_var: 0.998806357383728
        vf_loss: 0.04452107101678848
    load_time_ms: 0.729
    num_steps_sampled: 3300000
    num_steps_trained: 3250000
    sample_time_ms: 1779.126
    update_time_ms: 4.446
  iterations_since_restore: 125
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8881819957779404
    mean_inference_ms: 1.1122228163650105
    mean_processing_ms: 1.5620749360806767
  time_since_restore: 798.7793970108032
  time_this_iter_s: 6.550602912902832
  time_total_s: 798.7793970108032
  timestamp: 1563926526
  timesteps_since_restore: 3300000
  timesteps_this_iter: 26400
  timesteps_total: 3300000
  training_iteration: 125
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 798 s, 125 iter, 3300000 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.90623606480737
  episode_reward_mean: 18.147317527081483
  episode_reward_min: -2.102125478353736
  episodes_this_iter: 176
  episodes_total: 22176
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4584.818
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2972235679626465
        kl: 0.01626719906926155
        policy_loss: -0.014275562018156052
        total_loss: 0.4816890060901642
        vf_explained_var: 0.9879174828529358
        vf_loss: 0.4929145276546478
    load_time_ms: 0.736
    num_steps_sampled: 3326400
    num_steps_trained: 3276000
    sample_time_ms: 1779.762
    update_time_ms: 4.416
  iterations_since_restore: 126
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890945417203452
    mean_inference_ms: 1.1139775395539993
    mean_processing_ms: 1.5621370388878406
  time_since_restore: 805.9898586273193
  time_this_iter_s: 7.210461616516113
  time_total_s: 805.9898586273193
  timestamp: 1563926533
  timesteps_since_restore: 3326400
  timesteps_this_iter: 26400
  timesteps_total: 3326400
  training_iteration: 126
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 805 s, 126 iter, 3326400 ts, 18.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.61526303429858
  episode_reward_mean: 18.782107153385024
  episode_reward_min: -1.4261589689404295
  episodes_this_iter: 176
  episodes_total: 22352
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4525.717
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2808947563171387
        kl: 0.016361046582460403
        policy_loss: -0.023531215265393257
        total_loss: 0.026184264570474625
        vf_explained_var: 0.9987496733665466
        vf_loss: 0.04664778709411621
    load_time_ms: 0.742
    num_steps_sampled: 3352800
    num_steps_trained: 3302000
    sample_time_ms: 1776.676
    update_time_ms: 4.361
  iterations_since_restore: 127
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8877287826653253
    mean_inference_ms: 1.1127651525659406
    mean_processing_ms: 1.5622832720139768
  time_since_restore: 811.801187992096
  time_this_iter_s: 5.811329364776611
  time_total_s: 811.801187992096
  timestamp: 1563926539
  timesteps_since_restore: 3352800
  timesteps_this_iter: 26400
  timesteps_total: 3352800
  training_iteration: 127
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 811 s, 127 iter, 3352800 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.250206574600064
  episode_reward_mean: 18.941791212230655
  episode_reward_min: -0.30367537590445465
  episodes_this_iter: 176
  episodes_total: 22528
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4505.51
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2665787935256958
        kl: 0.016650192439556122
        policy_loss: -0.01996331475675106
        total_loss: 0.025833934545516968
        vf_explained_var: 0.9988471269607544
        vf_loss: 0.04267533868551254
    load_time_ms: 0.731
    num_steps_sampled: 3379200
    num_steps_trained: 3328000
    sample_time_ms: 1790.317
    update_time_ms: 4.277
  iterations_since_restore: 128
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8895173499985134
    mean_inference_ms: 1.1141901956179119
    mean_processing_ms: 1.5636570161504109
  time_since_restore: 817.7153589725494
  time_this_iter_s: 5.914170980453491
  time_total_s: 817.7153589725494
  timestamp: 1563926545
  timesteps_since_restore: 3379200
  timesteps_this_iter: 26400
  timesteps_total: 3379200
  training_iteration: 128
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 817 s, 128 iter, 3379200 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.68287583501641
  episode_reward_mean: 18.06646336113471
  episode_reward_min: -0.9789436566020966
  episodes_this_iter: 176
  episodes_total: 22704
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4489.088
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2552342414855957
        kl: 0.016778869554400444
        policy_loss: -0.022688981145620346
        total_loss: 0.02413875237107277
        vf_explained_var: 0.9986963868141174
        vf_loss: 0.04368169605731964
    load_time_ms: 0.739
    num_steps_sampled: 3405600
    num_steps_trained: 3354000
    sample_time_ms: 1778.888
    update_time_ms: 4.203
  iterations_since_restore: 129
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8892687154089671
    mean_inference_ms: 1.1128240013987034
    mean_processing_ms: 1.5632622346194922
  time_since_restore: 823.4197402000427
  time_this_iter_s: 5.704381227493286
  time_total_s: 823.4197402000427
  timestamp: 1563926551
  timesteps_since_restore: 3405600
  timesteps_this_iter: 26400
  timesteps_total: 3405600
  training_iteration: 129
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 823 s, 129 iter, 3405600 ts, 18.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.40308769480298
  episode_reward_mean: 19.06545595412034
  episode_reward_min: -1.012606351207535
  episodes_this_iter: 176
  episodes_total: 22880
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4490.276
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2451188564300537
        kl: 0.017280053347349167
        policy_loss: -0.021217992529273033
        total_loss: 0.02198614552617073
        vf_explained_var: 0.9989367723464966
        vf_loss: 0.039964135736227036
    load_time_ms: 0.739
    num_steps_sampled: 3432000
    num_steps_trained: 3380000
    sample_time_ms: 1787.35
    update_time_ms: 4.184
  iterations_since_restore: 130
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889547824171416
    mean_inference_ms: 1.1136905029497561
    mean_processing_ms: 1.562070939134624
  time_since_restore: 829.2183589935303
  time_this_iter_s: 5.798618793487549
  time_total_s: 829.2183589935303
  timestamp: 1563926556
  timesteps_since_restore: 3432000
  timesteps_this_iter: 26400
  timesteps_total: 3432000
  training_iteration: 130
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 829 s, 130 iter, 3432000 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.951336715929855
  episode_reward_mean: 17.915439247297822
  episode_reward_min: -3.2708698417050037
  episodes_this_iter: 176
  episodes_total: 23056
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4502.025
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2355637550354004
        kl: 0.018564186990261078
        policy_loss: -0.019095437601208687
        total_loss: 0.023843269795179367
        vf_explained_var: 0.998869776725769
        vf_loss: 0.03945792093873024
    load_time_ms: 0.738
    num_steps_sampled: 3458400
    num_steps_trained: 3406000
    sample_time_ms: 1775.167
    update_time_ms: 4.126
  iterations_since_restore: 131
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8909696777599918
    mean_inference_ms: 1.1140044164206009
    mean_processing_ms: 1.5637288598755175
  time_since_restore: 836.1273174285889
  time_this_iter_s: 6.908958435058594
  time_total_s: 836.1273174285889
  timestamp: 1563926563
  timesteps_since_restore: 3458400
  timesteps_this_iter: 26400
  timesteps_total: 3458400
  training_iteration: 131
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 836 s, 131 iter, 3458400 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.843320462434065
  episode_reward_mean: 17.791086066942135
  episode_reward_min: -8.693843350857797
  episodes_this_iter: 176
  episodes_total: 23232
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4503.801
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.230579137802124
        kl: 0.01693963259458542
        policy_loss: -0.01513016689568758
        total_loss: 0.03752361238002777
        vf_explained_var: 0.9985604882240295
        vf_loss: 0.04947759583592415
    load_time_ms: 0.735
    num_steps_sampled: 3484800
    num_steps_trained: 3432000
    sample_time_ms: 1785.62
    update_time_ms: 4.036
  iterations_since_restore: 132
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.891049013789531
    mean_inference_ms: 1.1126708578820497
    mean_processing_ms: 1.5629024956905098
  time_since_restore: 843.4520032405853
  time_this_iter_s: 7.32468581199646
  time_total_s: 843.4520032405853
  timestamp: 1563926571
  timesteps_since_restore: 3484800
  timesteps_this_iter: 26400
  timesteps_total: 3484800
  training_iteration: 132
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 843 s, 132 iter, 3484800 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-02-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.03207207305392
  episode_reward_mean: 18.129356418366978
  episode_reward_min: -0.6456044472492289
  episodes_this_iter: 176
  episodes_total: 23408
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4504.765
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2217921018600464
        kl: 0.01822695881128311
        policy_loss: -0.023514170199632645
        total_loss: 0.018286142498254776
        vf_explained_var: 0.9989087581634521
        vf_loss: 0.03838276118040085
    load_time_ms: 0.729
    num_steps_sampled: 3511200
    num_steps_trained: 3458000
    sample_time_ms: 1786.234
    update_time_ms: 4.099
  iterations_since_restore: 133
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889118384349294
    mean_inference_ms: 1.1132076863892166
    mean_processing_ms: 1.5626734869036296
  time_since_restore: 849.2051701545715
  time_this_iter_s: 5.753166913986206
  time_total_s: 849.2051701545715
  timestamp: 1563926576
  timesteps_since_restore: 3511200
  timesteps_this_iter: 26400
  timesteps_total: 3511200
  training_iteration: 133
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 849 s, 133 iter, 3511200 ts, 18.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.52975752714618
  episode_reward_mean: 18.973599960540184
  episode_reward_min: -6.013907623428226
  episodes_this_iter: 176
  episodes_total: 23584
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4503.342
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2099783420562744
        kl: 0.017849816009402275
        policy_loss: -0.02181960456073284
        total_loss: 0.02228129841387272
        vf_explained_var: 0.9988825917243958
        vf_loss: 0.040754061192274094
    load_time_ms: 0.726
    num_steps_sampled: 3537600
    num_steps_trained: 3484000
    sample_time_ms: 1789.964
    update_time_ms: 4.074
  iterations_since_restore: 134
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8896086551224252
    mean_inference_ms: 1.1127367544414695
    mean_processing_ms: 1.5617036279926966
  time_since_restore: 855.3875243663788
  time_this_iter_s: 6.182354211807251
  time_total_s: 855.3875243663788
  timestamp: 1563926583
  timesteps_since_restore: 3537600
  timesteps_this_iter: 26400
  timesteps_total: 3537600
  training_iteration: 134
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 855 s, 134 iter, 3537600 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.31345654599689
  episode_reward_mean: 18.687376938336598
  episode_reward_min: -7.660832026078026
  episodes_this_iter: 176
  episodes_total: 23760
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4487.926
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2002283334732056
        kl: 0.018152298405766487
        policy_loss: -0.020505430176854134
        total_loss: 0.02737084962427616
        vf_explained_var: 0.9987412095069885
        vf_loss: 0.044472720474004745
    load_time_ms: 0.731
    num_steps_sampled: 3564000
    num_steps_trained: 3510000
    sample_time_ms: 1758.388
    update_time_ms: 4.03
  iterations_since_restore: 135
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.893378264067016
    mean_inference_ms: 1.1151099963547744
    mean_processing_ms: 1.5645882769345982
  time_since_restore: 861.4673566818237
  time_this_iter_s: 6.079832315444946
  time_total_s: 861.4673566818237
  timestamp: 1563926589
  timesteps_since_restore: 3564000
  timesteps_this_iter: 26400
  timesteps_total: 3564000
  training_iteration: 135
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 861 s, 135 iter, 3564000 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.40773196008085
  episode_reward_mean: 18.545056502273823
  episode_reward_min: -1.3497327184547796
  episodes_this_iter: 176
  episodes_total: 23936
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4343.924
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1872221231460571
        kl: 0.019117752090096474
        policy_loss: -0.02582852728664875
        total_loss: 0.027863793075084686
        vf_explained_var: 0.9987140893936157
        vf_loss: 0.05010773986577988
    load_time_ms: 0.725
    num_steps_sampled: 3590400
    num_steps_trained: 3536000
    sample_time_ms: 1769.188
    update_time_ms: 4.019
  iterations_since_restore: 136
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889049765352527
    mean_inference_ms: 1.1135041648609674
    mean_processing_ms: 1.564131516864053
  time_since_restore: 867.3426737785339
  time_this_iter_s: 5.875317096710205
  time_total_s: 867.3426737785339
  timestamp: 1563926595
  timesteps_since_restore: 3590400
  timesteps_this_iter: 26400
  timesteps_total: 3590400
  training_iteration: 136
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 867 s, 136 iter, 3590400 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.47474936469021
  episode_reward_mean: 18.927516019828246
  episode_reward_min: -6.732749382149175
  episodes_this_iter: 176
  episodes_total: 24112
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4341.269
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1841751337051392
        kl: 0.018986277282238007
        policy_loss: -0.02278069034218788
        total_loss: 0.043605223298072815
        vf_explained_var: 0.9983333349227905
        vf_loss: 0.0628259927034378
    load_time_ms: 0.717
    num_steps_sampled: 3616800
    num_steps_trained: 3562000
    sample_time_ms: 1763.725
    update_time_ms: 3.933
  iterations_since_restore: 137
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8906076575916424
    mean_inference_ms: 1.1127077834558599
    mean_processing_ms: 1.5647956623322508
  time_since_restore: 873.070728302002
  time_this_iter_s: 5.728054523468018
  time_total_s: 873.070728302002
  timestamp: 1563926600
  timesteps_since_restore: 3616800
  timesteps_this_iter: 26400
  timesteps_total: 3616800
  training_iteration: 137
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 873 s, 137 iter, 3616800 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.03612993673527
  episode_reward_mean: 17.913575788011237
  episode_reward_min: -1.7957241774813748
  episodes_this_iter: 176
  episodes_total: 24288
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4457.805
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1699459552764893
        kl: 0.018440907821059227
        policy_loss: -0.019221143797039986
        total_loss: 0.018111377954483032
        vf_explained_var: 0.9990065097808838
        vf_loss: 0.03387484699487686
    load_time_ms: 0.714
    num_steps_sampled: 3643200
    num_steps_trained: 3588000
    sample_time_ms: 1758.613
    update_time_ms: 3.961
  iterations_since_restore: 138
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.89040673041346
    mean_inference_ms: 1.1132326847739427
    mean_processing_ms: 1.562950244277858
  time_since_restore: 880.1040081977844
  time_this_iter_s: 7.033279895782471
  time_total_s: 880.1040081977844
  timestamp: 1563926607
  timesteps_since_restore: 3643200
  timesteps_this_iter: 26400
  timesteps_total: 3643200
  training_iteration: 138
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 880 s, 138 iter, 3643200 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.32522510702666
  episode_reward_mean: 19.307834410875397
  episode_reward_min: -0.32783255829855107
  episodes_this_iter: 176
  episodes_total: 24464
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4485.637
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1649563312530518
        kl: 0.01827905885875225
        policy_loss: -0.019487768411636353
        total_loss: 0.03463632985949516
        vf_explained_var: 0.9987166523933411
        vf_loss: 0.05069677531719208
    load_time_ms: 0.706
    num_steps_sampled: 3669600
    num_steps_trained: 3614000
    sample_time_ms: 1759.778
    update_time_ms: 4.052
  iterations_since_restore: 139
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8889762317353558
    mean_inference_ms: 1.1121365808474548
    mean_processing_ms: 1.5628011178218466
  time_since_restore: 886.0991578102112
  time_this_iter_s: 5.995149612426758
  time_total_s: 886.0991578102112
  timestamp: 1563926613
  timesteps_since_restore: 3669600
  timesteps_this_iter: 26400
  timesteps_total: 3669600
  training_iteration: 139
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 886 s, 139 iter, 3669600 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.92144937425766
  episode_reward_mean: 18.98369278285334
  episode_reward_min: -2.041846120813227
  episodes_this_iter: 176
  episodes_total: 24640
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4629.359
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1536872386932373
        kl: 0.01630827970802784
        policy_loss: -0.020621776580810547
        total_loss: 0.021174238994717598
        vf_explained_var: 0.9989551901817322
        vf_loss: 0.03873821720480919
    load_time_ms: 0.693
    num_steps_sampled: 3696000
    num_steps_trained: 3640000
    sample_time_ms: 1768.708
    update_time_ms: 4.058
  iterations_since_restore: 140
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8891238137331294
    mean_inference_ms: 1.1125961148927297
    mean_processing_ms: 1.5641888785805287
  time_since_restore: 893.4287521839142
  time_this_iter_s: 7.329594373703003
  time_total_s: 893.4287521839142
  timestamp: 1563926621
  timesteps_since_restore: 3696000
  timesteps_this_iter: 26400
  timesteps_total: 3696000
  training_iteration: 140
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 893 s, 140 iter, 3696000 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.48922797131993
  episode_reward_mean: 18.623422666900634
  episode_reward_min: -5.313106476422401
  episodes_this_iter: 176
  episodes_total: 24816
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4519.927
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1383917331695557
        kl: 0.017075948417186737
        policy_loss: -0.021229248493909836
        total_loss: 0.03768376260995865
        vf_explained_var: 0.9985262155532837
        vf_loss: 0.05571126192808151
    load_time_ms: 0.687
    num_steps_sampled: 3722400
    num_steps_trained: 3666000
    sample_time_ms: 1726.832
    update_time_ms: 4.198
  iterations_since_restore: 141
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.891581731953213
    mean_inference_ms: 1.1131740953127391
    mean_processing_ms: 1.5638309011363363
  time_since_restore: 898.8241531848907
  time_this_iter_s: 5.3954010009765625
  time_total_s: 898.8241531848907
  timestamp: 1563926626
  timesteps_since_restore: 3722400
  timesteps_this_iter: 26400
  timesteps_total: 3722400
  training_iteration: 141
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 898 s, 141 iter, 3722400 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.50549879885255
  episode_reward_mean: 18.851789672704932
  episode_reward_min: -0.7541228394758213
  episodes_this_iter: 176
  episodes_total: 24992
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4415.38
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1298882961273193
        kl: 0.017571337521076202
        policy_loss: -0.020616544410586357
        total_loss: 0.020269405096769333
        vf_explained_var: 0.9989817142486572
        vf_loss: 0.037591323256492615
    load_time_ms: 0.686
    num_steps_sampled: 3748800
    num_steps_trained: 3692000
    sample_time_ms: 1748.1
    update_time_ms: 4.127
  iterations_since_restore: 142
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8901029170147026
    mean_inference_ms: 1.1131452325281124
    mean_processing_ms: 1.5638478313606654
  time_since_restore: 905.3121867179871
  time_this_iter_s: 6.4880335330963135
  time_total_s: 905.3121867179871
  timestamp: 1563926633
  timesteps_since_restore: 3748800
  timesteps_this_iter: 26400
  timesteps_total: 3748800
  training_iteration: 142
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 905 s, 142 iter, 3748800 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-03-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.09159086053954
  episode_reward_mean: 19.13028373504998
  episode_reward_min: -4.98936903381212
  episodes_this_iter: 176
  episodes_total: 25168
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4414.046
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1184659004211426
        kl: 0.019986357539892197
        policy_loss: -0.022778499871492386
        total_loss: 0.01872064732015133
        vf_explained_var: 0.9989988803863525
        vf_loss: 0.037751708179712296
    load_time_ms: 0.696
    num_steps_sampled: 3775200
    num_steps_trained: 3718000
    sample_time_ms: 1736.614
    update_time_ms: 3.943
  iterations_since_restore: 143
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8898370541258638
    mean_inference_ms: 1.1141141290452323
    mean_processing_ms: 1.5656598945390021
  time_since_restore: 910.935465335846
  time_this_iter_s: 5.623278617858887
  time_total_s: 910.935465335846
  timestamp: 1563926638
  timesteps_since_restore: 3775200
  timesteps_this_iter: 26400
  timesteps_total: 3775200
  training_iteration: 143
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 910 s, 143 iter, 3775200 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.55784990319027
  episode_reward_mean: 18.448087800526938
  episode_reward_min: -0.5033628895166651
  episodes_this_iter: 176
  episodes_total: 25344
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4430.637
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1085976362228394
        kl: 0.015831459313631058
        policy_loss: -0.018495630472898483
        total_loss: 0.01998160593211651
        vf_explained_var: 0.9989847540855408
        vf_loss: 0.035508837550878525
    load_time_ms: 0.704
    num_steps_sampled: 3801600
    num_steps_trained: 3744000
    sample_time_ms: 1743.761
    update_time_ms: 3.914
  iterations_since_restore: 144
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8899372051235723
    mean_inference_ms: 1.1124346035262755
    mean_processing_ms: 1.5639351742149585
  time_since_restore: 917.3551194667816
  time_this_iter_s: 6.419654130935669
  time_total_s: 917.3551194667816
  timestamp: 1563926645
  timesteps_since_restore: 3801600
  timesteps_this_iter: 26400
  timesteps_total: 3801600
  training_iteration: 144
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 917 s, 144 iter, 3801600 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.231969480291966
  episode_reward_mean: 19.045802071043923
  episode_reward_min: 0.17701851692060835
  episodes_this_iter: 176
  episodes_total: 25520
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4366.952
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.097484827041626
        kl: 0.01676274836063385
        policy_loss: -0.021727116778492928
        total_loss: 0.010326648131012917
        vf_explained_var: 0.9992491602897644
        vf_loss: 0.028910748660564423
    load_time_ms: 0.705
    num_steps_sampled: 3828000
    num_steps_trained: 3770000
    sample_time_ms: 1769.2
    update_time_ms: 3.898
  iterations_since_restore: 145
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8890533752591536
    mean_inference_ms: 1.111552196252549
    mean_processing_ms: 1.5632356237816163
  time_since_restore: 923.050478219986
  time_this_iter_s: 5.695358753204346
  time_total_s: 923.050478219986
  timestamp: 1563926650
  timesteps_since_restore: 3828000
  timesteps_this_iter: 26400
  timesteps_total: 3828000
  training_iteration: 145
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 923 s, 145 iter, 3828000 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.356133376058764
  episode_reward_mean: 19.239096563317574
  episode_reward_min: -0.5090806006303532
  episodes_this_iter: 176
  episodes_total: 25696
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4505.544
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0847437381744385
        kl: 0.01991243101656437
        policy_loss: -0.023949788883328438
        total_loss: 0.008099294267594814
        vf_explained_var: 0.9991937279701233
        vf_loss: 0.02831549569964409
    load_time_ms: 0.704
    num_steps_sampled: 3854400
    num_steps_trained: 3796000
    sample_time_ms: 1767.548
    update_time_ms: 3.926
  iterations_since_restore: 146
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888393148605195
    mean_inference_ms: 1.1116359475440232
    mean_processing_ms: 1.562205593874314
  time_since_restore: 930.2989728450775
  time_this_iter_s: 7.248494625091553
  time_total_s: 930.2989728450775
  timestamp: 1563926658
  timesteps_since_restore: 3854400
  timesteps_this_iter: 26400
  timesteps_total: 3854400
  training_iteration: 146
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 930 s, 146 iter, 3854400 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.970294536203276
  episode_reward_mean: 17.939208508024084
  episode_reward_min: -0.6861176538229101
  episodes_this_iter: 176
  episodes_total: 25872
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4509.568
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.084984302520752
        kl: 0.018603060394525528
        policy_loss: -0.02111785300076008
        total_loss: 0.01281009241938591
        vf_explained_var: 0.999099850654602
        vf_loss: 0.030439870432019234
    load_time_ms: 0.714
    num_steps_sampled: 3880800
    num_steps_trained: 3822000
    sample_time_ms: 1765.911
    update_time_ms: 4.048
  iterations_since_restore: 147
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8907463987915685
    mean_inference_ms: 1.1126197293506586
    mean_processing_ms: 1.5645616337042154
  time_since_restore: 936.0529248714447
  time_this_iter_s: 5.7539520263671875
  time_total_s: 936.0529248714447
  timestamp: 1563926663
  timesteps_since_restore: 3880800
  timesteps_this_iter: 26400
  timesteps_total: 3880800
  training_iteration: 147
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 936 s, 147 iter, 3880800 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.30107804926121
  episode_reward_mean: 18.944317699097862
  episode_reward_min: 0.3454927691169428
  episodes_this_iter: 176
  episodes_total: 26048
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4490.457
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.070821762084961
        kl: 0.019137399271130562
        policy_loss: -0.023444443941116333
        total_loss: 0.00986592099070549
        vf_explained_var: 0.9992254972457886
        vf_loss: 0.02972211129963398
    load_time_ms: 0.72
    num_steps_sampled: 3907200
    num_steps_trained: 3848000
    sample_time_ms: 1771.735
    update_time_ms: 4.051
  iterations_since_restore: 148
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8893964435706827
    mean_inference_ms: 1.1132514809244731
    mean_processing_ms: 1.5625074588652041
  time_since_restore: 942.9529461860657
  time_this_iter_s: 6.900021314620972
  time_total_s: 942.9529461860657
  timestamp: 1563926670
  timesteps_since_restore: 3907200
  timesteps_this_iter: 26400
  timesteps_total: 3907200
  training_iteration: 148
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 942 s, 148 iter, 3907200 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.123590449778064
  episode_reward_mean: 19.54229068507077
  episode_reward_min: -0.8131622141076462
  episodes_this_iter: 176
  episodes_total: 26224
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4463.406
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.070351481437683
        kl: 0.018508590757846832
        policy_loss: -0.01891404390335083
        total_loss: 0.014890585094690323
        vf_explained_var: 0.999271035194397
        vf_loss: 0.030334265902638435
    load_time_ms: 0.735
    num_steps_sampled: 3933600
    num_steps_trained: 3874000
    sample_time_ms: 1763.078
    update_time_ms: 4.112
  iterations_since_restore: 149
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.885235300611234
    mean_inference_ms: 1.1102489474403285
    mean_processing_ms: 1.5600495164361894
  time_since_restore: 948.5919010639191
  time_this_iter_s: 5.6389548778533936
  time_total_s: 948.5919010639191
  timestamp: 1563926676
  timesteps_since_restore: 3933600
  timesteps_this_iter: 26400
  timesteps_total: 3933600
  training_iteration: 149
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 948 s, 149 iter, 3933600 ts, 19.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.78250033039026
  episode_reward_mean: 19.073369796887068
  episode_reward_min: -7.16159002568208
  episodes_this_iter: 176
  episodes_total: 26400
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4461.955
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0552462339401245
        kl: 0.01737974025309086
        policy_loss: -0.02003435604274273
        total_loss: 0.010275288484990597
        vf_explained_var: 0.9992638826370239
        vf_loss: 0.027050942182540894
    load_time_ms: 0.743
    num_steps_sampled: 3960000
    num_steps_trained: 3900000
    sample_time_ms: 1773.969
    update_time_ms: 4.089
  iterations_since_restore: 150
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8911662799258648
    mean_inference_ms: 1.1124182881746927
    mean_processing_ms: 1.5630325964236216
  time_since_restore: 956.0148754119873
  time_this_iter_s: 7.422974348068237
  time_total_s: 956.0148754119873
  timestamp: 1563926684
  timesteps_since_restore: 3960000
  timesteps_this_iter: 26400
  timesteps_total: 3960000
  training_iteration: 150
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 956 s, 150 iter, 3960000 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.92693413316016
  episode_reward_mean: 18.069159793055068
  episode_reward_min: -0.967439913983043
  episodes_this_iter: 176
  episodes_total: 26576
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4457.399
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0488622188568115
        kl: 0.017528710886836052
        policy_loss: -0.015295431949198246
        total_loss: 0.03422711789608002
        vf_explained_var: 0.9986850023269653
        vf_loss: 0.04623591527342796
    load_time_ms: 0.745
    num_steps_sampled: 3986400
    num_steps_trained: 3926000
    sample_time_ms: 1806.056
    update_time_ms: 4.058
  iterations_since_restore: 151
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8895795983008912
    mean_inference_ms: 1.1114235613795531
    mean_processing_ms: 1.563420318495482
  time_since_restore: 961.6844491958618
  time_this_iter_s: 5.669573783874512
  time_total_s: 961.6844491958618
  timestamp: 1563926689
  timesteps_since_restore: 3986400
  timesteps_this_iter: 26400
  timesteps_total: 3986400
  training_iteration: 151
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 961 s, 151 iter, 3986400 ts, 18.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-04-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.69827541845767
  episode_reward_mean: 18.960942972048734
  episode_reward_min: -0.6093650911624297
  episodes_this_iter: 176
  episodes_total: 26752
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4419.22
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0350593328475952
        kl: 0.019785676151514053
        policy_loss: -0.024937260895967484
        total_loss: 0.005364743992686272
        vf_explained_var: 0.9993020296096802
        vf_loss: 0.0265921913087368
    load_time_ms: 0.744
    num_steps_sampled: 4012800
    num_steps_trained: 3952000
    sample_time_ms: 1782.494
    update_time_ms: 4.067
  iterations_since_restore: 152
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.887946177095233
    mean_inference_ms: 1.1116468332192182
    mean_processing_ms: 1.5628357209068395
  time_since_restore: 967.5549659729004
  time_this_iter_s: 5.870516777038574
  time_total_s: 967.5549659729004
  timestamp: 1563926695
  timesteps_since_restore: 4012800
  timesteps_this_iter: 26400
  timesteps_total: 4012800
  training_iteration: 152
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 967 s, 152 iter, 4012800 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.78392632369704
  episode_reward_mean: 18.700589025044078
  episode_reward_min: -8.562623659099785
  episodes_this_iter: 176
  episodes_total: 26928
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4418.967
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.026782751083374
        kl: 0.01796816848218441
        policy_loss: -0.016808073967695236
        total_loss: 0.020346183329820633
        vf_explained_var: 0.9991214275360107
        vf_loss: 0.03378523141145706
    load_time_ms: 0.736
    num_steps_sampled: 4039200
    num_steps_trained: 3978000
    sample_time_ms: 1795.948
    update_time_ms: 4.047
  iterations_since_restore: 153
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8891801545918419
    mean_inference_ms: 1.1120509835883452
    mean_processing_ms: 1.563338050843714
  time_since_restore: 973.3087494373322
  time_this_iter_s: 5.753783464431763
  time_total_s: 973.3087494373322
  timestamp: 1563926701
  timesteps_since_restore: 4039200
  timesteps_this_iter: 26400
  timesteps_total: 4039200
  training_iteration: 153
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 973 s, 153 iter, 4039200 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.49968296036444
  episode_reward_mean: 19.221814802591265
  episode_reward_min: -0.20901006227736504
  episodes_this_iter: 176
  episodes_total: 27104
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4372.518
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0152065753936768
        kl: 0.016977515071630478
        policy_loss: -0.016443779692053795
        total_loss: 0.013894544914364815
        vf_explained_var: 0.9992585778236389
        vf_loss: 0.027155039831995964
    load_time_ms: 0.735
    num_steps_sampled: 4065600
    num_steps_trained: 4004000
    sample_time_ms: 1754.382
    update_time_ms: 4.057
  iterations_since_restore: 154
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890056854720597
    mean_inference_ms: 1.1124271888136195
    mean_processing_ms: 1.5645236758301786
  time_since_restore: 978.8471710681915
  time_this_iter_s: 5.538421630859375
  time_total_s: 978.8471710681915
  timestamp: 1563926706
  timesteps_since_restore: 4065600
  timesteps_this_iter: 26400
  timesteps_total: 4065600
  training_iteration: 154
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 978 s, 154 iter, 4065600 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.5825986070293
  episode_reward_mean: 19.22951709951575
  episode_reward_min: -1.2197612163736995
  episodes_this_iter: 176
  episodes_total: 27280
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4452.286
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9985874891281128
        kl: 0.0196954645216465
        policy_loss: -0.02585121989250183
        total_loss: 0.0052151428535580635
        vf_explained_var: 0.9992436170578003
        vf_loss: 0.027373462915420532
    load_time_ms: 0.736
    num_steps_sampled: 4092000
    num_steps_trained: 4030000
    sample_time_ms: 1777.753
    update_time_ms: 4.024
  iterations_since_restore: 155
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8899297671495874
    mean_inference_ms: 1.1130668840533824
    mean_processing_ms: 1.5631968516813266
  time_since_restore: 985.5752947330475
  time_this_iter_s: 6.728123664855957
  time_total_s: 985.5752947330475
  timestamp: 1563926713
  timesteps_since_restore: 4092000
  timesteps_this_iter: 26400
  timesteps_total: 4092000
  training_iteration: 155
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 985 s, 155 iter, 4092000 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.73306189123643
  episode_reward_mean: 20.005851316530165
  episode_reward_min: 0.939465592967341
  episodes_this_iter: 176
  episodes_total: 27456
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4456.18
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9868590235710144
        kl: 0.0208395104855299
        policy_loss: -0.02239265851676464
        total_loss: 0.007965012453496456
        vf_explained_var: 0.9993388056755066
        vf_loss: 0.02645026333630085
    load_time_ms: 0.738
    num_steps_sampled: 4118400
    num_steps_trained: 4056000
    sample_time_ms: 1773.127
    update_time_ms: 3.98
  iterations_since_restore: 156
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.89121563706324
    mean_inference_ms: 1.1123329441126422
    mean_processing_ms: 1.5630982584422641
  time_since_restore: 992.8159513473511
  time_this_iter_s: 7.240656614303589
  time_total_s: 992.8159513473511
  timestamp: 1563926720
  timesteps_since_restore: 4118400
  timesteps_this_iter: 26400
  timesteps_total: 4118400
  training_iteration: 156
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 992 s, 156 iter, 4118400 ts, 20 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.65441952347885
  episode_reward_mean: 18.89552841422322
  episode_reward_min: -10.980393087376013
  episodes_this_iter: 176
  episodes_total: 27632
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4460.249
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9812372922897339
        kl: 0.013793275691568851
        policy_loss: -0.014824505895376205
        total_loss: 0.014999063685536385
        vf_explained_var: 0.999280571937561
        vf_loss: 0.025944214314222336
    load_time_ms: 0.732
    num_steps_sampled: 4144800
    num_steps_trained: 4082000
    sample_time_ms: 1766.034
    update_time_ms: 4.004
  iterations_since_restore: 157
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.88873375810587
    mean_inference_ms: 1.1128319427593665
    mean_processing_ms: 1.5636444598844628
  time_since_restore: 998.5399503707886
  time_this_iter_s: 5.7239990234375
  time_total_s: 998.5399503707886
  timestamp: 1563926726
  timesteps_since_restore: 4144800
  timesteps_this_iter: 26400
  timesteps_total: 4144800
  training_iteration: 157
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 998 s, 157 iter, 4144800 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.205266573293756
  episode_reward_mean: 19.05308813219344
  episode_reward_min: -5.492358784695391
  episodes_this_iter: 176
  episodes_total: 27808
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4376.166
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9756242036819458
        kl: 0.014806371182203293
        policy_loss: -0.017461247742176056
        total_loss: 0.03015601448714733
        vf_explained_var: 0.9988540410995483
        vf_loss: 0.043452970683574677
    load_time_ms: 0.72
    num_steps_sampled: 4171200
    num_steps_trained: 4108000
    sample_time_ms: 1750.157
    update_time_ms: 3.995
  iterations_since_restore: 158
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890677837689013
    mean_inference_ms: 1.111599245121509
    mean_processing_ms: 1.5617293548697595
  time_since_restore: 1004.4369812011719
  time_this_iter_s: 5.897030830383301
  time_total_s: 1004.4369812011719
  timestamp: 1563926732
  timesteps_since_restore: 4171200
  timesteps_this_iter: 26400
  timesteps_total: 4171200
  training_iteration: 158
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1004 s, 158 iter, 4171200 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.04410310378569
  episode_reward_mean: 20.265675471819687
  episode_reward_min: -0.548479624091864
  episodes_this_iter: 176
  episodes_total: 27984
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4377.714
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9672674536705017
        kl: 0.015681348741054535
        policy_loss: -0.019460581243038177
        total_loss: 0.008883117698132992
        vf_explained_var: 0.9993975162506104
        vf_loss: 0.023933319374918938
    load_time_ms: 0.705
    num_steps_sampled: 4197600
    num_steps_trained: 4134000
    sample_time_ms: 1779.7
    update_time_ms: 3.859
  iterations_since_restore: 159
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8891748820711178
    mean_inference_ms: 1.112109429120862
    mean_processing_ms: 1.5628585656926468
  time_since_restore: 1010.3851473331451
  time_this_iter_s: 5.948166131973267
  time_total_s: 1010.3851473331451
  timestamp: 1563926738
  timesteps_since_restore: 4197600
  timesteps_this_iter: 26400
  timesteps_total: 4197600
  training_iteration: 159
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1010 s, 159 iter, 4197600 ts, 20.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.230164458269094
  episode_reward_mean: 19.74929314328306
  episode_reward_min: 0.8515294956028066
  episodes_this_iter: 176
  episodes_total: 28160
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4292.973
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9548937678337097
        kl: 0.01671025902032852
        policy_loss: -0.020624008029699326
        total_loss: 0.010469164699316025
        vf_explained_var: 0.9992873668670654
        vf_loss: 0.02639341540634632
    load_time_ms: 0.708
    num_steps_sampled: 4224000
    num_steps_trained: 4160000
    sample_time_ms: 1749.069
    update_time_ms: 3.852
  iterations_since_restore: 160
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8894321128760703
    mean_inference_ms: 1.1139761263115966
    mean_processing_ms: 1.56262847547265
  time_since_restore: 1016.6527075767517
  time_this_iter_s: 6.267560243606567
  time_total_s: 1016.6527075767517
  timestamp: 1563926744
  timesteps_since_restore: 4224000
  timesteps_this_iter: 26400
  timesteps_total: 4224000
  training_iteration: 160
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1016 s, 160 iter, 4224000 ts, 19.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.27104779933286
  episode_reward_mean: 20.68671277635463
  episode_reward_min: -0.8243818206937493
  episodes_this_iter: 176
  episodes_total: 28336
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4294.564
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.950546383857727
        kl: 0.016838567331433296
        policy_loss: -0.02594287507236004
        total_loss: 0.003451023017987609
        vf_explained_var: 0.9993957281112671
        vf_loss: 0.024658052250742912
    load_time_ms: 0.706
    num_steps_sampled: 4250400
    num_steps_trained: 4186000
    sample_time_ms: 1782.336
    update_time_ms: 3.798
  iterations_since_restore: 161
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890728877220354
    mean_inference_ms: 1.1124133663044722
    mean_processing_ms: 1.5643800266358454
  time_since_restore: 1022.6700615882874
  time_this_iter_s: 6.0173540115356445
  time_total_s: 1022.6700615882874
  timestamp: 1563926750
  timesteps_since_restore: 4250400
  timesteps_this_iter: 26400
  timesteps_total: 4250400
  training_iteration: 161
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1022 s, 161 iter, 4250400 ts, 20.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-05-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.27783898151456
  episode_reward_mean: 19.14811095930892
  episode_reward_min: 0.17156918598786947
  episodes_this_iter: 176
  episodes_total: 28512
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4352.001
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.94216388463974
        kl: 0.016465844586491585
        policy_loss: -0.024447748437523842
        total_loss: 0.0046832445077598095
        vf_explained_var: 0.9992936253547668
        vf_loss: 0.02449997514486313
    load_time_ms: 0.704
    num_steps_sampled: 4276800
    num_steps_trained: 4212000
    sample_time_ms: 1756.733
    update_time_ms: 3.811
  iterations_since_restore: 162
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8878908180114609
    mean_inference_ms: 1.1118492111031406
    mean_processing_ms: 1.5625946996528
  time_since_restore: 1028.8606848716736
  time_this_iter_s: 6.1906232833862305
  time_total_s: 1028.8606848716736
  timestamp: 1563926757
  timesteps_since_restore: 4276800
  timesteps_this_iter: 26400
  timesteps_total: 4276800
  training_iteration: 162
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1028 s, 162 iter, 4276800 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.48452835775636
  episode_reward_mean: 19.427156613210784
  episode_reward_min: -0.6941029231778529
  episodes_this_iter: 176
  episodes_total: 28688
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4353.274
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9379569292068481
        kl: 0.014034722000360489
        policy_loss: -0.015930328518152237
        total_loss: 0.036214813590049744
        vf_explained_var: 0.9988307952880859
        vf_loss: 0.04819788038730621
    load_time_ms: 0.701
    num_steps_sampled: 4303200
    num_steps_trained: 4238000
    sample_time_ms: 1769.564
    update_time_ms: 3.884
  iterations_since_restore: 163
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.891069730829208
    mean_inference_ms: 1.114403749181821
    mean_processing_ms: 1.5657597483938637
  time_since_restore: 1034.7566223144531
  time_this_iter_s: 5.895937442779541
  time_total_s: 1034.7566223144531
  timestamp: 1563926762
  timesteps_since_restore: 4303200
  timesteps_this_iter: 26400
  timesteps_total: 4303200
  training_iteration: 163
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1034 s, 163 iter, 4303200 ts, 19.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.84129161133378
  episode_reward_mean: 18.49557425830389
  episode_reward_min: 2.0021507265830354
  episodes_this_iter: 176
  episodes_total: 28864
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4353.99
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9326492547988892
        kl: 0.014485065825283527
        policy_loss: -0.019136320799589157
        total_loss: 0.01723705418407917
        vf_explained_var: 0.9990387558937073
        vf_loss: 0.032299451529979706
    load_time_ms: 0.691
    num_steps_sampled: 4329600
    num_steps_trained: 4264000
    sample_time_ms: 1794.35
    update_time_ms: 3.87
  iterations_since_restore: 164
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8882803454373658
    mean_inference_ms: 1.1111757730300484
    mean_processing_ms: 1.5627173361280788
  time_since_restore: 1040.5497479438782
  time_this_iter_s: 5.793125629425049
  time_total_s: 1040.5497479438782
  timestamp: 1563926768
  timesteps_since_restore: 4329600
  timesteps_this_iter: 26400
  timesteps_total: 4329600
  training_iteration: 164
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1040 s, 164 iter, 4329600 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.0553341020154
  episode_reward_mean: 18.874991349824068
  episode_reward_min: -0.35776962296219406
  episodes_this_iter: 176
  episodes_total: 29040
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4419.358
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9234836101531982
        kl: 0.01734285242855549
        policy_loss: -0.02135510928928852
        total_loss: 0.00851967092603445
        vf_explained_var: 0.9992625713348389
        vf_loss: 0.024997103959321976
    load_time_ms: 0.684
    num_steps_sampled: 4356000
    num_steps_trained: 4290000
    sample_time_ms: 1747.014
    update_time_ms: 3.912
  iterations_since_restore: 165
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8898647562810418
    mean_inference_ms: 1.1136965311277613
    mean_processing_ms: 1.5649614320429917
  time_since_restore: 1047.4606628417969
  time_this_iter_s: 6.910914897918701
  time_total_s: 1047.4606628417969
  timestamp: 1563926775
  timesteps_since_restore: 4356000
  timesteps_this_iter: 26400
  timesteps_total: 4356000
  training_iteration: 165
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1047 s, 165 iter, 4356000 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.93484807266073
  episode_reward_mean: 20.110098786757142
  episode_reward_min: -0.7229710743044097
  episodes_this_iter: 176
  episodes_total: 29216
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4307.706
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.911119818687439
        kl: 0.016450999304652214
        policy_loss: -0.024829111993312836
        total_loss: 0.0025471264962106943
        vf_explained_var: 0.9994107484817505
        vf_loss: 0.0227493979036808
    load_time_ms: 0.688
    num_steps_sampled: 4382400
    num_steps_trained: 4316000
    sample_time_ms: 1758.55
    update_time_ms: 4.029
  iterations_since_restore: 166
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8902243937425403
    mean_inference_ms: 1.1117877567150167
    mean_processing_ms: 1.5626186811098102
  time_since_restore: 1053.6974811553955
  time_this_iter_s: 6.236818313598633
  time_total_s: 1053.6974811553955
  timestamp: 1563926781
  timesteps_since_restore: 4382400
  timesteps_this_iter: 26400
  timesteps_total: 4382400
  training_iteration: 166
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1053 s, 166 iter, 4382400 ts, 20.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.79363953870447
  episode_reward_mean: 19.235336708496888
  episode_reward_min: -0.10884962237024617
  episodes_this_iter: 176
  episodes_total: 29392
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4315.719
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.9045771956443787
        kl: 0.017362436279654503
        policy_loss: -0.01915314421057701
        total_loss: 0.0074133435264229774
        vf_explained_var: 0.9994252324104309
        vf_loss: 0.021683305501937866
    load_time_ms: 0.685
    num_steps_sampled: 4408800
    num_steps_trained: 4342000
    sample_time_ms: 1765.707
    update_time_ms: 3.98
  iterations_since_restore: 167
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8888717372775234
    mean_inference_ms: 1.1119817859112484
    mean_processing_ms: 1.5620362976028097
  time_since_restore: 1059.5724558830261
  time_this_iter_s: 5.874974727630615
  time_total_s: 1059.5724558830261
  timestamp: 1563926787
  timesteps_since_restore: 4408800
  timesteps_this_iter: 26400
  timesteps_total: 4408800
  training_iteration: 167
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1059 s, 167 iter, 4408800 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.91910327781363
  episode_reward_mean: 19.813648748604805
  episode_reward_min: 0.7570132533570353
  episodes_this_iter: 176
  episodes_total: 29568
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4299.116
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8946926593780518
        kl: 0.01912890560925007
        policy_loss: -0.02243354730308056
        total_loss: 0.004602392204105854
        vf_explained_var: 0.9994333982467651
        vf_loss: 0.021655935794115067
    load_time_ms: 0.691
    num_steps_sampled: 4435200
    num_steps_trained: 4368000
    sample_time_ms: 1774.801
    update_time_ms: 3.956
  iterations_since_restore: 168
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888719954140676
    mean_inference_ms: 1.1110665730733553
    mean_processing_ms: 1.5611469359239492
  time_since_restore: 1065.3943543434143
  time_this_iter_s: 5.821898460388184
  time_total_s: 1065.3943543434143
  timestamp: 1563926793
  timesteps_since_restore: 4435200
  timesteps_this_iter: 26400
  timesteps_total: 4435200
  training_iteration: 168
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1065 s, 168 iter, 4435200 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.74433799925057
  episode_reward_mean: 19.0466891567609
  episode_reward_min: -0.5063186872534884
  episodes_this_iter: 176
  episodes_total: 29744
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4325.965
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8920074105262756
        kl: 0.015326042659580708
        policy_loss: -0.021927498281002045
        total_loss: 0.006908237934112549
        vf_explained_var: 0.9993581771850586
        vf_loss: 0.024525292217731476
    load_time_ms: 0.693
    num_steps_sampled: 4461600
    num_steps_trained: 4394000
    sample_time_ms: 1772.31
    update_time_ms: 4.03
  iterations_since_restore: 169
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8904587196455807
    mean_inference_ms: 1.113336971945079
    mean_processing_ms: 1.5647573672703279
  time_since_restore: 1071.5873262882233
  time_this_iter_s: 6.19297194480896
  time_total_s: 1071.5873262882233
  timestamp: 1563926799
  timesteps_since_restore: 4461600
  timesteps_this_iter: 26400
  timesteps_total: 4461600
  training_iteration: 169
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1071 s, 169 iter, 4461600 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.15689446354152
  episode_reward_mean: 19.825343551873924
  episode_reward_min: 0.4638442054210743
  episodes_this_iter: 176
  episodes_total: 29920
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4325.723
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8804267048835754
        kl: 0.01799393817782402
        policy_loss: -0.02507108822464943
        total_loss: -0.0011014495976269245
        vf_explained_var: 0.9995079636573792
        vf_loss: 0.018908847123384476
    load_time_ms: 0.691
    num_steps_sampled: 4488000
    num_steps_trained: 4420000
    sample_time_ms: 1769.968
    update_time_ms: 4.076
  iterations_since_restore: 170
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8879916001905521
    mean_inference_ms: 1.1128559156514675
    mean_processing_ms: 1.5625001316436673
  time_since_restore: 1077.8371045589447
  time_this_iter_s: 6.2497782707214355
  time_total_s: 1077.8371045589447
  timestamp: 1563926806
  timesteps_since_restore: 4488000
  timesteps_this_iter: 26400
  timesteps_total: 4488000
  training_iteration: 170
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1077 s, 170 iter, 4488000 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.26539494611279
  episode_reward_mean: 17.724911701506542
  episode_reward_min: -0.7231024306408455
  episodes_this_iter: 176
  episodes_total: 30096
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4327.273
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8829535841941833
        kl: 0.015462774783372879
        policy_loss: -0.01725933328270912
        total_loss: 0.06765013933181763
        vf_explained_var: 0.9981205463409424
        vf_loss: 0.08056057244539261
    load_time_ms: 0.697
    num_steps_sampled: 4514400
    num_steps_trained: 4446000
    sample_time_ms: 1755.107
    update_time_ms: 4.069
  iterations_since_restore: 171
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8911043824329097
    mean_inference_ms: 1.1131969960269112
    mean_processing_ms: 1.5639672916492509
  time_since_restore: 1083.7214105129242
  time_this_iter_s: 5.884305953979492
  time_total_s: 1083.7214105129242
  timestamp: 1563926812
  timesteps_since_restore: 4514400
  timesteps_this_iter: 26400
  timesteps_total: 4514400
  training_iteration: 171
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1083 s, 171 iter, 4514400 ts, 17.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-06-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.1825332086697
  episode_reward_mean: 18.288644733842077
  episode_reward_min: -1.0892828827700136
  episodes_this_iter: 176
  episodes_total: 30272
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4267.43
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8716741800308228
        kl: 0.01726033166050911
        policy_loss: -0.024804752320051193
        total_loss: 0.0027795806527137756
        vf_explained_var: 0.9993705153465271
        vf_loss: 0.022729864344000816
    load_time_ms: 0.701
    num_steps_sampled: 4540800
    num_steps_trained: 4472000
    sample_time_ms: 1770.216
    update_time_ms: 4.052
  iterations_since_restore: 172
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889492755459469
    mean_inference_ms: 1.1128224011869294
    mean_processing_ms: 1.563428763714557
  time_since_restore: 1089.4627001285553
  time_this_iter_s: 5.7412896156311035
  time_total_s: 1089.4627001285553
  timestamp: 1563926817
  timesteps_since_restore: 4540800
  timesteps_this_iter: 26400
  timesteps_total: 4540800
  training_iteration: 172
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1089 s, 172 iter, 4540800 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.841489518285506
  episode_reward_mean: 19.85606999202201
  episode_reward_min: -0.40847378116293404
  episodes_this_iter: 176
  episodes_total: 30448
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4342.343
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8659782409667969
        kl: 0.017468415200710297
        policy_loss: -0.021021943539381027
        total_loss: 0.0035944445990025997
        vf_explained_var: 0.9994973540306091
        vf_loss: 0.019703391939401627
    load_time_ms: 0.705
    num_steps_sampled: 4567200
    num_steps_trained: 4498000
    sample_time_ms: 1763.761
    update_time_ms: 4.013
  iterations_since_restore: 173
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8894137106282904
    mean_inference_ms: 1.1129861538771584
    mean_processing_ms: 1.5629398864148036
  time_since_restore: 1096.0463314056396
  time_this_iter_s: 6.583631277084351
  time_total_s: 1096.0463314056396
  timestamp: 1563926824
  timesteps_since_restore: 4567200
  timesteps_this_iter: 26400
  timesteps_total: 4567200
  training_iteration: 173
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1096 s, 173 iter, 4567200 ts, 19.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.130588751114416
  episode_reward_mean: 19.84301913230935
  episode_reward_min: -0.6271270941241467
  episodes_this_iter: 176
  episodes_total: 30624
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4356.827
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8671159148216248
        kl: 0.017533035948872566
        policy_loss: -0.020670656114816666
        total_loss: 0.006690628826618195
        vf_explained_var: 0.9994308948516846
        vf_loss: 0.022430121898651123
    load_time_ms: 0.713
    num_steps_sampled: 4593600
    num_steps_trained: 4524000
    sample_time_ms: 1771.423
    update_time_ms: 4.148
  iterations_since_restore: 174
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8890053207290214
    mean_inference_ms: 1.1115655710619536
    mean_processing_ms: 1.5635696022840975
  time_since_restore: 1102.0617973804474
  time_this_iter_s: 6.015465974807739
  time_total_s: 1102.0617973804474
  timestamp: 1563926830
  timesteps_since_restore: 4593600
  timesteps_this_iter: 26400
  timesteps_total: 4593600
  training_iteration: 174
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1102 s, 174 iter, 4593600 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.083411132841306
  episode_reward_mean: 20.939026301575442
  episode_reward_min: 0.7636816293014012
  episodes_this_iter: 176
  episodes_total: 30800
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4353.394
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8580665588378906
        kl: 0.014502638019621372
        policy_loss: -0.01788424700498581
        total_loss: 0.012228678911924362
        vf_explained_var: 0.9993929266929626
        vf_loss: 0.02603405900299549
    load_time_ms: 0.712
    num_steps_sampled: 4620000
    num_steps_trained: 4550000
    sample_time_ms: 1802.126
    update_time_ms: 4.114
  iterations_since_restore: 175
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8892878189152191
    mean_inference_ms: 1.113876732255571
    mean_processing_ms: 1.5635640479371438
  time_since_restore: 1109.2434198856354
  time_this_iter_s: 7.181622505187988
  time_total_s: 1109.2434198856354
  timestamp: 1563926837
  timesteps_since_restore: 4620000
  timesteps_this_iter: 26400
  timesteps_total: 4620000
  training_iteration: 175
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1109 s, 175 iter, 4620000 ts, 20.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.361783961785214
  episode_reward_mean: 18.5938806899076
  episode_reward_min: -1.054974929087919
  episodes_this_iter: 176
  episodes_total: 30976
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4382.404
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8526097536087036
        kl: 0.016570117324590683
        policy_loss: -0.021872349083423615
        total_loss: 0.0029959569219499826
        vf_explained_var: 0.9994605183601379
        vf_loss: 0.020207960158586502
    load_time_ms: 0.705
    num_steps_sampled: 4646400
    num_steps_trained: 4576000
    sample_time_ms: 1773.226
    update_time_ms: 4.014
  iterations_since_restore: 176
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.887783834896225
    mean_inference_ms: 1.111295416327029
    mean_processing_ms: 1.562950144487428
  time_since_restore: 1115.4813406467438
  time_this_iter_s: 6.237920761108398
  time_total_s: 1115.4813406467438
  timestamp: 1563926843
  timesteps_since_restore: 4646400
  timesteps_this_iter: 26400
  timesteps_total: 4646400
  training_iteration: 176
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1115 s, 176 iter, 4646400 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.69583218341274
  episode_reward_mean: 20.59643778122669
  episode_reward_min: -0.4005988647485384
  episodes_this_iter: 176
  episodes_total: 31152
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4367.552
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8415985703468323
        kl: 0.018036391586065292
        policy_loss: -0.023730142042040825
        total_loss: 0.0009536563884466887
        vf_explained_var: 0.9994933009147644
        vf_loss: 0.019611068069934845
    load_time_ms: 0.708
    num_steps_sampled: 4672800
    num_steps_trained: 4602000
    sample_time_ms: 1787.35
    update_time_ms: 3.953
  iterations_since_restore: 177
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8912921882722495
    mean_inference_ms: 1.112388901927727
    mean_processing_ms: 1.5622457283094204
  time_since_restore: 1121.3491084575653
  time_this_iter_s: 5.867767810821533
  time_total_s: 1121.3491084575653
  timestamp: 1563926849
  timesteps_since_restore: 4672800
  timesteps_this_iter: 26400
  timesteps_total: 4672800
  training_iteration: 177
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1121 s, 177 iter, 4672800 ts, 20.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.55596727122716
  episode_reward_mean: 18.428013257820165
  episode_reward_min: 0.4876792351927223
  episodes_this_iter: 176
  episodes_total: 31328
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4425.86
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8341189026832581
        kl: 0.016285641118884087
        policy_loss: -0.020957492291927338
        total_loss: 0.007254242431372404
        vf_explained_var: 0.9993442296981812
        vf_loss: 0.023631397634744644
    load_time_ms: 0.707
    num_steps_sampled: 4699200
    num_steps_trained: 4628000
    sample_time_ms: 1776.834
    update_time_ms: 4.043
  iterations_since_restore: 178
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8899226702377374
    mean_inference_ms: 1.1145553203796175
    mean_processing_ms: 1.565636035028237
  time_since_restore: 1127.6518385410309
  time_this_iter_s: 6.302730083465576
  time_total_s: 1127.6518385410309
  timestamp: 1563926856
  timesteps_since_restore: 4699200
  timesteps_this_iter: 26400
  timesteps_total: 4699200
  training_iteration: 178
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1127 s, 178 iter, 4699200 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.27137470208331
  episode_reward_mean: 20.631745203242478
  episode_reward_min: -0.157118630460495
  episodes_this_iter: 176
  episodes_total: 31504
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4440.67
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.821002721786499
        kl: 0.018394844606518745
        policy_loss: -0.026755796745419502
        total_loss: -0.001413786318153143
        vf_explained_var: 0.9995293021202087
        vf_loss: 0.020168455317616463
    load_time_ms: 0.715
    num_steps_sampled: 4725600
    num_steps_trained: 4654000
    sample_time_ms: 1779.445
    update_time_ms: 4.032
  iterations_since_restore: 179
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8892562116337714
    mean_inference_ms: 1.1106532621096405
    mean_processing_ms: 1.561095106356981
  time_since_restore: 1134.022013425827
  time_this_iter_s: 6.370174884796143
  time_total_s: 1134.022013425827
  timestamp: 1563926862
  timesteps_since_restore: 4725600
  timesteps_this_iter: 26400
  timesteps_total: 4725600
  training_iteration: 179
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1134 s, 179 iter, 4725600 ts, 20.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.53923855275802
  episode_reward_mean: 20.422999030209603
  episode_reward_min: -4.531185435979305
  episodes_this_iter: 176
  episodes_total: 31680
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4383.312
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8212999105453491
        kl: 0.01783972606062889
        policy_loss: -0.018996959552168846
        total_loss: 0.019990693777799606
        vf_explained_var: 0.9991852045059204
        vf_loss: 0.03397022932767868
    load_time_ms: 0.716
    num_steps_sampled: 4752000
    num_steps_trained: 4680000
    sample_time_ms: 1779.434
    update_time_ms: 4.172
  iterations_since_restore: 180
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888866734917849
    mean_inference_ms: 1.1119133391109897
    mean_processing_ms: 1.56259137633398
  time_since_restore: 1139.689633846283
  time_this_iter_s: 5.667620420455933
  time_total_s: 1139.689633846283
  timestamp: 1563926868
  timesteps_since_restore: 4752000
  timesteps_this_iter: 26400
  timesteps_total: 4752000
  training_iteration: 180
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1139 s, 180 iter, 4752000 ts, 20.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-07-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.89198860456229
  episode_reward_mean: 19.96780657469011
  episode_reward_min: 0.3658766367083638
  episodes_this_iter: 176
  episodes_total: 31856
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4459.961
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8093487024307251
        kl: 0.019512422382831573
        policy_loss: -0.022601332515478134
        total_loss: -0.0012517459690570831
        vf_explained_var: 0.9996004700660706
        vf_loss: 0.015861714258790016
    load_time_ms: 0.716
    num_steps_sampled: 4778400
    num_steps_trained: 4706000
    sample_time_ms: 1786.799
    update_time_ms: 4.127
  iterations_since_restore: 181
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889410845129283
    mean_inference_ms: 1.1110809553730425
    mean_processing_ms: 1.5634803730409905
  time_since_restore: 1146.4161956310272
  time_this_iter_s: 6.726561784744263
  time_total_s: 1146.4161956310272
  timestamp: 1563926874
  timesteps_since_restore: 4778400
  timesteps_this_iter: 26400
  timesteps_total: 4778400
  training_iteration: 181
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1146 s, 181 iter, 4778400 ts, 20 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.40668466543458
  episode_reward_mean: 18.645544385716413
  episode_reward_min: -0.7081795256052649
  episodes_this_iter: 176
  episodes_total: 32032
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4457.166
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8007009029388428
        kl: 0.01801254414021969
        policy_loss: -0.021996377035975456
        total_loss: 0.0005769255221821368
        vf_explained_var: 0.9994919300079346
        vf_loss: 0.01750727742910385
    load_time_ms: 0.715
    num_steps_sampled: 4804800
    num_steps_trained: 4732000
    sample_time_ms: 1778.023
    update_time_ms: 4.208
  iterations_since_restore: 182
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8888964208099068
    mean_inference_ms: 1.1117321921491905
    mean_processing_ms: 1.5631244181707766
  time_since_restore: 1152.0426890850067
  time_this_iter_s: 5.626493453979492
  time_total_s: 1152.0426890850067
  timestamp: 1563926880
  timesteps_since_restore: 4804800
  timesteps_this_iter: 26400
  timesteps_total: 4804800
  training_iteration: 182
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1152 s, 182 iter, 4804800 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.02023104385116
  episode_reward_mean: 20.296837358163167
  episode_reward_min: -1.9345206532260506
  episodes_this_iter: 176
  episodes_total: 32208
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4522.256
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7963120937347412
        kl: 0.01678898185491562
        policy_loss: -0.026399292051792145
        total_loss: -0.006331772543489933
        vf_explained_var: 0.9996179938316345
        vf_loss: 0.01534561812877655
    load_time_ms: 0.716
    num_steps_sampled: 4831200
    num_steps_trained: 4758000
    sample_time_ms: 1792.789
    update_time_ms: 4.149
  iterations_since_restore: 183
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.892128129262278
    mean_inference_ms: 1.1145832985529516
    mean_processing_ms: 1.5661135526196366
  time_since_restore: 1159.4256870746613
  time_this_iter_s: 7.382997989654541
  time_total_s: 1159.4256870746613
  timestamp: 1563926887
  timesteps_since_restore: 4831200
  timesteps_this_iter: 26400
  timesteps_total: 4831200
  training_iteration: 183
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1159 s, 183 iter, 4831200 ts, 20.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.88895349128682
  episode_reward_mean: 20.004777179682556
  episode_reward_min: -0.0033201956041142774
  episodes_this_iter: 176
  episodes_total: 32384
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4565.294
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7901736497879028
        kl: 0.02008339762687683
        policy_loss: -0.021919341757893562
        total_loss: 0.0021868960466235876
        vf_explained_var: 0.9995291233062744
        vf_loss: 0.01845778338611126
    load_time_ms: 0.717
    num_steps_sampled: 4857600
    num_steps_trained: 4784000
    sample_time_ms: 1769.124
    update_time_ms: 4.151
  iterations_since_restore: 184
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8870813935844546
    mean_inference_ms: 1.1106529004874488
    mean_processing_ms: 1.5616870243423782
  time_since_restore: 1165.6358139514923
  time_this_iter_s: 6.210126876831055
  time_total_s: 1165.6358139514923
  timestamp: 1563926894
  timesteps_since_restore: 4857600
  timesteps_this_iter: 26400
  timesteps_total: 4857600
  training_iteration: 184
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1165 s, 184 iter, 4857600 ts, 20 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.589360885035696
  episode_reward_mean: 18.531531960269124
  episode_reward_min: -0.05670958605569746
  episodes_this_iter: 176
  episodes_total: 32560
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4494.57
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7862106561660767
        kl: 0.01278760377317667
        policy_loss: -0.022189395502209663
        total_loss: 0.00042862960253842175
        vf_explained_var: 0.9995447993278503
        vf_loss: 0.017223255708813667
    load_time_ms: 0.718
    num_steps_sampled: 4884000
    num_steps_trained: 4810000
    sample_time_ms: 1786.92
    update_time_ms: 4.164
  iterations_since_restore: 185
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8889025142757205
    mean_inference_ms: 1.1126194627201598
    mean_processing_ms: 1.563123421422555
  time_since_restore: 1172.2878720760345
  time_this_iter_s: 6.652058124542236
  time_total_s: 1172.2878720760345
  timestamp: 1563926900
  timesteps_since_restore: 4884000
  timesteps_this_iter: 26400
  timesteps_total: 4884000
  training_iteration: 185
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1172 s, 185 iter, 4884000 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.09001615529164
  episode_reward_mean: 19.6023128482734
  episode_reward_min: -1.9111343236210279
  episodes_this_iter: 176
  episodes_total: 32736
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4575.576
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7781431674957275
        kl: 0.014933646656572819
        policy_loss: -0.02300870046019554
        total_loss: 0.005172633565962315
        vf_explained_var: 0.9993985891342163
        vf_loss: 0.02188120223581791
    load_time_ms: 0.726
    num_steps_sampled: 4910400
    num_steps_trained: 4836000
    sample_time_ms: 1789.312
    update_time_ms: 4.15
  iterations_since_restore: 186
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8860678958098938
    mean_inference_ms: 1.1100930584903363
    mean_processing_ms: 1.5609590841728866
  time_since_restore: 1179.364197254181
  time_this_iter_s: 7.076325178146362
  time_total_s: 1179.364197254181
  timestamp: 1563926907
  timesteps_since_restore: 4910400
  timesteps_this_iter: 26400
  timesteps_total: 4910400
  training_iteration: 186
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1179 s, 186 iter, 4910400 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.97310920784307
  episode_reward_mean: 19.84759046362341
  episode_reward_min: 0.347857268292778
  episodes_this_iter: 176
  episodes_total: 32912
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4576.071
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7673121094703674
        kl: 0.01731697842478752
        policy_loss: -0.024261150509119034
        total_loss: -0.0025691064074635506
        vf_explained_var: 0.9996383190155029
        vf_loss: 0.014386444352567196
    load_time_ms: 0.729
    num_steps_sampled: 4936800
    num_steps_trained: 4862000
    sample_time_ms: 1783.057
    update_time_ms: 4.231
  iterations_since_restore: 187
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8905796174739118
    mean_inference_ms: 1.1132536847511652
    mean_processing_ms: 1.565211532918618
  time_since_restore: 1185.1745727062225
  time_this_iter_s: 5.810375452041626
  time_total_s: 1185.1745727062225
  timestamp: 1563926913
  timesteps_since_restore: 4936800
  timesteps_this_iter: 26400
  timesteps_total: 4936800
  training_iteration: 187
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1185 s, 187 iter, 4936800 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.35512778286221
  episode_reward_mean: 19.146166106081814
  episode_reward_min: -5.212914462108815
  episodes_this_iter: 176
  episodes_total: 33088
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4576.821
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7643970847129822
        kl: 0.013740753754973412
        policy_loss: -0.018866000697016716
        total_loss: 0.006326206959784031
        vf_explained_var: 0.9995139241218567
        vf_loss: 0.01939532905817032
    load_time_ms: 0.734
    num_steps_sampled: 4963200
    num_steps_trained: 4888000
    sample_time_ms: 1794.503
    update_time_ms: 4.223
  iterations_since_restore: 188
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8901379629224857
    mean_inference_ms: 1.1141834397887118
    mean_processing_ms: 1.5647347610359168
  time_since_restore: 1191.5993161201477
  time_this_iter_s: 6.424743413925171
  time_total_s: 1191.5993161201477
  timestamp: 1563926920
  timesteps_since_restore: 4963200
  timesteps_this_iter: 26400
  timesteps_total: 4963200
  training_iteration: 188
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1191 s, 188 iter, 4963200 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.26052982263641
  episode_reward_mean: 20.38233523079566
  episode_reward_min: -0.33431035013736127
  episodes_this_iter: 176
  episodes_total: 33264
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4604.132
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7615775465965271
        kl: 0.01486385241150856
        policy_loss: -0.02540530450642109
        total_loss: -0.004426480270922184
        vf_explained_var: 0.9996521472930908
        vf_loss: 0.014708138071000576
    load_time_ms: 0.735
    num_steps_sampled: 4989600
    num_steps_trained: 4914000
    sample_time_ms: 1784.452
    update_time_ms: 4.225
  iterations_since_restore: 189
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.89144814031231
    mean_inference_ms: 1.11277037485679
    mean_processing_ms: 1.5654926031539846
  time_since_restore: 1198.1405458450317
  time_this_iter_s: 6.541229724884033
  time_total_s: 1198.1405458450317
  timestamp: 1563926926
  timesteps_since_restore: 4989600
  timesteps_this_iter: 26400
  timesteps_total: 4989600
  training_iteration: 189
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1198 s, 189 iter, 4989600 ts, 20.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.82947411154042
  episode_reward_mean: 19.530335619010796
  episode_reward_min: -0.29864484058651
  episodes_this_iter: 176
  episodes_total: 33440
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4681.516
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7630649209022522
        kl: 0.013476556167006493
        policy_loss: -0.01728212460875511
        total_loss: 0.0244650449603796
        vf_explained_var: 0.9990456700325012
        vf_loss: 0.03606174513697624
    load_time_ms: 0.745
    num_steps_sampled: 5016000
    num_steps_trained: 4940000
    sample_time_ms: 1798.416
    update_time_ms: 4.068
  iterations_since_restore: 190
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.89099000923853
    mean_inference_ms: 1.1143492610793606
    mean_processing_ms: 1.5637916065963104
  time_since_restore: 1204.7230815887451
  time_this_iter_s: 6.582535743713379
  time_total_s: 1204.7230815887451
  timestamp: 1563926933
  timesteps_since_restore: 5016000
  timesteps_this_iter: 26400
  timesteps_total: 5016000
  training_iteration: 190
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1204 s, 190 iter, 5016000 ts, 19.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-08-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.87400253437887
  episode_reward_mean: 19.46216201108177
  episode_reward_min: -8.95083132642507
  episodes_this_iter: 176
  episodes_total: 33616
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4602.79
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7618289589881897
        kl: 0.014143860898911953
        policy_loss: -0.012398296035826206
        total_loss: 0.02549704909324646
        vf_explained_var: 0.9991751909255981
        vf_loss: 0.0319284126162529
    load_time_ms: 0.744
    num_steps_sampled: 5042400
    num_steps_trained: 4966000
    sample_time_ms: 1793.092
    update_time_ms: 4.153
  iterations_since_restore: 191
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8920162103826017
    mean_inference_ms: 1.1136629003283618
    mean_processing_ms: 1.5654814168118627
  time_since_restore: 1210.60782289505
  time_this_iter_s: 5.884741306304932
  time_total_s: 1210.60782289505
  timestamp: 1563926939
  timesteps_since_restore: 5042400
  timesteps_this_iter: 26400
  timesteps_total: 5042400
  training_iteration: 191
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1210 s, 191 iter, 5042400 ts, 19.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.86371545965718
  episode_reward_mean: 19.270072009750255
  episode_reward_min: -1.6067405416735447
  episodes_this_iter: 176
  episodes_total: 33792
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4746.991
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7649872899055481
        kl: 0.014045659452676773
        policy_loss: -0.020477673038840294
        total_loss: 0.005205052439123392
        vf_explained_var: 0.9994994401931763
        vf_loss: 0.019757213070988655
    load_time_ms: 0.746
    num_steps_sampled: 5068800
    num_steps_trained: 4992000
    sample_time_ms: 1793.534
    update_time_ms: 4.095
  iterations_since_restore: 192
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8879547423298235
    mean_inference_ms: 1.1117093552898594
    mean_processing_ms: 1.5633760592109247
  time_since_restore: 1217.6843993663788
  time_this_iter_s: 7.076576471328735
  time_total_s: 1217.6843993663788
  timestamp: 1563926946
  timesteps_since_restore: 5068800
  timesteps_this_iter: 26400
  timesteps_total: 5068800
  training_iteration: 192
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1217 s, 192 iter, 5068800 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.4571581104637
  episode_reward_mean: 19.880744241973385
  episode_reward_min: 0.9611666224016456
  episodes_this_iter: 176
  episodes_total: 33968
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4667.354
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7626902461051941
        kl: 0.013127636164426804
        policy_loss: -0.019595714285969734
        total_loss: 0.004203925374895334
        vf_explained_var: 0.9995510578155518
        vf_loss: 0.018261414021253586
    load_time_ms: 0.744
    num_steps_sampled: 5095200
    num_steps_trained: 5018000
    sample_time_ms: 1787.704
    update_time_ms: 4.353
  iterations_since_restore: 193
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8921967767296521
    mean_inference_ms: 1.114960153170751
    mean_processing_ms: 1.5652389006657943
  time_since_restore: 1224.2132568359375
  time_this_iter_s: 6.528857469558716
  time_total_s: 1224.2132568359375
  timestamp: 1563926952
  timesteps_since_restore: 5095200
  timesteps_this_iter: 26400
  timesteps_total: 5095200
  training_iteration: 193
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1224 s, 193 iter, 5095200 ts, 19.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.656990095667645
  episode_reward_mean: 18.378000924310967
  episode_reward_min: -1.5951081877887343
  episodes_this_iter: 176
  episodes_total: 34144
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4671.547
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7589890360832214
        kl: 0.013733502477407455
        policy_loss: -0.020809024572372437
        total_loss: 0.00862181093543768
        vf_explained_var: 0.9992856383323669
        vf_loss: 0.023637013509869576
    load_time_ms: 0.742
    num_steps_sampled: 5121600
    num_steps_trained: 5044000
    sample_time_ms: 1800.912
    update_time_ms: 4.386
  iterations_since_restore: 194
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.890949821053539
    mean_inference_ms: 1.1130174368554242
    mean_processing_ms: 1.5636307303657766
  time_since_restore: 1230.5983927249908
  time_this_iter_s: 6.385135889053345
  time_total_s: 1230.5983927249908
  timestamp: 1563926959
  timesteps_since_restore: 5121600
  timesteps_this_iter: 26400
  timesteps_total: 5121600
  training_iteration: 194
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1230 s, 194 iter, 5121600 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.67524633794033
  episode_reward_mean: 19.693881592617757
  episode_reward_min: -3.24943134835734
  episodes_this_iter: 176
  episodes_total: 34320
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4716.75
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7557891607284546
        kl: 0.01400538720190525
        policy_loss: -0.01732153818011284
        total_loss: 0.0057972571812570095
        vf_explained_var: 0.9995518326759338
        vf_loss: 0.017210273072123528
    load_time_ms: 0.742
    num_steps_sampled: 5148000
    num_steps_trained: 5070000
    sample_time_ms: 1775.998
    update_time_ms: 4.445
  iterations_since_restore: 195
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8893448354639866
    mean_inference_ms: 1.11130401954196
    mean_processing_ms: 1.5623613987608764
  time_since_restore: 1237.4551713466644
  time_this_iter_s: 6.856778621673584
  time_total_s: 1237.4551713466644
  timestamp: 1563926966
  timesteps_since_restore: 5148000
  timesteps_this_iter: 26400
  timesteps_total: 5148000
  training_iteration: 195
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1237 s, 195 iter, 5148000 ts, 19.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.604974236469836
  episode_reward_mean: 19.883050338489962
  episode_reward_min: -0.4290428961773993
  episodes_this_iter: 176
  episodes_total: 34496
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4720.085
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7563196420669556
        kl: 0.01553842332214117
        policy_loss: -0.02388862520456314
        total_loss: -0.0028779851272702217
        vf_explained_var: 0.9996260404586792
        vf_loss: 0.014455368742346764
    load_time_ms: 0.738
    num_steps_sampled: 5174400
    num_steps_trained: 5096000
    sample_time_ms: 1802.528
    update_time_ms: 4.557
  iterations_since_restore: 196
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8907722426429994
    mean_inference_ms: 1.114217235522747
    mean_processing_ms: 1.5663114698299732
  time_since_restore: 1244.830106973648
  time_this_iter_s: 7.374935626983643
  time_total_s: 1244.830106973648
  timestamp: 1563926973
  timesteps_since_restore: 5174400
  timesteps_this_iter: 26400
  timesteps_total: 5174400
  training_iteration: 196
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1244 s, 196 iter, 5174400 ts, 19.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.46780293207514
  episode_reward_mean: 18.205076502135356
  episode_reward_min: -0.008059286275238586
  episodes_this_iter: 176
  episodes_total: 34672
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4748.107
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7544757127761841
        kl: 0.015186164528131485
        policy_loss: -0.021492524072527885
        total_loss: -0.0015412234934046865
        vf_explained_var: 0.999602198600769
        vf_loss: 0.01354463491588831
    load_time_ms: 0.734
    num_steps_sampled: 5200800
    num_steps_trained: 5122000
    sample_time_ms: 1789.309
    update_time_ms: 4.564
  iterations_since_restore: 197
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8895549674178886
    mean_inference_ms: 1.1141764050612168
    mean_processing_ms: 1.5645135236760281
  time_since_restore: 1250.7888655662537
  time_this_iter_s: 5.958758592605591
  time_total_s: 1250.7888655662537
  timestamp: 1563926979
  timesteps_since_restore: 5200800
  timesteps_this_iter: 26400
  timesteps_total: 5200800
  training_iteration: 197
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1250 s, 197 iter, 5200800 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.051949505306986
  episode_reward_mean: 20.820849682271408
  episode_reward_min: -0.35172301686399293
  episodes_this_iter: 176
  episodes_total: 34848
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4690.385
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7467975616455078
        kl: 0.015104711055755615
        policy_loss: -0.02290978655219078
        total_loss: -0.002042352920398116
        vf_explained_var: 0.9996556639671326
        vf_loss: 0.014495134353637695
    load_time_ms: 0.733
    num_steps_sampled: 5227200
    num_steps_trained: 5148000
    sample_time_ms: 1795.025
    update_time_ms: 4.517
  iterations_since_restore: 198
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8894721008723316
    mean_inference_ms: 1.1119765399295147
    mean_processing_ms: 1.5624953145671263
  time_since_restore: 1256.6909050941467
  time_this_iter_s: 5.902039527893066
  time_total_s: 1256.6909050941467
  timestamp: 1563926985
  timesteps_since_restore: 5227200
  timesteps_this_iter: 26400
  timesteps_total: 5227200
  training_iteration: 198
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1256 s, 198 iter, 5227200 ts, 20.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.59963663181046
  episode_reward_mean: 18.988713621598976
  episode_reward_min: -0.6407453517923314
  episodes_this_iter: 176
  episodes_total: 35024
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4676.951
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7419713735580444
        kl: 0.01541440561413765
        policy_loss: -0.02258376218378544
        total_loss: -0.0013558748178184032
        vf_explained_var: 0.99960857629776
        vf_loss: 0.014724933542311192
    load_time_ms: 0.733
    num_steps_sampled: 5253600
    num_steps_trained: 5174000
    sample_time_ms: 1773.077
    update_time_ms: 4.47
  iterations_since_restore: 199
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.892304526175335
    mean_inference_ms: 1.1143588041685175
    mean_processing_ms: 1.5653274184600705
  time_since_restore: 1262.8782739639282
  time_this_iter_s: 6.187368869781494
  time_total_s: 1262.8782739639282
  timestamp: 1563926991
  timesteps_since_restore: 5253600
  timesteps_this_iter: 26400
  timesteps_total: 5253600
  training_iteration: 199
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1262 s, 199 iter, 5253600 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-09-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.58444221498451
  episode_reward_mean: 18.29621523396513
  episode_reward_min: -1.4334904644726163
  episodes_this_iter: 176
  episodes_total: 35200
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4598.051
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7438262701034546
        kl: 0.014232478104531765
        policy_loss: -0.016686903312802315
        total_loss: 0.007221617270261049
        vf_explained_var: 0.9994919896125793
        vf_loss: 0.017904195934534073
    load_time_ms: 0.716
    num_steps_sampled: 5280000
    num_steps_trained: 5200000
    sample_time_ms: 1787.817
    update_time_ms: 4.511
  iterations_since_restore: 200
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8908715678236918
    mean_inference_ms: 1.113814229301152
    mean_processing_ms: 1.5641406480187667
  time_since_restore: 1268.8172163963318
  time_this_iter_s: 5.9389424324035645
  time_total_s: 1268.8172163963318
  timestamp: 1563926997
  timesteps_since_restore: 5280000
  timesteps_this_iter: 26400
  timesteps_total: 5280000
  training_iteration: 200
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1268 s, 200 iter, 5280000 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.45252183612605
  episode_reward_mean: 19.21954184352729
  episode_reward_min: -11.833647319504117
  episodes_this_iter: 176
  episodes_total: 35376
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4742.277
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7377293705940247
        kl: 0.01607181318104267
        policy_loss: -0.01737809367477894
        total_loss: 0.01705789938569069
        vf_explained_var: 0.9992688298225403
        vf_loss: 0.027655702084302902
    load_time_ms: 0.718
    num_steps_sampled: 5306400
    num_steps_trained: 5226000
    sample_time_ms: 1743.01
    update_time_ms: 4.399
  iterations_since_restore: 201
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889085441388563
    mean_inference_ms: 1.1123566196020844
    mean_processing_ms: 1.565215243303308
  time_since_restore: 1275.7003071308136
  time_this_iter_s: 6.8830907344818115
  time_total_s: 1275.7003071308136
  timestamp: 1563927004
  timesteps_since_restore: 5306400
  timesteps_this_iter: 26400
  timesteps_total: 5306400
  training_iteration: 201
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1275 s, 201 iter, 5306400 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.743190192434184
  episode_reward_mean: 19.74705267435832
  episode_reward_min: -0.44316574569115397
  episodes_this_iter: 176
  episodes_total: 35552
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4744.068
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7321535348892212
        kl: 0.01451406441628933
        policy_loss: -0.023845236748456955
        total_loss: 0.003397180000320077
        vf_explained_var: 0.9994282126426697
        vf_loss: 0.021119292825460434
    load_time_ms: 0.72
    num_steps_sampled: 5332800
    num_steps_trained: 5252000
    sample_time_ms: 1769.067
    update_time_ms: 4.552
  iterations_since_restore: 202
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8915015402762527
    mean_inference_ms: 1.114496149394693
    mean_processing_ms: 1.5644324640863685
  time_since_restore: 1283.0582168102264
  time_this_iter_s: 7.357909679412842
  time_total_s: 1283.0582168102264
  timestamp: 1563927011
  timesteps_since_restore: 5332800
  timesteps_this_iter: 26400
  timesteps_total: 5332800
  training_iteration: 202
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1283 s, 202 iter, 5332800 ts, 19.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.00738479175267
  episode_reward_mean: 18.528260851413926
  episode_reward_min: -4.8086125049448505
  episodes_this_iter: 176
  episodes_total: 35728
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4682.506
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7237584590911865
        kl: 0.014414005912840366
        policy_loss: -0.023557892069220543
        total_loss: -0.003698734799399972
        vf_explained_var: 0.9996039271354675
        vf_loss: 0.013778249733150005
    load_time_ms: 0.723
    num_steps_sampled: 5359200
    num_steps_trained: 5278000
    sample_time_ms: 1753.865
    update_time_ms: 4.428
  iterations_since_restore: 203
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8906613379844104
    mean_inference_ms: 1.1132687533106354
    mean_processing_ms: 1.5644389335226199
  time_since_restore: 1288.815556049347
  time_this_iter_s: 5.757339239120483
  time_total_s: 1288.815556049347
  timestamp: 1563927017
  timesteps_since_restore: 5359200
  timesteps_this_iter: 26400
  timesteps_total: 5359200
  training_iteration: 203
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1288 s, 203 iter, 5359200 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.30059608396661
  episode_reward_mean: 19.848037079269194
  episode_reward_min: -3.8957546891584065
  episodes_this_iter: 176
  episodes_total: 35904
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4720.029
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7236671447753906
        kl: 0.013545623980462551
        policy_loss: -0.019300539046525955
        total_loss: 0.000985823106020689
        vf_explained_var: 0.9996135234832764
        vf_loss: 0.014571799896657467
    load_time_ms: 0.726
    num_steps_sampled: 5385600
    num_steps_trained: 5304000
    sample_time_ms: 1761.89
    update_time_ms: 4.291
  iterations_since_restore: 204
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8911266795591262
    mean_inference_ms: 1.1147548183652907
    mean_processing_ms: 1.5652993153224584
  time_since_restore: 1295.6569159030914
  time_this_iter_s: 6.841359853744507
  time_total_s: 1295.6569159030914
  timestamp: 1563927024
  timesteps_since_restore: 5385600
  timesteps_this_iter: 26400
  timesteps_total: 5385600
  training_iteration: 204
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1295 s, 204 iter, 5385600 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.10076639885522
  episode_reward_mean: 19.356810918927497
  episode_reward_min: -13.062350028763541
  episodes_this_iter: 176
  episodes_total: 36080
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4604.428
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7251180410385132
        kl: 0.012433771975338459
        policy_loss: -0.015067292377352715
        total_loss: 0.02207530289888382
        vf_explained_var: 0.9991760849952698
        vf_loss: 0.03189709410071373
    load_time_ms: 0.725
    num_steps_sampled: 5412000
    num_steps_trained: 5330000
    sample_time_ms: 1764.902
    update_time_ms: 4.297
  iterations_since_restore: 205
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.889495346845437
    mean_inference_ms: 1.1114493085350994
    mean_processing_ms: 1.5630859342348964
  time_since_restore: 1301.3845810890198
  time_this_iter_s: 5.727665185928345
  time_total_s: 1301.3845810890198
  timestamp: 1563927030
  timesteps_since_restore: 5412000
  timesteps_this_iter: 26400
  timesteps_total: 5412000
  training_iteration: 205
  2019-07-24 02:10:50,973	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-24 02:10:50,987	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1301 s, 205 iter, 5412000 ts, 19.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.99380183994638
  episode_reward_mean: 19.868883464841726
  episode_reward_min: -5.381322978234462
  episodes_this_iter: 176
  episodes_total: 36256
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4505.17
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7162030339241028
        kl: 0.013961846008896828
        policy_loss: -0.01956048421561718
        total_loss: 0.0035873926244676113
        vf_explained_var: 0.9995560050010681
        vf_loss: 0.017257722094655037
    load_time_ms: 0.727
    num_steps_sampled: 5438400
    num_steps_trained: 5356000
    sample_time_ms: 1756.85
    update_time_ms: 4.172
  iterations_since_restore: 206
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8911602878798797
    mean_inference_ms: 1.113873210463756
    mean_processing_ms: 1.5633237651659164
  time_since_restore: 1307.6810710430145
  time_this_iter_s: 6.296489953994751
  time_total_s: 1307.6810710430145
  timestamp: 1563927036
  timesteps_since_restore: 5438400
  timesteps_this_iter: 26400
  timesteps_total: 5438400
  training_iteration: 206
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1307 s, 206 iter, 5438400 ts, 19.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.015589015362586
  episode_reward_mean: 19.4142628124532
  episode_reward_min: -0.3749464435162034
  episodes_this_iter: 176
  episodes_total: 36432
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4616.998
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7064518332481384
        kl: 0.016624370589852333
        policy_loss: -0.023929182440042496
        total_loss: -0.0035562876146286726
        vf_explained_var: 0.9996300935745239
        vf_loss: 0.013359487988054752
    load_time_ms: 0.724
    num_steps_sampled: 5464800
    num_steps_trained: 5382000
    sample_time_ms: 1762.901
    update_time_ms: 4.034
  iterations_since_restore: 207
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.8912502746432998
    mean_inference_ms: 1.1128798669987074
    mean_processing_ms: 1.5651664856902465
  time_since_restore: 1314.8210558891296
  time_this_iter_s: 7.139984846115112
  time_total_s: 1314.8210558891296
  timestamp: 1563927043
  timesteps_since_restore: 5464800
  timesteps_this_iter: 26400
  timesteps_total: 5464800
  training_iteration: 207
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=30529], 1314 s, 207 iter, 5464800 ts, 19.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew

Result for PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-10-50
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.44353344874787
  episode_reward_mean: 21.358557190570707
  episode_reward_min: -0.5001369927266222
  episodes_this_iter: 176
  episodes_total: 36608
  experiment_id: 53b78cc0032c4f85ab134c2fc9ebe152
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4762.166
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.696738600730896
        kl: 0.017222652211785316
        policy_loss: -0.02524218149483204
        total_loss: -0.004351236391812563
        vf_explained_var: 0.9996935129165649
        vf_loss: 0.013625139370560646
    load_time_ms: 0.722
    num_steps_sampled: 5491200
    num_steps_trained: 5408000
    sample_time_ms: 1754.142
    update_time_ms: 4.202
  iterations_since_restore: 208
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 30529
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.888723591694597
    mean_inference_ms: 1.1124032471135457
    mean_processing_ms: 1.563602204130453
  time_since_restore: 1322.0918536186218
  time_this_iter_s: 7.2707977294921875
  time_total_s: 1322.0918536186218
  timestamp: 1563927050
  timesteps_since_restore: 5491200
  timesteps_this_iter: 26400
  timesteps_total: 5491200
  training_iteration: 208
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 6.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'PENDING': 1})
PENDING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

[2m[36m(pid=31141)[0m 2019-07-24 02:10:51,107	WARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
[2m[36m(pid=31141)[0m 2019-07-24 02:10:51.113282: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31141)[0m [32m [     0.03125s,  INFO] TimeLimit:
[2m[36m(pid=31141)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31141)[0m - action_space = Box(2,)
[2m[36m(pid=31141)[0m - observation_space = Box(9,)
[2m[36m(pid=31141)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31141)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31141)[0m - _max_episode_steps = 150
[2m[36m(pid=31141)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31141)[0m 2019-07-24 02:10:51,302	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=31141)[0m 
[2m[36m(pid=31141)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=31141)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=31141)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=31141)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=31141)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=31141)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=31141)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=31141)[0m 
[2m[36m(pid=31141)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31141)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31141)[0m 2019-07-24 02:10:52,077	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fed981292b0>}
[2m[36m(pid=31141)[0m 2019-07-24 02:10:52,077	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fed9b7b6dd8>}
[2m[36m(pid=31141)[0m 2019-07-24 02:10:52,077	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': MeanStdFilter((9,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
[2m[36m(pid=31141)[0m [32m [     0.99757s,  INFO] TimeLimit:
[2m[36m(pid=31141)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31141)[0m - action_space = Box(2,)
[2m[36m(pid=31141)[0m - observation_space = Box(9,)
[2m[36m(pid=31141)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31141)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31141)[0m - _max_episode_steps = 150
[2m[36m(pid=31141)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31141)[0m [32m [     0.99796s,  INFO] TimeLimit:
[2m[36m(pid=31141)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31141)[0m - action_space = Box(2,)
[2m[36m(pid=31141)[0m - observation_space = Box(9,)
[2m[36m(pid=31141)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31141)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31141)[0m - _max_episode_steps = 150
[2m[36m(pid=31141)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31141)[0m [32m [     0.99833s,  INFO] TimeLimit:
[2m[36m(pid=31141)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31141)[0m - action_space = Box(2,)
[2m[36m(pid=31141)[0m - observation_space = Box(9,)
[2m[36m(pid=31141)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31141)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31141)[0m - _max_episode_steps = 150
[2m[36m(pid=31141)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31103)[0m 2019-07-24 02:10:52,167	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31103)[0m 2019-07-24 02:10:52.168063: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31077)[0m 2019-07-24 02:10:52,156	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31077)[0m 2019-07-24 02:10:52.157215: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31103)[0m [32m [     0.03529s,  INFO] TimeLimit:
[2m[36m(pid=31103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31103)[0m - action_space = Box(2,)
[2m[36m(pid=31103)[0m - observation_space = Box(9,)
[2m[36m(pid=31103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31103)[0m - _max_episode_steps = 150
[2m[36m(pid=31103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31077)[0m [32m [     0.02538s,  INFO] TimeLimit:
[2m[36m(pid=31077)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31077)[0m - action_space = Box(2,)
[2m[36m(pid=31077)[0m - observation_space = Box(9,)
[2m[36m(pid=31077)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31077)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31077)[0m - _max_episode_steps = 150
[2m[36m(pid=31077)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31063)[0m [32m [     0.02209s,  INFO] TimeLimit:
[2m[36m(pid=31063)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31063)[0m - action_space = Box(2,)
[2m[36m(pid=31063)[0m - observation_space = Box(9,)
[2m[36m(pid=31063)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31063)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31063)[0m - _max_episode_steps = 150
[2m[36m(pid=31063)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31063)[0m 2019-07-24 02:10:52,138	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31063)[0m 2019-07-24 02:10:52.138841: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31141)[0m 2019-07-24 02:10:52,159	INFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/gpu:0']
[2m[36m(pid=31044)[0m [32m [     0.03630s,  INFO] TimeLimit:
[2m[36m(pid=31044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31044)[0m - action_space = Box(2,)
[2m[36m(pid=31044)[0m - observation_space = Box(9,)
[2m[36m(pid=31044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31044)[0m - _max_episode_steps = 150
[2m[36m(pid=31044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31044)[0m 2019-07-24 02:10:52,190	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31044)[0m 2019-07-24 02:10:52.190832: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31138)[0m [32m [     0.03556s,  INFO] TimeLimit:
[2m[36m(pid=31138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31138)[0m - action_space = Box(2,)
[2m[36m(pid=31138)[0m - observation_space = Box(9,)
[2m[36m(pid=31138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31138)[0m - _max_episode_steps = 150
[2m[36m(pid=31138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31138)[0m 2019-07-24 02:10:52,190	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31138)[0m 2019-07-24 02:10:52.191079: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31037)[0m 2019-07-24 02:10:52,222	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31037)[0m 2019-07-24 02:10:52.223280: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31042)[0m [32m [     0.04009s,  INFO] TimeLimit:
[2m[36m(pid=31042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31042)[0m - action_space = Box(2,)
[2m[36m(pid=31042)[0m - observation_space = Box(9,)
[2m[36m(pid=31042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31042)[0m - _max_episode_steps = 150
[2m[36m(pid=31042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31037)[0m [32m [     0.03652s,  INFO] TimeLimit:
[2m[36m(pid=31037)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31037)[0m - action_space = Box(2,)
[2m[36m(pid=31037)[0m - observation_space = Box(9,)
[2m[36m(pid=31037)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31037)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31037)[0m - _max_episode_steps = 150
[2m[36m(pid=31037)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31033)[0m [32m [     0.03708s,  INFO] TimeLimit:
[2m[36m(pid=31033)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31033)[0m - action_space = Box(2,)
[2m[36m(pid=31033)[0m - observation_space = Box(9,)
[2m[36m(pid=31033)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31033)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31033)[0m - _max_episode_steps = 150
[2m[36m(pid=31033)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31042)[0m 2019-07-24 02:10:52,243	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31042)[0m 2019-07-24 02:10:52.244101: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31051)[0m 2019-07-24 02:10:52,217	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31051)[0m 2019-07-24 02:10:52.217770: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31051)[0m [32m [     0.03666s,  INFO] TimeLimit:
[2m[36m(pid=31051)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31051)[0m - action_space = Box(2,)
[2m[36m(pid=31051)[0m - observation_space = Box(9,)
[2m[36m(pid=31051)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31051)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31051)[0m - _max_episode_steps = 150
[2m[36m(pid=31051)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31047)[0m [32m [     0.03752s,  INFO] TimeLimit:
[2m[36m(pid=31047)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31047)[0m - action_space = Box(2,)
[2m[36m(pid=31047)[0m - observation_space = Box(9,)
[2m[36m(pid=31047)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31047)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31047)[0m - _max_episode_steps = 150
[2m[36m(pid=31047)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31047)[0m 2019-07-24 02:10:52,198	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31047)[0m 2019-07-24 02:10:52.198772: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31034)[0m [32m [     0.03453s,  INFO] TimeLimit:
[2m[36m(pid=31034)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31034)[0m - action_space = Box(2,)
[2m[36m(pid=31034)[0m - observation_space = Box(9,)
[2m[36m(pid=31034)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31034)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31034)[0m - _max_episode_steps = 150
[2m[36m(pid=31034)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31034)[0m 2019-07-24 02:10:52,246	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31034)[0m 2019-07-24 02:10:52.247656: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31033)[0m 2019-07-24 02:10:52,245	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31033)[0m 2019-07-24 02:10:52.245707: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31063)[0m 2019-07-24 02:10:52,337	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31063)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=31063)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31063)[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=31063)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=31063)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=31063)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=31063)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=31063)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31063)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31063)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31063)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31077)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31077)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31063)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31063)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31044)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31044)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31034)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31034)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31051)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31051)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31047)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31047)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31138)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31138)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31103)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31103)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31033)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31033)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31037)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31037)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31042)[0m /home/amr/anaconda3/envs/kayray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[2m[36m(pid=31042)[0m   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[2m[36m(pid=31077)[0m [32m [     1.56894s,  INFO] TimeLimit:
[2m[36m(pid=31077)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31077)[0m - action_space = Box(2,)
[2m[36m(pid=31077)[0m - observation_space = Box(9,)
[2m[36m(pid=31077)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31077)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31077)[0m - _max_episode_steps = 150
[2m[36m(pid=31077)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31063)[0m [32m [     1.57620s,  INFO] TimeLimit:
[2m[36m(pid=31063)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31063)[0m - action_space = Box(2,)
[2m[36m(pid=31063)[0m - observation_space = Box(9,)
[2m[36m(pid=31063)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31063)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31063)[0m - _max_episode_steps = 150
[2m[36m(pid=31063)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31063)[0m [32m [     1.57705s,  INFO] TimeLimit:
[2m[36m(pid=31063)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31063)[0m - action_space = Box(2,)
[2m[36m(pid=31063)[0m - observation_space = Box(9,)
[2m[36m(pid=31063)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31063)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31063)[0m - _max_episode_steps = 150
[2m[36m(pid=31063)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31063)[0m [32m [     1.57787s,  INFO] TimeLimit:
[2m[36m(pid=31063)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31063)[0m - action_space = Box(2,)
[2m[36m(pid=31063)[0m - observation_space = Box(9,)
[2m[36m(pid=31063)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31063)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31063)[0m - _max_episode_steps = 150
[2m[36m(pid=31063)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31077)[0m [32m [     1.56987s,  INFO] TimeLimit:
[2m[36m(pid=31077)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31077)[0m - action_space = Box(2,)
[2m[36m(pid=31077)[0m - observation_space = Box(9,)
[2m[36m(pid=31077)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31077)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31077)[0m - _max_episode_steps = 150
[2m[36m(pid=31077)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31077)[0m [32m [     1.57074s,  INFO] TimeLimit:
[2m[36m(pid=31077)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31077)[0m - action_space = Box(2,)
[2m[36m(pid=31077)[0m - observation_space = Box(9,)
[2m[36m(pid=31077)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31077)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31077)[0m - _max_episode_steps = 150
[2m[36m(pid=31077)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31034)[0m [32m [     1.59199s,  INFO] TimeLimit:
[2m[36m(pid=31034)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31034)[0m - action_space = Box(2,)
[2m[36m(pid=31034)[0m - observation_space = Box(9,)
[2m[36m(pid=31034)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31034)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31034)[0m - _max_episode_steps = 150
[2m[36m(pid=31034)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31034)[0m [32m [     1.59275s,  INFO] TimeLimit:
[2m[36m(pid=31034)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31034)[0m - action_space = Box(2,)
[2m[36m(pid=31034)[0m - observation_space = Box(9,)
[2m[36m(pid=31034)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31034)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31034)[0m - _max_episode_steps = 150
[2m[36m(pid=31034)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31034)[0m [32m [     1.59334s,  INFO] TimeLimit:
[2m[36m(pid=31034)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31034)[0m - action_space = Box(2,)
[2m[36m(pid=31034)[0m - observation_space = Box(9,)
[2m[36m(pid=31034)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31034)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31034)[0m - _max_episode_steps = 150
[2m[36m(pid=31034)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31033)[0m [32m [     1.59519s,  INFO] TimeLimit:
[2m[36m(pid=31033)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31033)[0m - action_space = Box(2,)
[2m[36m(pid=31033)[0m - observation_space = Box(9,)
[2m[36m(pid=31033)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31033)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31033)[0m - _max_episode_steps = 150
[2m[36m(pid=31033)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31033)[0m [32m [     1.59602s,  INFO] TimeLimit:
[2m[36m(pid=31033)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31033)[0m - action_space = Box(2,)
[2m[36m(pid=31033)[0m - observation_space = Box(9,)
[2m[36m(pid=31033)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31033)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31033)[0m - _max_episode_steps = 150
[2m[36m(pid=31033)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31033)[0m [32m [     1.59681s,  INFO] TimeLimit:
[2m[36m(pid=31033)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31033)[0m - action_space = Box(2,)
[2m[36m(pid=31033)[0m - observation_space = Box(9,)
[2m[36m(pid=31033)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31033)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31033)[0m - _max_episode_steps = 150
[2m[36m(pid=31033)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31051)[0m [32m [     1.61523s,  INFO] TimeLimit:
[2m[36m(pid=31051)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31051)[0m - action_space = Box(2,)
[2m[36m(pid=31051)[0m - observation_space = Box(9,)
[2m[36m(pid=31051)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31051)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31051)[0m - _max_episode_steps = 150
[2m[36m(pid=31051)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31051)[0m [32m [     1.61604s,  INFO] TimeLimit:
[2m[36m(pid=31051)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31051)[0m - action_space = Box(2,)
[2m[36m(pid=31051)[0m - observation_space = Box(9,)
[2m[36m(pid=31051)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31051)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31051)[0m - _max_episode_steps = 150
[2m[36m(pid=31051)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31051)[0m [32m [     1.61682s,  INFO] TimeLimit:
[2m[36m(pid=31051)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31051)[0m - action_space = Box(2,)
[2m[36m(pid=31051)[0m - observation_space = Box(9,)
[2m[36m(pid=31051)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31051)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31051)[0m - _max_episode_steps = 150
[2m[36m(pid=31051)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31044)[0m [32m [     1.61316s,  INFO] TimeLimit:
[2m[36m(pid=31044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31044)[0m - action_space = Box(2,)
[2m[36m(pid=31044)[0m - observation_space = Box(9,)
[2m[36m(pid=31044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31044)[0m - _max_episode_steps = 150
[2m[36m(pid=31044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31044)[0m [32m [     1.61401s,  INFO] TimeLimit:
[2m[36m(pid=31044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31044)[0m - action_space = Box(2,)
[2m[36m(pid=31044)[0m - observation_space = Box(9,)
[2m[36m(pid=31044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31044)[0m - _max_episode_steps = 150
[2m[36m(pid=31044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31044)[0m [32m [     1.61482s,  INFO] TimeLimit:
[2m[36m(pid=31044)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31044)[0m - action_space = Box(2,)
[2m[36m(pid=31044)[0m - observation_space = Box(9,)
[2m[36m(pid=31044)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31044)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31044)[0m - _max_episode_steps = 150
[2m[36m(pid=31044)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31047)[0m [32m [     1.64088s,  INFO] TimeLimit:
[2m[36m(pid=31047)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31047)[0m - action_space = Box(2,)
[2m[36m(pid=31047)[0m - observation_space = Box(9,)
[2m[36m(pid=31047)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31047)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31047)[0m - _max_episode_steps = 150
[2m[36m(pid=31047)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31047)[0m [32m [     1.64147s,  INFO] TimeLimit:
[2m[36m(pid=31047)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31047)[0m - action_space = Box(2,)
[2m[36m(pid=31047)[0m - observation_space = Box(9,)
[2m[36m(pid=31047)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31047)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31047)[0m - _max_episode_steps = 150
[2m[36m(pid=31047)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31047)[0m [32m [     1.64201s,  INFO] TimeLimit:
[2m[36m(pid=31047)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31047)[0m - action_space = Box(2,)
[2m[36m(pid=31047)[0m - observation_space = Box(9,)
[2m[36m(pid=31047)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31047)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31047)[0m - _max_episode_steps = 150
[2m[36m(pid=31047)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31138)[0m [32m [     1.62785s,  INFO] TimeLimit:
[2m[36m(pid=31138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31138)[0m - action_space = Box(2,)
[2m[36m(pid=31138)[0m - observation_space = Box(9,)
[2m[36m(pid=31138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31138)[0m - _max_episode_steps = 150
[2m[36m(pid=31138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31138)[0m [32m [     1.62866s,  INFO] TimeLimit:
[2m[36m(pid=31138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31138)[0m - action_space = Box(2,)
[2m[36m(pid=31138)[0m - observation_space = Box(9,)
[2m[36m(pid=31138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31138)[0m - _max_episode_steps = 150
[2m[36m(pid=31138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31138)[0m [32m [     1.62944s,  INFO] TimeLimit:
[2m[36m(pid=31138)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31138)[0m - action_space = Box(2,)
[2m[36m(pid=31138)[0m - observation_space = Box(9,)
[2m[36m(pid=31138)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31138)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31138)[0m - _max_episode_steps = 150
[2m[36m(pid=31138)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31103)[0m [32m [     1.67547s,  INFO] TimeLimit:
[2m[36m(pid=31103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31103)[0m - action_space = Box(2,)
[2m[36m(pid=31103)[0m - observation_space = Box(9,)
[2m[36m(pid=31103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31103)[0m - _max_episode_steps = 150
[2m[36m(pid=31103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31103)[0m [32m [     1.67600s,  INFO] TimeLimit:
[2m[36m(pid=31103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31103)[0m - action_space = Box(2,)
[2m[36m(pid=31103)[0m - observation_space = Box(9,)
[2m[36m(pid=31103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31103)[0m - _max_episode_steps = 150
[2m[36m(pid=31103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31103)[0m [32m [     1.67651s,  INFO] TimeLimit:
[2m[36m(pid=31103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31103)[0m - action_space = Box(2,)
[2m[36m(pid=31103)[0m - observation_space = Box(9,)
[2m[36m(pid=31103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31103)[0m - _max_episode_steps = 150
[2m[36m(pid=31103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31042)[0m [32m [     1.64677s,  INFO] TimeLimit:
[2m[36m(pid=31042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31042)[0m - action_space = Box(2,)
[2m[36m(pid=31042)[0m - observation_space = Box(9,)
[2m[36m(pid=31042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31042)[0m - _max_episode_steps = 150
[2m[36m(pid=31042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31042)[0m [32m [     1.64717s,  INFO] TimeLimit:
[2m[36m(pid=31042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31042)[0m - action_space = Box(2,)
[2m[36m(pid=31042)[0m - observation_space = Box(9,)
[2m[36m(pid=31042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31042)[0m - _max_episode_steps = 150
[2m[36m(pid=31042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31042)[0m [32m [     1.64757s,  INFO] TimeLimit:
[2m[36m(pid=31042)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31042)[0m - action_space = Box(2,)
[2m[36m(pid=31042)[0m - observation_space = Box(9,)
[2m[36m(pid=31042)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31042)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31042)[0m - _max_episode_steps = 150
[2m[36m(pid=31042)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31037)[0m [32m [     1.63637s,  INFO] TimeLimit:
[2m[36m(pid=31037)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31037)[0m - action_space = Box(2,)
[2m[36m(pid=31037)[0m - observation_space = Box(9,)
[2m[36m(pid=31037)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31037)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31037)[0m - _max_episode_steps = 150
[2m[36m(pid=31037)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31037)[0m [32m [     1.63686s,  INFO] TimeLimit:
[2m[36m(pid=31037)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31037)[0m - action_space = Box(2,)
[2m[36m(pid=31037)[0m - observation_space = Box(9,)
[2m[36m(pid=31037)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31037)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31037)[0m - _max_episode_steps = 150
[2m[36m(pid=31037)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31037)[0m [32m [     1.63730s,  INFO] TimeLimit:
[2m[36m(pid=31037)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31037)[0m - action_space = Box(2,)
[2m[36m(pid=31037)[0m - observation_space = Box(9,)
[2m[36m(pid=31037)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31037)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31037)[0m - _max_episode_steps = 150
[2m[36m(pid=31037)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,788	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,835	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.229, max=0.979, mean=0.191)},
[2m[36m(pid=31063)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.367, max=0.93, mean=0.055)},
[2m[36m(pid=31063)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.969, max=0.035, mean=-0.251)},
[2m[36m(pid=31063)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.691, max=0.723, mean=0.034)}}
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,836	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=31063)[0m   1: {'agent0': None},
[2m[36m(pid=31063)[0m   2: {'agent0': None},
[2m[36m(pid=31063)[0m   3: {'agent0': None}}
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,836	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.229, max=0.979, mean=0.191)
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,836	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0)
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,839	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31063)[0m                                   'env_id': 0,
[2m[36m(pid=31063)[0m                                   'info': None,
[2m[36m(pid=31063)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31063)[0m                                   'rnn_state': []},
[2m[36m(pid=31063)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31063)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31063)[0m                                   'env_id': 1,
[2m[36m(pid=31063)[0m                                   'info': None,
[2m[36m(pid=31063)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.707, max=0.707, mean=-0.236),
[2m[36m(pid=31063)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31063)[0m                                   'rnn_state': []},
[2m[36m(pid=31063)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31063)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31063)[0m                                   'env_id': 2,
[2m[36m(pid=31063)[0m                                   'info': None,
[2m[36m(pid=31063)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.134, max=0.795, mean=-0.382),
[2m[36m(pid=31063)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31063)[0m                                   'rnn_state': []},
[2m[36m(pid=31063)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31063)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31063)[0m                                   'env_id': 3,
[2m[36m(pid=31063)[0m                                   'info': None,
[2m[36m(pid=31063)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.354, max=1.462, mean=0.041),
[2m[36m(pid=31063)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31063)[0m                                   'rnn_state': []},
[2m[36m(pid=31063)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,839	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=31063)[0m 2019-07-24 02:10:54,876	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-1.812, max=1.616, mean=-0.129),
[2m[36m(pid=31063)[0m                       [],
[2m[36m(pid=31063)[0m                       { 'action_prob': np.ndarray((4,), dtype=float32, min=0.008, max=0.149, mean=0.086),
[2m[36m(pid=31063)[0m                         'behaviour_logits': np.ndarray((4, 4), dtype=float32, min=-0.006, max=0.008, mean=0.001),
[2m[36m(pid=31063)[0m                         'vf_preds': np.ndarray((4,), dtype=float32, min=-0.005, max=0.002, mean=-0.001)})}
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m 2019-07-24 02:10:55,314	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((150,), dtype=float32, min=0.0, max=0.159, mean=0.078),
[2m[36m(pid=31063)[0m                         'actions': np.ndarray((150, 2), dtype=float32, min=-2.305, max=3.416, mean=0.034),
[2m[36m(pid=31063)[0m                         'advantages': np.ndarray((150,), dtype=float32, min=-9.927, max=13.37, mean=-2.308),
[2m[36m(pid=31063)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                         'behaviour_logits': np.ndarray((150, 4), dtype=float32, min=-0.011, max=0.004, mean=-0.001),
[2m[36m(pid=31063)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=31063)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=58691758.0, max=58691758.0, mean=58691758.0),
[2m[36m(pid=31063)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=31063)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-2.256, max=2.312, mean=0.025),
[2m[36m(pid=31063)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-2.256, max=2.312, mean=0.024),
[2m[36m(pid=31063)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-2.305, max=3.416, mean=0.036),
[2m[36m(pid=31063)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-1.764, max=2.623, mean=-0.065),
[2m[36m(pid=31063)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-1.764, max=2.623, mean=-0.075),
[2m[36m(pid=31063)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=31063)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m                         'value_targets': np.ndarray((150,), dtype=float32, min=-9.928, max=13.365, mean=-2.308),
[2m[36m(pid=31063)[0m                         'vf_preds': np.ndarray((150,), dtype=float32, min=-0.006, max=0.005, mean=-0.001)},
[2m[36m(pid=31063)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m 2019-07-24 02:10:55,855	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31063)[0m { 'data': { 'action_prob': np.ndarray((900,), dtype=float32, min=0.0, max=0.159, mean=0.081),
[2m[36m(pid=31063)[0m             'actions': np.ndarray((900, 2), dtype=float32, min=-3.55, max=3.416, mean=0.013),
[2m[36m(pid=31063)[0m             'advantages': np.ndarray((900,), dtype=float32, min=-23.205, max=27.006, mean=-3.815),
[2m[36m(pid=31063)[0m             'agent_index': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31063)[0m             'behaviour_logits': np.ndarray((900, 4), dtype=float32, min=-0.011, max=0.012, mean=-0.0),
[2m[36m(pid=31063)[0m             'dones': np.ndarray((900,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=31063)[0m             'eps_id': np.ndarray((900,), dtype=int64, min=21792356.0, max=1723471520.0, mean=995026869.0),
[2m[36m(pid=31063)[0m             'infos': np.ndarray((900,), dtype=object, head={}),
[2m[36m(pid=31063)[0m             'new_obs': np.ndarray((900, 9), dtype=float32, min=-3.681, max=4.14, mean=-0.025),
[2m[36m(pid=31063)[0m             'obs': np.ndarray((900, 9), dtype=float32, min=-3.681, max=4.14, mean=-0.026),
[2m[36m(pid=31063)[0m             'prev_actions': np.ndarray((900, 2), dtype=float32, min=-3.55, max=3.416, mean=0.012),
[2m[36m(pid=31063)[0m             'prev_rewards': np.ndarray((900,), dtype=float32, min=-5.16, max=4.309, mean=-0.086),
[2m[36m(pid=31063)[0m             'rewards': np.ndarray((900,), dtype=float32, min=-5.16, max=4.309, mean=-0.089),
[2m[36m(pid=31063)[0m             't': np.ndarray((900,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=31063)[0m             'unroll_id': np.ndarray((900,), dtype=int64, min=0.0, max=1.0, mean=0.333),
[2m[36m(pid=31063)[0m             'value_targets': np.ndarray((900,), dtype=float32, min=-23.202, max=27.003, mean=-3.815),
[2m[36m(pid=31063)[0m             'vf_preds': np.ndarray((900,), dtype=float32, min=-0.01, max=0.009, mean=0.0)},
[2m[36m(pid=31063)[0m   'type': 'SampleBatch'}
[2m[36m(pid=31063)[0m 
[2m[36m(pid=31141)[0m 2019-07-24 02:10:57,144	INFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:
[2m[36m(pid=31141)[0m 
[2m[36m(pid=31141)[0m { 'inputs': [ np.ndarray((26100, 2), dtype=float32, min=-4.077, max=4.329, mean=-0.007),
[2m[36m(pid=31141)[0m               np.ndarray((26100,), dtype=float32, min=-24.276, max=21.613, mean=-0.105),
[2m[36m(pid=31141)[0m               np.ndarray((26100, 9), dtype=float32, min=-6.65, max=15.202, mean=0.001),
[2m[36m(pid=31141)[0m               np.ndarray((26100, 2), dtype=float32, min=-4.077, max=4.329, mean=-0.007),
[2m[36m(pid=31141)[0m               np.ndarray((26100,), dtype=float32, min=-4.044, max=4.142, mean=-0.0),
[2m[36m(pid=31141)[0m               np.ndarray((26100, 4), dtype=float32, min=-0.012, max=0.013, mean=0.0),
[2m[36m(pid=31141)[0m               np.ndarray((26100,), dtype=float32, min=-47.884, max=34.331, mean=-7.268),
[2m[36m(pid=31141)[0m               np.ndarray((26100,), dtype=float32, min=-0.015, max=0.014, mean=-0.0)],
[2m[36m(pid=31141)[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=31141)[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
[2m[36m(pid=31141)[0m   'state_inputs': []}
[2m[36m(pid=31141)[0m 
[2m[36m(pid=31141)[0m 2019-07-24 02:10:57,145	INFO multi_gpu_impl.py:191 -- Divided 26100 rollout sequences, each of length 1, among 1 devices.
Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.406491672694397
  episode_reward_mean: -15.872879077119684
  episode_reward_min: -48.90769458353401
  episodes_this_iter: 174
  episodes_total: 174
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4585.834
    learner:
      default_policy:
        cur_kl_coeff: 1.0
        cur_lr: 9.999999747378752e-05
        entropy: 2.8376641273498535
        kl: 0.0013740068534389138
        policy_loss: -0.0029728407971560955
        total_loss: 92.54314422607422
        vf_explained_var: 0.10643844306468964
        vf_loss: 92.54475402832031
    load_time_ms: 20.144
    num_steps_sampled: 26100
    num_steps_trained: 26000
    sample_time_ms: 2377.384
    update_time_ms: 184.427
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.1207054366846882
    mean_inference_ms: 1.1563929487426594
    mean_processing_ms: 0.8715568126250428
  time_since_restore: 7.202416896820068
  time_this_iter_s: 7.202416896820068
  time_total_s: 7.202416896820068
  timestamp: 1563927061
  timesteps_since_restore: 26100
  timesteps_this_iter: 26100
  timesteps_total: 26100
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 7 s, 1 iter, 26100 ts, -15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.442741297005892
  episode_reward_mean: -13.224595219844032
  episode_reward_min: -47.36249468310105
  episodes_this_iter: 174
  episodes_total: 348
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4426.347
    learner:
      default_policy:
        cur_kl_coeff: 0.5
        cur_lr: 9.999999747378752e-05
        entropy: 2.8300271034240723
        kl: 0.0026764869689941406
        policy_loss: -0.004095221869647503
        total_loss: 71.99154663085938
        vf_explained_var: 0.28167012333869934
        vf_loss: 71.99430847167969
    load_time_ms: 10.669
    num_steps_sampled: 52200
    num_steps_trained: 52000
    sample_time_ms: 2236.408
    update_time_ms: 94.155
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0926304551590535
    mean_inference_ms: 1.1396124580063287
    mean_processing_ms: 0.8795717534867429
  time_since_restore: 13.587757349014282
  time_this_iter_s: 6.385340452194214
  time_total_s: 13.587757349014282
  timestamp: 1563927068
  timesteps_since_restore: 52200
  timesteps_this_iter: 26100
  timesteps_total: 52200
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 13 s, 2 iter, 52200 ts, -13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 8.79263595891105
  episode_reward_mean: -13.56330956002159
  episode_reward_min: -39.88296754061847
  episodes_this_iter: 174
  episodes_total: 522
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4277.126
    learner:
      default_policy:
        cur_kl_coeff: 0.25
        cur_lr: 9.999999747378752e-05
        entropy: 2.827314853668213
        kl: 0.0048902384005486965
        policy_loss: -0.005218158476054668
        total_loss: 62.960289001464844
        vf_explained_var: 0.34910979866981506
        vf_loss: 62.964290618896484
    load_time_ms: 7.38
    num_steps_sampled: 78300
    num_steps_trained: 78000
    sample_time_ms: 2201.108
    update_time_ms: 64.072
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0795893268622692
    mean_inference_ms: 1.1280110883787666
    mean_processing_ms: 0.8779484980249995
  time_since_restore: 19.71758460998535
  time_this_iter_s: 6.129827260971069
  time_total_s: 19.71758460998535
  timestamp: 1563927074
  timesteps_since_restore: 78300
  timesteps_this_iter: 26100
  timesteps_total: 78300
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 19 s, 3 iter, 78300 ts, -13.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.439936854323093
  episode_reward_mean: -12.126713495906126
  episode_reward_min: -47.29518598430967
  episodes_this_iter: 174
  episodes_total: 696
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4204.416
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8139092922210693
        kl: 0.008299597539007664
        policy_loss: -0.006995540577918291
        total_loss: 60.444068908691406
        vf_explained_var: 0.3692808449268341
        vf_loss: 60.45002746582031
    load_time_ms: 5.735
    num_steps_sampled: 104400
    num_steps_trained: 104000
    sample_time_ms: 2195.299
    update_time_ms: 48.991
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0750752678437216
    mean_inference_ms: 1.1258238161302185
    mean_processing_ms: 0.8806259177012096
  time_since_restore: 25.902968883514404
  time_this_iter_s: 6.185384273529053
  time_total_s: 25.902968883514404
  timestamp: 1563927080
  timesteps_since_restore: 104400
  timesteps_this_iter: 26100
  timesteps_total: 104400
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 25 s, 4 iter, 104400 ts, -12.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 22.992742151500437
  episode_reward_mean: -11.44841230649485
  episode_reward_min: -40.26268441793585
  episodes_this_iter: 174
  episodes_total: 870
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4317.079
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8132829666137695
        kl: 0.006651615723967552
        policy_loss: -0.006249356083571911
        total_loss: 53.06045913696289
        vf_explained_var: 0.39631932973861694
        vf_loss: 53.06587219238281
    load_time_ms: 4.738
    num_steps_sampled: 130500
    num_steps_trained: 130000
    sample_time_ms: 2188.374
    update_time_ms: 39.908
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0713352262909719
    mean_inference_ms: 1.1212486676523954
    mean_processing_ms: 0.881417485198736
  time_since_restore: 32.85392785072327
  time_this_iter_s: 6.950958967208862
  time_total_s: 32.85392785072327
  timestamp: 1563927087
  timesteps_since_restore: 130500
  timesteps_this_iter: 26100
  timesteps_total: 130500
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 32 s, 5 iter, 130500 ts, -11.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.931478494480057
  episode_reward_mean: -10.861249645585845
  episode_reward_min: -51.456207821819575
  episodes_this_iter: 174
  episodes_total: 1044
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4495.732
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.8024911880493164
        kl: 0.00657282117754221
        policy_loss: -0.007144623436033726
        total_loss: 50.66514205932617
        vf_explained_var: 0.43921521306037903
        vf_loss: 50.67146301269531
    load_time_ms: 4.086
    num_steps_sampled: 156600
    num_steps_trained: 156000
    sample_time_ms: 2181.826
    update_time_ms: 34.026
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0693698600960988
    mean_inference_ms: 1.1187779301180778
    mean_processing_ms: 0.8824674538340698
  time_since_restore: 40.41914343833923
  time_this_iter_s: 7.565215587615967
  time_total_s: 40.41914343833923
  timestamp: 1563927095
  timesteps_since_restore: 156600
  timesteps_this_iter: 26100
  timesteps_total: 156600
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 40 s, 6 iter, 156600 ts, -10.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.110081786021293
  episode_reward_mean: -8.141717499911772
  episode_reward_min: -38.58432858777562
  episodes_this_iter: 174
  episodes_total: 1218
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4534.661
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.791825294494629
        kl: 0.007157139014452696
        policy_loss: -0.007802980486303568
        total_loss: 39.48938751220703
        vf_explained_var: 0.4559236168861389
        vf_loss: 39.49629592895508
    load_time_ms: 3.621
    num_steps_sampled: 182700
    num_steps_trained: 182000
    sample_time_ms: 2162.28
    update_time_ms: 29.918
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0673439076207183
    mean_inference_ms: 1.1187515087619753
    mean_processing_ms: 0.8803645318857004
  time_since_restore: 47.25689625740051
  time_this_iter_s: 6.837752819061279
  time_total_s: 47.25689625740051
  timestamp: 1563927101
  timesteps_since_restore: 182700
  timesteps_this_iter: 26100
  timesteps_total: 182700
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 47 s, 7 iter, 182700 ts, -8.14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.436524707486733
  episode_reward_mean: -6.137958578422529
  episode_reward_min: -33.114333507998424
  episodes_this_iter: 174
  episodes_total: 1392
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4500.988
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7875266075134277
        kl: 0.007408935111016035
        policy_loss: -0.008205367252230644
        total_loss: 39.971412658691406
        vf_explained_var: 0.3780738413333893
        vf_loss: 39.97869110107422
    load_time_ms: 3.26
    num_steps_sampled: 208800
    num_steps_trained: 208000
    sample_time_ms: 2162.878
    update_time_ms: 26.704
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0625022026244615
    mean_inference_ms: 1.1149034226484886
    mean_processing_ms: 0.8777936692450236
  time_since_restore: 53.71264958381653
  time_this_iter_s: 6.455753326416016
  time_total_s: 53.71264958381653
  timestamp: 1563927108
  timesteps_since_restore: 208800
  timesteps_this_iter: 26100
  timesteps_total: 208800
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 53 s, 8 iter, 208800 ts, -6.14 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-11-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.635053221635385
  episode_reward_mean: -5.773553735058918
  episode_reward_min: -39.687116951022816
  episodes_this_iter: 174
  episodes_total: 1566
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4576.284
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.779054880142212
        kl: 0.00761399744078517
        policy_loss: -0.008309494704008102
        total_loss: 28.439708709716797
        vf_explained_var: 0.5041689276695251
        vf_loss: 28.44706916809082
    load_time_ms: 2.983
    num_steps_sampled: 234900
    num_steps_trained: 234000
    sample_time_ms: 2153.506
    update_time_ms: 24.169
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0626327721991031
    mean_inference_ms: 1.115992231975523
    mean_processing_ms: 0.8782618138878757
  time_since_restore: 60.99515199661255
  time_this_iter_s: 7.2825024127960205
  time_total_s: 60.99515199661255
  timestamp: 1563927115
  timesteps_since_restore: 234900
  timesteps_this_iter: 26100
  timesteps_total: 234900
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 60 s, 9 iter, 234900 ts, -5.77 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.986589986406802
  episode_reward_mean: -3.888441716030782
  episode_reward_min: -38.86495072941012
  episodes_this_iter: 174
  episodes_total: 1740
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4560.338
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7760119438171387
        kl: 0.00779541814699769
        policy_loss: -0.009715928696095943
        total_loss: 26.42418098449707
        vf_explained_var: 0.49975886940956116
        vf_loss: 26.432920455932617
    load_time_ms: 2.763
    num_steps_sampled: 261000
    num_steps_trained: 260000
    sample_time_ms: 2155.544
    update_time_ms: 22.205
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0605615857517272
    mean_inference_ms: 1.1141349203901865
    mean_processing_ms: 0.8784588312957887
  time_since_restore: 67.60871005058289
  time_this_iter_s: 6.613558053970337
  time_total_s: 67.60871005058289
  timestamp: 1563927122
  timesteps_since_restore: 261000
  timesteps_this_iter: 26100
  timesteps_total: 261000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 67 s, 10 iter, 261000 ts, -3.89 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.939728908887712
  episode_reward_mean: -1.4350740881513495
  episode_reward_min: -30.638895722844737
  episodes_this_iter: 174
  episodes_total: 1914
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4641.458
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7637290954589844
        kl: 0.008592024445533752
        policy_loss: -0.009005112573504448
        total_loss: 21.73089599609375
        vf_explained_var: 0.555618166923523
        vf_loss: 21.738828659057617
    load_time_ms: 0.827
    num_steps_sampled: 287100
    num_steps_trained: 286000
    sample_time_ms: 2129.031
    update_time_ms: 4.136
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0617887478799355
    mean_inference_ms: 1.1148616855304638
    mean_processing_ms: 0.8802049013050385
  time_since_restore: 75.14257550239563
  time_this_iter_s: 7.533865451812744
  time_total_s: 75.14257550239563
  timestamp: 1563927129
  timesteps_since_restore: 287100
  timesteps_this_iter: 26100
  timesteps_total: 287100
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 75 s, 11 iter, 287100 ts, -1.44 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.837465896619516
  episode_reward_mean: 0.6243751072374688
  episode_reward_min: -25.00795533226387
  episodes_this_iter: 174
  episodes_total: 2088
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4754.512
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7520151138305664
        kl: 0.007348419167101383
        policy_loss: -0.008367331698536873
        total_loss: 18.5577392578125
        vf_explained_var: 0.6303539872169495
        vf_loss: 18.565189361572266
    load_time_ms: 0.789
    num_steps_sampled: 313200
    num_steps_trained: 312000
    sample_time_ms: 2134.461
    update_time_ms: 4.251
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0591990460976433
    mean_inference_ms: 1.1131277294270887
    mean_processing_ms: 0.8782488252594628
  time_since_restore: 82.71694374084473
  time_this_iter_s: 7.574368238449097
  time_total_s: 82.71694374084473
  timestamp: 1563927137
  timesteps_since_restore: 313200
  timesteps_this_iter: 26100
  timesteps_total: 313200
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 82 s, 12 iter, 313200 ts, 0.624 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.920968576777145
  episode_reward_mean: 0.6370397644806837
  episode_reward_min: -23.307816919356537
  episodes_this_iter: 174
  episodes_total: 2262
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4897.246
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.746802806854248
        kl: 0.007921604439616203
        policy_loss: -0.00822716299444437
        total_loss: 18.61115264892578
        vf_explained_var: 0.6143680810928345
        vf_loss: 18.618389129638672
    load_time_ms: 0.788
    num_steps_sampled: 339300
    num_steps_trained: 338000
    sample_time_ms: 2132.278
    update_time_ms: 4.371
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0574320379649753
    mean_inference_ms: 1.1138629922805248
    mean_processing_ms: 0.876930577578023
  time_since_restore: 90.25888013839722
  time_this_iter_s: 7.54193639755249
  time_total_s: 90.25888013839722
  timestamp: 1563927145
  timesteps_since_restore: 339300
  timesteps_this_iter: 26100
  timesteps_total: 339300
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 90 s, 13 iter, 339300 ts, 0.637 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.905291751989875
  episode_reward_mean: 2.609070438604322
  episode_reward_min: -36.56334908236235
  episodes_this_iter: 174
  episodes_total: 2436
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5038.688
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.734588146209717
        kl: 0.00812950823456049
        policy_loss: -0.009569864720106125
        total_loss: 16.92888832092285
        vf_explained_var: 0.6531175374984741
        vf_loss: 16.937442779541016
    load_time_ms: 0.792
    num_steps_sampled: 365400
    num_steps_trained: 364000
    sample_time_ms: 2128.627
    update_time_ms: 4.476
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.056032166362815
    mean_inference_ms: 1.112075083215327
    mean_processing_ms: 0.8764410353466019
  time_since_restore: 97.82717108726501
  time_this_iter_s: 7.568290948867798
  time_total_s: 97.82717108726501
  timestamp: 1563927152
  timesteps_since_restore: 365400
  timesteps_this_iter: 26100
  timesteps_total: 365400
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 97 s, 14 iter, 365400 ts, 2.61 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 26.060031990897265
  episode_reward_mean: 2.418215289617856
  episode_reward_min: -32.11861031605757
  episodes_this_iter: 174
  episodes_total: 2610
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4961.015
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7154698371887207
        kl: 0.008151236921548843
        policy_loss: -0.009867825545370579
        total_loss: 15.171622276306152
        vf_explained_var: 0.7063592672348022
        vf_loss: 15.180474281311035
    load_time_ms: 0.796
    num_steps_sampled: 391500
    num_steps_trained: 390000
    sample_time_ms: 2121.392
    update_time_ms: 4.672
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0569638228462512
    mean_inference_ms: 1.112977640669829
    mean_processing_ms: 0.8782810722314054
  time_since_restore: 103.92947196960449
  time_this_iter_s: 6.1023008823394775
  time_total_s: 103.92947196960449
  timestamp: 1563927158
  timesteps_since_restore: 391500
  timesteps_this_iter: 26100
  timesteps_total: 391500
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 103 s, 15 iter, 391500 ts, 2.42 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.05228263992889
  episode_reward_mean: 4.287211951114382
  episode_reward_min: -26.705761495937985
  episodes_this_iter: 174
  episodes_total: 2784
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4818.599
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.7052230834960938
        kl: 0.008390898816287518
        policy_loss: -0.011761471629142761
        total_loss: 14.477858543395996
        vf_explained_var: 0.6709773540496826
        vf_loss: 14.488570213317871
    load_time_ms: 0.786
    num_steps_sampled: 417600
    num_steps_trained: 416000
    sample_time_ms: 2117.234
    update_time_ms: 4.534
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.056691656369742
    mean_inference_ms: 1.1132000216860443
    mean_processing_ms: 0.8793528792119167
  time_since_restore: 110.0222659111023
  time_this_iter_s: 6.092793941497803
  time_total_s: 110.0222659111023
  timestamp: 1563927164
  timesteps_since_restore: 417600
  timesteps_this_iter: 26100
  timesteps_total: 417600
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 110 s, 16 iter, 417600 ts, 4.29 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.128517132961367
  episode_reward_mean: 5.962319643193922
  episode_reward_min: -15.693722859946515
  episodes_this_iter: 174
  episodes_total: 2958
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4768.689
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6956281661987305
        kl: 0.007689809426665306
        policy_loss: -0.009971773251891136
        total_loss: 9.258825302124023
        vf_explained_var: 0.7618483304977417
        vf_loss: 9.26783561706543
    load_time_ms: 0.805
    num_steps_sampled: 443700
    num_steps_trained: 442000
    sample_time_ms: 2127.404
    update_time_ms: 4.342
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0555458926426826
    mean_inference_ms: 1.1105353828763462
    mean_processing_ms: 0.8785998069463978
  time_since_restore: 116.4600841999054
  time_this_iter_s: 6.437818288803101
  time_total_s: 116.4600841999054
  timestamp: 1563927171
  timesteps_since_restore: 443700
  timesteps_this_iter: 26100
  timesteps_total: 443700
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 116 s, 17 iter, 443700 ts, 5.96 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-12-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.33843906894332
  episode_reward_mean: 5.245416676136149
  episode_reward_min: -28.715179705213863
  episodes_this_iter: 174
  episodes_total: 3132
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4882.798
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.685011386871338
        kl: 0.00875348411500454
        policy_loss: -0.010824405588209629
        total_loss: 7.433985710144043
        vf_explained_var: 0.8053551912307739
        vf_loss: 7.443716049194336
    load_time_ms: 0.805
    num_steps_sampled: 469800
    num_steps_trained: 468000
    sample_time_ms: 2121.513
    update_time_ms: 4.338
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0546656534652268
    mean_inference_ms: 1.1110274379539153
    mean_processing_ms: 0.8779988153639317
  time_since_restore: 124.00033044815063
  time_this_iter_s: 7.540246248245239
  time_total_s: 124.00033044815063
  timestamp: 1563927178
  timesteps_since_restore: 469800
  timesteps_this_iter: 26100
  timesteps_total: 469800
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 124 s, 18 iter, 469800 ts, 5.25 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.67040029196508
  episode_reward_mean: 7.066022495674257
  episode_reward_min: -17.633228139892733
  episodes_this_iter: 174
  episodes_total: 3306
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4907.932
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6718485355377197
        kl: 0.0069650462828576565
        policy_loss: -0.009049157612025738
        total_loss: 6.391079902648926
        vf_explained_var: 0.8346753716468811
        vf_loss: 6.399258613586426
    load_time_ms: 0.807
    num_steps_sampled: 495900
    num_steps_trained: 494000
    sample_time_ms: 2127.574
    update_time_ms: 4.456
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0524666904066964
    mean_inference_ms: 1.109836615574373
    mean_processing_ms: 0.8772165187735788
  time_since_restore: 131.5970733165741
  time_this_iter_s: 7.596742868423462
  time_total_s: 131.5970733165741
  timestamp: 1563927186
  timesteps_since_restore: 495900
  timesteps_this_iter: 26100
  timesteps_total: 495900
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 131 s, 19 iter, 495900 ts, 7.07 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 28.868708044472243
  episode_reward_mean: 6.707278682928717
  episode_reward_min: -14.150119434904632
  episodes_this_iter: 174
  episodes_total: 3480
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4865.971
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.66450834274292
        kl: 0.009280175901949406
        policy_loss: -0.010635829530656338
        total_loss: 4.3619794845581055
        vf_explained_var: 0.8679757714271545
        vf_loss: 4.371455669403076
    load_time_ms: 0.807
    num_steps_sampled: 522000
    num_steps_trained: 520000
    sample_time_ms: 2124.05
    update_time_ms: 4.502
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0523685144285795
    mean_inference_ms: 1.1106231634763126
    mean_processing_ms: 0.8774368709299596
  time_since_restore: 137.7554452419281
  time_this_iter_s: 6.158371925354004
  time_total_s: 137.7554452419281
  timestamp: 1563927192
  timesteps_since_restore: 522000
  timesteps_this_iter: 26100
  timesteps_total: 522000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 137 s, 20 iter, 522000 ts, 6.71 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.932793358863258
  episode_reward_mean: 8.002503113176266
  episode_reward_min: -14.772032767666452
  episodes_this_iter: 174
  episodes_total: 3654
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4785.795
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6511852741241455
        kl: 0.00977486465126276
        policy_loss: -0.011898274533450603
        total_loss: 5.099331378936768
        vf_explained_var: 0.8724510669708252
        vf_loss: 5.1100077629089355
    load_time_ms: 0.81
    num_steps_sampled: 548100
    num_steps_trained: 546000
    sample_time_ms: 2125.404
    update_time_ms: 4.501
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0532079815623745
    mean_inference_ms: 1.1112219111256605
    mean_processing_ms: 0.8790487330706566
  time_since_restore: 144.4996247291565
  time_this_iter_s: 6.7441794872283936
  time_total_s: 144.4996247291565
  timestamp: 1563927199
  timesteps_since_restore: 548100
  timesteps_this_iter: 26100
  timesteps_total: 548100
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 144 s, 21 iter, 548100 ts, 8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.20040021413814
  episode_reward_mean: 9.516539991784946
  episode_reward_min: -10.854093802599978
  episodes_this_iter: 174
  episodes_total: 3828
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4643.807
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6353936195373535
        kl: 0.008881459012627602
        policy_loss: -0.011065738275647163
        total_loss: 3.998401165008545
        vf_explained_var: 0.8917300701141357
        vf_loss: 4.008357048034668
    load_time_ms: 0.807
    num_steps_sampled: 574200
    num_steps_trained: 572000
    sample_time_ms: 2128.09
    update_time_ms: 4.448
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0516362938126853
    mean_inference_ms: 1.109692220477663
    mean_processing_ms: 0.8774818928618737
  time_since_restore: 150.67583632469177
  time_this_iter_s: 6.176211595535278
  time_total_s: 150.67583632469177
  timestamp: 1563927205
  timesteps_since_restore: 574200
  timesteps_this_iter: 26100
  timesteps_total: 574200
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 150 s, 22 iter, 574200 ts, 9.52 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.463022139844163
  episode_reward_mean: 8.818163609860006
  episode_reward_min: -18.988961681553583
  episodes_this_iter: 174
  episodes_total: 4002
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4643.94
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.6181702613830566
        kl: 0.007804708555340767
        policy_loss: -0.010725812055170536
        total_loss: 5.010249137878418
        vf_explained_var: 0.8704316020011902
        vf_loss: 5.0199995040893555
    load_time_ms: 0.81
    num_steps_sampled: 600300
    num_steps_trained: 598000
    sample_time_ms: 2125.822
    update_time_ms: 4.348
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0523659787339865
    mean_inference_ms: 1.110888362392951
    mean_processing_ms: 0.8778247082242016
  time_since_restore: 158.19459176063538
  time_this_iter_s: 7.5187554359436035
  time_total_s: 158.19459176063538
  timestamp: 1563927213
  timesteps_since_restore: 600300
  timesteps_this_iter: 26100
  timesteps_total: 600300
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 158 s, 23 iter, 600300 ts, 8.82 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.710456375634095
  episode_reward_mean: 9.612909260674181
  episode_reward_min: -13.559838631582576
  episodes_this_iter: 174
  episodes_total: 4176
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4630.284
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5994479656219482
        kl: 0.010090518742799759
        policy_loss: -0.013187935575842857
        total_loss: 3.7501261234283447
        vf_explained_var: 0.8986988067626953
        vf_loss: 3.7620527744293213
    load_time_ms: 0.802
    num_steps_sampled: 626400
    num_steps_trained: 624000
    sample_time_ms: 2128.813
    update_time_ms: 4.374
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0517435165243412
    mean_inference_ms: 1.1101730201926727
    mean_processing_ms: 0.8776625817844
  time_since_restore: 165.65387201309204
  time_this_iter_s: 7.459280252456665
  time_total_s: 165.65387201309204
  timestamp: 1563927220
  timesteps_since_restore: 626400
  timesteps_this_iter: 26100
  timesteps_total: 626400
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 165 s, 24 iter, 626400 ts, 9.61 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.246008462352822
  episode_reward_mean: 9.657623295282887
  episode_reward_min: -8.811641353591675
  episodes_this_iter: 174
  episodes_total: 4350
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4774.96
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5866949558258057
        kl: 0.00861135683953762
        policy_loss: -0.010583536699414253
        total_loss: 2.2469098567962646
        vf_explained_var: 0.9306491613388062
        vf_loss: 2.2564172744750977
    load_time_ms: 0.797
    num_steps_sampled: 652500
    num_steps_trained: 650000
    sample_time_ms: 2126.31
    update_time_ms: 4.273
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0511493040437887
    mean_inference_ms: 1.1106181119739174
    mean_processing_ms: 0.8786658876246676
  time_since_restore: 173.1805076599121
  time_this_iter_s: 7.526635646820068
  time_total_s: 173.1805076599121
  timestamp: 1563927228
  timesteps_since_restore: 652500
  timesteps_this_iter: 26100
  timesteps_total: 652500
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 173 s, 25 iter, 652500 ts, 9.66 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-13-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.382996878019327
  episode_reward_mean: 11.07043273360967
  episode_reward_min: -8.536357341322368
  episodes_this_iter: 174
  episodes_total: 4524
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4776.296
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.574476957321167
        kl: 0.009090474806725979
        policy_loss: -0.01325593888759613
        total_loss: 2.2653446197509766
        vf_explained_var: 0.9381533265113831
        vf_loss: 2.2774643898010254
    load_time_ms: 0.8
    num_steps_sampled: 678600
    num_steps_trained: 676000
    sample_time_ms: 2129.183
    update_time_ms: 4.379
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.051251382600186
    mean_inference_ms: 1.110943715672401
    mean_processing_ms: 0.8787984831437854
  time_since_restore: 179.3164827823639
  time_this_iter_s: 6.135975122451782
  time_total_s: 179.3164827823639
  timestamp: 1563927234
  timesteps_since_restore: 678600
  timesteps_this_iter: 26100
  timesteps_total: 678600
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 179 s, 26 iter, 678600 ts, 11.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.19179698472543
  episode_reward_mean: 11.830053701870781
  episode_reward_min: -7.677169745646075
  episodes_this_iter: 174
  episodes_total: 4698
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4891.915
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.559476375579834
        kl: 0.009061692282557487
        policy_loss: -0.011522899381816387
        total_loss: 1.842666506767273
        vf_explained_var: 0.949282705783844
        vf_loss: 1.8530564308166504
    load_time_ms: 0.773
    num_steps_sampled: 704700
    num_steps_trained: 702000
    sample_time_ms: 2126.318
    update_time_ms: 4.435
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0528522461810006
    mean_inference_ms: 1.111952822288668
    mean_processing_ms: 0.8808150368411747
  time_since_restore: 186.8850862979889
  time_this_iter_s: 7.568603515625
  time_total_s: 186.8850862979889
  timestamp: 1563927241
  timesteps_since_restore: 704700
  timesteps_this_iter: 26100
  timesteps_total: 704700
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 186 s, 27 iter, 704700 ts, 11.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.863575648099836
  episode_reward_mean: 10.266157494905302
  episode_reward_min: -10.289140527211035
  episodes_this_iter: 174
  episodes_total: 4872
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4867.925
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.5443692207336426
        kl: 0.008571864105761051
        policy_loss: -0.011845321394503117
        total_loss: 1.9740592241287231
        vf_explained_var: 0.9399235844612122
        vf_loss: 1.9848332405090332
    load_time_ms: 0.772
    num_steps_sampled: 730800
    num_steps_trained: 728000
    sample_time_ms: 2129.858
    update_time_ms: 4.501
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0523127390411655
    mean_inference_ms: 1.1114692691606123
    mean_processing_ms: 0.8801521832844923
  time_since_restore: 194.22075653076172
  time_this_iter_s: 7.335670232772827
  time_total_s: 194.22075653076172
  timestamp: 1563927249
  timesteps_since_restore: 730800
  timesteps_this_iter: 26100
  timesteps_total: 730800
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 194 s, 28 iter, 730800 ts, 10.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.860414004889535
  episode_reward_mean: 11.90635031491114
  episode_reward_min: -6.838743199606232
  episodes_this_iter: 174
  episodes_total: 5046
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4768.574
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.531036615371704
        kl: 0.009273101575672626
        policy_loss: -0.012243772856891155
        total_loss: 1.493693470954895
        vf_explained_var: 0.9591843485832214
        vf_loss: 1.5047780275344849
    load_time_ms: 0.767
    num_steps_sampled: 756900
    num_steps_trained: 754000
    sample_time_ms: 2123.948
    update_time_ms: 4.454
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0508459066706708
    mean_inference_ms: 1.1100091427499936
    mean_processing_ms: 0.8777901846555828
  time_since_restore: 200.7597644329071
  time_this_iter_s: 6.539007902145386
  time_total_s: 200.7597644329071
  timestamp: 1563927255
  timesteps_since_restore: 756900
  timesteps_this_iter: 26100
  timesteps_total: 756900
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 200 s, 29 iter, 756900 ts, 11.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.385196291367436
  episode_reward_mean: 11.473913431119326
  episode_reward_min: -9.100940645812058
  episodes_this_iter: 174
  episodes_total: 5220
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4883.239
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.513284921646118
        kl: 0.009212285280227661
        policy_loss: -0.010649546049535275
        total_loss: 1.3307710886001587
        vf_explained_var: 0.9613930583000183
        vf_loss: 1.3402689695358276
    load_time_ms: 0.772
    num_steps_sampled: 783000
    num_steps_trained: 780000
    sample_time_ms: 2130.206
    update_time_ms: 4.372
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0524755155921288
    mean_inference_ms: 1.1111108185787208
    mean_processing_ms: 0.8796846740879605
  time_since_restore: 208.12991499900818
  time_this_iter_s: 7.370150566101074
  time_total_s: 208.12991499900818
  timestamp: 1563927263
  timesteps_since_restore: 783000
  timesteps_this_iter: 26100
  timesteps_total: 783000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 208 s, 30 iter, 783000 ts, 11.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.65571119266752
  episode_reward_mean: 10.451685073085764
  episode_reward_min: -12.949335227941742
  episodes_this_iter: 174
  episodes_total: 5394
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4959.554
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4984688758850098
        kl: 0.009640919044613838
        policy_loss: -0.012378731742501259
        total_loss: 1.3580288887023926
        vf_explained_var: 0.9549494981765747
        vf_loss: 1.3692023754119873
    load_time_ms: 0.773
    num_steps_sampled: 809100
    num_steps_trained: 806000
    sample_time_ms: 2122.632
    update_time_ms: 4.454
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0497535120234134
    mean_inference_ms: 1.1092501008286841
    mean_processing_ms: 0.8776372619853121
  time_since_restore: 215.5731179714203
  time_this_iter_s: 7.443202972412109
  time_total_s: 215.5731179714203
  timestamp: 1563927270
  timesteps_since_restore: 809100
  timesteps_this_iter: 26100
  timesteps_total: 809100
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 215 s, 31 iter, 809100 ts, 10.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.613627970114024
  episode_reward_mean: 11.840616698504713
  episode_reward_min: -7.907905368810492
  episodes_this_iter: 174
  episodes_total: 5568
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4958.908
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.483550548553467
        kl: 0.008640036918222904
        policy_loss: -0.010940204374492168
        total_loss: 1.06260347366333
        vf_explained_var: 0.9685486555099487
        vf_loss: 1.0724637508392334
    load_time_ms: 0.773
    num_steps_sampled: 835200
    num_steps_trained: 832000
    sample_time_ms: 2121.89
    update_time_ms: 4.444
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0504314835964683
    mean_inference_ms: 1.1095852993918909
    mean_processing_ms: 0.878450513961024
  time_since_restore: 221.73567628860474
  time_this_iter_s: 6.162558317184448
  time_total_s: 221.73567628860474
  timestamp: 1563927276
  timesteps_since_restore: 835200
  timesteps_this_iter: 26100
  timesteps_total: 835200
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 221 s, 32 iter, 835200 ts, 11.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.08535613073202
  episode_reward_mean: 12.965553657281717
  episode_reward_min: -12.744230008258107
  episodes_this_iter: 174
  episodes_total: 5742
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4814.773
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4763357639312744
        kl: 0.009755163453519344
        policy_loss: -0.013678476214408875
        total_loss: 1.142722487449646
        vf_explained_var: 0.9687485098838806
        vf_loss: 1.1551815271377563
    load_time_ms: 0.763
    num_steps_sampled: 861300
    num_steps_trained: 858000
    sample_time_ms: 2124.589
    update_time_ms: 4.404
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0519185539181968
    mean_inference_ms: 1.1112301780072997
    mean_processing_ms: 0.8794780768012268
  time_since_restore: 227.8352792263031
  time_this_iter_s: 6.099602937698364
  time_total_s: 227.8352792263031
  timestamp: 1563927282
  timesteps_since_restore: 861300
  timesteps_this_iter: 26100
  timesteps_total: 861300
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 227 s, 33 iter, 861300 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.240032155078097
  episode_reward_mean: 11.259273257913387
  episode_reward_min: -6.916055540346139
  episodes_this_iter: 174
  episodes_total: 5916
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4830.114
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4570562839508057
        kl: 0.008717366494238377
        policy_loss: -0.01136725302785635
        total_loss: 0.9824486374855042
        vf_explained_var: 0.9674092531204224
        vf_loss: 0.9927263259887695
    load_time_ms: 0.767
    num_steps_sampled: 887400
    num_steps_trained: 884000
    sample_time_ms: 2120.351
    update_time_ms: 4.284
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.050997238130637
    mean_inference_ms: 1.110169336433102
    mean_processing_ms: 0.8800104034656807
  time_since_restore: 235.40736365318298
  time_this_iter_s: 7.572084426879883
  time_total_s: 235.40736365318298
  timestamp: 1563927290
  timesteps_since_restore: 887400
  timesteps_this_iter: 26100
  timesteps_total: 887400
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 235 s, 34 iter, 887400 ts, 11.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-14-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.69577869585491
  episode_reward_mean: 13.36543763178913
  episode_reward_min: -7.093751720457326
  episodes_this_iter: 174
  episodes_total: 6090
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4685.719
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4367268085479736
        kl: 0.009090196341276169
        policy_loss: -0.012652137316763401
        total_loss: 0.9662506580352783
        vf_explained_var: 0.9735845327377319
        vf_loss: 0.9777665734291077
    load_time_ms: 0.769
    num_steps_sampled: 913500
    num_steps_trained: 910000
    sample_time_ms: 2130.515
    update_time_ms: 4.344
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488410094979668
    mean_inference_ms: 1.1092138927603252
    mean_processing_ms: 0.8776277385663303
  time_since_restore: 241.58803987503052
  time_this_iter_s: 6.180676221847534
  time_total_s: 241.58803987503052
  timestamp: 1563927296
  timesteps_since_restore: 913500
  timesteps_this_iter: 26100
  timesteps_total: 913500
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 241 s, 35 iter, 913500 ts, 13.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.05334090360646
  episode_reward_mean: 13.007867381206564
  episode_reward_min: -7.539143927348744
  episodes_this_iter: 174
  episodes_total: 6264
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4749.307
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.4191877841949463
        kl: 0.00962795503437519
        policy_loss: -0.01171765848994255
        total_loss: 1.0950424671173096
        vf_explained_var: 0.9726543426513672
        vf_loss: 1.1055567264556885
    load_time_ms: 0.772
    num_steps_sampled: 939600
    num_steps_trained: 936000
    sample_time_ms: 2125.662
    update_time_ms: 4.218
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0512609102450277
    mean_inference_ms: 1.1108515448930367
    mean_processing_ms: 0.8794039994828095
  time_since_restore: 248.31177353858948
  time_this_iter_s: 6.72373366355896
  time_total_s: 248.31177353858948
  timestamp: 1563927303
  timesteps_since_restore: 939600
  timesteps_this_iter: 26100
  timesteps_total: 939600
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 248 s, 36 iter, 939600 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.134421673771385
  episode_reward_mean: 13.453916625981762
  episode_reward_min: -7.9008437073474695
  episodes_this_iter: 174
  episodes_total: 6438
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4665.31
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.405198812484741
        kl: 0.009017734788358212
        policy_loss: -0.012897877022624016
        total_loss: 0.8895242214202881
        vf_explained_var: 0.9727872014045715
        vf_loss: 0.9012948274612427
    load_time_ms: 0.776
    num_steps_sampled: 965700
    num_steps_trained: 962000
    sample_time_ms: 2120.199
    update_time_ms: 4.272
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.04959930783428
    mean_inference_ms: 1.1098620570916466
    mean_processing_ms: 0.8785881221780827
  time_since_restore: 254.98452925682068
  time_this_iter_s: 6.672755718231201
  time_total_s: 254.98452925682068
  timestamp: 1563927310
  timesteps_since_restore: 965700
  timesteps_this_iter: 26100
  timesteps_total: 965700
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 254 s, 37 iter, 965700 ts, 13.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.67356395092044
  episode_reward_mean: 12.172219862987149
  episode_reward_min: -11.55638283129456
  episodes_this_iter: 174
  episodes_total: 6612
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4577.052
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3940513134002686
        kl: 0.009070727974176407
        policy_loss: -0.011804452165961266
        total_loss: 0.8473170399665833
        vf_explained_var: 0.9741040468215942
        vf_loss: 0.8579877018928528
    load_time_ms: 0.783
    num_steps_sampled: 991800
    num_steps_trained: 988000
    sample_time_ms: 2118.451
    update_time_ms: 4.234
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0501355171442743
    mean_inference_ms: 1.1098390989942122
    mean_processing_ms: 0.8789607757134633
  time_since_restore: 261.41693353652954
  time_this_iter_s: 6.432404279708862
  time_total_s: 261.41693353652954
  timestamp: 1563927316
  timesteps_since_restore: 991800
  timesteps_this_iter: 26100
  timesteps_total: 991800
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 261 s, 38 iter, 991800 ts, 12.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.38964207810847
  episode_reward_mean: 11.724321261719506
  episode_reward_min: -6.918789454530668
  episodes_this_iter: 174
  episodes_total: 6786
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4548.12
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3715767860412598
        kl: 0.01063649170100689
        policy_loss: -0.012839148752391338
        total_loss: 0.6723580956459045
        vf_explained_var: 0.9752978086471558
        vf_loss: 0.6838675737380981
    load_time_ms: 0.785
    num_steps_sampled: 1017900
    num_steps_trained: 1014000
    sample_time_ms: 2120.125
    update_time_ms: 4.187
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.050459220911409
    mean_inference_ms: 1.1100196321472748
    mean_processing_ms: 0.8796850615669566
  time_since_restore: 267.6829483509064
  time_this_iter_s: 6.266014814376831
  time_total_s: 267.6829483509064
  timestamp: 1563927322
  timesteps_since_restore: 1017900
  timesteps_this_iter: 26100
  timesteps_total: 1017900
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 267 s, 39 iter, 1017900 ts, 11.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.02145113794164
  episode_reward_mean: 13.009388762525411
  episode_reward_min: -9.433282379752322
  episodes_this_iter: 174
  episodes_total: 6960
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4573.874
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.359320640563965
        kl: 0.010709226131439209
        policy_loss: -0.012799585238099098
        total_loss: 0.7751557230949402
        vf_explained_var: 0.9777908325195312
        vf_loss: 0.7866165637969971
    load_time_ms: 0.782
    num_steps_sampled: 1044000
    num_steps_trained: 1040000
    sample_time_ms: 2115.182
    update_time_ms: 4.143
  iterations_since_restore: 40
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490541060464889
    mean_inference_ms: 1.1086225729008774
    mean_processing_ms: 0.8784082084687235
  time_since_restore: 275.2604923248291
  time_this_iter_s: 7.5775439739227295
  time_total_s: 275.2604923248291
  timestamp: 1563927330
  timesteps_since_restore: 1044000
  timesteps_this_iter: 26100
  timesteps_total: 1044000
  training_iteration: 40
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 275 s, 40 iter, 1044000 ts, 13 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-38
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.95314885896647
  episode_reward_mean: 12.683797702532816
  episode_reward_min: -5.425068672492795
  episodes_this_iter: 174
  episodes_total: 7134
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4577.605
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.34554386138916
        kl: 0.010083505883812904
        policy_loss: -0.015308304689824581
        total_loss: 0.7168290019035339
        vf_explained_var: 0.9768403768539429
        vf_loss: 0.7308769226074219
    load_time_ms: 0.772
    num_steps_sampled: 1070100
    num_steps_trained: 1066000
    sample_time_ms: 2128.773
    update_time_ms: 4.109
  iterations_since_restore: 41
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493748646106824
    mean_inference_ms: 1.1093504050327372
    mean_processing_ms: 0.8780633811052239
  time_since_restore: 282.86671924591064
  time_this_iter_s: 7.606226921081543
  time_total_s: 282.86671924591064
  timestamp: 1563927338
  timesteps_since_restore: 1070100
  timesteps_this_iter: 26100
  timesteps_total: 1070100
  training_iteration: 41
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 282 s, 41 iter, 1070100 ts, 12.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.87772420602302
  episode_reward_mean: 12.365059267848938
  episode_reward_min: -10.376353954301802
  episodes_this_iter: 174
  episodes_total: 7308
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4593.074
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3324146270751953
        kl: 0.010370243340730667
        policy_loss: -0.012851540930569172
        total_loss: 0.6785900592803955
        vf_explained_var: 0.978122353553772
        vf_loss: 0.6901453733444214
    load_time_ms: 0.779
    num_steps_sampled: 1096200
    num_steps_trained: 1092000
    sample_time_ms: 2119.405
    update_time_ms: 4.19
  iterations_since_restore: 42
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0503925049754543
    mean_inference_ms: 1.1106108248481525
    mean_processing_ms: 0.8795121739259346
  time_since_restore: 289.09112548828125
  time_this_iter_s: 6.2244062423706055
  time_total_s: 289.09112548828125
  timestamp: 1563927344
  timesteps_since_restore: 1096200
  timesteps_this_iter: 26100
  timesteps_total: 1096200
  training_iteration: 42
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 289 s, 42 iter, 1096200 ts, 12.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.84759373847984
  episode_reward_mean: 13.855879513598653
  episode_reward_min: -7.4190901353744
  episodes_this_iter: 174
  episodes_total: 7482
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4737.042
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3166604042053223
        kl: 0.011753162369132042
        policy_loss: -0.01144191063940525
        total_loss: 0.6018440127372742
        vf_explained_var: 0.9815900325775146
        vf_loss: 0.6118167638778687
    load_time_ms: 0.779
    num_steps_sampled: 1122300
    num_steps_trained: 1118000
    sample_time_ms: 2118.967
    update_time_ms: 4.227
  iterations_since_restore: 43
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0509359354209378
    mean_inference_ms: 1.1102440527742754
    mean_processing_ms: 0.8796059508669822
  time_since_restore: 296.63091349601746
  time_this_iter_s: 7.539788007736206
  time_total_s: 296.63091349601746
  timestamp: 1563927351
  timesteps_since_restore: 1122300
  timesteps_this_iter: 26100
  timesteps_total: 1122300
  training_iteration: 43
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 296 s, 43 iter, 1122300 ts, 13.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-15-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.398792679556784
  episode_reward_mean: 13.271936690854755
  episode_reward_min: -8.256739422229977
  episodes_this_iter: 174
  episodes_total: 7656
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4734.275
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.3013806343078613
        kl: 0.011455515399575233
        policy_loss: -0.014546670950949192
        total_loss: 0.6389977931976318
        vf_explained_var: 0.9798893332481384
        vf_loss: 0.6521125435829163
    load_time_ms: 0.786
    num_steps_sampled: 1148400
    num_steps_trained: 1144000
    sample_time_ms: 2119.974
    update_time_ms: 4.29
  iterations_since_restore: 44
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488881596205997
    mean_inference_ms: 1.1086452893578465
    mean_processing_ms: 0.8779836838054459
  time_since_restore: 304.1874659061432
  time_this_iter_s: 7.556552410125732
  time_total_s: 304.1874659061432
  timestamp: 1563927359
  timesteps_since_restore: 1148400
  timesteps_this_iter: 26100
  timesteps_total: 1148400
  training_iteration: 44
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 304 s, 44 iter, 1148400 ts, 13.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.28068906493558
  episode_reward_mean: 13.118524275317647
  episode_reward_min: -6.6744389639441675
  episodes_this_iter: 174
  episodes_total: 7830
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4778.39
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.286543846130371
        kl: 0.011623849160969257
        policy_loss: -0.013500096276402473
        total_loss: 0.5217956900596619
        vf_explained_var: 0.9830237627029419
        vf_loss: 0.5338428020477295
    load_time_ms: 0.783
    num_steps_sampled: 1174500
    num_steps_trained: 1170000
    sample_time_ms: 2112.808
    update_time_ms: 4.205
  iterations_since_restore: 45
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480410667493139
    mean_inference_ms: 1.1083512221571
    mean_processing_ms: 0.8775871039708991
  time_since_restore: 310.73892164230347
  time_this_iter_s: 6.551455736160278
  time_total_s: 310.73892164230347
  timestamp: 1563927365
  timesteps_since_restore: 1174500
  timesteps_this_iter: 26100
  timesteps_total: 1174500
  training_iteration: 45
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 310 s, 45 iter, 1174500 ts, 13.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.526706666768035
  episode_reward_mean: 13.663707794581388
  episode_reward_min: -13.697289263646821
  episodes_this_iter: 174
  episodes_total: 8004
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4810.103
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.277406930923462
        kl: 0.010049315169453621
        policy_loss: -0.011517203412950039
        total_loss: 0.7223536968231201
        vf_explained_var: 0.9793888330459595
        vf_loss: 0.7326146364212036
    load_time_ms: 0.777
    num_steps_sampled: 1200600
    num_steps_trained: 1196000
    sample_time_ms: 2122.04
    update_time_ms: 4.276
  iterations_since_restore: 46
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485902247898458
    mean_inference_ms: 1.108758191060433
    mean_processing_ms: 0.8776081541279926
  time_since_restore: 317.8732261657715
  time_this_iter_s: 7.134304523468018
  time_total_s: 317.8732261657715
  timestamp: 1563927373
  timesteps_since_restore: 1200600
  timesteps_this_iter: 26100
  timesteps_total: 1200600
  training_iteration: 46
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 317 s, 46 iter, 1200600 ts, 13.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.564615184528016
  episode_reward_mean: 14.362513266527175
  episode_reward_min: -8.446374463109853
  episodes_this_iter: 174
  episodes_total: 8178
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4747.034
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2630529403686523
        kl: 0.010452562011778355
        policy_loss: -0.014161000028252602
        total_loss: 0.650981605052948
        vf_explained_var: 0.9809735417366028
        vf_loss: 0.6638360619544983
    load_time_ms: 0.782
    num_steps_sampled: 1226700
    num_steps_trained: 1222000
    sample_time_ms: 2125.552
    update_time_ms: 4.266
  iterations_since_restore: 47
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488784996780776
    mean_inference_ms: 1.109976419872291
    mean_processing_ms: 0.8785181622744411
  time_since_restore: 323.9493730068207
  time_this_iter_s: 6.076146841049194
  time_total_s: 323.9493730068207
  timestamp: 1563927379
  timesteps_since_restore: 1226700
  timesteps_this_iter: 26100
  timesteps_total: 1226700
  training_iteration: 47
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 323 s, 47 iter, 1226700 ts, 14.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.266167318931714
  episode_reward_mean: 14.33874425505061
  episode_reward_min: -7.076026905291163
  episodes_this_iter: 174
  episodes_total: 8352
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4861.943
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2548744678497314
        kl: 0.010978453792631626
        policy_loss: -0.015573689714074135
        total_loss: 0.5348090529441833
        vf_explained_var: 0.9842996597290039
        vf_loss: 0.5490103960037231
    load_time_ms: 0.78
    num_steps_sampled: 1252800
    num_steps_trained: 1248000
    sample_time_ms: 2125.398
    update_time_ms: 4.16
  iterations_since_restore: 48
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0507381337183375
    mean_inference_ms: 1.1113715447040522
    mean_processing_ms: 0.8802519151344173
  time_since_restore: 331.53123140335083
  time_this_iter_s: 7.581858396530151
  time_total_s: 331.53123140335083
  timestamp: 1563927386
  timesteps_since_restore: 1252800
  timesteps_this_iter: 26100
  timesteps_total: 1252800
  training_iteration: 48
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 331 s, 48 iter, 1252800 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 33.879395095579476
  episode_reward_mean: 15.124014176481634
  episode_reward_min: -4.575528272272384
  episodes_this_iter: 174
  episodes_total: 8526
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4846.277
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.2351064682006836
        kl: 0.01204814575612545
        policy_loss: -0.014926297590136528
        total_loss: 0.47366073727607727
        vf_explained_var: 0.9869580864906311
        vf_loss: 0.48708102107048035
    load_time_ms: 0.784
    num_steps_sampled: 1278900
    num_steps_trained: 1274000
    sample_time_ms: 2129.241
    update_time_ms: 4.205
  iterations_since_restore: 49
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.049430516538843
    mean_inference_ms: 1.1106457660048459
    mean_processing_ms: 0.8787838485997346
  time_since_restore: 337.6786906719208
  time_this_iter_s: 6.147459268569946
  time_total_s: 337.6786906719208
  timestamp: 1563927392
  timesteps_since_restore: 1278900
  timesteps_this_iter: 26100
  timesteps_total: 1278900
  training_iteration: 49
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 337 s, 49 iter, 1278900 ts, 15.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.70339797504361
  episode_reward_mean: 14.667236810347491
  episode_reward_min: -5.117564607154756
  episodes_this_iter: 174
  episodes_total: 8700
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4845.091
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.216198205947876
        kl: 0.011693034321069717
        policy_loss: -0.014792103320360184
        total_loss: 0.46746304631233215
        vf_explained_var: 0.9855921268463135
        vf_loss: 0.4807935059070587
    load_time_ms: 0.784
    num_steps_sampled: 1305000
    num_steps_trained: 1300000
    sample_time_ms: 2129.258
    update_time_ms: 4.255
  iterations_since_restore: 50
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0486481018908191
    mean_inference_ms: 1.109021337536936
    mean_processing_ms: 0.8783656183445967
  time_since_restore: 345.24636244773865
  time_this_iter_s: 7.567671775817871
  time_total_s: 345.24636244773865
  timestamp: 1563927400
  timesteps_since_restore: 1305000
  timesteps_this_iter: 26100
  timesteps_total: 1305000
  training_iteration: 50
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 345 s, 50 iter, 1305000 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.22502372107193
  episode_reward_mean: 15.302027383608847
  episode_reward_min: -8.889871696123699
  episodes_this_iter: 174
  episodes_total: 8874
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4717.147
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.203840494155884
        kl: 0.00972037110477686
        policy_loss: -0.012920111417770386
        total_loss: 0.6216385960578918
        vf_explained_var: 0.9829155206680298
        vf_loss: 0.6333436369895935
    load_time_ms: 0.788
    num_steps_sampled: 1331100
    num_steps_trained: 1326000
    sample_time_ms: 2117.065
    update_time_ms: 4.401
  iterations_since_restore: 51
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048699485150922
    mean_inference_ms: 1.1098242008809653
    mean_processing_ms: 0.8785863142580493
  time_since_restore: 351.4500298500061
  time_this_iter_s: 6.203667402267456
  time_total_s: 351.4500298500061
  timestamp: 1563927406
  timesteps_since_restore: 1331100
  timesteps_this_iter: 26100
  timesteps_total: 1331100
  training_iteration: 51
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 351 s, 51 iter, 1331100 ts, 15.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-16-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.895651869036165
  episode_reward_mean: 13.185492004084646
  episode_reward_min: -4.855529789353392
  episodes_this_iter: 174
  episodes_total: 9048
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4847.091
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.186270236968994
        kl: 0.013009165413677692
        policy_loss: -0.012661576271057129
        total_loss: 0.4779471158981323
        vf_explained_var: 0.9834398031234741
        vf_loss: 0.48898252844810486
    load_time_ms: 0.785
    num_steps_sampled: 1357200
    num_steps_trained: 1352000
    sample_time_ms: 2128.866
    update_time_ms: 4.256
  iterations_since_restore: 52
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.050838129743285
    mean_inference_ms: 1.111556371353575
    mean_processing_ms: 0.88025752715734
  time_since_restore: 359.0939290523529
  time_this_iter_s: 7.643899202346802
  time_total_s: 359.0939290523529
  timestamp: 1563927414
  timesteps_since_restore: 1357200
  timesteps_this_iter: 26100
  timesteps_total: 1357200
  training_iteration: 52
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 359 s, 52 iter, 1357200 ts, 13.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.190571342905436
  episode_reward_mean: 14.292689030645889
  episode_reward_min: -5.952840046690729
  episodes_this_iter: 174
  episodes_total: 9222
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4847.386
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1642274856567383
        kl: 0.01207038201391697
        policy_loss: -0.01545904390513897
        total_loss: 0.44491058588027954
        vf_explained_var: 0.9860217571258545
        vf_loss: 0.4588609039783478
    load_time_ms: 0.793
    num_steps_sampled: 1383300
    num_steps_trained: 1378000
    sample_time_ms: 2130.607
    update_time_ms: 4.414
  iterations_since_restore: 53
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0478935295149268
    mean_inference_ms: 1.1093856632269996
    mean_processing_ms: 0.877808360513538
  time_since_restore: 366.6562304496765
  time_this_iter_s: 7.562301397323608
  time_total_s: 366.6562304496765
  timestamp: 1563927422
  timesteps_since_restore: 1383300
  timesteps_this_iter: 26100
  timesteps_total: 1383300
  training_iteration: 53
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 366 s, 53 iter, 1383300 ts, 14.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.046796530405004
  episode_reward_mean: 15.497560407916968
  episode_reward_min: -9.288585094255055
  episodes_this_iter: 174
  episodes_total: 9396
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4721.333
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.14819073677063
        kl: 0.011819175444543362
        policy_loss: -0.015290324576199055
        total_loss: 0.4183182120323181
        vf_explained_var: 0.9883953332901001
        vf_loss: 0.4321311414241791
    load_time_ms: 0.782
    num_steps_sampled: 1409400
    num_steps_trained: 1404000
    sample_time_ms: 2127.272
    update_time_ms: 4.47
  iterations_since_restore: 54
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0492708963303956
    mean_inference_ms: 1.1099885655026818
    mean_processing_ms: 0.8784692000269461
  time_since_restore: 372.9138672351837
  time_this_iter_s: 6.257636785507202
  time_total_s: 372.9138672351837
  timestamp: 1563927428
  timesteps_since_restore: 1409400
  timesteps_this_iter: 26100
  timesteps_total: 1409400
  training_iteration: 54
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 372 s, 54 iter, 1409400 ts, 15.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.39298697519046
  episode_reward_mean: 14.840303858730916
  episode_reward_min: -5.523697006460386
  episodes_this_iter: 174
  episodes_total: 9570
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4820.151
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1384952068328857
        kl: 0.011621375568211079
        policy_loss: -0.015883268788456917
        total_loss: 0.42870840430259705
        vf_explained_var: 0.9862864017486572
        vf_loss: 0.44313904643058777
    load_time_ms: 0.784
    num_steps_sampled: 1435500
    num_steps_trained: 1430000
    sample_time_ms: 2124.635
    update_time_ms: 4.406
  iterations_since_restore: 55
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481068889214122
    mean_inference_ms: 1.1091274624286072
    mean_processing_ms: 0.8774319928614891
  time_since_restore: 380.4282400608063
  time_this_iter_s: 7.514372825622559
  time_total_s: 380.4282400608063
  timestamp: 1563927435
  timesteps_since_restore: 1435500
  timesteps_this_iter: 26100
  timesteps_total: 1435500
  training_iteration: 55
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 380 s, 55 iter, 1435500 ts, 14.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.304637526520594
  episode_reward_mean: 15.71440186778155
  episode_reward_min: -3.513391128748045
  episodes_this_iter: 174
  episodes_total: 9744
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4802.217
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.1125378608703613
        kl: 0.010518022812902927
        policy_loss: -0.014391753822565079
        total_loss: 0.3537512421607971
        vf_explained_var: 0.9893775582313538
        vf_loss: 0.36682823300361633
    load_time_ms: 0.786
    num_steps_sampled: 1461600
    num_steps_trained: 1456000
    sample_time_ms: 2127.49
    update_time_ms: 4.475
  iterations_since_restore: 56
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.050164996457464
    mean_inference_ms: 1.1115101375928957
    mean_processing_ms: 0.8799948147423574
  time_since_restore: 387.41212916374207
  time_this_iter_s: 6.983889102935791
  time_total_s: 387.41212916374207
  timestamp: 1563927442
  timesteps_since_restore: 1461600
  timesteps_this_iter: 26100
  timesteps_total: 1461600
  training_iteration: 56
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 387 s, 56 iter, 1461600 ts, 15.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.52235342673343
  episode_reward_mean: 14.457787060008176
  episode_reward_min: -5.834501087235983
  episodes_this_iter: 174
  episodes_total: 9918
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4919.379
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.093658208847046
        kl: 0.011016125790774822
        policy_loss: -0.014685387723147869
        total_loss: 0.4178805649280548
        vf_explained_var: 0.9871587157249451
        vf_loss: 0.4311889410018921
    load_time_ms: 0.778
    num_steps_sampled: 1487700
    num_steps_trained: 1482000
    sample_time_ms: 2122.747
    update_time_ms: 4.449
  iterations_since_restore: 57
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048685321392725
    mean_inference_ms: 1.1095204977244144
    mean_processing_ms: 0.8782096962862562
  time_since_restore: 394.61494612693787
  time_this_iter_s: 7.202816963195801
  time_total_s: 394.61494612693787
  timestamp: 1563927450
  timesteps_since_restore: 1487700
  timesteps_this_iter: 26100
  timesteps_total: 1487700
  training_iteration: 57
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 394 s, 57 iter, 1487700 ts, 14.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.46716114859053
  episode_reward_mean: 15.387244418404713
  episode_reward_min: -5.167819065099749
  episodes_this_iter: 174
  episodes_total: 10092
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4775.437
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.074070930480957
        kl: 0.01156225148588419
        policy_loss: -0.016678692772984505
        total_loss: 0.3671010732650757
        vf_explained_var: 0.9890996217727661
        vf_loss: 0.38233453035354614
    load_time_ms: 0.784
    num_steps_sampled: 1513800
    num_steps_trained: 1508000
    sample_time_ms: 2127.035
    update_time_ms: 4.624
  iterations_since_restore: 58
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0487963905137436
    mean_inference_ms: 1.110458438338979
    mean_processing_ms: 0.8790091535123026
  time_since_restore: 400.79830503463745
  time_this_iter_s: 6.183358907699585
  time_total_s: 400.79830503463745
  timestamp: 1563927456
  timesteps_since_restore: 1513800
  timesteps_this_iter: 26100
  timesteps_total: 1513800
  training_iteration: 58
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 400 s, 58 iter, 1513800 ts, 15.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.0117209510166
  episode_reward_mean: 16.418105699899527
  episode_reward_min: -4.045733680201906
  episodes_this_iter: 174
  episodes_total: 10266
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4818.702
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0525100231170654
        kl: 0.013792400248348713
        policy_loss: -0.01706862263381481
        total_loss: 0.32519325613975525
        vf_explained_var: 0.9912021160125732
        vf_loss: 0.34053781628608704
    load_time_ms: 0.783
    num_steps_sampled: 1539900
    num_steps_trained: 1534000
    sample_time_ms: 2122.465
    update_time_ms: 4.518
  iterations_since_restore: 59
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0499259782170252
    mean_inference_ms: 1.1110235528166725
    mean_processing_ms: 0.8799605193432916
  time_since_restore: 407.3342020511627
  time_this_iter_s: 6.5358970165252686
  time_total_s: 407.3342020511627
  timestamp: 1563927462
  timesteps_since_restore: 1539900
  timesteps_this_iter: 26100
  timesteps_total: 1539900
  training_iteration: 59
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 407 s, 59 iter, 1539900 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.00056250067382
  episode_reward_mean: 15.211535709901344
  episode_reward_min: -3.9693361205541557
  episodes_this_iter: 174
  episodes_total: 10440
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4707.312
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0337600708007812
        kl: 0.011992274783551693
        policy_loss: -0.013210556469857693
        total_loss: 0.3458153009414673
        vf_explained_var: 0.9894205927848816
        vf_loss: 0.3575267791748047
    load_time_ms: 0.784
    num_steps_sampled: 1566000
    num_steps_trained: 1560000
    sample_time_ms: 2126.17
    update_time_ms: 4.53
  iterations_since_restore: 60
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0482141291548623
    mean_inference_ms: 1.1088699806842799
    mean_processing_ms: 0.877956103445814
  time_since_restore: 413.8215193748474
  time_this_iter_s: 6.487317323684692
  time_total_s: 413.8215193748474
  timestamp: 1563927469
  timesteps_since_restore: 1566000
  timesteps_this_iter: 26100
  timesteps_total: 1566000
  training_iteration: 60
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 413 s, 60 iter, 1566000 ts, 15.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-17-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.05617777472539
  episode_reward_mean: 14.724014336796929
  episode_reward_min: -5.929883477479309
  episodes_this_iter: 174
  episodes_total: 10614
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4836.032
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.029165029525757
        kl: 0.011586272157728672
        policy_loss: -0.013902292586863041
        total_loss: 0.48617473244667053
        vf_explained_var: 0.9852965474128723
        vf_loss: 0.49862879514694214
    load_time_ms: 0.785
    num_steps_sampled: 1592100
    num_steps_trained: 1586000
    sample_time_ms: 2126.657
    update_time_ms: 4.406
  iterations_since_restore: 61
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485790742642784
    mean_inference_ms: 1.1102691036758154
    mean_processing_ms: 0.8780669248383621
  time_since_restore: 421.3200933933258
  time_this_iter_s: 7.4985740184783936
  time_total_s: 421.3200933933258
  timestamp: 1563927476
  timesteps_since_restore: 1592100
  timesteps_this_iter: 26100
  timesteps_total: 1592100
  training_iteration: 61
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 421 s, 61 iter, 1592100 ts, 14.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.141551406775584
  episode_reward_mean: 17.01404552691073
  episode_reward_min: -4.500611819402747
  episodes_this_iter: 174
  episodes_total: 10788
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4833.19
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 2.0062315464019775
        kl: 0.012557808309793472
        policy_loss: -0.015246504917740822
        total_loss: 0.358005166053772
        vf_explained_var: 0.9912804365158081
        vf_loss: 0.37168189883232117
    load_time_ms: 0.78
    num_steps_sampled: 1618200
    num_steps_trained: 1612000
    sample_time_ms: 2123.083
    update_time_ms: 4.528
  iterations_since_restore: 62
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047727291910191
    mean_inference_ms: 1.108655419018419
    mean_processing_ms: 0.8776294178222839
  time_since_restore: 428.9012906551361
  time_this_iter_s: 7.581197261810303
  time_total_s: 428.9012906551361
  timestamp: 1563927484
  timesteps_since_restore: 1618200
  timesteps_this_iter: 26100
  timesteps_total: 1618200
  training_iteration: 62
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 428 s, 62 iter, 1618200 ts, 17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.11671801465127
  episode_reward_mean: 14.881069694000459
  episode_reward_min: -4.2157616420191335
  episodes_this_iter: 174
  episodes_total: 10962
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4690.914
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9896820783615112
        kl: 0.012513279914855957
        policy_loss: -0.014789647422730923
        total_loss: 0.2706220746040344
        vf_explained_var: 0.9917651414871216
        vf_loss: 0.28384754061698914
    load_time_ms: 0.774
    num_steps_sampled: 1644300
    num_steps_trained: 1638000
    sample_time_ms: 2120.345
    update_time_ms: 4.458
  iterations_since_restore: 63
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485201049596995
    mean_inference_ms: 1.10955176569828
    mean_processing_ms: 0.8783313378710847
  time_since_restore: 435.0076072216034
  time_this_iter_s: 6.106316566467285
  time_total_s: 435.0076072216034
  timestamp: 1563927490
  timesteps_since_restore: 1644300
  timesteps_this_iter: 26100
  timesteps_total: 1644300
  training_iteration: 63
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 435 s, 63 iter, 1644300 ts, 14.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.41284299378012
  episode_reward_mean: 16.214686925515746
  episode_reward_min: -2.923750648351413
  episodes_this_iter: 174
  episodes_total: 11136
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4797.829
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9703527688980103
        kl: 0.011401111260056496
        policy_loss: -0.015885623171925545
        total_loss: 0.2652721405029297
        vf_explained_var: 0.9924132823944092
        vf_loss: 0.2797326445579529
    load_time_ms: 0.771
    num_steps_sampled: 1670400
    num_steps_trained: 1664000
    sample_time_ms: 2122.908
    update_time_ms: 4.358
  iterations_since_restore: 64
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490731058618108
    mean_inference_ms: 1.110644250850165
    mean_processing_ms: 0.8790346710636263
  time_since_restore: 442.3622486591339
  time_this_iter_s: 7.354641437530518
  time_total_s: 442.3622486591339
  timestamp: 1563927497
  timesteps_since_restore: 1670400
  timesteps_this_iter: 26100
  timesteps_total: 1670400
  training_iteration: 64
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 442 s, 64 iter, 1670400 ts, 16.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.06350659907218
  episode_reward_mean: 15.630076673066057
  episode_reward_min: -4.336659979943894
  episodes_this_iter: 174
  episodes_total: 11310
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4655.239
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9571020603179932
        kl: 0.012474654242396355
        policy_loss: -0.013939991593360901
        total_loss: 0.28670188784599304
        vf_explained_var: 0.9907722473144531
        vf_loss: 0.29908251762390137
    load_time_ms: 0.772
    num_steps_sampled: 1696500
    num_steps_trained: 1690000
    sample_time_ms: 2133.21
    update_time_ms: 4.454
  iterations_since_restore: 65
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0487490565812798
    mean_inference_ms: 1.1103344599093505
    mean_processing_ms: 0.8791403464227873
  time_since_restore: 448.5508625507355
  time_this_iter_s: 6.1886138916015625
  time_total_s: 448.5508625507355
  timestamp: 1563927504
  timesteps_since_restore: 1696500
  timesteps_this_iter: 26100
  timesteps_total: 1696500
  training_iteration: 65
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 448 s, 65 iter, 1696500 ts, 15.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.53405932849032
  episode_reward_mean: 16.186347143927907
  episode_reward_min: -4.433521976422097
  episodes_this_iter: 174
  episodes_total: 11484
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4718.319
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9356706142425537
        kl: 0.012274281121790409
        policy_loss: -0.015609283931553364
        total_loss: 0.2443508356809616
        vf_explained_var: 0.9929341077804565
        vf_loss: 0.25842583179473877
    load_time_ms: 0.779
    num_steps_sampled: 1722600
    num_steps_trained: 1716000
    sample_time_ms: 2121.699
    update_time_ms: 4.352
  iterations_since_restore: 66
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.050035608125385
    mean_inference_ms: 1.111408031259314
    mean_processing_ms: 0.8801201817682892
  time_since_restore: 456.0517177581787
  time_this_iter_s: 7.500855207443237
  time_total_s: 456.0517177581787
  timestamp: 1563927511
  timesteps_since_restore: 1722600
  timesteps_this_iter: 26100
  timesteps_total: 1722600
  training_iteration: 66
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 456 s, 66 iter, 1722600 ts, 16.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.37567321485264
  episode_reward_mean: 15.771682234151907
  episode_reward_min: -3.6945292640079748
  episodes_this_iter: 174
  episodes_total: 11658
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4604.992
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.9130055904388428
        kl: 0.013162215240299702
        policy_loss: -0.01831715740263462
        total_loss: 0.23183776438236237
        vf_explained_var: 0.9932772517204285
        vf_loss: 0.24850966036319733
    load_time_ms: 0.784
    num_steps_sampled: 1748700
    num_steps_trained: 1742000
    sample_time_ms: 2133.889
    update_time_ms: 4.433
  iterations_since_restore: 67
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0486076322279798
    mean_inference_ms: 1.1101313086087239
    mean_processing_ms: 0.8787940326720046
  time_since_restore: 462.2402606010437
  time_this_iter_s: 6.18854284286499
  time_total_s: 462.2402606010437
  timestamp: 1563927517
  timesteps_since_restore: 1748700
  timesteps_this_iter: 26100
  timesteps_total: 1748700
  training_iteration: 67
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 462 s, 67 iter, 1748700 ts, 15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.566816162822505
  episode_reward_mean: 16.780109389010658
  episode_reward_min: -3.584314325485605
  episodes_this_iter: 174
  episodes_total: 11832
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4618.494
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8976496458053589
        kl: 0.013176171109080315
        policy_loss: -0.01799648068845272
        total_loss: 0.20249639451503754
        vf_explained_var: 0.9939765930175781
        vf_loss: 0.21884585916996002
    load_time_ms: 0.774
    num_steps_sampled: 1774800
    num_steps_trained: 1768000
    sample_time_ms: 2125.962
    update_time_ms: 4.399
  iterations_since_restore: 68
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0494618739086348
    mean_inference_ms: 1.1108871414460593
    mean_processing_ms: 0.8798022738817887
  time_since_restore: 468.4787230491638
  time_this_iter_s: 6.238462448120117
  time_total_s: 468.4787230491638
  timestamp: 1563927524
  timesteps_since_restore: 1774800
  timesteps_this_iter: 26100
  timesteps_total: 1774800
  training_iteration: 68
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 468 s, 68 iter, 1774800 ts, 16.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.10978538297358
  episode_reward_mean: 15.786576998227751
  episode_reward_min: -14.00980845315862
  episodes_this_iter: 174
  episodes_total: 12006
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4718.21
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8801707029342651
        kl: 0.010751036927103996
        policy_loss: -0.01457519456744194
        total_loss: 0.29480284452438354
        vf_explained_var: 0.9919905066490173
        vf_loss: 0.30803415179252625
    load_time_ms: 0.771
    num_steps_sampled: 1800900
    num_steps_trained: 1794000
    sample_time_ms: 2136.015
    update_time_ms: 4.385
  iterations_since_restore: 69
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0478554191889415
    mean_inference_ms: 1.1090639827199198
    mean_processing_ms: 0.8781421670991574
  time_since_restore: 476.1135869026184
  time_this_iter_s: 7.63486385345459
  time_total_s: 476.1135869026184
  timestamp: 1563927531
  timesteps_since_restore: 1800900
  timesteps_this_iter: 26100
  timesteps_total: 1800900
  training_iteration: 69
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 476 s, 69 iter, 1800900 ts, 15.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-18-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.475610972410664
  episode_reward_mean: 15.887471166865776
  episode_reward_min: -6.133882990925058
  episodes_this_iter: 174
  episodes_total: 12180
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4749.343
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8743727207183838
        kl: 0.011137878522276878
        policy_loss: -0.01586494781076908
        total_loss: 0.2116081714630127
        vf_explained_var: 0.9934129118919373
        vf_loss: 0.22608087956905365
    load_time_ms: 0.769
    num_steps_sampled: 1827000
    num_steps_trained: 1820000
    sample_time_ms: 2122.761
    update_time_ms: 4.462
  iterations_since_restore: 70
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480999617511166
    mean_inference_ms: 1.1091192944442025
    mean_processing_ms: 0.8781277140923754
  time_since_restore: 482.78129172325134
  time_this_iter_s: 6.667704820632935
  time_total_s: 482.78129172325134
  timestamp: 1563927538
  timesteps_since_restore: 1827000
  timesteps_this_iter: 26100
  timesteps_total: 1827000
  training_iteration: 70
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 482 s, 70 iter, 1827000 ts, 15.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.045273830207435
  episode_reward_mean: 17.771379241916005
  episode_reward_min: -2.5934053432625186
  episodes_this_iter: 174
  episodes_total: 12354
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4752.128
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8596583604812622
        kl: 0.01234239898622036
        policy_loss: -0.016126085072755814
        total_loss: 0.1996014416217804
        vf_explained_var: 0.9944663643836975
        vf_loss: 0.2141847461462021
    load_time_ms: 0.768
    num_steps_sampled: 1853100
    num_steps_trained: 1846000
    sample_time_ms: 2137.394
    update_time_ms: 4.416
  iterations_since_restore: 71
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480212104946645
    mean_inference_ms: 1.108718385756896
    mean_processing_ms: 0.8786696960895489
  time_since_restore: 490.45400285720825
  time_this_iter_s: 7.672711133956909
  time_total_s: 490.45400285720825
  timestamp: 1563927546
  timesteps_since_restore: 1853100
  timesteps_this_iter: 26100
  timesteps_total: 1853100
  training_iteration: 71
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 490 s, 71 iter, 1853100 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.39262355521674
  episode_reward_mean: 16.966567456954184
  episode_reward_min: -2.3550490885425046
  episodes_this_iter: 174
  episodes_total: 12528
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4756.134
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.845792293548584
        kl: 0.012568452395498753
        policy_loss: -0.016781674697995186
        total_loss: 0.20407341420650482
        vf_explained_var: 0.9944028258323669
        vf_loss: 0.2192840278148651
    load_time_ms: 0.766
    num_steps_sampled: 1879200
    num_steps_trained: 1872000
    sample_time_ms: 2109.932
    update_time_ms: 4.381
  iterations_since_restore: 72
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048666457043806
    mean_inference_ms: 1.1093538534691314
    mean_processing_ms: 0.8788666950010519
  time_since_restore: 497.8001778125763
  time_this_iter_s: 7.346174955368042
  time_total_s: 497.8001778125763
  timestamp: 1563927553
  timesteps_since_restore: 1879200
  timesteps_this_iter: 26100
  timesteps_total: 1879200
  training_iteration: 72
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 497 s, 72 iter, 1879200 ts, 17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.173829177677746
  episode_reward_mean: 17.233886752291962
  episode_reward_min: -3.9431602491417803
  episodes_this_iter: 174
  episodes_total: 12702
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4755.442
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.8260862827301025
        kl: 0.012457217089831829
        policy_loss: -0.01903391443192959
        total_loss: 0.20243382453918457
        vf_explained_var: 0.9942556619644165
        vf_loss: 0.21991059184074402
    load_time_ms: 0.771
    num_steps_sampled: 1905300
    num_steps_trained: 1898000
    sample_time_ms: 2118.095
    update_time_ms: 4.4
  iterations_since_restore: 73
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481491749456258
    mean_inference_ms: 1.1099463567998413
    mean_processing_ms: 0.8788839090968725
  time_since_restore: 503.982253074646
  time_this_iter_s: 6.182075262069702
  time_total_s: 503.982253074646
  timestamp: 1563927559
  timesteps_since_restore: 1905300
  timesteps_this_iter: 26100
  timesteps_total: 1905300
  training_iteration: 73
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 503 s, 73 iter, 1905300 ts, 17.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.7068473398605
  episode_reward_mean: 17.25596006904416
  episode_reward_min: -12.90459281527962
  episodes_this_iter: 174
  episodes_total: 12876
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4775.434
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.816983938217163
        kl: 0.013122646138072014
        policy_loss: -0.017114756628870964
        total_loss: 0.19685512781143188
        vf_explained_var: 0.9946318864822388
        vf_loss: 0.21232955157756805
    load_time_ms: 0.778
    num_steps_sampled: 1931400
    num_steps_trained: 1924000
    sample_time_ms: 2121.647
    update_time_ms: 4.39
  iterations_since_restore: 74
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493467274118522
    mean_inference_ms: 1.110864028651408
    mean_processing_ms: 0.8802133858027367
  time_since_restore: 511.57442355155945
  time_this_iter_s: 7.592170476913452
  time_total_s: 511.57442355155945
  timestamp: 1563927567
  timesteps_since_restore: 1931400
  timesteps_this_iter: 26100
  timesteps_total: 1931400
  training_iteration: 74
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 511 s, 74 iter, 1931400 ts, 17.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.99404714278808
  episode_reward_mean: 16.44369674940014
  episode_reward_min: -1.8508413533196904
  episodes_this_iter: 174
  episodes_total: 13050
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4773.662
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7986537218093872
        kl: 0.012662151828408241
        policy_loss: -0.018937379121780396
        total_loss: 0.154808908700943
        vf_explained_var: 0.9953394532203674
        vf_loss: 0.17216350138187408
    load_time_ms: 0.774
    num_steps_sampled: 1957500
    num_steps_trained: 1950000
    sample_time_ms: 2115.988
    update_time_ms: 4.39
  iterations_since_restore: 75
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047764654354134
    mean_inference_ms: 1.1095751471865232
    mean_processing_ms: 0.8785193245143356
  time_since_restore: 517.6894187927246
  time_this_iter_s: 6.114995241165161
  time_total_s: 517.6894187927246
  timestamp: 1563927573
  timesteps_since_restore: 1957500
  timesteps_this_iter: 26100
  timesteps_total: 1957500
  training_iteration: 75
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 517 s, 75 iter, 1957500 ts, 16.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.55655002068368
  episode_reward_mean: 18.7619635206156
  episode_reward_min: -10.875079558497625
  episodes_this_iter: 174
  episodes_total: 13224
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4774.954
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7881560325622559
        kl: 0.013121020048856735
        policy_loss: -0.01698393188416958
        total_loss: 0.219333678483963
        vf_explained_var: 0.9947119355201721
        vf_loss: 0.2346774786710739
    load_time_ms: 0.769
    num_steps_sampled: 1983600
    num_steps_trained: 1976000
    sample_time_ms: 2118.325
    update_time_ms: 4.391
  iterations_since_restore: 76
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0491009046226325
    mean_inference_ms: 1.1108590849319533
    mean_processing_ms: 0.8798204403110619
  time_since_restore: 525.2261242866516
  time_this_iter_s: 7.536705493927002
  time_total_s: 525.2261242866516
  timestamp: 1563927580
  timesteps_since_restore: 1983600
  timesteps_this_iter: 26100
  timesteps_total: 1983600
  training_iteration: 76
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 525 s, 76 iter, 1983600 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.321945543896305
  episode_reward_mean: 17.624245487641296
  episode_reward_min: -7.374334207505989
  episodes_this_iter: 174
  episodes_total: 13398
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4833.131
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7796714305877686
        kl: 0.013288898393511772
        policy_loss: -0.01853061094880104
        total_loss: 0.18269740045070648
        vf_explained_var: 0.9948323369026184
        vf_loss: 0.19956691563129425
    load_time_ms: 0.769
    num_steps_sampled: 2009700
    num_steps_trained: 2002000
    sample_time_ms: 2119.048
    update_time_ms: 4.377
  iterations_since_restore: 77
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0483365370832773
    mean_inference_ms: 1.1100168593462691
    mean_processing_ms: 0.8792819569332423
  time_since_restore: 532.0052311420441
  time_this_iter_s: 6.779106855392456
  time_total_s: 532.0052311420441
  timestamp: 1563927587
  timesteps_since_restore: 2009700
  timesteps_this_iter: 26100
  timesteps_total: 2009700
  training_iteration: 77
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 532 s, 77 iter, 2009700 ts, 17.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-19-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.08157150075387
  episode_reward_mean: 17.34011962850002
  episode_reward_min: -14.486877565959203
  episodes_this_iter: 174
  episodes_total: 13572
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4961.798
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7562057971954346
        kl: 0.014020879752933979
        policy_loss: -0.015780877321958542
        total_loss: 0.15758320689201355
        vf_explained_var: 0.995733380317688
        vf_loss: 0.1716114729642868
    load_time_ms: 0.779
    num_steps_sampled: 2035800
    num_steps_trained: 2028000
    sample_time_ms: 2124.95
    update_time_ms: 4.376
  iterations_since_restore: 78
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0494838114818588
    mean_inference_ms: 1.110503048097016
    mean_processing_ms: 0.8799093575590059
  time_since_restore: 539.5961203575134
  time_this_iter_s: 7.59088921546936
  time_total_s: 539.5961203575134
  timestamp: 1563927595
  timesteps_since_restore: 2035800
  timesteps_this_iter: 26100
  timesteps_total: 2035800
  training_iteration: 78
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 539 s, 78 iter, 2035800 ts, 17.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.57242879807282
  episode_reward_mean: 17.430538248717617
  episode_reward_min: -3.5064691834495125
  episodes_this_iter: 174
  episodes_total: 13746
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4817.302
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.738852858543396
        kl: 0.013110573403537273
        policy_loss: -0.014698786661028862
        total_loss: 0.18100714683532715
        vf_explained_var: 0.995069146156311
        vf_loss: 0.1940671056509018
    load_time_ms: 0.78
    num_steps_sampled: 2061900
    num_steps_trained: 2054000
    sample_time_ms: 2115.069
    update_time_ms: 4.653
  iterations_since_restore: 79
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488753258641175
    mean_inference_ms: 1.1101739096396916
    mean_processing_ms: 0.8796816511926518
  time_since_restore: 545.6865510940552
  time_this_iter_s: 6.090430736541748
  time_total_s: 545.6865510940552
  timestamp: 1563927601
  timesteps_since_restore: 2061900
  timesteps_this_iter: 26100
  timesteps_total: 2061900
  training_iteration: 79
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 545 s, 79 iter, 2061900 ts, 17.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.006897605496825
  episode_reward_mean: 17.237968646491442
  episode_reward_min: -2.657779933738327
  episodes_this_iter: 174
  episodes_total: 13920
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4837.513
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7219624519348145
        kl: 0.014911347068846226
        policy_loss: -0.015282758511602879
        total_loss: 0.14001263678073883
        vf_explained_var: 0.9958928227424622
        vf_loss: 0.1534314751625061
    load_time_ms: 0.778
    num_steps_sampled: 2088000
    num_steps_trained: 2080000
    sample_time_ms: 2120.545
    update_time_ms: 4.521
  iterations_since_restore: 80
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0484102825405577
    mean_inference_ms: 1.1095153494741317
    mean_processing_ms: 0.8785894005369899
  time_since_restore: 552.6105151176453
  time_this_iter_s: 6.923964023590088
  time_total_s: 552.6105151176453
  timestamp: 1563927608
  timesteps_since_restore: 2088000
  timesteps_this_iter: 26100
  timesteps_total: 2088000
  training_iteration: 80
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 552 s, 80 iter, 2088000 ts, 17.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.424185416386095
  episode_reward_mean: 17.80917873447041
  episode_reward_min: -2.970510233877192
  episodes_this_iter: 174
  episodes_total: 14094
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4835.544
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.7065348625183105
        kl: 0.013660220429301262
        policy_loss: -0.01737990975379944
        total_loss: 0.11931789666414261
        vf_explained_var: 0.9966368079185486
        vf_loss: 0.13499028980731964
    load_time_ms: 0.775
    num_steps_sampled: 2114100
    num_steps_trained: 2106000
    sample_time_ms: 2108.036
    update_time_ms: 4.534
  iterations_since_restore: 81
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481055774472197
    mean_inference_ms: 1.1093606863186407
    mean_processing_ms: 0.878654272580135
  time_since_restore: 560.1383533477783
  time_this_iter_s: 7.527838230133057
  time_total_s: 560.1383533477783
  timestamp: 1563927615
  timesteps_since_restore: 2114100
  timesteps_this_iter: 26100
  timesteps_total: 2114100
  training_iteration: 81
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 560 s, 81 iter, 2114100 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.45871487282166
  episode_reward_mean: 17.747511878110874
  episode_reward_min: -9.772384228304164
  episodes_this_iter: 174
  episodes_total: 14268
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4690.396
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.687473177909851
        kl: 0.014778518117964268
        policy_loss: -0.01625511422753334
        total_loss: 0.1232260912656784
        vf_explained_var: 0.9963732957839966
        vf_loss: 0.13763388991355896
    load_time_ms: 0.775
    num_steps_sampled: 2140200
    num_steps_trained: 2132000
    sample_time_ms: 2133.267
    update_time_ms: 4.602
  iterations_since_restore: 82
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0499358969502763
    mean_inference_ms: 1.1111339179305735
    mean_processing_ms: 0.880780129053394
  time_since_restore: 566.2816181182861
  time_this_iter_s: 6.1432647705078125
  time_total_s: 566.2816181182861
  timestamp: 1563927622
  timesteps_since_restore: 2140200
  timesteps_this_iter: 26100
  timesteps_total: 2140200
  training_iteration: 82
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 566 s, 82 iter, 2140200 ts, 17.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.20470962206448
  episode_reward_mean: 18.459329598675392
  episode_reward_min: -2.4142860072276053
  episodes_this_iter: 174
  episodes_total: 14442
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4688.911
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6704459190368652
        kl: 0.013575087301433086
        policy_loss: -0.018962223082780838
        total_loss: 0.10104702413082123
        vf_explained_var: 0.9972482919692993
        vf_loss: 0.11831235885620117
    load_time_ms: 0.773
    num_steps_sampled: 2166300
    num_steps_trained: 2158000
    sample_time_ms: 2128.459
    update_time_ms: 4.455
  iterations_since_restore: 83
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.049845120811854
    mean_inference_ms: 1.1116281533670194
    mean_processing_ms: 0.8808274761641435
  time_since_restore: 572.3986103534698
  time_this_iter_s: 6.116992235183716
  time_total_s: 572.3986103534698
  timestamp: 1563927628
  timesteps_since_restore: 2166300
  timesteps_this_iter: 26100
  timesteps_total: 2166300
  training_iteration: 83
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 572 s, 83 iter, 2166300 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.91113420725556
  episode_reward_mean: 18.91302166311215
  episode_reward_min: -2.8652392755395635
  episodes_this_iter: 174
  episodes_total: 14616
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4687.726
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6551579236984253
        kl: 0.013400771655142307
        policy_loss: -0.01799551397562027
        total_loss: 0.09583103656768799
        vf_explained_var: 0.9973419904708862
        vf_loss: 0.11215145885944366
    load_time_ms: 0.781
    num_steps_sampled: 2192400
    num_steps_trained: 2184000
    sample_time_ms: 2127.621
    update_time_ms: 4.422
  iterations_since_restore: 84
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485707934234147
    mean_inference_ms: 1.110211834939069
    mean_processing_ms: 0.8793336169078307
  time_since_restore: 579.9681599140167
  time_this_iter_s: 7.569549560546875
  time_total_s: 579.9681599140167
  timestamp: 1563927635
  timesteps_since_restore: 2192400
  timesteps_this_iter: 26100
  timesteps_total: 2192400
  training_iteration: 84
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 579 s, 84 iter, 2192400 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.29008960899099
  episode_reward_mean: 18.482918920335248
  episode_reward_min: -2.069064288316082
  episodes_this_iter: 174
  episodes_total: 14790
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4687.425
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6358281373977661
        kl: 0.013734421692788601
        policy_loss: -0.02025488018989563
        total_loss: 0.10165007412433624
        vf_explained_var: 0.9969063401222229
        vf_loss: 0.1201881542801857
    load_time_ms: 0.786
    num_steps_sampled: 2218500
    num_steps_trained: 2210000
    sample_time_ms: 2129.075
    update_time_ms: 4.446
  iterations_since_restore: 85
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0495447962633742
    mean_inference_ms: 1.111387407413279
    mean_processing_ms: 0.8800318935060927
  time_since_restore: 586.0946378707886
  time_this_iter_s: 6.126477956771851
  time_total_s: 586.0946378707886
  timestamp: 1563927641
  timesteps_since_restore: 2218500
  timesteps_this_iter: 26100
  timesteps_total: 2218500
  training_iteration: 85
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 586 s, 85 iter, 2218500 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.94434328660994
  episode_reward_mean: 19.535286927717753
  episode_reward_min: -2.4943951428917863
  episodes_this_iter: 174
  episodes_total: 14964
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4692.031
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.6181851625442505
        kl: 0.014658169820904732
        policy_loss: -0.02116101235151291
        total_loss: 0.09023304283618927
        vf_explained_var: 0.9974369406700134
        vf_loss: 0.10956177115440369
    load_time_ms: 0.789
    num_steps_sampled: 2244600
    num_steps_trained: 2236000
    sample_time_ms: 2125.225
    update_time_ms: 4.461
  iterations_since_restore: 86
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490730720529047
    mean_inference_ms: 1.1107625651109851
    mean_processing_ms: 0.8796227005306293
  time_since_restore: 593.6408584117889
  time_this_iter_s: 7.546220541000366
  time_total_s: 593.6408584117889
  timestamp: 1563927649
  timesteps_since_restore: 2244600
  timesteps_this_iter: 26100
  timesteps_total: 2244600
  training_iteration: 86
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 593 s, 86 iter, 2244600 ts, 19.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-20-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.86724608722599
  episode_reward_mean: 16.708571061018997
  episode_reward_min: -2.1459813169078457
  episodes_this_iter: 174
  episodes_total: 15138
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4776.805
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5972298383712769
        kl: 0.015058958902955055
        policy_loss: -0.020543260499835014
        total_loss: 0.07891186326742172
        vf_explained_var: 0.9970130324363708
        vf_loss: 0.09757275879383087
    load_time_ms: 0.781
    num_steps_sampled: 2270700
    num_steps_trained: 2262000
    sample_time_ms: 2119.953
    update_time_ms: 4.492
  iterations_since_restore: 87
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0484624605640727
    mean_inference_ms: 1.1105239831830527
    mean_processing_ms: 0.8792874283338545
  time_since_restore: 601.2169988155365
  time_this_iter_s: 7.576140403747559
  time_total_s: 601.2169988155365
  timestamp: 1563927657
  timesteps_since_restore: 2270700
  timesteps_this_iter: 26100
  timesteps_total: 2270700
  training_iteration: 87
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 601 s, 87 iter, 2270700 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.39306421146383
  episode_reward_mean: 17.85679588266223
  episode_reward_min: -2.4376887146601645
  episodes_this_iter: 174
  episodes_total: 15312
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4634.775
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5782036781311035
        kl: 0.01466731633991003
        policy_loss: -0.01932419463992119
        total_loss: 0.07379066199064255
        vf_explained_var: 0.9975365996360779
        vf_loss: 0.09128144383430481
    load_time_ms: 0.782
    num_steps_sampled: 2296800
    num_steps_trained: 2288000
    sample_time_ms: 2119.322
    update_time_ms: 4.51
  iterations_since_restore: 88
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0482178197713192
    mean_inference_ms: 1.1098149238675326
    mean_processing_ms: 0.8790687465899385
  time_since_restore: 607.3748595714569
  time_this_iter_s: 6.15786075592041
  time_total_s: 607.3748595714569
  timestamp: 1563927663
  timesteps_since_restore: 2296800
  timesteps_this_iter: 26100
  timesteps_total: 2296800
  training_iteration: 88
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 607 s, 88 iter, 2296800 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.4753860427123
  episode_reward_mean: 16.901904775165107
  episode_reward_min: -6.48207062070556
  episodes_this_iter: 174
  episodes_total: 15486
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4778.084
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.559596300125122
        kl: 0.015086580999195576
        policy_loss: -0.020728766918182373
        total_loss: 0.07696668803691864
        vf_explained_var: 0.9973713755607605
        vf_loss: 0.09580963104963303
    load_time_ms: 0.784
    num_steps_sampled: 2322900
    num_steps_trained: 2314000
    sample_time_ms: 2123.486
    update_time_ms: 4.313
  iterations_since_restore: 89
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0492345353246642
    mean_inference_ms: 1.1110731322569583
    mean_processing_ms: 0.8800454099905878
  time_since_restore: 614.941882610321
  time_this_iter_s: 7.567023038864136
  time_total_s: 614.941882610321
  timestamp: 1563927670
  timesteps_since_restore: 2322900
  timesteps_this_iter: 26100
  timesteps_total: 2322900
  training_iteration: 89
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 614 s, 89 iter, 2322900 ts, 16.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.57567902973468
  episode_reward_mean: 16.66196432234765
  episode_reward_min: -10.938703187391575
  episodes_this_iter: 174
  episodes_total: 15660
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4838.831
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5476199388504028
        kl: 0.014235458336770535
        policy_loss: -0.017644235864281654
        total_loss: 0.10583473742008209
        vf_explained_var: 0.9964245557785034
        vf_loss: 0.12169954180717468
    load_time_ms: 0.782
    num_steps_sampled: 2349000
    num_steps_trained: 2340000
    sample_time_ms: 2124.273
    update_time_ms: 4.467
  iterations_since_restore: 90
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481748576364236
    mean_inference_ms: 1.1095625244154026
    mean_processing_ms: 0.8789556862685596
  time_since_restore: 622.4847157001495
  time_this_iter_s: 7.542833089828491
  time_total_s: 622.4847157001495
  timestamp: 1563927678
  timesteps_since_restore: 2349000
  timesteps_this_iter: 26100
  timesteps_total: 2349000
  training_iteration: 90
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 622 s, 90 iter, 2349000 ts, 16.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.858562538174915
  episode_reward_mean: 17.82717931274582
  episode_reward_min: -2.4132293544492684
  episodes_this_iter: 174
  episodes_total: 15834
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4710.754
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.5317364931106567
        kl: 0.01363283395767212
        policy_loss: -0.016392387449741364
        total_loss: 0.06934556365013123
        vf_explained_var: 0.9977149367332458
        vf_loss: 0.08403385430574417
    load_time_ms: 0.788
    num_steps_sampled: 2375100
    num_steps_trained: 2366000
    sample_time_ms: 2128.762
    update_time_ms: 4.626
  iterations_since_restore: 91
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0476540338191271
    mean_inference_ms: 1.1094224378452313
    mean_processing_ms: 0.8780567236560457
  time_since_restore: 628.7738285064697
  time_this_iter_s: 6.28911280632019
  time_total_s: 628.7738285064697
  timestamp: 1563927684
  timesteps_since_restore: 2375100
  timesteps_this_iter: 26100
  timesteps_total: 2375100
  training_iteration: 91
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 628 s, 91 iter, 2375100 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.036078403340376
  episode_reward_mean: 18.41780670527952
  episode_reward_min: -3.2964573545875018
  episodes_this_iter: 174
  episodes_total: 16008
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4789.285
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.515067458152771
        kl: 0.01626107469201088
        policy_loss: -0.01934182643890381
        total_loss: 0.06836097687482834
        vf_explained_var: 0.9976580142974854
        vf_loss: 0.08567016571760178
    load_time_ms: 0.791
    num_steps_sampled: 2401200
    num_steps_trained: 2392000
    sample_time_ms: 2127.364
    update_time_ms: 4.499
  iterations_since_restore: 92
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490306857727727
    mean_inference_ms: 1.1113061677209455
    mean_processing_ms: 0.8797805462285384
  time_since_restore: 635.6898968219757
  time_this_iter_s: 6.9160683155059814
  time_total_s: 635.6898968219757
  timestamp: 1563927691
  timesteps_since_restore: 2401200
  timesteps_this_iter: 26100
  timesteps_total: 2401200
  training_iteration: 92
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 635 s, 92 iter, 2401200 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.176821370099056
  episode_reward_mean: 16.97860893599793
  episode_reward_min: -4.16361663401831
  episodes_this_iter: 174
  episodes_total: 16182
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4932.894
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.500852346420288
        kl: 0.015472611412405968
        policy_loss: -0.01877112127840519
        total_loss: 0.058212339878082275
        vf_explained_var: 0.997803807258606
        vf_loss: 0.07504938542842865
    load_time_ms: 0.792
    num_steps_sampled: 2427300
    num_steps_trained: 2418000
    sample_time_ms: 2133.412
    update_time_ms: 4.57
  iterations_since_restore: 93
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047654245364236
    mean_inference_ms: 1.1093636295633378
    mean_processing_ms: 0.8780715030681628
  time_since_restore: 643.3097031116486
  time_this_iter_s: 7.619806289672852
  time_total_s: 643.3097031116486
  timestamp: 1563927699
  timesteps_since_restore: 2427300
  timesteps_this_iter: 26100
  timesteps_total: 2427300
  training_iteration: 93
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 643 s, 93 iter, 2427300 ts, 17 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.89751523377705
  episode_reward_mean: 19.100435735421218
  episode_reward_min: -0.8187250030264192
  episodes_this_iter: 174
  episodes_total: 16356
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4854.15
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4865278005599976
        kl: 0.014971782453358173
        policy_loss: -0.021630652248859406
        total_loss: 0.057626448571681976
        vf_explained_var: 0.9980730414390564
        vf_loss: 0.07738562673330307
    load_time_ms: 0.779
    num_steps_sampled: 2453400
    num_steps_trained: 2444000
    sample_time_ms: 2127.536
    update_time_ms: 4.66
  iterations_since_restore: 94
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0500458496171092
    mean_inference_ms: 1.1119107885093018
    mean_processing_ms: 0.8807328767846699
  time_since_restore: 650.0311281681061
  time_this_iter_s: 6.7214250564575195
  time_total_s: 650.0311281681061
  timestamp: 1563927706
  timesteps_since_restore: 2453400
  timesteps_this_iter: 26100
  timesteps_total: 2453400
  training_iteration: 94
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 650 s, 94 iter, 2453400 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.6024914430871
  episode_reward_mean: 19.029982224860074
  episode_reward_min: -0.8915765897207955
  episodes_this_iter: 174
  episodes_total: 16530
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4854.173
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4779025316238403
        kl: 0.01650639995932579
        policy_loss: -0.020664852112531662
        total_loss: 0.07144195586442947
        vf_explained_var: 0.9977283477783203
        vf_loss: 0.09004350751638412
    load_time_ms: 0.78
    num_steps_sampled: 2479500
    num_steps_trained: 2470000
    sample_time_ms: 2121.131
    update_time_ms: 4.614
  iterations_since_restore: 95
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048853958372479
    mean_inference_ms: 1.1104280515685199
    mean_processing_ms: 0.8791783624925354
  time_since_restore: 656.0936486721039
  time_this_iter_s: 6.062520503997803
  time_total_s: 656.0936486721039
  timestamp: 1563927712
  timesteps_since_restore: 2479500
  timesteps_this_iter: 26100
  timesteps_total: 2479500
  training_iteration: 95
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 656 s, 95 iter, 2479500 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-21-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.66200337124331
  episode_reward_mean: 18.518290934654157
  episode_reward_min: -7.492801671553351
  episodes_this_iter: 174
  episodes_total: 16704
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4853.437
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4561952352523804
        kl: 0.01514407154172659
        policy_loss: -0.02095583826303482
        total_loss: 0.05653596296906471
        vf_explained_var: 0.9981457591056824
        vf_loss: 0.07559879124164581
    load_time_ms: 0.779
    num_steps_sampled: 2505600
    num_steps_trained: 2496000
    sample_time_ms: 2136.936
    update_time_ms: 4.632
  iterations_since_restore: 96
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481595842292593
    mean_inference_ms: 1.1103072622634302
    mean_processing_ms: 0.8797581930830968
  time_since_restore: 663.7891602516174
  time_this_iter_s: 7.69551157951355
  time_total_s: 663.7891602516174
  timestamp: 1563927719
  timesteps_since_restore: 2505600
  timesteps_this_iter: 26100
  timesteps_total: 2505600
  training_iteration: 96
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 663 s, 96 iter, 2505600 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.644229055515304
  episode_reward_mean: 18.630117919467818
  episode_reward_min: -2.0028606419603054
  episodes_this_iter: 174
  episodes_total: 16878
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4851.635
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4339605569839478
        kl: 0.016424696892499924
        policy_loss: -0.01708611100912094
        total_loss: 0.04940561577677727
        vf_explained_var: 0.9982967972755432
        vf_loss: 0.06443863362073898
    load_time_ms: 0.78
    num_steps_sampled: 2531700
    num_steps_trained: 2522000
    sample_time_ms: 2131.187
    update_time_ms: 4.621
  iterations_since_restore: 97
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488501634610188
    mean_inference_ms: 1.1105039517736426
    mean_processing_ms: 0.8797150172571142
  time_since_restore: 671.290598154068
  time_this_iter_s: 7.5014379024505615
  time_total_s: 671.290598154068
  timestamp: 1563927727
  timesteps_since_restore: 2531700
  timesteps_this_iter: 26100
  timesteps_total: 2531700
  training_iteration: 97
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 671 s, 97 iter, 2531700 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.60955027128042
  episode_reward_mean: 19.350247219438174
  episode_reward_min: -0.8669824099346574
  episodes_this_iter: 174
  episodes_total: 17052
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4991.392
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.4110357761383057
        kl: 0.019202835857868195
        policy_loss: -0.02119620516896248
        total_loss: 0.03874460980296135
        vf_explained_var: 0.9985167980194092
        vf_loss: 0.05754045397043228
    load_time_ms: 0.768
    num_steps_sampled: 2557800
    num_steps_trained: 2548000
    sample_time_ms: 2136.789
    update_time_ms: 4.599
  iterations_since_restore: 98
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485195006635626
    mean_inference_ms: 1.1105342640666687
    mean_processing_ms: 0.8797276717750504
  time_since_restore: 678.9067568778992
  time_this_iter_s: 7.616158723831177
  time_total_s: 678.9067568778992
  timestamp: 1563927734
  timesteps_since_restore: 2557800
  timesteps_this_iter: 26100
  timesteps_total: 2557800
  training_iteration: 98
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 678 s, 98 iter, 2557800 ts, 19.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.09163720876144
  episode_reward_mean: 18.461537536251807
  episode_reward_min: -13.393637301665436
  episodes_this_iter: 174
  episodes_total: 17226
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4908.892
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3955988883972168
        kl: 0.014007276855409145
        policy_loss: -0.013274552300572395
        total_loss: 0.09982990473508835
        vf_explained_var: 0.9970731735229492
        vf_loss: 0.1113535538315773
    load_time_ms: 0.772
    num_steps_sampled: 2583900
    num_steps_trained: 2574000
    sample_time_ms: 2131.493
    update_time_ms: 4.624
  iterations_since_restore: 99
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0483537186036325
    mean_inference_ms: 1.1105278431552106
    mean_processing_ms: 0.8794132523926436
  time_since_restore: 685.5937077999115
  time_this_iter_s: 6.686950922012329
  time_total_s: 685.5937077999115
  timestamp: 1563927741
  timesteps_since_restore: 2583900
  timesteps_this_iter: 26100
  timesteps_total: 2583900
  training_iteration: 99
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 685 s, 99 iter, 2583900 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.48364455906898
  episode_reward_mean: 17.593917630245382
  episode_reward_min: -0.6877831620899033
  episodes_this_iter: 174
  episodes_total: 17400
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4911.242
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3758317232131958
        kl: 0.015551482327282429
        policy_loss: -0.02185073308646679
        total_loss: 0.032808806747198105
        vf_explained_var: 0.9984749555587769
        vf_loss: 0.05271560326218605
    load_time_ms: 0.778
    num_steps_sampled: 2610000
    num_steps_trained: 2600000
    sample_time_ms: 2130.926
    update_time_ms: 4.513
  iterations_since_restore: 100
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0491239638234224
    mean_inference_ms: 1.1114930269585008
    mean_processing_ms: 0.880220547978994
  time_since_restore: 693.1517055034637
  time_this_iter_s: 7.557997703552246
  time_total_s: 693.1517055034637
  timestamp: 1563927749
  timesteps_since_restore: 2610000
  timesteps_this_iter: 26100
  timesteps_total: 2610000
  training_iteration: 100
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 693 s, 100 iter, 2610000 ts, 17.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.52384437505774
  episode_reward_mean: 18.64784812580102
  episode_reward_min: -7.5780260848434375
  episodes_this_iter: 174
  episodes_total: 17574
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4924.283
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.365156888961792
        kl: 0.01631496101617813
        policy_loss: -0.012679753825068474
        total_loss: 0.07986407727003098
        vf_explained_var: 0.997697114944458
        vf_loss: 0.09050446003675461
    load_time_ms: 0.774
    num_steps_sampled: 2636100
    num_steps_trained: 2626000
    sample_time_ms: 2142.603
    update_time_ms: 4.465
  iterations_since_restore: 101
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0475599269911555
    mean_inference_ms: 1.1101449431198536
    mean_processing_ms: 0.8784380463994651
  time_since_restore: 699.6879589557648
  time_this_iter_s: 6.536253452301025
  time_total_s: 699.6879589557648
  timestamp: 1563927755
  timesteps_since_restore: 2636100
  timesteps_this_iter: 26100
  timesteps_total: 2636100
  training_iteration: 101
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 699 s, 101 iter, 2636100 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.03762823352231
  episode_reward_mean: 18.851312601413326
  episode_reward_min: -0.9330894565459428
  episodes_this_iter: 174
  episodes_total: 17748
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4966.206
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3529287576675415
        kl: 0.015191652812063694
        policy_loss: -0.01577315479516983
        total_loss: 0.062055520713329315
        vf_explained_var: 0.9981686472892761
        vf_loss: 0.07592970877885818
    load_time_ms: 0.772
    num_steps_sampled: 2662200
    num_steps_trained: 2652000
    sample_time_ms: 2119.646
    update_time_ms: 4.452
  iterations_since_restore: 102
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481157037063757
    mean_inference_ms: 1.1102536099117148
    mean_processing_ms: 0.8789067150641798
  time_since_restore: 706.7944476604462
  time_this_iter_s: 7.1064887046813965
  time_total_s: 706.7944476604462
  timestamp: 1563927762
  timesteps_since_restore: 2662200
  timesteps_this_iter: 26100
  timesteps_total: 2662200
  training_iteration: 102
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 706 s, 102 iter, 2662200 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.124715704253454
  episode_reward_mean: 17.828866773484204
  episode_reward_min: -3.5643107365195466
  episodes_this_iter: 174
  episodes_total: 17922
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4967.955
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3404529094696045
        kl: 0.016593655571341515
        policy_loss: -0.018884383141994476
        total_loss: 0.04610748589038849
        vf_explained_var: 0.9983208179473877
        vf_loss: 0.06291766464710236
    load_time_ms: 0.77
    num_steps_sampled: 2688300
    num_steps_trained: 2678000
    sample_time_ms: 2121.021
    update_time_ms: 4.55
  iterations_since_restore: 103
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481658592227203
    mean_inference_ms: 1.1105995713990306
    mean_processing_ms: 0.8790074082991609
  time_since_restore: 714.4454145431519
  time_this_iter_s: 7.6509668827056885
  time_total_s: 714.4454145431519
  timestamp: 1563927770
  timesteps_since_restore: 2688300
  timesteps_this_iter: 26100
  timesteps_total: 2688300
  training_iteration: 103
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 714 s, 103 iter, 2688300 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-22-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.82569337858724
  episode_reward_mean: 18.888909676530293
  episode_reward_min: -0.8726987759155427
  episodes_this_iter: 174
  episodes_total: 18096
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4906.945
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.322920322418213
        kl: 0.015502800233662128
        policy_loss: -0.022023025900125504
        total_loss: 0.02980664372444153
        vf_explained_var: 0.9986025094985962
        vf_loss: 0.049891822040081024
    load_time_ms: 0.775
    num_steps_sampled: 2714400
    num_steps_trained: 2704000
    sample_time_ms: 2115.726
    update_time_ms: 4.566
  iterations_since_restore: 104
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0487494105043231
    mean_inference_ms: 1.1110263410589798
    mean_processing_ms: 0.8790581818033051
  time_since_restore: 720.5023398399353
  time_this_iter_s: 6.056925296783447
  time_total_s: 720.5023398399353
  timestamp: 1563927776
  timesteps_since_restore: 2714400
  timesteps_this_iter: 26100
  timesteps_total: 2714400
  training_iteration: 104
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 720 s, 104 iter, 2714400 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.53630965683549
  episode_reward_mean: 19.06938330588559
  episode_reward_min: -15.365762713980883
  episodes_this_iter: 174
  episodes_total: 18270
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5050.703
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.3063507080078125
        kl: 0.017321348190307617
        policy_loss: -0.017648795619606972
        total_loss: 0.037992604076862335
        vf_explained_var: 0.9985677003860474
        vf_loss: 0.0534762367606163
    load_time_ms: 0.783
    num_steps_sampled: 2740500
    num_steps_trained: 2730000
    sample_time_ms: 2127.082
    update_time_ms: 4.535
  iterations_since_restore: 105
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0491605681879344
    mean_inference_ms: 1.1116627508195236
    mean_processing_ms: 0.8796771511036812
  time_since_restore: 728.1211948394775
  time_this_iter_s: 7.618854999542236
  time_total_s: 728.1211948394775
  timestamp: 1563927784
  timesteps_since_restore: 2740500
  timesteps_this_iter: 26100
  timesteps_total: 2740500
  training_iteration: 105
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 728 s, 105 iter, 2740500 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.30155022982528
  episode_reward_mean: 17.933560762385166
  episode_reward_min: -3.764197218734746
  episodes_this_iter: 174
  episodes_total: 18444
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4906.95
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.297261118888855
        kl: 0.016752926632761955
        policy_loss: -0.022575097158551216
        total_loss: 0.03799528628587723
        vf_explained_var: 0.9985204935073853
        vf_loss: 0.058476269245147705
    load_time_ms: 0.794
    num_steps_sampled: 2766600
    num_steps_trained: 2756000
    sample_time_ms: 2121.261
    update_time_ms: 4.649
  iterations_since_restore: 106
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0479174472196509
    mean_inference_ms: 1.1104753389149906
    mean_processing_ms: 0.8792600176336601
  time_since_restore: 734.3197772502899
  time_this_iter_s: 6.198582410812378
  time_total_s: 734.3197772502899
  timestamp: 1563927790
  timesteps_since_restore: 2766600
  timesteps_this_iter: 26100
  timesteps_total: 2766600
  training_iteration: 106
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 734 s, 106 iter, 2766600 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.57452662157042
  episode_reward_mean: 18.98405399016522
  episode_reward_min: 0.21996960775059657
  episodes_this_iter: 174
  episodes_total: 18618
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4826.017
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.288512945175171
        kl: 0.014333193190395832
        policy_loss: -0.017304889857769012
        total_loss: 0.04230671375989914
        vf_explained_var: 0.9985266923904419
        vf_loss: 0.05781995505094528
    load_time_ms: 0.802
    num_steps_sampled: 2792700
    num_steps_trained: 2782000
    sample_time_ms: 2123.087
    update_time_ms: 4.533
  iterations_since_restore: 107
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481755943541173
    mean_inference_ms: 1.110621306414618
    mean_processing_ms: 0.8789678668596115
  time_since_restore: 741.0250606536865
  time_this_iter_s: 6.7052834033966064
  time_total_s: 741.0250606536865
  timestamp: 1563927797
  timesteps_since_restore: 2792700
  timesteps_this_iter: 26100
  timesteps_total: 2792700
  training_iteration: 107
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 741 s, 107 iter, 2792700 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.4321051534989
  episode_reward_mean: 18.296478401622227
  episode_reward_min: -1.2263341319987677
  episodes_this_iter: 174
  episodes_total: 18792
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4764.635
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.2787545919418335
        kl: 0.015382408164441586
        policy_loss: -0.0207778699696064
        total_loss: 0.04639166221022606
        vf_explained_var: 0.9983652830123901
        vf_loss: 0.06524673104286194
    load_time_ms: 0.802
    num_steps_sampled: 2818800
    num_steps_trained: 2808000
    sample_time_ms: 2112.108
    update_time_ms: 4.489
  iterations_since_restore: 108
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493057996386888
    mean_inference_ms: 1.1119981554457024
    mean_processing_ms: 0.8808479761079392
  time_since_restore: 747.9153571128845
  time_this_iter_s: 6.890296459197998
  time_total_s: 747.9153571128845
  timestamp: 1563927804
  timesteps_since_restore: 2818800
  timesteps_this_iter: 26100
  timesteps_total: 2818800
  training_iteration: 108
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 747 s, 108 iter, 2818800 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.99661362289483
  episode_reward_mean: 19.740813659089014
  episode_reward_min: -1.7575367610255093
  episodes_this_iter: 174
  episodes_total: 18966
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4704.56
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.2692221403121948
        kl: 0.015790637582540512
        policy_loss: -0.019548039883375168
        total_loss: 0.06452690064907074
        vf_explained_var: 0.9980523586273193
        vf_loss: 0.08210110664367676
    load_time_ms: 0.794
    num_steps_sampled: 2844900
    num_steps_trained: 2834000
    sample_time_ms: 2117.74
    update_time_ms: 4.471
  iterations_since_restore: 109
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0473471492200974
    mean_inference_ms: 1.1102919793176966
    mean_processing_ms: 0.8784719665443821
  time_since_restore: 754.0557780265808
  time_this_iter_s: 6.140420913696289
  time_total_s: 754.0557780265808
  timestamp: 1563927810
  timesteps_since_restore: 2844900
  timesteps_this_iter: 26100
  timesteps_total: 2844900
  training_iteration: 109
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 754 s, 109 iter, 2844900 ts, 19.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.70896368862231
  episode_reward_mean: 18.16439943314342
  episode_reward_min: -1.2007454101476494
  episodes_this_iter: 174
  episodes_total: 19140
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4641.49
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.2625676393508911
        kl: 0.01632586121559143
        policy_loss: -0.021681683138012886
        total_loss: 0.02641507238149643
        vf_explained_var: 0.9987494945526123
        vf_loss: 0.046056028455495834
    load_time_ms: 0.793
    num_steps_sampled: 2871000
    num_steps_trained: 2860000
    sample_time_ms: 2117.578
    update_time_ms: 4.424
  iterations_since_restore: 110
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0477327217115768
    mean_inference_ms: 1.1103685581448048
    mean_processing_ms: 0.8786293403531618
  time_since_restore: 760.9798934459686
  time_this_iter_s: 6.924115419387817
  time_total_s: 760.9798934459686
  timestamp: 1563927817
  timesteps_since_restore: 2871000
  timesteps_this_iter: 26100
  timesteps_total: 2871000
  training_iteration: 110
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 760 s, 110 iter, 2871000 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.685536389108265
  episode_reward_mean: 18.720920100615345
  episode_reward_min: -0.8080041721938908
  episodes_this_iter: 174
  episodes_total: 19314
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4745.905
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.2486807107925415
        kl: 0.019180821254849434
        policy_loss: -0.024022139608860016
        total_loss: 0.0321853868663311
        vf_explained_var: 0.9985382556915283
        vf_loss: 0.05380993336439133
    load_time_ms: 0.791
    num_steps_sampled: 2897100
    num_steps_trained: 2886000
    sample_time_ms: 2115.424
    update_time_ms: 4.373
  iterations_since_restore: 111
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481988904473376
    mean_inference_ms: 1.1109676551803493
    mean_processing_ms: 0.878825123579588
  time_since_restore: 768.5392181873322
  time_this_iter_s: 7.559324741363525
  time_total_s: 768.5392181873322
  timestamp: 1563927824
  timesteps_since_restore: 2897100
  timesteps_this_iter: 26100
  timesteps_total: 2897100
  training_iteration: 111
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 768 s, 111 iter, 2897100 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.49484260488724
  episode_reward_mean: 19.276462476379447
  episode_reward_min: -0.23641053819013585
  episodes_this_iter: 174
  episodes_total: 19488
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4743.003
    learner:
      default_policy:
        cur_kl_coeff: 0.125
        cur_lr: 9.999999747378752e-05
        entropy: 1.232102870941162
        kl: 0.020556895062327385
        policy_loss: -0.023906361311674118
        total_loss: 0.024445684626698494
        vf_explained_var: 0.9989300966262817
        vf_loss: 0.045782431960105896
    load_time_ms: 0.793
    num_steps_sampled: 2923200
    num_steps_trained: 2912000
    sample_time_ms: 2132.878
    update_time_ms: 4.424
  iterations_since_restore: 112
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0492772689286602
    mean_inference_ms: 1.1114984330708266
    mean_processing_ms: 0.8802274117464981
  time_since_restore: 775.7914574146271
  time_this_iter_s: 7.252239227294922
  time_total_s: 775.7914574146271
  timestamp: 1563927832
  timesteps_since_restore: 2923200
  timesteps_this_iter: 26100
  timesteps_total: 2923200
  training_iteration: 112
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 775 s, 112 iter, 2923200 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-23-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.538944594770875
  episode_reward_mean: 18.44866900357274
  episode_reward_min: 0.019036033793076383
  episodes_this_iter: 174
  episodes_total: 19662
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4698.424
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2203338146209717
        kl: 0.015457596629858017
        policy_loss: -0.02241368405520916
        total_loss: 0.017576489597558975
        vf_explained_var: 0.999031662940979
        vf_loss: 0.037091877311468124
    load_time_ms: 0.796
    num_steps_sampled: 2949300
    num_steps_trained: 2938000
    sample_time_ms: 2123.415
    update_time_ms: 4.371
  iterations_since_restore: 113
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480788324678463
    mean_inference_ms: 1.1109701592106422
    mean_processing_ms: 0.8786977001606979
  time_since_restore: 782.8969552516937
  time_this_iter_s: 7.10549783706665
  time_total_s: 782.8969552516937
  timestamp: 1563927839
  timesteps_since_restore: 2949300
  timesteps_this_iter: 26100
  timesteps_total: 2949300
  training_iteration: 113
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 782 s, 113 iter, 2949300 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.937193404193316
  episode_reward_mean: 18.754770662107635
  episode_reward_min: -1.2437837066209751
  episodes_this_iter: 174
  episodes_total: 19836
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4713.309
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.208868145942688
        kl: 0.015664540231227875
        policy_loss: -0.023130862042307854
        total_loss: 0.01517927274107933
        vf_explained_var: 0.9990671873092651
        vf_loss: 0.035373032093048096
    load_time_ms: 0.796
    num_steps_sampled: 2975400
    num_steps_trained: 2964000
    sample_time_ms: 2136.335
    update_time_ms: 4.244
  iterations_since_restore: 114
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0486739147995412
    mean_inference_ms: 1.1118923919694652
    mean_processing_ms: 0.8796803917221139
  time_since_restore: 789.2316083908081
  time_this_iter_s: 6.33465313911438
  time_total_s: 789.2316083908081
  timestamp: 1563927845
  timesteps_since_restore: 2975400
  timesteps_this_iter: 26100
  timesteps_total: 2975400
  training_iteration: 114
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 789 s, 114 iter, 2975400 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.981670478348285
  episode_reward_mean: 17.903925808088403
  episode_reward_min: -1.322853142163893
  episodes_this_iter: 174
  episodes_total: 20010
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4646.736
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.2038500308990479
        kl: 0.014396624639630318
        policy_loss: -0.020050058141350746
        total_loss: 0.016373518854379654
        vf_explained_var: 0.9990947246551514
        vf_loss: 0.033724211156368256
    load_time_ms: 0.788
    num_steps_sampled: 3001500
    num_steps_trained: 2990000
    sample_time_ms: 2126.91
    update_time_ms: 4.167
  iterations_since_restore: 115
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048875858488863
    mean_inference_ms: 1.1121208744806483
    mean_processing_ms: 0.8804518399139696
  time_since_restore: 796.0861630439758
  time_this_iter_s: 6.854554653167725
  time_total_s: 796.0861630439758
  timestamp: 1563927852
  timesteps_since_restore: 3001500
  timesteps_this_iter: 26100
  timesteps_total: 3001500
  training_iteration: 115
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 796 s, 115 iter, 3001500 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.84870919782533
  episode_reward_mean: 19.29696320348982
  episode_reward_min: -0.9902736554697388
  episodes_this_iter: 174
  episodes_total: 20184
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4646.185
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.204695463180542
        kl: 0.014524958096444607
        policy_loss: -0.020290348678827286
        total_loss: 0.05951138213276863
        vf_explained_var: 0.9981478452682495
        vf_loss: 0.07707830518484116
    load_time_ms: 0.779
    num_steps_sampled: 3027600
    num_steps_trained: 3016000
    sample_time_ms: 2126.861
    update_time_ms: 4.14
  iterations_since_restore: 116
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0476339374018773
    mean_inference_ms: 1.1106082104930377
    mean_processing_ms: 0.8792407028456719
  time_since_restore: 802.2763111591339
  time_this_iter_s: 6.190148115158081
  time_total_s: 802.2763111591339
  timestamp: 1563927858
  timesteps_since_restore: 3027600
  timesteps_this_iter: 26100
  timesteps_total: 3027600
  training_iteration: 116
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 802 s, 116 iter, 3027600 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.39699620462568
  episode_reward_mean: 18.377079095194812
  episode_reward_min: -0.6927620181610652
  episodes_this_iter: 174
  episodes_total: 20358
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4597.963
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1929219961166382
        kl: 0.016260797157883644
        policy_loss: -0.014889664016664028
        total_loss: 0.04365363344550133
        vf_explained_var: 0.9984939098358154
        vf_loss: 0.055494390428066254
    load_time_ms: 0.777
    num_steps_sampled: 3053700
    num_steps_trained: 3042000
    sample_time_ms: 2128.164
    update_time_ms: 4.094
  iterations_since_restore: 117
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0494100741675154
    mean_inference_ms: 1.1127174240242832
    mean_processing_ms: 0.8800721350550222
  time_since_restore: 808.5114443302155
  time_this_iter_s: 6.235133171081543
  time_total_s: 808.5114443302155
  timestamp: 1563927864
  timesteps_since_restore: 3053700
  timesteps_this_iter: 26100
  timesteps_total: 3053700
  training_iteration: 117
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 808 s, 117 iter, 3053700 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.654710635051806
  episode_reward_mean: 18.792259229887346
  episode_reward_min: -0.5370546380506511
  episodes_this_iter: 174
  episodes_total: 20532
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4637.105
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1838642358779907
        kl: 0.014667367562651634
        policy_loss: -0.01983673684298992
        total_loss: 0.030835213139653206
        vf_explained_var: 0.9986643195152283
        vf_loss: 0.04792181774973869
    load_time_ms: 0.782
    num_steps_sampled: 3079800
    num_steps_trained: 3068000
    sample_time_ms: 2138.116
    update_time_ms: 4.055
  iterations_since_restore: 118
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0486629372930403
    mean_inference_ms: 1.111556475966766
    mean_processing_ms: 0.8799252106733696
  time_since_restore: 815.8928184509277
  time_this_iter_s: 7.38137412071228
  time_total_s: 815.8928184509277
  timestamp: 1563927872
  timesteps_since_restore: 3079800
  timesteps_this_iter: 26100
  timesteps_total: 3079800
  training_iteration: 118
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 815 s, 118 iter, 3079800 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.73665147925665
  episode_reward_mean: 18.657727011138718
  episode_reward_min: -3.2040640140811947
  episodes_this_iter: 174
  episodes_total: 20706
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4779.879
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1778439283370972
        kl: 0.016150575131177902
        policy_loss: -0.023315714672207832
        total_loss: 0.03225412592291832
        vf_explained_var: 0.9987151026725769
        vf_loss: 0.05254160612821579
    load_time_ms: 0.787
    num_steps_sampled: 3105900
    num_steps_trained: 3094000
    sample_time_ms: 2131.495
    update_time_ms: 4.07
  iterations_since_restore: 119
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480280317375168
    mean_inference_ms: 1.1110726421516655
    mean_processing_ms: 0.8791882589819977
  time_since_restore: 823.4000294208527
  time_this_iter_s: 7.507210969924927
  time_total_s: 823.4000294208527
  timestamp: 1563927879
  timesteps_since_restore: 3105900
  timesteps_this_iter: 26100
  timesteps_total: 3105900
  training_iteration: 119
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 823 s, 119 iter, 3105900 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.195708248456285
  episode_reward_mean: 18.218238733024076
  episode_reward_min: -1.2286739457438782
  episodes_this_iter: 174
  episodes_total: 20880
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4763.205
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1667793989181519
        kl: 0.016327716410160065
        policy_loss: -0.02239319495856762
        total_loss: 0.024105872958898544
        vf_explained_var: 0.9987922310829163
        vf_loss: 0.043437615036964417
    load_time_ms: 0.787
    num_steps_sampled: 3132000
    num_steps_trained: 3120000
    sample_time_ms: 2126.509
    update_time_ms: 4.205
  iterations_since_restore: 120
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0486793975028468
    mean_inference_ms: 1.1120698117598649
    mean_processing_ms: 0.8803559426833537
  time_since_restore: 830.1095876693726
  time_this_iter_s: 6.7095582485198975
  time_total_s: 830.1095876693726
  timestamp: 1563927886
  timesteps_since_restore: 3132000
  timesteps_this_iter: 26100
  timesteps_total: 3132000
  training_iteration: 120
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 830 s, 120 iter, 3132000 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-24-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.93701833505183
  episode_reward_mean: 19.53396037857863
  episode_reward_min: -0.7900690745306047
  episodes_this_iter: 174
  episodes_total: 21054
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4645.972
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1547930240631104
        kl: 0.013991103507578373
        policy_loss: -0.018548931926488876
        total_loss: 0.02062462456524372
        vf_explained_var: 0.9990605115890503
        vf_loss: 0.03655022382736206
    load_time_ms: 0.794
    num_steps_sampled: 3158100
    num_steps_trained: 3146000
    sample_time_ms: 2124.992
    update_time_ms: 4.199
  iterations_since_restore: 121
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0479829298131773
    mean_inference_ms: 1.1109595628414566
    mean_processing_ms: 0.8787459988983551
  time_since_restore: 836.4807167053223
  time_this_iter_s: 6.371129035949707
  time_total_s: 836.4807167053223
  timestamp: 1563927892
  timesteps_since_restore: 3158100
  timesteps_this_iter: 26100
  timesteps_total: 3158100
  training_iteration: 121
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 836 s, 121 iter, 3158100 ts, 19.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.89137567723817
  episode_reward_mean: 18.172627549114267
  episode_reward_min: -1.7275076646506051
  episodes_this_iter: 174
  episodes_total: 21228
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4647.531
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1460564136505127
        kl: 0.01745862141251564
        policy_loss: -0.0241163931787014
        total_loss: 0.013439486734569073
        vf_explained_var: 0.9990986585617065
        vf_loss: 0.034282386302948
    load_time_ms: 0.795
    num_steps_sampled: 3184200
    num_steps_trained: 3172000
    sample_time_ms: 2124.77
    update_time_ms: 4.21
  iterations_since_restore: 122
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0501619595760951
    mean_inference_ms: 1.1129348082662989
    mean_processing_ms: 0.8808633433135588
  time_since_restore: 843.7456777095795
  time_this_iter_s: 7.264961004257202
  time_total_s: 843.7456777095795
  timestamp: 1563927900
  timesteps_since_restore: 3184200
  timesteps_this_iter: 26100
  timesteps_total: 3184200
  training_iteration: 122
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 843 s, 122 iter, 3184200 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.15768207311414
  episode_reward_mean: 18.781858213915754
  episode_reward_min: -0.5701362378436727
  episodes_this_iter: 174
  episodes_total: 21402
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4688.897
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1347193717956543
        kl: 0.014997833408415318
        policy_loss: -0.022110771387815475
        total_loss: 0.017385652288794518
        vf_explained_var: 0.9989861249923706
        vf_loss: 0.03668433055281639
    load_time_ms: 0.793
    num_steps_sampled: 3210300
    num_steps_trained: 3198000
    sample_time_ms: 2134.503
    update_time_ms: 4.238
  iterations_since_restore: 123
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0478717541702387
    mean_inference_ms: 1.1106843972139968
    mean_processing_ms: 0.8786555910197693
  time_since_restore: 851.3660893440247
  time_this_iter_s: 7.62041163444519
  time_total_s: 851.3660893440247
  timestamp: 1563927907
  timesteps_since_restore: 3210300
  timesteps_this_iter: 26100
  timesteps_total: 3210300
  training_iteration: 123
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 851 s, 123 iter, 3210300 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.378235887248835
  episode_reward_mean: 18.511100840639298
  episode_reward_min: -0.19758196850688733
  episodes_this_iter: 174
  episodes_total: 21576
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4670.695
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.129655361175537
        kl: 0.01719169318675995
        policy_loss: -0.014798540621995926
        total_loss: 0.040867287665605545
        vf_explained_var: 0.9985904693603516
        vf_loss: 0.05244237557053566
    load_time_ms: 0.798
    num_steps_sampled: 3236400
    num_steps_trained: 3224000
    sample_time_ms: 2124.037
    update_time_ms: 4.385
  iterations_since_restore: 124
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488431625622585
    mean_inference_ms: 1.1123032037937008
    mean_processing_ms: 0.8804814129476752
  time_since_restore: 857.4152097702026
  time_this_iter_s: 6.0491204261779785
  time_total_s: 857.4152097702026
  timestamp: 1563927913
  timesteps_since_restore: 3236400
  timesteps_this_iter: 26100
  timesteps_total: 3236400
  training_iteration: 124
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 857 s, 124 iter, 3236400 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.1055018954975
  episode_reward_mean: 18.052181180208386
  episode_reward_min: -0.6726290408820799
  episodes_this_iter: 174
  episodes_total: 21750
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4711.782
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1223673820495605
        kl: 0.017466427758336067
        policy_loss: -0.02230040915310383
        total_loss: 0.01216843817383051
        vf_explained_var: 0.9991915225982666
        vf_loss: 0.031193895265460014
    load_time_ms: 0.799
    num_steps_sampled: 3262500
    num_steps_trained: 3250000
    sample_time_ms: 2128.654
    update_time_ms: 4.448
  iterations_since_restore: 125
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0496850485492677
    mean_inference_ms: 1.1122917011006705
    mean_processing_ms: 0.8804499079986743
  time_since_restore: 864.7284326553345
  time_this_iter_s: 7.313222885131836
  time_total_s: 864.7284326553345
  timestamp: 1563927921
  timesteps_since_restore: 3262500
  timesteps_this_iter: 26100
  timesteps_total: 3262500
  training_iteration: 125
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 864 s, 125 iter, 3262500 ts, 18.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.914729579078795
  episode_reward_mean: 18.314821971659246
  episode_reward_min: -2.73927310810403
  episodes_this_iter: 174
  episodes_total: 21924
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4709.43
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.1106526851654053
        kl: 0.017717041075229645
        policy_loss: -0.01958456262946129
        total_loss: 0.0385187491774559
        vf_explained_var: 0.9985206127166748
        vf_loss: 0.05478135868906975
    load_time_ms: 0.796
    num_steps_sampled: 3288600
    num_steps_trained: 3276000
    sample_time_ms: 2130.594
    update_time_ms: 4.422
  iterations_since_restore: 126
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.049156062373083
    mean_inference_ms: 1.11240591530464
    mean_processing_ms: 0.8800829949652679
  time_since_restore: 870.9145514965057
  time_this_iter_s: 6.186118841171265
  time_total_s: 870.9145514965057
  timestamp: 1563927927
  timesteps_since_restore: 3288600
  timesteps_this_iter: 26100
  timesteps_total: 3288600
  training_iteration: 126
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 870 s, 126 iter, 3288600 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.077995423054
  episode_reward_mean: 19.251141004542657
  episode_reward_min: -1.9384844775386618
  episodes_this_iter: 174
  episodes_total: 22098
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4773.403
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.102355718612671
        kl: 0.015100168064236641
        policy_loss: -0.014172634109854698
        total_loss: 0.08490484207868576
        vf_explained_var: 0.9975731372833252
        vf_loss: 0.09624619781970978
    load_time_ms: 0.801
    num_steps_sampled: 3314700
    num_steps_trained: 3302000
    sample_time_ms: 2129.925
    update_time_ms: 4.474
  iterations_since_restore: 127
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0476351601551799
    mean_inference_ms: 1.1112036297786911
    mean_processing_ms: 0.8791096343487074
  time_since_restore: 877.7849242687225
  time_this_iter_s: 6.870372772216797
  time_total_s: 877.7849242687225
  timestamp: 1563927934
  timesteps_since_restore: 3314700
  timesteps_this_iter: 26100
  timesteps_total: 3314700
  training_iteration: 127
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 877 s, 127 iter, 3314700 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.03139502869316
  episode_reward_mean: 19.228973939688352
  episode_reward_min: -3.7600074901764393
  episodes_this_iter: 174
  episodes_total: 22272
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4731.305
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0916177034378052
        kl: 0.018583381548523903
        policy_loss: -0.01939540170133114
        total_loss: 0.0225501861423254
        vf_explained_var: 0.9990272521972656
        vf_loss: 0.03846120461821556
    load_time_ms: 0.805
    num_steps_sampled: 3340800
    num_steps_trained: 3328000
    sample_time_ms: 2125.154
    update_time_ms: 4.485
  iterations_since_restore: 128
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048124437857483
    mean_inference_ms: 1.1108925466145325
    mean_processing_ms: 0.879151056333677
  time_since_restore: 884.6970036029816
  time_this_iter_s: 6.912079334259033
  time_total_s: 884.6970036029816
  timestamp: 1563927941
  timesteps_since_restore: 3340800
  timesteps_this_iter: 26100
  timesteps_total: 3340800
  training_iteration: 128
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 884 s, 128 iter, 3340800 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.41489335631331
  episode_reward_mean: 19.382477170966332
  episode_reward_min: -1.4755400139450865
  episodes_this_iter: 174
  episodes_total: 22446
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4733.067
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.082824468612671
        kl: 0.01708369329571724
        policy_loss: -0.023442277684807777
        total_loss: 0.01833486184477806
        vf_explained_var: 0.9990339875221252
        vf_loss: 0.038573943078517914
    load_time_ms: 0.802
    num_steps_sampled: 3366900
    num_steps_trained: 3354000
    sample_time_ms: 2127.742
    update_time_ms: 4.55
  iterations_since_restore: 129
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048967075314992
    mean_inference_ms: 1.1123136862285417
    mean_processing_ms: 0.880363230048117
  time_since_restore: 892.2475023269653
  time_this_iter_s: 7.550498723983765
  time_total_s: 892.2475023269653
  timestamp: 1563927948
  timesteps_since_restore: 3366900
  timesteps_this_iter: 26100
  timesteps_total: 3366900
  training_iteration: 129
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 892 s, 129 iter, 3366900 ts, 19.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-25-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.35404676014971
  episode_reward_mean: 19.633224411673318
  episode_reward_min: -0.778581564374656
  episodes_this_iter: 174
  episodes_total: 22620
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4810.758
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.07303786277771
        kl: 0.016873802989721298
        policy_loss: -0.021577615290880203
        total_loss: 0.009468946605920792
        vf_explained_var: 0.9993458390235901
        vf_loss: 0.02788272500038147
    load_time_ms: 0.8
    num_steps_sampled: 3393000
    num_steps_trained: 3380000
    sample_time_ms: 2137.096
    update_time_ms: 4.577
  iterations_since_restore: 130
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493744805743532
    mean_inference_ms: 1.112736253941473
    mean_processing_ms: 0.880641421567131
  time_since_restore: 899.8289031982422
  time_this_iter_s: 7.5814008712768555
  time_total_s: 899.8289031982422
  timestamp: 1563927956
  timesteps_since_restore: 3393000
  timesteps_this_iter: 26100
  timesteps_total: 3393000
  training_iteration: 130
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 899 s, 130 iter, 3393000 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.38214102789987
  episode_reward_mean: 19.087192470475653
  episode_reward_min: -4.440147646467724
  episodes_this_iter: 174
  episodes_total: 22794
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4937.895
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0607272386550903
        kl: 0.019411779940128326
        policy_loss: -0.020489342510700226
        total_loss: 0.018775325268507004
        vf_explained_var: 0.9991424083709717
        vf_loss: 0.03562495857477188
    load_time_ms: 0.794
    num_steps_sampled: 3419100
    num_steps_trained: 3406000
    sample_time_ms: 2126.042
    update_time_ms: 4.569
  iterations_since_restore: 131
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0487479287799288
    mean_inference_ms: 1.112024772708761
    mean_processing_ms: 0.8793228199681775
  time_since_restore: 907.3644592761993
  time_this_iter_s: 7.535556077957153
  time_total_s: 907.3644592761993
  timestamp: 1563927963
  timesteps_since_restore: 3419100
  timesteps_this_iter: 26100
  timesteps_total: 3419100
  training_iteration: 131
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 907 s, 131 iter, 3419100 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.30445165431067
  episode_reward_mean: 17.832463280286788
  episode_reward_min: -1.0988957921182982
  episodes_this_iter: 174
  episodes_total: 22968
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4961.933
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.048680067062378
        kl: 0.01634327508509159
        policy_loss: -0.021037664264440536
        total_loss: 0.010624127462506294
        vf_explained_var: 0.9992135167121887
        vf_loss: 0.028597423806786537
    load_time_ms: 0.793
    num_steps_sampled: 3445200
    num_steps_trained: 3432000
    sample_time_ms: 2131.769
    update_time_ms: 4.584
  iterations_since_restore: 132
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048445015628364
    mean_inference_ms: 1.112166451785787
    mean_processing_ms: 0.8802809267283374
  time_since_restore: 914.9290170669556
  time_this_iter_s: 7.564557790756226
  time_total_s: 914.9290170669556
  timestamp: 1563927971
  timesteps_since_restore: 3445200
  timesteps_this_iter: 26100
  timesteps_total: 3445200
  training_iteration: 132
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 914 s, 132 iter, 3445200 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.29384999854223
  episode_reward_mean: 17.789182384035364
  episode_reward_min: -5.665363798990972
  episodes_this_iter: 174
  episodes_total: 23142
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4964.486
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0383009910583496
        kl: 0.017197417095303535
        policy_loss: -0.022283058613538742
        total_loss: 0.04333868995308876
        vf_explained_var: 0.9982318878173828
        vf_loss: 0.06239723786711693
    load_time_ms: 0.79
    num_steps_sampled: 3471300
    num_steps_trained: 3458000
    sample_time_ms: 2122.185
    update_time_ms: 4.596
  iterations_since_restore: 133
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490058751971314
    mean_inference_ms: 1.112843984989623
    mean_processing_ms: 0.8804744701482372
  time_since_restore: 922.4810726642609
  time_this_iter_s: 7.552055597305298
  time_total_s: 922.4810726642609
  timestamp: 1563927979
  timesteps_since_restore: 3471300
  timesteps_this_iter: 26100
  timesteps_total: 3471300
  training_iteration: 133
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 922 s, 133 iter, 3471300 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.161106151781574
  episode_reward_mean: 18.241591173239712
  episode_reward_min: -0.7411142263986888
  episodes_this_iter: 174
  episodes_total: 23316
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4968.129
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.02180016040802
        kl: 0.016936693340539932
        policy_loss: -0.024452468380331993
        total_loss: 0.007184111047536135
        vf_explained_var: 0.9992082118988037
        vf_loss: 0.028460949659347534
    load_time_ms: 0.783
    num_steps_sampled: 3497400
    num_steps_trained: 3484000
    sample_time_ms: 2125.444
    update_time_ms: 4.542
  iterations_since_restore: 134
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048506301196837
    mean_inference_ms: 1.1119733253481379
    mean_processing_ms: 0.8791961613921299
  time_since_restore: 928.5982420444489
  time_this_iter_s: 6.117169380187988
  time_total_s: 928.5982420444489
  timestamp: 1563927985
  timesteps_since_restore: 3497400
  timesteps_this_iter: 26100
  timesteps_total: 3497400
  training_iteration: 134
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 928 s, 134 iter, 3497400 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.374472046056106
  episode_reward_mean: 20.411199248790016
  episode_reward_min: -0.872891046930507
  episodes_this_iter: 174
  episodes_total: 23490
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4948.109
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0124359130859375
        kl: 0.015968499705195427
        policy_loss: -0.02427363023161888
        total_loss: 0.004538070876151323
        vf_explained_var: 0.9993903636932373
        vf_loss: 0.02581760473549366
    load_time_ms: 0.784
    num_steps_sampled: 3523500
    num_steps_trained: 3510000
    sample_time_ms: 2125.063
    update_time_ms: 4.616
  iterations_since_restore: 135
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047796806399749
    mean_inference_ms: 1.1109062664996785
    mean_processing_ms: 0.8789125921913753
  time_since_restore: 935.7087118625641
  time_this_iter_s: 7.110469818115234
  time_total_s: 935.7087118625641
  timestamp: 1563927992
  timesteps_since_restore: 3523500
  timesteps_this_iter: 26100
  timesteps_total: 3523500
  training_iteration: 135
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 935 s, 135 iter, 3523500 ts, 20.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.13398198287132
  episode_reward_mean: 18.915916637906978
  episode_reward_min: -0.4623537074231405
  episodes_this_iter: 174
  episodes_total: 23664
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5090.802
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 1.0043686628341675
        kl: 0.017583182081580162
        policy_loss: -0.026212003082036972
        total_loss: 0.0023000785149633884
        vf_explained_var: 0.9993306398391724
        vf_loss: 0.025215230882167816
    load_time_ms: 0.785
    num_steps_sampled: 3549600
    num_steps_trained: 3536000
    sample_time_ms: 2119.505
    update_time_ms: 4.641
  iterations_since_restore: 136
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0475723521763045
    mean_inference_ms: 1.1108516694400248
    mean_processing_ms: 0.8785143715616124
  time_since_restore: 943.2700521945953
  time_this_iter_s: 7.56134033203125
  time_total_s: 943.2700521945953
  timestamp: 1563927999
  timesteps_since_restore: 3549600
  timesteps_this_iter: 26100
  timesteps_total: 3549600
  training_iteration: 136
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 943 s, 136 iter, 3549600 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.655188747798775
  episode_reward_mean: 19.623765373842883
  episode_reward_min: 0.4158645991574942
  episodes_this_iter: 174
  episodes_total: 23838
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 5011.75
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9942826628684998
        kl: 0.01872403174638748
        policy_loss: -0.02231295220553875
        total_loss: 0.007607094012200832
        vf_explained_var: 0.9993674755096436
        vf_loss: 0.026409294456243515
    load_time_ms: 0.785
    num_steps_sampled: 3575700
    num_steps_trained: 3562000
    sample_time_ms: 2123.69
    update_time_ms: 4.734
  iterations_since_restore: 137
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480570660087631
    mean_inference_ms: 1.1118474254196045
    mean_processing_ms: 0.8793240590106596
  time_since_restore: 949.3914120197296
  time_this_iter_s: 6.121359825134277
  time_total_s: 949.3914120197296
  timestamp: 1563928006
  timesteps_since_restore: 3575700
  timesteps_this_iter: 26100
  timesteps_total: 3575700
  training_iteration: 137
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 949 s, 137 iter, 3575700 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.87503087092238
  episode_reward_mean: 18.295515403700442
  episode_reward_min: -0.2664247585731573
  episodes_this_iter: 174
  episodes_total: 24012
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4936.749
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.984548032283783
        kl: 0.016390111297369003
        policy_loss: -0.021672772243618965
        total_loss: 0.010389505885541439
        vf_explained_var: 0.9991697669029236
        vf_loss: 0.02898913249373436
    load_time_ms: 0.784
    num_steps_sampled: 3601800
    num_steps_trained: 3588000
    sample_time_ms: 2125.306
    update_time_ms: 4.705
  iterations_since_restore: 138
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0479328717555276
    mean_inference_ms: 1.1115369733765668
    mean_processing_ms: 0.8788918589945311
  time_since_restore: 955.5679650306702
  time_this_iter_s: 6.176553010940552
  time_total_s: 955.5679650306702
  timestamp: 1563928012
  timesteps_since_restore: 3601800
  timesteps_this_iter: 26100
  timesteps_total: 3601800
  training_iteration: 138
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 955 s, 138 iter, 3601800 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-26-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.62868278964104
  episode_reward_mean: 18.87103631863635
  episode_reward_min: -0.9337239970772738
  episodes_this_iter: 174
  episodes_total: 24186
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4836.431
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9871321320533752
        kl: 0.01775129698216915
        policy_loss: -0.013105765916407108
        total_loss: 0.051502350717782974
        vf_explained_var: 0.9985067844390869
        vf_loss: 0.061279747635126114
    load_time_ms: 0.786
    num_steps_sampled: 3627900
    num_steps_trained: 3614000
    sample_time_ms: 2127.069
    update_time_ms: 4.59
  iterations_since_restore: 139
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047998511829916
    mean_inference_ms: 1.1114021376773944
    mean_processing_ms: 0.8789579216360202
  time_since_restore: 962.1297032833099
  time_this_iter_s: 6.5617382526397705
  time_total_s: 962.1297032833099
  timestamp: 1563928018
  timesteps_since_restore: 3627900
  timesteps_this_iter: 26100
  timesteps_total: 3627900
  training_iteration: 139
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 962 s, 139 iter, 3627900 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.1884198904038
  episode_reward_mean: 18.1663311015424
  episode_reward_min: -1.5366920729825935
  episodes_this_iter: 174
  episodes_total: 24360
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4695.212
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9836365580558777
        kl: 0.014796724542975426
        policy_loss: -0.018317995592951775
        total_loss: 0.030882814899086952
        vf_explained_var: 0.9986953139305115
        vf_loss: 0.04642641916871071
    load_time_ms: 0.791
    num_steps_sampled: 3654000
    num_steps_trained: 3640000
    sample_time_ms: 2122.341
    update_time_ms: 4.479
  iterations_since_restore: 140
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0489714164045425
    mean_inference_ms: 1.1121580598081326
    mean_processing_ms: 0.880114604423477
  time_since_restore: 968.2472002506256
  time_this_iter_s: 6.117496967315674
  time_total_s: 968.2472002506256
  timestamp: 1563928024
  timesteps_since_restore: 3654000
  timesteps_this_iter: 26100
  timesteps_total: 3654000
  training_iteration: 140
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 968 s, 140 iter, 3654000 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.60197166853738
  episode_reward_mean: 19.23150134249828
  episode_reward_min: -0.3979291975275347
  episodes_this_iter: 174
  episodes_total: 24534
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4615.555
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9711732268333435
        kl: 0.019003871828317642
        policy_loss: -0.023101964965462685
        total_loss: 0.011022250168025494
        vf_explained_var: 0.9991960525512695
        vf_loss: 0.030560988932847977
    load_time_ms: 0.787
    num_steps_sampled: 3680100
    num_steps_trained: 3666000
    sample_time_ms: 2124.46
    update_time_ms: 4.408
  iterations_since_restore: 141
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0482217091661523
    mean_inference_ms: 1.1113413787460504
    mean_processing_ms: 0.8788755485060445
  time_since_restore: 975.0033540725708
  time_this_iter_s: 6.75615382194519
  time_total_s: 975.0033540725708
  timestamp: 1563928031
  timesteps_since_restore: 3680100
  timesteps_this_iter: 26100
  timesteps_total: 3680100
  training_iteration: 141
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 975 s, 141 iter, 3680100 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.95985042369163
  episode_reward_mean: 18.626997838754622
  episode_reward_min: 0.11789836571539969
  episodes_this_iter: 174
  episodes_total: 24708
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4616.793
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9579555988311768
        kl: 0.016790492460131645
        policy_loss: -0.02199810929596424
        total_loss: 0.007124269846826792
        vf_explained_var: 0.9992765188217163
        vf_loss: 0.02597416192293167
    load_time_ms: 0.786
    num_steps_sampled: 3706200
    num_steps_trained: 3692000
    sample_time_ms: 2131.415
    update_time_ms: 4.413
  iterations_since_restore: 142
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0476701693730877
    mean_inference_ms: 1.1106598213131915
    mean_processing_ms: 0.8788808891292043
  time_since_restore: 982.6509895324707
  time_this_iter_s: 7.647635459899902
  time_total_s: 982.6509895324707
  timestamp: 1563928039
  timesteps_since_restore: 3706200
  timesteps_this_iter: 26100
  timesteps_total: 3706200
  training_iteration: 142
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 982 s, 142 iter, 3706200 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.44792666975818
  episode_reward_mean: 18.606048892286406
  episode_reward_min: -0.6825766157679231
  episodes_this_iter: 174
  episodes_total: 24882
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4615.946
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9468084573745728
        kl: 0.01733541302382946
        policy_loss: -0.02645101398229599
        total_loss: 0.0017713832203298807
        vf_explained_var: 0.9993314743041992
        vf_loss: 0.02497200295329094
    load_time_ms: 0.793
    num_steps_sampled: 3732300
    num_steps_trained: 3718000
    sample_time_ms: 2131.115
    update_time_ms: 4.411
  iterations_since_restore: 143
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.049085976303067
    mean_inference_ms: 1.1126167712739878
    mean_processing_ms: 0.8806382882966881
  time_since_restore: 990.191468000412
  time_this_iter_s: 7.540478467941284
  time_total_s: 990.191468000412
  timestamp: 1563928046
  timesteps_since_restore: 3732300
  timesteps_this_iter: 26100
  timesteps_total: 3732300
  training_iteration: 143
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 990 s, 143 iter, 3732300 ts, 18.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.55355131404823
  episode_reward_mean: 20.342722144897664
  episode_reward_min: -0.8179526521125376
  episodes_this_iter: 174
  episodes_total: 25056
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4756.84
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9408369064331055
        kl: 0.01653766818344593
        policy_loss: -0.023797940462827682
        total_loss: 0.01765906997025013
        vf_explained_var: 0.9991951584815979
        vf_loss: 0.038356199860572815
    load_time_ms: 0.792
    num_steps_sampled: 3758400
    num_steps_trained: 3744000
    sample_time_ms: 2135.198
    update_time_ms: 4.509
  iterations_since_restore: 144
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047239660919408
    mean_inference_ms: 1.1111143324488562
    mean_processing_ms: 0.8786853744165727
  time_since_restore: 997.7631099224091
  time_this_iter_s: 7.57164192199707
  time_total_s: 997.7631099224091
  timestamp: 1563928054
  timesteps_since_restore: 3758400
  timesteps_this_iter: 26100
  timesteps_total: 3758400
  training_iteration: 144
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 997 s, 144 iter, 3758400 ts, 20.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.428206150166545
  episode_reward_mean: 18.892595396800147
  episode_reward_min: -0.8989561556423745
  episodes_this_iter: 174
  episodes_total: 25230
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4687.711
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9373306035995483
        kl: 0.014693235047161579
        policy_loss: -0.017752395942807198
        total_loss: 0.06454918533563614
        vf_explained_var: 0.9979283213615417
        vf_loss: 0.07954660058021545
    load_time_ms: 0.786
    num_steps_sampled: 3784500
    num_steps_trained: 3770000
    sample_time_ms: 2129.065
    update_time_ms: 4.558
  iterations_since_restore: 145
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048396497433392
    mean_inference_ms: 1.1122312132304342
    mean_processing_ms: 0.8802638909866055
  time_since_restore: 1004.1182377338409
  time_this_iter_s: 6.355127811431885
  time_total_s: 1004.1182377338409
  timestamp: 1563928060
  timesteps_since_restore: 3784500
  timesteps_this_iter: 26100
  timesteps_total: 3784500
  training_iteration: 145
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1004 s, 145 iter, 3784500 ts, 18.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.261868094522015
  episode_reward_mean: 19.55300915991327
  episode_reward_min: -8.733166235270945
  episodes_this_iter: 174
  episodes_total: 25404
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4607.282
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9267300963401794
        kl: 0.016655180603265762
        policy_loss: -0.01987667940557003
        total_loss: 0.028215033933520317
        vf_explained_var: 0.9989368915557861
        vf_loss: 0.0449688620865345
    load_time_ms: 0.787
    num_steps_sampled: 3810600
    num_steps_trained: 3796000
    sample_time_ms: 2132.659
    update_time_ms: 4.452
  iterations_since_restore: 146
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481854405908568
    mean_inference_ms: 1.1120685174878777
    mean_processing_ms: 0.8795174086414503
  time_since_restore: 1010.9093677997589
  time_this_iter_s: 6.791130065917969
  time_total_s: 1010.9093677997589
  timestamp: 1563928067
  timesteps_since_restore: 3810600
  timesteps_this_iter: 26100
  timesteps_total: 3810600
  training_iteration: 146
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1010 s, 146 iter, 3810600 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-27-54
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.980336454518806
  episode_reward_mean: 19.7872421981883
  episode_reward_min: -0.9734623076164411
  episodes_this_iter: 174
  episodes_total: 25578
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4623.363
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9246301054954529
        kl: 0.017371047288179398
        policy_loss: -0.024541983380913734
        total_loss: 0.011229246854782104
        vf_explained_var: 0.9992119073867798
        vf_loss: 0.03251415491104126
    load_time_ms: 0.778
    num_steps_sampled: 3836700
    num_steps_trained: 3822000
    sample_time_ms: 2132.881
    update_time_ms: 4.403
  iterations_since_restore: 147
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490913545038831
    mean_inference_ms: 1.1118909034561761
    mean_processing_ms: 0.8801301899625424
  time_since_restore: 1017.1930766105652
  time_this_iter_s: 6.283708810806274
  time_total_s: 1017.1930766105652
  timestamp: 1563928074
  timesteps_since_restore: 3836700
  timesteps_this_iter: 26100
  timesteps_total: 3836700
  training_iteration: 147
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1017 s, 147 iter, 3836700 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.09129008675929
  episode_reward_mean: 18.747539419841516
  episode_reward_min: -0.28686674103416676
  episodes_this_iter: 174
  episodes_total: 25752
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4765.068
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9106553196907043
        kl: 0.017423249781131744
        policy_loss: -0.02705073170363903
        total_loss: 0.00038296988350339234
        vf_explained_var: 0.9993652105331421
        vf_loss: 0.024166841059923172
    load_time_ms: 0.772
    num_steps_sampled: 3862800
    num_steps_trained: 3848000
    sample_time_ms: 2125.456
    update_time_ms: 4.365
  iterations_since_restore: 148
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0479015188472525
    mean_inference_ms: 1.1116261440115816
    mean_processing_ms: 0.8794311732707606
  time_since_restore: 1024.7145774364471
  time_this_iter_s: 7.521500825881958
  time_total_s: 1024.7145774364471
  timestamp: 1563928081
  timesteps_since_restore: 3862800
  timesteps_this_iter: 26100
  timesteps_total: 3862800
  training_iteration: 148
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1024 s, 148 iter, 3862800 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.62125291774371
  episode_reward_mean: 19.199883776091834
  episode_reward_min: -0.7774188108751882
  episodes_this_iter: 174
  episodes_total: 25926
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4865.441
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.9000973105430603
        kl: 0.019229646772146225
        policy_loss: -0.025926321744918823
        total_loss: 0.0005284249782562256
        vf_explained_var: 0.9994305968284607
        vf_loss: 0.022849183529615402
    load_time_ms: 0.772
    num_steps_sampled: 3888900
    num_steps_trained: 3874000
    sample_time_ms: 2136.853
    update_time_ms: 4.455
  iterations_since_restore: 149
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481325629050102
    mean_inference_ms: 1.112022325092033
    mean_processing_ms: 0.8797072833599537
  time_since_restore: 1032.396691083908
  time_this_iter_s: 7.6821136474609375
  time_total_s: 1032.396691083908
  timestamp: 1563928089
  timesteps_since_restore: 3888900
  timesteps_this_iter: 26100
  timesteps_total: 3888900
  training_iteration: 149
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1032 s, 149 iter, 3888900 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.00380299717648
  episode_reward_mean: 19.960335018780746
  episode_reward_min: 0.19609438147546784
  episodes_this_iter: 174
  episodes_total: 26100
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4892.967
    learner:
      default_policy:
        cur_kl_coeff: 0.1875
        cur_lr: 9.999999747378752e-05
        entropy: 0.8916571140289307
        kl: 0.022115278989076614
        policy_loss: -0.020167510956525803
        total_loss: 0.0057221767492592335
        vf_explained_var: 0.9994588494300842
        vf_loss: 0.02174307219684124
    load_time_ms: 0.773
    num_steps_sampled: 3915000
    num_steps_trained: 3900000
    sample_time_ms: 2133.847
    update_time_ms: 4.544
  iterations_since_restore: 150
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0476290285320078
    mean_inference_ms: 1.111051780016915
    mean_processing_ms: 0.8786428661166185
  time_since_restore: 1038.7601959705353
  time_this_iter_s: 6.363504886627197
  time_total_s: 1038.7601959705353
  timestamp: 1563928095
  timesteps_since_restore: 3915000
  timesteps_this_iter: 26100
  timesteps_total: 3915000
  training_iteration: 150
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1038 s, 150 iter, 3915000 ts, 20 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.933894786607866
  episode_reward_mean: 18.432703302967408
  episode_reward_min: -1.1773004664177817
  episodes_this_iter: 174
  episodes_total: 26274
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4829.221
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8946648836135864
        kl: 0.01458807848393917
        policy_loss: -0.020058896392583847
        total_loss: 0.02285349741578102
        vf_explained_var: 0.998993992805481
        vf_loss: 0.03880949690937996
    load_time_ms: 0.786
    num_steps_sampled: 3941100
    num_steps_trained: 3926000
    sample_time_ms: 2135.304
    update_time_ms: 4.567
  iterations_since_restore: 151
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.04860312124383
    mean_inference_ms: 1.111989262219728
    mean_processing_ms: 0.879625156644707
  time_since_restore: 1044.8926565647125
  time_this_iter_s: 6.132460594177246
  time_total_s: 1044.8926565647125
  timestamp: 1563928101
  timesteps_since_restore: 3941100
  timesteps_this_iter: 26100
  timesteps_total: 3941100
  training_iteration: 151
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1044 s, 151 iter, 3941100 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.627232321191734
  episode_reward_mean: 19.092729760635162
  episode_reward_min: -4.4399927132473405
  episodes_this_iter: 174
  episodes_total: 26448
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4741.464
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8878732323646545
        kl: 0.015433738939464092
        policy_loss: -0.018367882817983627
        total_loss: 0.042598143219947815
        vf_explained_var: 0.9984773993492126
        vf_loss: 0.05662528797984123
    load_time_ms: 0.79
    num_steps_sampled: 3967200
    num_steps_trained: 3952000
    sample_time_ms: 2124.338
    update_time_ms: 4.469
  iterations_since_restore: 152
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480238094913887
    mean_inference_ms: 1.1117029966112948
    mean_processing_ms: 0.8795733747338905
  time_since_restore: 1051.5490601062775
  time_this_iter_s: 6.656403541564941
  time_total_s: 1051.5490601062775
  timestamp: 1563928108
  timesteps_since_restore: 3967200
  timesteps_this_iter: 26100
  timesteps_total: 3967200
  training_iteration: 152
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1051 s, 152 iter, 3967200 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.77783556301858
  episode_reward_mean: 20.147233760307653
  episode_reward_min: -1.2102596533195136
  episodes_this_iter: 174
  episodes_total: 26622
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4599.448
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8816331028938293
        kl: 0.014914369210600853
        policy_loss: -0.020111067220568657
        total_loss: 0.01547741424292326
        vf_explained_var: 0.9992081522941589
        vf_loss: 0.03139382228255272
    load_time_ms: 0.788
    num_steps_sampled: 3993300
    num_steps_trained: 3978000
    sample_time_ms: 2135.715
    update_time_ms: 4.394
  iterations_since_restore: 153
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480275298755786
    mean_inference_ms: 1.1115858245239567
    mean_processing_ms: 0.8792035434769565
  time_since_restore: 1057.7777636051178
  time_this_iter_s: 6.228703498840332
  time_total_s: 1057.7777636051178
  timestamp: 1563928114
  timesteps_since_restore: 3993300
  timesteps_this_iter: 26100
  timesteps_total: 3993300
  training_iteration: 153
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1057 s, 153 iter, 3993300 ts, 20.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.46107550740766
  episode_reward_mean: 19.209326711694217
  episode_reward_min: -0.2070994436433918
  episodes_this_iter: 174
  episodes_total: 26796
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4596.15
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8678277134895325
        kl: 0.017499642446637154
        policy_loss: -0.024283649399876595
        total_loss: 0.00199676351621747
        vf_explained_var: 0.9994099736213684
        vf_loss: 0.021358642727136612
    load_time_ms: 0.795
    num_steps_sampled: 4019400
    num_steps_trained: 4004000
    sample_time_ms: 2132.044
    update_time_ms: 4.242
  iterations_since_restore: 154
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485330842198446
    mean_inference_ms: 1.111974971036316
    mean_processing_ms: 0.8796981643478915
  time_since_restore: 1065.278659105301
  time_this_iter_s: 7.5008955001831055
  time_total_s: 1065.278659105301
  timestamp: 1563928122
  timesteps_since_restore: 4019400
  timesteps_this_iter: 26100
  timesteps_total: 4019400
  training_iteration: 154
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1065 s, 154 iter, 4019400 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.060164075263245
  episode_reward_mean: 18.54838936373055
  episode_reward_min: -1.6164724164067459
  episodes_this_iter: 174
  episodes_total: 26970
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4646.603
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.86672443151474
        kl: 0.016667310148477554
        policy_loss: -0.023441914469003677
        total_loss: 0.02140069380402565
        vf_explained_var: 0.9989141821861267
        vf_loss: 0.04015493392944336
    load_time_ms: 0.809
    num_steps_sampled: 4045500
    num_steps_trained: 4030000
    sample_time_ms: 2135.856
    update_time_ms: 4.191
  iterations_since_restore: 155
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0475396773037697
    mean_inference_ms: 1.1114633949033388
    mean_processing_ms: 0.8788592842143482
  time_since_restore: 1072.1764900684357
  time_this_iter_s: 6.897830963134766
  time_total_s: 1072.1764900684357
  timestamp: 1563928129
  timesteps_since_restore: 4045500
  timesteps_this_iter: 26100
  timesteps_total: 4045500
  training_iteration: 155
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1072 s, 155 iter, 4045500 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-28-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.76687088607313
  episode_reward_mean: 18.338192331835504
  episode_reward_min: 0.4877219579446467
  episodes_this_iter: 174
  episodes_total: 27144
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4597.494
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8662071824073792
        kl: 0.015862397849559784
        policy_loss: -0.01706053502857685
        total_loss: 0.015126552432775497
        vf_explained_var: 0.999208390712738
        vf_loss: 0.027725789695978165
    load_time_ms: 0.808
    num_steps_sampled: 4071600
    num_steps_trained: 4056000
    sample_time_ms: 2133.363
    update_time_ms: 4.223
  iterations_since_restore: 156
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490994406682121
    mean_inference_ms: 1.112961085596842
    mean_processing_ms: 0.8807613731368401
  time_since_restore: 1078.4502937793732
  time_this_iter_s: 6.2738037109375
  time_total_s: 1078.4502937793732
  timestamp: 1563928135
  timesteps_since_restore: 4071600
  timesteps_this_iter: 26100
  timesteps_total: 4071600
  training_iteration: 156
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1078 s, 156 iter, 4071600 ts, 18.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.23914306242723
  episode_reward_mean: 19.706407316443983
  episode_reward_min: -0.5694803915072676
  episodes_this_iter: 174
  episodes_total: 27318
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4659.536
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8511886596679688
        kl: 0.014825524762272835
        policy_loss: -0.026375535875558853
        total_loss: 0.0010553353931754827
        vf_explained_var: 0.9994822144508362
        vf_loss: 0.023261193186044693
    load_time_ms: 0.817
    num_steps_sampled: 4097700
    num_steps_trained: 4082000
    sample_time_ms: 2139.099
    update_time_ms: 4.127
  iterations_since_restore: 157
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048002635573333
    mean_inference_ms: 1.1114991349124776
    mean_processing_ms: 0.8793129942209228
  time_since_restore: 1085.4129815101624
  time_this_iter_s: 6.962687730789185
  time_total_s: 1085.4129815101624
  timestamp: 1563928142
  timesteps_since_restore: 4097700
  timesteps_this_iter: 26100
  timesteps_total: 4097700
  training_iteration: 157
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1085 s, 157 iter, 4097700 ts, 19.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.953387690043705
  episode_reward_mean: 18.988114849039494
  episode_reward_min: -0.04306130856119694
  episodes_this_iter: 174
  episodes_total: 27492
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4514.591
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8391196131706238
        kl: 0.015613611787557602
        policy_loss: -0.02314871922135353
        total_loss: 0.00564546650275588
        vf_explained_var: 0.9993813037872314
        vf_loss: 0.024402856826782227
    load_time_ms: 0.816
    num_steps_sampled: 4123800
    num_steps_trained: 4108000
    sample_time_ms: 2137.905
    update_time_ms: 4.205
  iterations_since_restore: 158
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480123438435482
    mean_inference_ms: 1.111452081620412
    mean_processing_ms: 0.879327863784064
  time_since_restore: 1091.4698173999786
  time_this_iter_s: 6.056835889816284
  time_total_s: 1091.4698173999786
  timestamp: 1563928148
  timesteps_since_restore: 4123800
  timesteps_this_iter: 26100
  timesteps_total: 4123800
  training_iteration: 158
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1091 s, 158 iter, 4123800 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.13276134756026
  episode_reward_mean: 19.567572520393597
  episode_reward_min: -0.3281481584803988
  episodes_this_iter: 174
  episodes_total: 27666
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4369.212
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8323961496353149
        kl: 0.015790553763508797
        policy_loss: -0.022507932037115097
        total_loss: -0.0004210359766148031
        vf_explained_var: 0.9995085000991821
        vf_loss: 0.017645804211497307
    load_time_ms: 0.818
    num_steps_sampled: 4149900
    num_steps_trained: 4134000
    sample_time_ms: 2131.181
    update_time_ms: 4.087
  iterations_since_restore: 159
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0492088904603787
    mean_inference_ms: 1.1128045864999243
    mean_processing_ms: 0.8807102116551326
  time_since_restore: 1097.6260659694672
  time_this_iter_s: 6.156248569488525
  time_total_s: 1097.6260659694672
  timestamp: 1563928154
  timesteps_since_restore: 4149900
  timesteps_this_iter: 26100
  timesteps_total: 4149900
  training_iteration: 159
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1097 s, 159 iter, 4149900 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.384296738391136
  episode_reward_mean: 19.244650152713344
  episode_reward_min: 0.03755562233221264
  episodes_this_iter: 174
  episodes_total: 27840
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4482.729
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8240240812301636
        kl: 0.016592152416706085
        policy_loss: -0.025089144706726074
        total_loss: -0.0016603521071374416
        vf_explained_var: 0.9995123147964478
        vf_loss: 0.018762249499559402
    load_time_ms: 0.806
    num_steps_sampled: 4176000
    num_steps_trained: 4160000
    sample_time_ms: 2143.943
    update_time_ms: 3.964
  iterations_since_restore: 160
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493950995871855
    mean_inference_ms: 1.1127304474149873
    mean_processing_ms: 0.8803619043873866
  time_since_restore: 1105.2560920715332
  time_this_iter_s: 7.63002610206604
  time_total_s: 1105.2560920715332
  timestamp: 1563928162
  timesteps_since_restore: 4176000
  timesteps_this_iter: 26100
  timesteps_total: 4176000
  training_iteration: 160
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1105 s, 160 iter, 4176000 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.571984338093856
  episode_reward_mean: 20.849390479921205
  episode_reward_min: -0.21008757760034272
  episodes_this_iter: 174
  episodes_total: 28014
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4481.762
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.8122662305831909
        kl: 0.015764547511935234
        policy_loss: -0.020254284143447876
        total_loss: 0.019768359139561653
        vf_explained_var: 0.9991753697395325
        vf_loss: 0.03558887168765068
    load_time_ms: 0.803
    num_steps_sampled: 4202100
    num_steps_trained: 4186000
    sample_time_ms: 2135.49
    update_time_ms: 4.049
  iterations_since_restore: 161
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0480322462314484
    mean_inference_ms: 1.1110874031720182
    mean_processing_ms: 0.879483456442434
  time_since_restore: 1111.2947597503662
  time_this_iter_s: 6.038667678833008
  time_total_s: 1111.2947597503662
  timestamp: 1563928168
  timesteps_since_restore: 4202100
  timesteps_this_iter: 26100
  timesteps_total: 4202100
  training_iteration: 161
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1111 s, 161 iter, 4202100 ts, 20.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.249664247289935
  episode_reward_mean: 17.86523052922447
  episode_reward_min: -5.181041718418505
  episodes_this_iter: 174
  episodes_total: 28188
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4505.825
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7999125719070435
        kl: 0.016323663294315338
        policy_loss: -0.02207106165587902
        total_loss: 0.006784930359572172
        vf_explained_var: 0.9992269277572632
        vf_loss: 0.02426496148109436
    load_time_ms: 0.803
    num_steps_sampled: 4228200
    num_steps_trained: 4212000
    sample_time_ms: 2143.82
    update_time_ms: 4.033
  iterations_since_restore: 162
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0486618395259726
    mean_inference_ms: 1.1125212196891712
    mean_processing_ms: 0.8805926260349651
  time_since_restore: 1118.2741112709045
  time_this_iter_s: 6.97935152053833
  time_total_s: 1118.2741112709045
  timestamp: 1563928175
  timesteps_since_restore: 4228200
  timesteps_this_iter: 26100
  timesteps_total: 4228200
  training_iteration: 162
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1118 s, 162 iter, 4228200 ts, 17.9 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.35333305028189
  episode_reward_mean: 18.355925883862234
  episode_reward_min: -0.5419274894273262
  episodes_this_iter: 174
  episodes_total: 28362
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4643.387
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7964624166488647
        kl: 0.01734839752316475
        policy_loss: -0.023215655237436295
        total_loss: 0.0019037810852751136
        vf_explained_var: 0.9994415044784546
        vf_loss: 0.02024020068347454
    load_time_ms: 0.796
    num_steps_sampled: 4254300
    num_steps_trained: 4238000
    sample_time_ms: 2130.753
    update_time_ms: 4.053
  iterations_since_restore: 163
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488420937275957
    mean_inference_ms: 1.1121979493405574
    mean_processing_ms: 0.8799851491569792
  time_since_restore: 1125.7511994838715
  time_this_iter_s: 7.477088212966919
  time_total_s: 1125.7511994838715
  timestamp: 1563928182
  timesteps_since_restore: 4254300
  timesteps_this_iter: 26100
  timesteps_total: 4254300
  training_iteration: 163
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1125 s, 163 iter, 4254300 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.38144678729394
  episode_reward_mean: 18.809984148654294
  episode_reward_min: -9.405871242635255
  episodes_this_iter: 174
  episodes_total: 28536
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4647.788
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7940164804458618
        kl: 0.016603050753474236
        policy_loss: -0.021444443613290787
        total_loss: 0.03007896989583969
        vf_explained_var: 0.9988364577293396
        vf_loss: 0.04685380682349205
    load_time_ms: 0.79
    num_steps_sampled: 4280400
    num_steps_trained: 4264000
    sample_time_ms: 2133.46
    update_time_ms: 4.156
  iterations_since_restore: 164
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493395463642752
    mean_inference_ms: 1.1129793132307433
    mean_processing_ms: 0.8804562077237017
  time_since_restore: 1133.324874162674
  time_this_iter_s: 7.57367467880249
  time_total_s: 1133.324874162674
  timestamp: 1563928190
  timesteps_since_restore: 4280400
  timesteps_this_iter: 26100
  timesteps_total: 4280400
  training_iteration: 164
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1133 s, 164 iter, 4280400 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-29-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.80540622939845
  episode_reward_mean: 18.66527382088102
  episode_reward_min: -2.976194555535951
  episodes_this_iter: 174
  episodes_total: 28710
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4568.674
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7860864996910095
        kl: 0.015888657420873642
        policy_loss: -0.022500205785036087
        total_loss: 0.0008379562059417367
        vf_explained_var: 0.9994522333145142
        vf_loss: 0.01886947639286518
    load_time_ms: 0.787
    num_steps_sampled: 4306500
    num_steps_trained: 4290000
    sample_time_ms: 2136.492
    update_time_ms: 4.113
  iterations_since_restore: 165
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490656083684586
    mean_inference_ms: 1.112715482126074
    mean_processing_ms: 0.8805998553802841
  time_since_restore: 1139.4610195159912
  time_this_iter_s: 6.136145353317261
  time_total_s: 1139.4610195159912
  timestamp: 1563928196
  timesteps_since_restore: 4306500
  timesteps_this_iter: 26100
  timesteps_total: 4306500
  training_iteration: 165
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1139 s, 165 iter, 4306500 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.528498278462855
  episode_reward_mean: 18.217668448883103
  episode_reward_min: -0.10192332390421958
  episodes_this_iter: 174
  episodes_total: 28884
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4570.664
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7817738056182861
        kl: 0.01659487932920456
        policy_loss: -0.02521323412656784
        total_loss: -0.0030755274929106236
        vf_explained_var: 0.9995125532150269
        vf_loss: 0.017470398917794228
    load_time_ms: 0.783
    num_steps_sampled: 4332600
    num_steps_trained: 4316000
    sample_time_ms: 2129.65
    update_time_ms: 4.064
  iterations_since_restore: 166
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.047856261572465
    mean_inference_ms: 1.1110341488332147
    mean_processing_ms: 0.8788737560617516
  time_since_restore: 1145.686339378357
  time_this_iter_s: 6.225319862365723
  time_total_s: 1145.686339378357
  timestamp: 1563928202
  timesteps_since_restore: 4332600
  timesteps_this_iter: 26100
  timesteps_total: 4332600
  training_iteration: 166
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1145 s, 166 iter, 4332600 ts, 18.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.924018556598426
  episode_reward_mean: 18.836892101370015
  episode_reward_min: -0.22553871333425984
  episodes_this_iter: 174
  episodes_total: 29058
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4508.689
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7672730088233948
        kl: 0.017192814499139786
        policy_loss: -0.026139236986637115
        total_loss: -0.003994989208877087
        vf_explained_var: 0.9995508790016174
        vf_loss: 0.01730877161026001
    load_time_ms: 0.782
    num_steps_sampled: 4358700
    num_steps_trained: 4342000
    sample_time_ms: 2129.984
    update_time_ms: 4.091
  iterations_since_restore: 167
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0483156141406553
    mean_inference_ms: 1.1117407678038334
    mean_processing_ms: 0.8793828210321476
  time_since_restore: 1152.0302011966705
  time_this_iter_s: 6.343861818313599
  time_total_s: 1152.0302011966705
  timestamp: 1563928209
  timesteps_since_restore: 4358700
  timesteps_this_iter: 26100
  timesteps_total: 4358700
  training_iteration: 167
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1152 s, 167 iter, 4358700 ts, 18.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.83241906699259
  episode_reward_mean: 18.43280476443491
  episode_reward_min: -0.6934851549316177
  episodes_this_iter: 174
  episodes_total: 29232
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4508.158
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7688115835189819
        kl: 0.013588966801762581
        policy_loss: -0.019421307370066643
        total_loss: 0.0025619990192353725
        vf_explained_var: 0.9995008111000061
        vf_loss: 0.018161406740546227
    load_time_ms: 0.79
    num_steps_sampled: 4384800
    num_steps_trained: 4368000
    sample_time_ms: 2131.984
    update_time_ms: 4.085
  iterations_since_restore: 168
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0478621182850605
    mean_inference_ms: 1.1117407209264043
    mean_processing_ms: 0.8790564836164149
  time_since_restore: 1158.102020740509
  time_this_iter_s: 6.071819543838501
  time_total_s: 1158.102020740509
  timestamp: 1563928215
  timesteps_since_restore: 4384800
  timesteps_this_iter: 26100
  timesteps_total: 4384800
  training_iteration: 168
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1158 s, 168 iter, 4384800 ts, 18.4 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.195224799134884
  episode_reward_mean: 18.952039670208237
  episode_reward_min: -1.5439307881211903
  episodes_this_iter: 174
  episodes_total: 29406
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4649.132
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.75820392370224
        kl: 0.01495403703302145
        policy_loss: -0.017392918467521667
        total_loss: 0.002954262774437666
        vf_explained_var: 0.999598503112793
        vf_loss: 0.016141360625624657
    load_time_ms: 0.81
    num_steps_sampled: 4410900
    num_steps_trained: 4394000
    sample_time_ms: 2137.362
    update_time_ms: 4.054
  iterations_since_restore: 169
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0488405160803775
    mean_inference_ms: 1.1126459356394631
    mean_processing_ms: 0.880051663866152
  time_since_restore: 1165.7283563613892
  time_this_iter_s: 7.626335620880127
  time_total_s: 1165.7283563613892
  timestamp: 1563928222
  timesteps_since_restore: 4410900
  timesteps_this_iter: 26100
  timesteps_total: 4410900
  training_iteration: 169
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1165 s, 169 iter, 4410900 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.65758384227328
  episode_reward_mean: 18.509490446644442
  episode_reward_min: 0.09145680903527782
  episodes_this_iter: 174
  episodes_total: 29580
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4605.617
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7456269860267639
        kl: 0.016265273094177246
        policy_loss: -0.02728075534105301
        total_loss: -0.007729482837021351
        vf_explained_var: 0.9996022582054138
        vf_loss: 0.014976664446294308
    load_time_ms: 0.818
    num_steps_sampled: 4437000
    num_steps_trained: 4420000
    sample_time_ms: 2128.783
    update_time_ms: 4.204
  iterations_since_restore: 170
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0502250761197616
    mean_inference_ms: 1.113672881647932
    mean_processing_ms: 0.8810913688827272
  time_since_restore: 1172.837102651596
  time_this_iter_s: 7.108746290206909
  time_total_s: 1172.837102651596
  timestamp: 1563928230
  timesteps_since_restore: 4437000
  timesteps_this_iter: 26100
  timesteps_total: 4437000
  training_iteration: 170
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1172 s, 170 iter, 4437000 ts, 18.5 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.446809204165305
  episode_reward_mean: 20.614387276396382
  episode_reward_min: -1.9061955814038605
  episodes_this_iter: 174
  episodes_total: 29754
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4749.309
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7368446588516235
        kl: 0.018140774220228195
        policy_loss: -0.028139285743236542
        total_loss: -0.006128818728029728
        vf_explained_var: 0.9996042847633362
        vf_loss: 0.016908371821045876
    load_time_ms: 0.82
    num_steps_sampled: 4463100
    num_steps_trained: 4446000
    sample_time_ms: 2128.713
    update_time_ms: 4.163
  iterations_since_restore: 171
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0479206262896277
    mean_inference_ms: 1.1111288881580221
    mean_processing_ms: 0.8791666036861593
  time_since_restore: 1180.3159375190735
  time_this_iter_s: 7.478834867477417
  time_total_s: 1180.3159375190735
  timestamp: 1563928237
  timesteps_since_restore: 4463100
  timesteps_this_iter: 26100
  timesteps_total: 4463100
  training_iteration: 171
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1180 s, 171 iter, 4463100 ts, 20.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.8320815850234
  episode_reward_mean: 19.294181502191957
  episode_reward_min: -7.013161481478122
  episodes_this_iter: 174
  episodes_total: 29928
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4757.687
    learner:
      default_policy:
        cur_kl_coeff: 0.28125
        cur_lr: 9.999999747378752e-05
        entropy: 0.7380551099777222
        kl: 0.020676851272583008
        policy_loss: -0.012106279842555523
        total_loss: 0.032576508820056915
        vf_explained_var: 0.9990469217300415
        vf_loss: 0.03886742144823074
    load_time_ms: 0.818
    num_steps_sampled: 4489200
    num_steps_trained: 4472000
    sample_time_ms: 2135.455
    update_time_ms: 4.309
  iterations_since_restore: 172
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0493377143931248
    mean_inference_ms: 1.1127940386177146
    mean_processing_ms: 0.8806640342087776
  time_since_restore: 1187.4461970329285
  time_this_iter_s: 7.1302595138549805
  time_total_s: 1187.4461970329285
  timestamp: 1563928244
  timesteps_since_restore: 4489200
  timesteps_this_iter: 26100
  timesteps_total: 4489200
  training_iteration: 172
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1187 s, 172 iter, 4489200 ts, 19.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.58188420301883
  episode_reward_mean: 19.23051448273726
  episode_reward_min: -0.8433535087083777
  episodes_this_iter: 174
  episodes_total: 30102
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4761.486
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7301194667816162
        kl: 0.01572270505130291
        policy_loss: -0.016063744202256203
        total_loss: 0.008623822592198849
        vf_explained_var: 0.9995280504226685
        vf_loss: 0.018054550513625145
    load_time_ms: 0.826
    num_steps_sampled: 4515300
    num_steps_trained: 4498000
    sample_time_ms: 2139.968
    update_time_ms: 4.211
  iterations_since_restore: 173
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0485972140777606
    mean_inference_ms: 1.1121610482504158
    mean_processing_ms: 0.8801541847345192
  time_since_restore: 1195.0046677589417
  time_this_iter_s: 7.558470726013184
  time_total_s: 1195.0046677589417
  timestamp: 1563928252
  timesteps_since_restore: 4515300
  timesteps_this_iter: 26100
  timesteps_total: 4515300
  training_iteration: 173
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1195 s, 173 iter, 4515300 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-30-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.86719879775382
  episode_reward_mean: 19.592058725575463
  episode_reward_min: -0.4398105401221031
  episodes_this_iter: 174
  episodes_total: 30276
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4616.255
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7264193892478943
        kl: 0.012500019744038582
        policy_loss: -0.02331591583788395
        total_loss: 0.002836274914443493
        vf_explained_var: 0.9994934797286987
        vf_loss: 0.020878741517663002
    load_time_ms: 0.821
    num_steps_sampled: 4541400
    num_steps_trained: 4524000
    sample_time_ms: 2136.306
    update_time_ms: 4.265
  iterations_since_restore: 174
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0484088958689521
    mean_inference_ms: 1.1119977114272481
    mean_processing_ms: 0.8795501006332488
  time_since_restore: 1201.0863535404205
  time_this_iter_s: 6.081685781478882
  time_total_s: 1201.0863535404205
  timestamp: 1563928258
  timesteps_since_restore: 4541400
  timesteps_this_iter: 26100
  timesteps_total: 4541400
  training_iteration: 174
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1201 s, 174 iter, 4541400 ts, 19.6 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.744433401020025
  episode_reward_mean: 20.285973546731597
  episode_reward_min: 0.20316179334707446
  episodes_this_iter: 174
  episodes_total: 30450
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4616.92
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.716480016708374
        kl: 0.013434810563921928
        policy_loss: -0.02095833607017994
        total_loss: 0.01016966998577118
        vf_explained_var: 0.9993982315063477
        vf_loss: 0.025460196658968925
    load_time_ms: 0.823
    num_steps_sampled: 4567500
    num_steps_trained: 4550000
    sample_time_ms: 2138.129
    update_time_ms: 4.218
  iterations_since_restore: 175
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481644107017565
    mean_inference_ms: 1.1116661555358225
    mean_processing_ms: 0.8797819216976055
  time_since_restore: 1207.2468371391296
  time_this_iter_s: 6.1604835987091064
  time_total_s: 1207.2468371391296
  timestamp: 1563928264
  timesteps_since_restore: 4567500
  timesteps_this_iter: 26100
  timesteps_total: 4567500
  training_iteration: 175
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1207 s, 175 iter, 4567500 ts, 20.3 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.110875560878625
  episode_reward_mean: 19.22748258522352
  episode_reward_min: -1.2034641900082812
  episodes_this_iter: 174
  episodes_total: 30624
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4746.302
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.709426999092102
        kl: 0.013436724431812763
        policy_loss: -0.020062385126948357
        total_loss: 0.002640967955812812
        vf_explained_var: 0.9995620250701904
        vf_loss: 0.017034731805324554
    load_time_ms: 0.823
    num_steps_sampled: 4593600
    num_steps_trained: 4576000
    sample_time_ms: 2143.431
    update_time_ms: 4.246
  iterations_since_restore: 176
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0490962865920408
    mean_inference_ms: 1.1124539526153099
    mean_processing_ms: 0.8803469779934248
  time_since_restore: 1214.8226482868195
  time_this_iter_s: 7.575811147689819
  time_total_s: 1214.8226482868195
  timestamp: 1563928272
  timesteps_since_restore: 4593600
  timesteps_this_iter: 26100
  timesteps_total: 4593600
  training_iteration: 176
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1214 s, 176 iter, 4593600 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.73548424709421
  episode_reward_mean: 18.722242293568563
  episode_reward_min: 0.979470906776423
  episodes_this_iter: 174
  episodes_total: 30798
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4732.465
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.7004817724227905
        kl: 0.014375047758221626
        policy_loss: -0.021483350545167923
        total_loss: 0.006910573225468397
        vf_explained_var: 0.9993906617164612
        vf_loss: 0.022329451516270638
    load_time_ms: 0.829
    num_steps_sampled: 4619700
    num_steps_trained: 4602000
    sample_time_ms: 2141.486
    update_time_ms: 4.373
  iterations_since_restore: 177
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0494447450868656
    mean_inference_ms: 1.1128889715530357
    mean_processing_ms: 0.8800697177701058
  time_since_restore: 1221.0100672245026
  time_this_iter_s: 6.1874189376831055
  time_total_s: 1221.0100672245026
  timestamp: 1563928278
  timesteps_since_restore: 4619700
  timesteps_this_iter: 26100
  timesteps_total: 4619700
  training_iteration: 177
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1221 s, 177 iter, 4619700 ts, 18.7 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.39306709595369
  episode_reward_mean: 19.82587657465988
  episode_reward_min: 1.5002130415041346
  episodes_this_iter: 174
  episodes_total: 30972
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4873.564
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6912974119186401
        kl: 0.016200527548789978
        policy_loss: -0.024915022775530815
        total_loss: -0.0009408898185938597
        vf_explained_var: 0.9995637536048889
        vf_loss: 0.01713954098522663
    load_time_ms: 0.825
    num_steps_sampled: 4645800
    num_steps_trained: 4628000
    sample_time_ms: 2144.29
    update_time_ms: 4.372
  iterations_since_restore: 178
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0489649246306925
    mean_inference_ms: 1.1126261065654488
    mean_processing_ms: 0.8802345679108813
  time_since_restore: 1228.5263345241547
  time_this_iter_s: 7.5162672996521
  time_total_s: 1228.5263345241547
  timestamp: 1563928285
  timesteps_since_restore: 4645800
  timesteps_this_iter: 26100
  timesteps_total: 4645800
  training_iteration: 178
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1228 s, 178 iter, 4645800 ts, 19.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.368510591383604
  episode_reward_mean: 19.154665672705654
  episode_reward_min: 0.056961077786166055
  episodes_this_iter: 174
  episodes_total: 31146
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4734.554
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6909440755844116
        kl: 0.016494112089276314
        policy_loss: -0.015945864841341972
        total_loss: 0.025007741525769234
        vf_explained_var: 0.9991582632064819
        vf_loss: 0.03399515151977539
    load_time_ms: 0.807
    num_steps_sampled: 4671900
    num_steps_trained: 4654000
    sample_time_ms: 2136.044
    update_time_ms: 4.537
  iterations_since_restore: 179
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.04980615969187
    mean_inference_ms: 1.1135098499445217
    mean_processing_ms: 0.8812510668720258
  time_since_restore: 1234.6749877929688
  time_this_iter_s: 6.148653268814087
  time_total_s: 1234.6749877929688
  timestamp: 1563928292
  timesteps_since_restore: 4671900
  timesteps_this_iter: 26100
  timesteps_total: 4671900
  training_iteration: 179
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1234 s, 179 iter, 4671900 ts, 19.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.61051306886968
  episode_reward_mean: 17.757973713406056
  episode_reward_min: -0.5393370133670831
  episodes_this_iter: 174
  episodes_total: 31320
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4780.9
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6867300868034363
        kl: 0.01585122011601925
        policy_loss: -0.01951526664197445
        total_loss: 0.006046158727258444
        vf_explained_var: 0.9995061159133911
        vf_loss: 0.018874188885092735
    load_time_ms: 0.802
    num_steps_sampled: 4698000
    num_steps_trained: 4680000
    sample_time_ms: 2136.755
    update_time_ms: 4.392
  iterations_since_restore: 180
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.049102422899491
    mean_inference_ms: 1.1122587042251573
    mean_processing_ms: 0.8796715191932635
  time_since_restore: 1242.2543277740479
  time_this_iter_s: 7.579339981079102
  time_total_s: 1242.2543277740479
  timestamp: 1563928299
  timesteps_since_restore: 4698000
  timesteps_this_iter: 26100
  timesteps_total: 4698000
  training_iteration: 180
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1242 s, 180 iter, 4698000 ts, 17.8 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.30000511914365
  episode_reward_mean: 19.140568859968422
  episode_reward_min: 0.6635027509525271
  episodes_this_iter: 174
  episodes_total: 31494
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4639.536
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6784441471099854
        kl: 0.013213849626481533
        policy_loss: -0.018444735556840897
        total_loss: 0.03998555243015289
        vf_explained_var: 0.9985592365264893
        vf_loss: 0.05285569652915001
    load_time_ms: 0.804
    num_steps_sampled: 4724100
    num_steps_trained: 4706000
    sample_time_ms: 2136.008
    update_time_ms: 4.449
  iterations_since_restore: 181
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.048360775939224
    mean_inference_ms: 1.1117813032560584
    mean_processing_ms: 0.8798935030655805
  time_since_restore: 1248.3083591461182
  time_this_iter_s: 6.0540313720703125
  time_total_s: 1248.3083591461182
  timestamp: 1563928305
  timesteps_since_restore: 4724100
  timesteps_this_iter: 26100
  timesteps_total: 4724100
  training_iteration: 181
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1248 s, 181 iter, 4724100 ts, 19.1 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew
2019-07-24 02:32:07,825	INFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-31-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.03532378963726
  episode_reward_mean: 20.192528054934165
  episode_reward_min: -0.5446969565460619
  episodes_this_iter: 174
  episodes_total: 31668
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4628.778
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6719785332679749
        kl: 0.013129950501024723
        policy_loss: -0.02368062362074852
        total_loss: -0.0019740161951631308
        vf_explained_var: 0.9995862245559692
        vf_loss: 0.01616740971803665
    load_time_ms: 0.805
    num_steps_sampled: 4750200
    num_steps_trained: 4732000
    sample_time_ms: 2136.511
    update_time_ms: 4.344
  iterations_since_restore: 182
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0481554241719948
    mean_inference_ms: 1.1117754478606627
    mean_processing_ms: 0.8792812904681777
  time_since_restore: 1255.3374083042145
  time_this_iter_s: 7.0290491580963135
  time_total_s: 1255.3374083042145
  timestamp: 1563928312
  timesteps_since_restore: 4750200
  timesteps_this_iter: 26100
  timesteps_total: 4750200
  training_iteration: 182
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1255 s, 182 iter, 4750200 ts, 20.2 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-32-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.970312873898294
  episode_reward_mean: 19.006885964256583
  episode_reward_min: -0.582292530193314
  episodes_this_iter: 174
  episodes_total: 31842
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4629.067
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6698249578475952
        kl: 0.014126977883279324
        policy_loss: -0.019339608028531075
        total_loss: 0.04881419986486435
        vf_explained_var: 0.9983975291252136
        vf_loss: 0.062193986028432846
    load_time_ms: 0.806
    num_steps_sampled: 4776300
    num_steps_trained: 4758000
    sample_time_ms: 2136.355
    update_time_ms: 4.412
  iterations_since_restore: 183
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0492928042968055
    mean_inference_ms: 1.1123307291907323
    mean_processing_ms: 0.8800051689472655
  time_since_restore: 1262.8995757102966
  time_this_iter_s: 7.562167406082153
  time_total_s: 1262.8995757102966
  timestamp: 1563928320
  timesteps_since_restore: 4776300
  timesteps_this_iter: 26100
  timesteps_total: 4776300
  training_iteration: 183
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=31141], 1262 s, 183 iter, 4776300 ts, 19 rew
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew

Result for PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_02-32-07
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 38.532868259593485
  episode_reward_mean: 21.269950694737027
  episode_reward_min: 3.211131531089146
  episodes_this_iter: 174
  episodes_total: 32016
  experiment_id: 60286fbce0a343a4bf45a137b68d4730
  hostname: navel-notebook-1
  info:
    grad_time_ms: 4774.304
    learner:
      default_policy:
        cur_kl_coeff: 0.421875
        cur_lr: 9.999999747378752e-05
        entropy: 0.6611513495445251
        kl: 0.014365334063768387
        policy_loss: -0.029619941487908363
        total_loss: -0.005818742793053389
        vf_explained_var: 0.9996141195297241
        vf_loss: 0.017740823328495026
    load_time_ms: 0.818
    num_steps_sampled: 4802400
    num_steps_trained: 4784000
    sample_time_ms: 2132.948
    update_time_ms: 4.316
  iterations_since_restore: 184
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31141
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.0499528117665637
    mean_inference_ms: 1.1134891383808614
    mean_processing_ms: 0.8813718486590841
  time_since_restore: 1270.401971578598
  time_this_iter_s: 7.502395868301392
  time_total_s: 1270.401971578598
  timestamp: 1563928327
  timesteps_since_restore: 4802400
  timesteps_this_iter: 26100
  timesteps_total: 4802400
  training_iteration: 184
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 4.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 3})
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31141], 1270 s, 184 iter, 4802400 ts, 21.3 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 4.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-ppo
Number of trials: 3 ({'TERMINATED': 3})
TERMINATED trials:
 - PPO_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30530], 1479 s, 176 iter, 6388800 ts, 21.3 rew
 - PPO_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=30529], 1322 s, 208 iter, 5491200 ts, 21.4 rew
 - PPO_RoboschoolReacher-v1_2_num_envs_per_worker=4:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31141], 1270 s, 184 iter, 4802400 ts, 21.3 rew

[32m [  4099.72105s,  INFO] Experiment took 4099.51506 seconds | 68.32525 minutes | 1.13875 hours [0m
