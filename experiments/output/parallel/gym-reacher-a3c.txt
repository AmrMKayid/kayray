2019-07-18 03:57:48,318	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-18_03-57-48_318035_21286/logs.
2019-07-18 03:57:48,424	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-18 03:57:48,536	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-18 03:57:48,540	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-18 03:57:48,574	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-18_03-57-48_318035_21286/logs.
2019-07-18 03:57:48,574	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-18 03:57:48,732	INFO tune.py:61 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()
2019-07-18 03:57:48,732	INFO tune.py:233 -- Starting a new experiment.
2019-07-18 03:57:48,789	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-18 03:57:48,890	WARNING util.py:64 -- The `start_trial` operation took 0.14803457260131836 seconds to complete, which may be a performance bottleneck.
[32m [     0.21867s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.21899s,  INFO] Experiment configs: 
 {
  "gym-reacher-a3c": {
    "env": "RoboschoolReacher-v1",
    "run": "A3C",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 50,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 18,
      "training_iteration": 500
    },
    "config": {
      "sample_batch_size": 50,
      "clip_rewards": false,
      "num_gpus": 1,
      "num_workers": {
        "grid_search": [
          11,
          7
        ]
      },
      "num_envs_per_worker": 1
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 2.3/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 2.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING

[2m[36m(pid=21333)[0m [32m [     0.01605s,  INFO] TimeLimit:
[2m[36m(pid=21333)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21333)[0m - action_space = Box(2,)
[2m[36m(pid=21333)[0m - observation_space = Box(9,)
[2m[36m(pid=21333)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21333)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21333)[0m - _max_episode_steps = 150
[2m[36m(pid=21333)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21333)[0m 2019-07-18 03:57:50.749308: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21333)[0m 2019-07-18 03:57:50,813	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21333)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=21333)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21333)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=21333)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=21333)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=21333)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=21333)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21333)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21333)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21333)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,067	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.A3CTFPolicy object at 0x7ff90cf2d400>}
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,067	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7ff90d541358>}
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,067	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7ff90d5411d0>}
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,079	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.92, max=0.159, mean=-0.241)}}
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,080	INFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,080	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.92, max=0.159, mean=-0.241)
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,080	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.92, max=0.159, mean=-0.241)
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,081	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=21333)[0m                                   'env_id': 0,
[2m[36m(pid=21333)[0m                                   'info': None,
[2m[36m(pid=21333)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.92, max=0.159, mean=-0.241),
[2m[36m(pid=21333)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21333)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=21333)[0m                                   'rnn_state': []},
[2m[36m(pid=21333)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,083	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,099	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.51, max=0.174, mean=-0.168),
[2m[36m(pid=21333)[0m                       [],
[2m[36m(pid=21333)[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.137, max=0.137, mean=0.137),
[2m[36m(pid=21333)[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.183, max=0.183, mean=0.183)})}
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m 2019-07-18 03:57:51,237	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.002, max=0.159, mean=0.075),
[2m[36m(pid=21333)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-2.7, max=2.415, mean=0.09),
[2m[36m(pid=21333)[0m                         'advantages': np.ndarray((50,), dtype=float32, min=-12.711, max=0.878, mean=-5.266),
[2m[36m(pid=21333)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21333)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21333)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=1895104929.0, max=1895104929.0, mean=1895104929.0),
[2m[36m(pid=21333)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=21333)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-0.999, max=2.07, mean=-0.039),
[2m[36m(pid=21333)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-0.999, max=2.07, mean=-0.047),
[2m[36m(pid=21333)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.7, max=2.415, mean=0.091),
[2m[36m(pid=21333)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-1.641, max=0.515, mean=-0.26),
[2m[36m(pid=21333)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-1.641, max=0.515, mean=-0.258),
[2m[36m(pid=21333)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=21333)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21333)[0m                         'value_targets': np.ndarray((50,), dtype=float32, min=-12.569, max=0.908, mean=-5.177),
[2m[36m(pid=21333)[0m                         'vf_preds': np.ndarray((50,), dtype=float32, min=-0.064, max=0.337, mean=0.089)},
[2m[36m(pid=21333)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21531)[0m [32m [     0.03851s,  INFO] TimeLimit:
[2m[36m(pid=21531)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21531)[0m - action_space = Box(2,)
[2m[36m(pid=21531)[0m - observation_space = Box(9,)
[2m[36m(pid=21531)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21531)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21531)[0m - _max_episode_steps = 150
[2m[36m(pid=21531)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21531)[0m 2019-07-18 03:57:53,474	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21531)[0m 2019-07-18 03:57:53.474818: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21338)[0m [32m [     0.02632s,  INFO] TimeLimit:
[2m[36m(pid=21338)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21338)[0m - action_space = Box(2,)
[2m[36m(pid=21338)[0m - observation_space = Box(9,)
[2m[36m(pid=21338)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21338)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21338)[0m - _max_episode_steps = 150
[2m[36m(pid=21338)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21338)[0m 2019-07-18 03:57:53,558	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21338)[0m 2019-07-18 03:57:53.559423: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21337)[0m 2019-07-18 03:57:53,644	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21337)[0m 2019-07-18 03:57:53.645178: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21337)[0m [32m [     0.02669s,  INFO] TimeLimit:
[2m[36m(pid=21337)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21337)[0m - action_space = Box(2,)
[2m[36m(pid=21337)[0m - observation_space = Box(9,)
[2m[36m(pid=21337)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21337)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21337)[0m - _max_episode_steps = 150
[2m[36m(pid=21337)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21339)[0m 2019-07-18 03:57:53,716	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21339)[0m 2019-07-18 03:57:53.716747: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21339)[0m [32m [     0.03838s,  INFO] TimeLimit:
[2m[36m(pid=21339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21339)[0m - action_space = Box(2,)
[2m[36m(pid=21339)[0m - observation_space = Box(9,)
[2m[36m(pid=21339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21339)[0m - _max_episode_steps = 150
[2m[36m(pid=21339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21330)[0m [32m [     0.03833s,  INFO] TimeLimit:
[2m[36m(pid=21330)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21330)[0m - action_space = Box(2,)
[2m[36m(pid=21330)[0m - observation_space = Box(9,)
[2m[36m(pid=21330)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21330)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21330)[0m - _max_episode_steps = 150
[2m[36m(pid=21330)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21330)[0m 2019-07-18 03:57:53,691	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21330)[0m 2019-07-18 03:57:53.691871: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21336)[0m [32m [     0.03972s,  INFO] TimeLimit:
[2m[36m(pid=21336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21336)[0m - action_space = Box(2,)
[2m[36m(pid=21336)[0m - observation_space = Box(9,)
[2m[36m(pid=21336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21336)[0m - _max_episode_steps = 150
[2m[36m(pid=21336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21336)[0m 2019-07-18 03:57:53,685	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21336)[0m 2019-07-18 03:57:53.685813: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21334)[0m 2019-07-18 03:57:53,779	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21334)[0m 2019-07-18 03:57:53.780377: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21334)[0m [32m [     0.02802s,  INFO] TimeLimit:
[2m[36m(pid=21334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21334)[0m - action_space = Box(2,)
[2m[36m(pid=21334)[0m - observation_space = Box(9,)
[2m[36m(pid=21334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21334)[0m - _max_episode_steps = 150
[2m[36m(pid=21334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21535)[0m 2019-07-18 03:57:53,772	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21535)[0m 2019-07-18 03:57:53.773195: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21535)[0m [32m [     0.03948s,  INFO] TimeLimit:
[2m[36m(pid=21535)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21535)[0m - action_space = Box(2,)
[2m[36m(pid=21535)[0m - observation_space = Box(9,)
[2m[36m(pid=21535)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21535)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21535)[0m - _max_episode_steps = 150
[2m[36m(pid=21535)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21335)[0m 2019-07-18 03:57:53,819	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21335)[0m 2019-07-18 03:57:53.820462: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21335)[0m [32m [     0.03979s,  INFO] TimeLimit:
[2m[36m(pid=21335)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21335)[0m - action_space = Box(2,)
[2m[36m(pid=21335)[0m - observation_space = Box(9,)
[2m[36m(pid=21335)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21335)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21335)[0m - _max_episode_steps = 150
[2m[36m(pid=21335)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21331)[0m 2019-07-18 03:57:53,803	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21331)[0m 2019-07-18 03:57:53.804006: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21331)[0m [32m [     0.04110s,  INFO] TimeLimit:
[2m[36m(pid=21331)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21331)[0m - action_space = Box(2,)
[2m[36m(pid=21331)[0m - observation_space = Box(9,)
[2m[36m(pid=21331)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21331)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21331)[0m - _max_episode_steps = 150
[2m[36m(pid=21331)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21336)[0m 2019-07-18 03:57:53,841	INFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21336)[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=21336)[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21336)[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,
[2m[36m(pid=21336)[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=21336)[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 9) dtype=float32>,
[2m[36m(pid=21336)[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,
[2m[36m(pid=21336)[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21336)[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21336)[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
[2m[36m(pid=21336)[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21340)[0m 2019-07-18 03:57:53,853	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=21340)[0m 2019-07-18 03:57:53.853886: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=21340)[0m [32m [     0.04000s,  INFO] TimeLimit:
[2m[36m(pid=21340)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=21340)[0m - action_space = Box(2,)
[2m[36m(pid=21340)[0m - observation_space = Box(9,)
[2m[36m(pid=21340)[0m - reward_range = (-inf, inf)
[2m[36m(pid=21340)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=21340)[0m - _max_episode_steps = 150
[2m[36m(pid=21340)[0m - _elapsed_steps = None [0m
[2m[36m(pid=21333)[0m 2019-07-18 03:57:54,551	INFO rollout_worker.py:529 -- Apply gradients:
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21333)[0m [ np.ndarray((9, 256), dtype=float32, min=-0.518, max=0.542, mean=-0.001),
[2m[36m(pid=21333)[0m   np.ndarray((256,), dtype=float32, min=-0.8, max=0.624, mean=-0.01),
[2m[36m(pid=21333)[0m   np.ndarray((256, 256), dtype=float32, min=-0.538, max=0.537, mean=0.0),
[2m[36m(pid=21333)[0m   np.ndarray((256,), dtype=float32, min=-0.641, max=0.751, mean=0.007),
[2m[36m(pid=21333)[0m   np.ndarray((256, 4), dtype=float32, min=-3.047, max=3.592, mean=0.006),
[2m[36m(pid=21333)[0m   np.ndarray((4,), dtype=float32, min=-0.024, max=4.909, mean=2.191),
[2m[36m(pid=21333)[0m   np.ndarray((256, 1), dtype=float32, min=-3.396, max=2.795, mean=-0.019),
[2m[36m(pid=21333)[0m   np.ndarray((1,), dtype=float32, min=-4.65, max=-4.65, mean=-4.65)]
[2m[36m(pid=21333)[0m 
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,516	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.638, max=0.77, mean=0.046)}}
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,516	INFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,516	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.638, max=0.77, mean=0.046)
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,517	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.638, max=0.77, mean=0.046)
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,518	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=21336)[0m                                   'env_id': 0,
[2m[36m(pid=21336)[0m                                   'info': None,
[2m[36m(pid=21336)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.638, max=0.77, mean=0.046),
[2m[36m(pid=21336)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=21336)[0m                                   'rnn_state': []},
[2m[36m(pid=21336)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,522	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,535	INFO rollout_worker.py:428 -- Generating sample batch of size 50
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,575	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.442, max=1.077, mean=0.76),
[2m[36m(pid=21336)[0m                       [],
[2m[36m(pid=21336)[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.08, max=0.08, mean=0.08),
[2m[36m(pid=21336)[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.362, max=-0.362, mean=-0.362)})}
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,720	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.001, max=0.158, mean=0.084),
[2m[36m(pid=21336)[0m                         'actions': np.ndarray((50, 2), dtype=float32, min=-2.438, max=2.599, mean=-0.122),
[2m[36m(pid=21336)[0m                         'advantages': np.ndarray((50,), dtype=float32, min=-20.576, max=1.428, mean=-9.125),
[2m[36m(pid=21336)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=753745265.0, max=753745265.0, mean=753745265.0),
[2m[36m(pid=21336)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=21336)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-2.943, max=1.516, mean=-0.142),
[2m[36m(pid=21336)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-2.943, max=1.516, mean=-0.139),
[2m[36m(pid=21336)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.438, max=2.599, mean=-0.109),
[2m[36m(pid=21336)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-2.193, max=5.188, mean=-0.131),
[2m[36m(pid=21336)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-2.193, max=5.188, mean=-0.16),
[2m[36m(pid=21336)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=21336)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m                         'value_targets': np.ndarray((50,), dtype=float32, min=-20.702, max=1.393, mean=-9.328),
[2m[36m(pid=21336)[0m                         'vf_preds': np.ndarray((50,), dtype=float32, min=-0.586, max=0.143, mean=-0.203)},
[2m[36m(pid=21336)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,731	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.001, max=0.158, mean=0.084),
[2m[36m(pid=21336)[0m             'actions': np.ndarray((50, 2), dtype=float32, min=-2.438, max=2.599, mean=-0.122),
[2m[36m(pid=21336)[0m             'advantages': np.ndarray((50,), dtype=float32, min=-20.576, max=1.428, mean=-9.125),
[2m[36m(pid=21336)[0m             'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m             'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m             'eps_id': np.ndarray((50,), dtype=int64, min=753745265.0, max=753745265.0, mean=753745265.0),
[2m[36m(pid=21336)[0m             'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=21336)[0m             'new_obs': np.ndarray((50, 9), dtype=float32, min=-2.943, max=1.516, mean=-0.142),
[2m[36m(pid=21336)[0m             'obs': np.ndarray((50, 9), dtype=float32, min=-2.943, max=1.516, mean=-0.139),
[2m[36m(pid=21336)[0m             'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.438, max=2.599, mean=-0.109),
[2m[36m(pid=21336)[0m             'prev_rewards': np.ndarray((50,), dtype=float32, min=-2.193, max=5.188, mean=-0.131),
[2m[36m(pid=21336)[0m             'rewards': np.ndarray((50,), dtype=float32, min=-2.193, max=5.188, mean=-0.16),
[2m[36m(pid=21336)[0m             't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=21336)[0m             'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m             'value_targets': np.ndarray((50,), dtype=float32, min=-20.702, max=1.393, mean=-9.328),
[2m[36m(pid=21336)[0m             'vf_preds': np.ndarray((50,), dtype=float32, min=-0.586, max=0.143, mean=-0.203)},
[2m[36m(pid=21336)[0m   'type': 'SampleBatch'}
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m 2019-07-18 03:57:54,736	INFO rollout_worker.py:498 -- Compute gradients on:
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'data': { 'action_prob': np.ndarray((50,), dtype=float32, min=0.001, max=0.158, mean=0.084),
[2m[36m(pid=21336)[0m             'actions': np.ndarray((50, 2), dtype=float32, min=-2.438, max=2.599, mean=-0.122),
[2m[36m(pid=21336)[0m             'advantages': np.ndarray((50,), dtype=float32, min=-20.576, max=1.428, mean=-9.125),
[2m[36m(pid=21336)[0m             'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m             'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m             'eps_id': np.ndarray((50,), dtype=int64, min=753745265.0, max=753745265.0, mean=753745265.0),
[2m[36m(pid=21336)[0m             'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=21336)[0m             'new_obs': np.ndarray((50, 9), dtype=float32, min=-2.943, max=1.516, mean=-0.142),
[2m[36m(pid=21336)[0m             'obs': np.ndarray((50, 9), dtype=float32, min=-2.943, max=1.516, mean=-0.139),
[2m[36m(pid=21336)[0m             'prev_actions': np.ndarray((50, 2), dtype=float32, min=-2.438, max=2.599, mean=-0.109),
[2m[36m(pid=21336)[0m             'prev_rewards': np.ndarray((50,), dtype=float32, min=-2.193, max=5.188, mean=-0.131),
[2m[36m(pid=21336)[0m             'rewards': np.ndarray((50,), dtype=float32, min=-2.193, max=5.188, mean=-0.16),
[2m[36m(pid=21336)[0m             't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=21336)[0m             'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=21336)[0m             'value_targets': np.ndarray((50,), dtype=float32, min=-20.702, max=1.393, mean=-9.328),
[2m[36m(pid=21336)[0m             'vf_preds': np.ndarray((50,), dtype=float32, min=-0.586, max=0.143, mean=-0.203)},
[2m[36m(pid=21336)[0m   'type': 'SampleBatch'}
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m 2019-07-18 03:57:55,023	INFO rollout_worker.py:523 -- Compute grad info:
[2m[36m(pid=21336)[0m 
[2m[36m(pid=21336)[0m { 'batch_count': 50,
[2m[36m(pid=21336)[0m   'learner_stats': { 'cur_lr': 9.999999747378752e-05,
[2m[36m(pid=21336)[0m                      'grad_gnorm': 40.000008,
[2m[36m(pid=21336)[0m                      'model': {},
[2m[36m(pid=21336)[0m                      'policy_entropy': 141.91519,
[2m[36m(pid=21336)[0m                      'policy_loss': -1285.212,
[2m[36m(pid=21336)[0m                      'var_gnorm': 22.649506,
[2m[36m(pid=21336)[0m                      'vf_explained_var': -0.0011421442,
[2m[36m(pid=21336)[0m                      'vf_loss': 3342.1238}}
[2m[36m(pid=21336)[0m 
Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-57-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.366045218569317
  episode_reward_mean: -15.991983629823515
  episode_reward_min: -43.75020728296896
  episodes_this_iter: 71
  episodes_total: 71
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.82
    dispatch_time_ms: 7.058
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.00000762939453
      model: {}
      policy_entropy: 142.6671905517578
      policy_loss: -906.228271484375
      var_gnorm: 22.65283203125
      vf_explained_var: 0.04981297254562378
      vf_loss: 1020.625732421875
    num_steps_sampled: 10000
    num_steps_trained: 10000
    wait_time_ms: 1.233
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.44318155173451107
    mean_inference_ms: 1.6750008486284653
    mean_processing_ms: 0.2879207677060008
  time_since_restore: 5.702159881591797
  time_this_iter_s: 5.702159881591797
  time_total_s: 5.702159881591797
  timestamp: 1563415076
  timesteps_since_restore: 10000
  timesteps_this_iter: 10000
  timesteps_total: 10000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 4.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 5 s, 1 iter, 10000 ts, -16 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.25953142386737
  episode_reward_mean: -12.642333312896481
  episode_reward_min: -49.1374101621386
  episodes_this_iter: 200
  episodes_total: 271
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.739
    dispatch_time_ms: 6.402
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 143.84109497070312
      policy_loss: 496.71905517578125
      var_gnorm: 22.660184860229492
      vf_explained_var: 0.11034470796585083
      vf_loss: 1429.502685546875
    num_steps_sampled: 40000
    num_steps_trained: 40000
    wait_time_ms: 0.941
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.421876847545908
    mean_inference_ms: 1.427283746563198
    mean_processing_ms: 0.2864376326227238
  time_since_restore: 11.287948608398438
  time_this_iter_s: 5.585788726806641
  time_total_s: 11.287948608398438
  timestamp: 1563415082
  timesteps_since_restore: 40000
  timesteps_this_iter: 30000
  timesteps_total: 40000
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 11 s, 2 iter, 40000 ts, -12.6 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.179763397359139
  episode_reward_mean: -10.843895231099298
  episode_reward_min: -41.57634669655468
  episodes_this_iter: 201
  episodes_total: 472
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.677
    dispatch_time_ms: 6.208
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: 137.7593231201172
      policy_loss: -125.32127380371094
      var_gnorm: 22.68638038635254
      vf_explained_var: -0.10349559783935547
      vf_loss: 307.8099365234375
    num_steps_sampled: 70000
    num_steps_trained: 70000
    wait_time_ms: 0.873
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4179611386641433
    mean_inference_ms: 1.37267244131292
    mean_processing_ms: 0.28556318684624615
  time_since_restore: 16.691265106201172
  time_this_iter_s: 5.403316497802734
  time_total_s: 16.691265106201172
  timestamp: 1563415088
  timesteps_since_restore: 70000
  timesteps_this_iter: 30000
  timesteps_total: 70000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 16 s, 3 iter, 70000 ts, -10.8 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.52437140344773
  episode_reward_mean: -9.576660063526507
  episode_reward_min: -42.51285139909521
  episodes_this_iter: 198
  episodes_total: 670
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.557
    dispatch_time_ms: 6.256
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 133.19471740722656
      policy_loss: -345.48406982421875
      var_gnorm: 22.71847152709961
      vf_explained_var: 0.36640065908432007
      vf_loss: 366.63671875
    num_steps_sampled: 100000
    num_steps_trained: 100000
    wait_time_ms: 0.992
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4156666655197212
    mean_inference_ms: 1.3515076424437718
    mean_processing_ms: 0.2847370408073351
  time_since_restore: 22.084001779556274
  time_this_iter_s: 5.3927366733551025
  time_total_s: 22.084001779556274
  timestamp: 1563415093
  timesteps_since_restore: 100000
  timesteps_this_iter: 30000
  timesteps_total: 100000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 22 s, 4 iter, 100000 ts, -9.58 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.1106711725333
  episode_reward_mean: -11.474460247037495
  episode_reward_min: -40.3878057759079
  episodes_this_iter: 201
  episodes_total: 871
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.79
    dispatch_time_ms: 6.528
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.00000762939453
      model: {}
      policy_entropy: 137.9020233154297
      policy_loss: 462.42474365234375
      var_gnorm: 22.76837921142578
      vf_explained_var: 0.03342270851135254
      vf_loss: 1843.0474853515625
    num_steps_sampled: 130000
    num_steps_trained: 130000
    wait_time_ms: 3.95
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41471802271373337
    mean_inference_ms: 1.3391976432488664
    mean_processing_ms: 0.283700450550386
  time_since_restore: 27.512576580047607
  time_this_iter_s: 5.428574800491333
  time_total_s: 27.512576580047607
  timestamp: 1563415098
  timesteps_since_restore: 130000
  timesteps_this_iter: 30000
  timesteps_total: 130000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 27 s, 5 iter, 130000 ts, -11.5 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 21.33368119300447
  episode_reward_mean: -9.60234141587639
  episode_reward_min: -41.47226968877719
  episodes_this_iter: 199
  episodes_total: 1070
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.824
    dispatch_time_ms: 4.722
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: 130.66900634765625
      policy_loss: 204.0422821044922
      var_gnorm: 22.803260803222656
      vf_explained_var: 0.059222280979156494
      vf_loss: 462.3531799316406
    num_steps_sampled: 160000
    num_steps_trained: 160000
    wait_time_ms: 3.718
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4141572295475704
    mean_inference_ms: 1.330591668397547
    mean_processing_ms: 0.28366977482340844
  time_since_restore: 32.94528007507324
  time_this_iter_s: 5.432703495025635
  time_total_s: 32.94528007507324
  timestamp: 1563415104
  timesteps_since_restore: 160000
  timesteps_this_iter: 30000
  timesteps_total: 160000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 32 s, 6 iter, 160000 ts, -9.6 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.66817621290497
  episode_reward_mean: -10.641371357911785
  episode_reward_min: -42.77984599667328
  episodes_this_iter: 201
  episodes_total: 1271
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.913
    dispatch_time_ms: 4.857
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 130.21209716796875
      policy_loss: 1194.6778564453125
      var_gnorm: 22.827083587646484
      vf_explained_var: 0.2175203561782837
      vf_loss: 3014.728515625
    num_steps_sampled: 190000
    num_steps_trained: 190000
    wait_time_ms: 0.986
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4137192102474028
    mean_inference_ms: 1.325573441443309
    mean_processing_ms: 0.2829678140657517
  time_since_restore: 38.406975746154785
  time_this_iter_s: 5.461695671081543
  time_total_s: 38.406975746154785
  timestamp: 1563415109
  timesteps_since_restore: 190000
  timesteps_this_iter: 30000
  timesteps_total: 190000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 5.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 38 s, 7 iter, 190000 ts, -10.6 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.62079890375868
  episode_reward_mean: -11.005851410011868
  episode_reward_min: -45.722874597907236
  episodes_this_iter: 197
  episodes_total: 1468
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.695
    dispatch_time_ms: 6.776
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 39.999996185302734
      model: {}
      policy_entropy: 131.4560089111328
      policy_loss: 546.391845703125
      var_gnorm: 22.852548599243164
      vf_explained_var: -0.06262660026550293
      vf_loss: 596.1976318359375
    num_steps_sampled: 220000
    num_steps_trained: 220000
    wait_time_ms: 1.82
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4134554174262303
    mean_inference_ms: 1.3224833407216061
    mean_processing_ms: 0.2825426604698184
  time_since_restore: 43.795562982559204
  time_this_iter_s: 5.388587236404419
  time_total_s: 43.795562982559204
  timestamp: 1563415115
  timesteps_since_restore: 220000
  timesteps_this_iter: 30000
  timesteps_total: 220000
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 43 s, 8 iter, 220000 ts, -11 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.442760319305675
  episode_reward_mean: -10.426559395600087
  episode_reward_min: -51.446705816666274
  episodes_this_iter: 201
  episodes_total: 1669
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.081
    dispatch_time_ms: 4.706
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: 130.46324157714844
      policy_loss: 53.84465026855469
      var_gnorm: 22.865154266357422
      vf_explained_var: 0.0943564772605896
      vf_loss: 42.66576385498047
    num_steps_sampled: 250000
    num_steps_trained: 250000
    wait_time_ms: 5.951
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41315841734136943
    mean_inference_ms: 1.3197422383632187
    mean_processing_ms: 0.2822814527709765
  time_since_restore: 49.247897148132324
  time_this_iter_s: 5.45233416557312
  time_total_s: 49.247897148132324
  timestamp: 1563415120
  timesteps_since_restore: 250000
  timesteps_this_iter: 30000
  timesteps_total: 250000
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 49 s, 9 iter, 250000 ts, -10.4 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.233789665062737
  episode_reward_mean: -7.644181141004264
  episode_reward_min: -35.98907056567215
  episodes_this_iter: 201
  episodes_total: 1870
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.265
    dispatch_time_ms: 4.856
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 39.999996185302734
      model: {}
      policy_entropy: 125.78008270263672
      policy_loss: 395.2055358886719
      var_gnorm: 22.881032943725586
      vf_explained_var: 0.39156895875930786
      vf_loss: 613.2459716796875
    num_steps_sampled: 280000
    num_steps_trained: 280000
    wait_time_ms: 5.838
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4129247948884827
    mean_inference_ms: 1.3173178687368878
    mean_processing_ms: 0.28193608140572896
  time_since_restore: 54.68813896179199
  time_this_iter_s: 5.440241813659668
  time_total_s: 54.68813896179199
  timestamp: 1563415126
  timesteps_since_restore: 280000
  timesteps_this_iter: 30000
  timesteps_total: 280000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 54 s, 10 iter, 280000 ts, -7.64 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.43961517039266
  episode_reward_mean: -8.430971524794971
  episode_reward_min: -30.502275481622835
  episodes_this_iter: 197
  episodes_total: 2067
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.818
    dispatch_time_ms: 5.913
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.00000762939453
      model: {}
      policy_entropy: 115.21015930175781
      policy_loss: 28.498493194580078
      var_gnorm: 22.9121150970459
      vf_explained_var: -0.10445630550384521
      vf_loss: 22.758264541625977
    num_steps_sampled: 310000
    num_steps_trained: 310000
    wait_time_ms: 3.396
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41298204137503547
    mean_inference_ms: 1.3165955340695004
    mean_processing_ms: 0.2813554370427576
  time_since_restore: 60.086386919021606
  time_this_iter_s: 5.398247957229614
  time_total_s: 60.086386919021606
  timestamp: 1563415131
  timesteps_since_restore: 310000
  timesteps_this_iter: 30000
  timesteps_total: 310000
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 60 s, 11 iter, 310000 ts, -8.43 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-58-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.471871085512177
  episode_reward_mean: -7.044403713131612
  episode_reward_min: -31.92119488166473
  episodes_this_iter: 200
  episodes_total: 2267
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.479
    dispatch_time_ms: 5.378
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: 117.75689697265625
      policy_loss: 137.99777221679688
      var_gnorm: 22.918960571289062
      vf_explained_var: 0.18952572345733643
      vf_loss: 83.93975830078125
    num_steps_sampled: 340000
    num_steps_trained: 340000
    wait_time_ms: 6.442
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4129099523783533
    mean_inference_ms: 1.3159592249752274
    mean_processing_ms: 0.28114283318524097
  time_since_restore: 65.51315641403198
  time_this_iter_s: 5.426769495010376
  time_total_s: 65.51315641403198
  timestamp: 1563415137
  timesteps_since_restore: 340000
  timesteps_this_iter: 30000
  timesteps_total: 340000
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 65 s, 12 iter, 340000 ts, -7.04 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.170334758020452
  episode_reward_mean: -7.033060246217685
  episode_reward_min: -31.58286943848133
  episodes_this_iter: 200
  episodes_total: 2467
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.792
    dispatch_time_ms: 5.443
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 123.41487121582031
      policy_loss: 135.1668243408203
      var_gnorm: 22.93948745727539
      vf_explained_var: 0.021792590618133545
      vf_loss: 148.78253173828125
    num_steps_sampled: 370000
    num_steps_trained: 370000
    wait_time_ms: 5.229
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4127719289878036
    mean_inference_ms: 1.315214224222416
    mean_processing_ms: 0.28080832316175497
  time_since_restore: 70.93178820610046
  time_this_iter_s: 5.4186317920684814
  time_total_s: 70.93178820610046
  timestamp: 1563415142
  timesteps_since_restore: 370000
  timesteps_this_iter: 30000
  timesteps_total: 370000
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 6.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 70 s, 13 iter, 370000 ts, -7.03 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.676720776264053
  episode_reward_mean: -5.939532641234098
  episode_reward_min: -32.03551794157752
  episodes_this_iter: 199
  episodes_total: 2666
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.746
    dispatch_time_ms: 4.995
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 39.999996185302734
      model: {}
      policy_entropy: 103.43404388427734
      policy_loss: -98.07117462158203
      var_gnorm: 22.947265625
      vf_explained_var: 0.20710265636444092
      vf_loss: 91.61090850830078
    num_steps_sampled: 400000
    num_steps_trained: 400000
    wait_time_ms: 7.517
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4125267450836622
    mean_inference_ms: 1.3147352527011615
    mean_processing_ms: 0.28042221598854383
  time_since_restore: 76.40303564071655
  time_this_iter_s: 5.471247434616089
  time_total_s: 76.40303564071655
  timestamp: 1563415147
  timesteps_since_restore: 400000
  timesteps_this_iter: 30000
  timesteps_total: 400000
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 76 s, 14 iter, 400000 ts, -5.94 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.877066329298803
  episode_reward_mean: -5.462982110393397
  episode_reward_min: -32.051136739308355
  episodes_this_iter: 198
  episodes_total: 2864
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.514
    dispatch_time_ms: 5.096
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 104.23848724365234
      policy_loss: 38.243900299072266
      var_gnorm: 22.9660701751709
      vf_explained_var: -0.2556065320968628
      vf_loss: 35.87446212768555
    num_steps_sampled: 430000
    num_steps_trained: 430000
    wait_time_ms: 5.535
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41253446533615107
    mean_inference_ms: 1.314296832913943
    mean_processing_ms: 0.28028688721536377
  time_since_restore: 81.83789706230164
  time_this_iter_s: 5.434861421585083
  time_total_s: 81.83789706230164
  timestamp: 1563415153
  timesteps_since_restore: 430000
  timesteps_this_iter: 30000
  timesteps_total: 430000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 81 s, 15 iter, 430000 ts, -5.46 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.525897529723842
  episode_reward_mean: -7.408366776987461
  episode_reward_min: -31.07955217136727
  episodes_this_iter: 204
  episodes_total: 3068
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.754
    dispatch_time_ms: 4.108
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 39.99999237060547
      model: {}
      policy_entropy: 95.06375122070312
      policy_loss: -13.567181587219238
      var_gnorm: 22.986421585083008
      vf_explained_var: -0.3336615562438965
      vf_loss: 4.090642929077148
    num_steps_sampled: 460000
    num_steps_trained: 460000
    wait_time_ms: 6.608
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41254154264726683
    mean_inference_ms: 1.3136029357890442
    mean_processing_ms: 0.2802732042443156
  time_since_restore: 87.31732153892517
  time_this_iter_s: 5.479424476623535
  time_total_s: 87.31732153892517
  timestamp: 1563415158
  timesteps_since_restore: 460000
  timesteps_this_iter: 30000
  timesteps_total: 460000
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 87 s, 16 iter, 460000 ts, -7.41 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.03346893657898
  episode_reward_mean: -4.797977506357659
  episode_reward_min: -31.610077744277298
  episodes_this_iter: 198
  episodes_total: 3266
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.733
    dispatch_time_ms: 5.619
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: 112.84259033203125
      policy_loss: -150.34129333496094
      var_gnorm: 23.010730743408203
      vf_explained_var: 0.4392181634902954
      vf_loss: 400.87255859375
    num_steps_sampled: 490000
    num_steps_trained: 490000
    wait_time_ms: 3.133
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4125533971262061
    mean_inference_ms: 1.31368459292356
    mean_processing_ms: 0.280041631002342
  time_since_restore: 92.73009085655212
  time_this_iter_s: 5.412769317626953
  time_total_s: 92.73009085655212
  timestamp: 1563415164
  timesteps_since_restore: 490000
  timesteps_this_iter: 30000
  timesteps_total: 490000
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 92 s, 17 iter, 490000 ts, -4.8 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.648636214480725
  episode_reward_mean: -4.693274170872694
  episode_reward_min: -29.335783083236134
  episodes_this_iter: 202
  episodes_total: 3468
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.06
    dispatch_time_ms: 5.454
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 95.20658874511719
      policy_loss: 111.66328430175781
      var_gnorm: 23.015079498291016
      vf_explained_var: 0.03544127941131592
      vf_loss: 573.858154296875
    num_steps_sampled: 520000
    num_steps_trained: 520000
    wait_time_ms: 6.015
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41255464850817153
    mean_inference_ms: 1.3133649626683759
    mean_processing_ms: 0.2797413918105388
  time_since_restore: 98.22751712799072
  time_this_iter_s: 5.497426271438599
  time_total_s: 98.22751712799072
  timestamp: 1563415169
  timesteps_since_restore: 520000
  timesteps_this_iter: 30000
  timesteps_total: 520000
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 98 s, 18 iter, 520000 ts, -4.69 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.08574966219094
  episode_reward_mean: -4.128123257870887
  episode_reward_min: -32.32144960247183
  episodes_this_iter: 198
  episodes_total: 3666
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.867
    dispatch_time_ms: 4.282
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 39.999996185302734
      model: {}
      policy_entropy: 91.13709259033203
      policy_loss: 10.863975524902344
      var_gnorm: 23.02430534362793
      vf_explained_var: 0.23601078987121582
      vf_loss: 29.190662384033203
    num_steps_sampled: 550000
    num_steps_trained: 550000
    wait_time_ms: 4.005
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4125986588060618
    mean_inference_ms: 1.3130707169435456
    mean_processing_ms: 0.27987100555420813
  time_since_restore: 103.63537549972534
  time_this_iter_s: 5.407858371734619
  time_total_s: 103.63537549972534
  timestamp: 1563415175
  timesteps_since_restore: 550000
  timesteps_this_iter: 30000
  timesteps_total: 550000
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 7.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 103 s, 19 iter, 550000 ts, -4.13 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.682098074482536
  episode_reward_mean: -5.315416153055104
  episode_reward_min: -30.340133251883632
  episodes_this_iter: 200
  episodes_total: 3866
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.849
    dispatch_time_ms: 6.413
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.00000762939453
      model: {}
      policy_entropy: 89.25048065185547
      policy_loss: 4.164132118225098
      var_gnorm: 23.03789710998535
      vf_explained_var: -0.28584933280944824
      vf_loss: 3.2910821437835693
    num_steps_sampled: 580000
    num_steps_trained: 580000
    wait_time_ms: 1.663
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41253405124114567
    mean_inference_ms: 1.312535193047377
    mean_processing_ms: 0.2796269283296026
  time_since_restore: 109.08271336555481
  time_this_iter_s: 5.447337865829468
  time_total_s: 109.08271336555481
  timestamp: 1563415180
  timesteps_since_restore: 580000
  timesteps_this_iter: 30000
  timesteps_total: 580000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 109 s, 20 iter, 580000 ts, -5.32 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 17.426313624105468
  episode_reward_mean: -4.851262128700179
  episode_reward_min: -30.07480884084601
  episodes_this_iter: 199
  episodes_total: 4065
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.141
    dispatch_time_ms: 4.739
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: 103.5737075805664
      policy_loss: -308.9823913574219
      var_gnorm: 23.054908752441406
      vf_explained_var: -0.22536969184875488
      vf_loss: 1270.4884033203125
    num_steps_sampled: 610000
    num_steps_trained: 610000
    wait_time_ms: 5.561
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4126050878365121
    mean_inference_ms: 1.3123401794799041
    mean_processing_ms: 0.2795263116675218
  time_since_restore: 114.53704929351807
  time_this_iter_s: 5.454335927963257
  time_total_s: 114.53704929351807
  timestamp: 1563415186
  timesteps_since_restore: 610000
  timesteps_this_iter: 30000
  timesteps_total: 610000
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 114 s, 21 iter, 610000 ts, -4.85 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.235286935753155
  episode_reward_mean: -3.891279380477143
  episode_reward_min: -30.678640155772296
  episodes_this_iter: 203
  episodes_total: 4268
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.67
    dispatch_time_ms: 6.008
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 103.7607192993164
      policy_loss: -75.02517700195312
      var_gnorm: 23.078731536865234
      vf_explained_var: 0.006114184856414795
      vf_loss: 54.61898422241211
    num_steps_sampled: 640000
    num_steps_trained: 640000
    wait_time_ms: 6.586
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4126261436168873
    mean_inference_ms: 1.3119072769404854
    mean_processing_ms: 0.2794127060510363
  time_since_restore: 120.00311350822449
  time_this_iter_s: 5.466064214706421
  time_total_s: 120.00311350822449
  timestamp: 1563415191
  timesteps_since_restore: 640000
  timesteps_this_iter: 30000
  timesteps_total: 640000
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 120 s, 22 iter, 640000 ts, -3.89 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_03-59-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.044181834228755
  episode_reward_mean: -4.082181809726056
  episode_reward_min: -35.221457125932496
  episodes_this_iter: 199
  episodes_total: 4467
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.962
    dispatch_time_ms: 5.431
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 87.7115249633789
      policy_loss: -5.099520206451416
      var_gnorm: 23.103309631347656
      vf_explained_var: 0.5818818807601929
      vf_loss: 0.612062931060791
    num_steps_sampled: 670000
    num_steps_trained: 670000
    wait_time_ms: 5.461
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41253828069946835
    mean_inference_ms: 1.311929521541592
    mean_processing_ms: 0.27917402356758897
  time_since_restore: 125.43893718719482
  time_this_iter_s: 5.435823678970337
  time_total_s: 125.43893718719482
  timestamp: 1563415197
  timesteps_since_restore: 670000
  timesteps_this_iter: 30000
  timesteps_total: 670000
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 125 s, 23 iter, 670000 ts, -4.08 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.000662099656044
  episode_reward_mean: -3.3540343658652105
  episode_reward_min: -32.91971299915354
  episodes_this_iter: 199
  episodes_total: 4666
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.083
    dispatch_time_ms: 5.638
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 89.52928161621094
      policy_loss: 0.7014450430870056
      var_gnorm: 23.12049674987793
      vf_explained_var: 0.6073623895645142
      vf_loss: 0.6234546303749084
    num_steps_sampled: 700000
    num_steps_trained: 700000
    wait_time_ms: 5.298
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4126238873918956
    mean_inference_ms: 1.3118261790702848
    mean_processing_ms: 0.2790170077831065
  time_since_restore: 130.87544345855713
  time_this_iter_s: 5.436506271362305
  time_total_s: 130.87544345855713
  timestamp: 1563415202
  timesteps_since_restore: 700000
  timesteps_this_iter: 30000
  timesteps_total: 700000
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 130 s, 24 iter, 700000 ts, -3.35 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.824428169503342
  episode_reward_mean: -27.489089494557398
  episode_reward_min: -189.68618599456374
  episodes_this_iter: 202
  episodes_total: 4868
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.627
    dispatch_time_ms: 5.535
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.00000762939453
      model: {}
      policy_entropy: -16.655784606933594
      policy_loss: 1182.6199951171875
      var_gnorm: 23.237817764282227
      vf_explained_var: 0.04078710079193115
      vf_loss: 2300.913818359375
    num_steps_sampled: 730000
    num_steps_trained: 730000
    wait_time_ms: 8.756
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41249286618327075
    mean_inference_ms: 1.3116216099692568
    mean_processing_ms: 0.2789406499687348
  time_since_restore: 136.3589792251587
  time_this_iter_s: 5.4835357666015625
  time_total_s: 136.3589792251587
  timestamp: 1563415208
  timesteps_since_restore: 730000
  timesteps_this_iter: 30000
  timesteps_total: 730000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 136 s, 25 iter, 730000 ts, -27.5 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -4.909835153408504
  episode_reward_mean: -100.84837636027953
  episode_reward_min: -191.2342501089559
  episodes_this_iter: 201
  episodes_total: 5069
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.172
    dispatch_time_ms: 5.332
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: -369.6964111328125
      policy_loss: -178242887614464.0
      var_gnorm: 23.497791290283203
      vf_explained_var: -0.0028684139251708984
      vf_loss: 5619.35888671875
    num_steps_sampled: 760000
    num_steps_trained: 760000
    wait_time_ms: 3.939
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41278233365726724
    mean_inference_ms: 1.3111020671434572
    mean_processing_ms: 0.27896086013395616
  time_since_restore: 141.8969748020172
  time_this_iter_s: 5.5379955768585205
  time_total_s: 141.8969748020172
  timestamp: 1563415213
  timesteps_since_restore: 760000
  timesteps_this_iter: 30000
  timesteps_total: 760000
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 141 s, 26 iter, 760000 ts, -101 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -2.2472346309074336
  episode_reward_mean: -97.995732333548
  episode_reward_min: -203.2087270410601
  episodes_this_iter: 200
  episodes_total: 5269
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.887
    dispatch_time_ms: 5.792
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 347.17523193359375
      policy_loss: -2992.198974609375
      var_gnorm: 23.487682342529297
      vf_explained_var: 0.20488786697387695
      vf_loss: 653.3011474609375
    num_steps_sampled: 790000
    num_steps_trained: 790000
    wait_time_ms: 1.417
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4129470226549307
    mean_inference_ms: 1.310703984603344
    mean_processing_ms: 0.2790937245873166
  time_since_restore: 147.35502815246582
  time_this_iter_s: 5.458053350448608
  time_total_s: 147.35502815246582
  timestamp: 1563415219
  timesteps_since_restore: 790000
  timesteps_this_iter: 30000
  timesteps_total: 790000
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 147 s, 27 iter, 790000 ts, -98 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: -14.855950114938873
  episode_reward_mean: -106.29063906137463
  episode_reward_min: -194.16022849854352
  episodes_this_iter: 200
  episodes_total: 5469
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.71
    dispatch_time_ms: 5.16
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.000003814697266
      model: {}
      policy_entropy: -306.3455505371094
      policy_loss: -7.753131510831514e+16
      var_gnorm: 23.442659378051758
      vf_explained_var: 0.0016922950744628906
      vf_loss: 9241.7626953125
    num_steps_sampled: 820000
    num_steps_trained: 820000
    wait_time_ms: 3.437
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4132162803309666
    mean_inference_ms: 1.3105213647918128
    mean_processing_ms: 0.2790972032992132
  time_since_restore: 152.78870034217834
  time_this_iter_s: 5.433672189712524
  time_total_s: 152.78870034217834
  timestamp: 1563415224
  timesteps_since_restore: 820000
  timesteps_this_iter: 30000
  timesteps_total: 820000
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 152 s, 28 iter, 820000 ts, -106 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.380248933062129
  episode_reward_mean: -37.51456086319276
  episode_reward_min: -187.99841588377762
  episodes_this_iter: 201
  episodes_total: 5670
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.653
    dispatch_time_ms: 5.249
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 40.0
      model: {}
      policy_entropy: 551.8475952148438
      policy_loss: 5093.7958984375
      var_gnorm: 23.964113235473633
      vf_explained_var: 0.042805016040802
      vf_loss: 785.498046875
    num_steps_sampled: 850000
    num_steps_trained: 850000
    wait_time_ms: 2.544
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41336503588303725
    mean_inference_ms: 1.3103519406686783
    mean_processing_ms: 0.2791490753676346
  time_since_restore: 158.2605984210968
  time_this_iter_s: 5.471898078918457
  time_total_s: 158.2605984210968
  timestamp: 1563415230
  timesteps_since_restore: 850000
  timesteps_this_iter: 30000
  timesteps_total: 850000
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 158 s, 29 iter, 850000 ts, -37.5 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 6.239971637333923
  episode_reward_mean: -66.91912909560442
  episode_reward_min: -181.7031535888415
  episodes_this_iter: 199
  episodes_total: 5869
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.439
    dispatch_time_ms: 5.883
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -596.1432495117188
      policy_loss: -1.6101238988477513e+22
      var_gnorm: 24.02593421936035
      vf_explained_var: -0.04775857925415039
      vf_loss: 9709.8515625
    num_steps_sampled: 880000
    num_steps_trained: 880000
    wait_time_ms: 5.283
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.413448071913977
    mean_inference_ms: 1.3102777579778988
    mean_processing_ms: 0.279122447196492
  time_since_restore: 163.70011377334595
  time_this_iter_s: 5.4395153522491455
  time_total_s: 163.70011377334595
  timestamp: 1563415235
  timesteps_since_restore: 880000
  timesteps_this_iter: 30000
  timesteps_total: 880000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 9.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 163 s, 30 iter, 880000 ts, -66.9 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 0.4177629277317497
  episode_reward_mean: -70.3256190228372
  episode_reward_min: -170.63391737250384
  episodes_this_iter: 198
  episodes_total: 6067
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.583
    dispatch_time_ms: 6.0
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -586.7884521484375
      policy_loss: -1.1591196539627598e+21
      var_gnorm: 24.02972984313965
      vf_explained_var: 0.01089465618133545
      vf_loss: 7711.373046875
    num_steps_sampled: 910000
    num_steps_trained: 910000
    wait_time_ms: 1.188
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4135467549675799
    mean_inference_ms: 1.3101895828921635
    mean_processing_ms: 0.2790917294956518
  time_since_restore: 169.15872979164124
  time_this_iter_s: 5.458616018295288
  time_total_s: 169.15872979164124
  timestamp: 1563415241
  timesteps_since_restore: 910000
  timesteps_this_iter: 30000
  timesteps_total: 910000
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 169 s, 31 iter, 910000 ts, -70.3 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 4.5276860700038135
  episode_reward_mean: -72.78584536388766
  episode_reward_min: -184.40011485924234
  episodes_this_iter: 209
  episodes_total: 6276
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.923
    dispatch_time_ms: 4.925
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: 10.392982482910156
      policy_loss: -1.291643368246136e+20
      var_gnorm: 24.03260040283203
      vf_explained_var: 0.4540162682533264
      vf_loss: 857.3432006835938
    num_steps_sampled: 940000
    num_steps_trained: 940000
    wait_time_ms: 1.291
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4136319897477103
    mean_inference_ms: 1.30982245592537
    mean_processing_ms: 0.2789829538492302
  time_since_restore: 174.85461854934692
  time_this_iter_s: 5.6958887577056885
  time_total_s: 174.85461854934692
  timestamp: 1563415246
  timesteps_since_restore: 940000
  timesteps_this_iter: 30000
  timesteps_total: 940000
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 174 s, 32 iter, 940000 ts, -72.8 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 2.809144002961029
  episode_reward_mean: -65.74905490455284
  episode_reward_min: -171.2819863818801
  episodes_this_iter: 188
  episodes_total: 6464
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.728
    dispatch_time_ms: 4.822
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -499.76446533203125
      policy_loss: -9.213019387211358e+20
      var_gnorm: 23.986324310302734
      vf_explained_var: -0.16557908058166504
      vf_loss: 4480.3115234375
    num_steps_sampled: 970000
    num_steps_trained: 970000
    wait_time_ms: 4.989
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41390845783424934
    mean_inference_ms: 1.3101456033817866
    mean_processing_ms: 0.2791830517588787
  time_since_restore: 180.0569851398468
  time_this_iter_s: 5.202366590499878
  time_total_s: 180.0569851398468
  timestamp: 1563415252
  timesteps_since_restore: 970000
  timesteps_this_iter: 30000
  timesteps_total: 970000
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 180 s, 33 iter, 970000 ts, -65.7 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-00-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 5.560827735147907
  episode_reward_mean: -69.15027669107138
  episode_reward_min: -174.85914605584512
  episodes_this_iter: 201
  episodes_total: 6665
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.695
    dispatch_time_ms: 5.399
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -467.1107177734375
      policy_loss: -2.7425556577049274e+21
      var_gnorm: 23.988618850708008
      vf_explained_var: -0.060392022132873535
      vf_loss: 3942.841064453125
    num_steps_sampled: 1000000
    num_steps_trained: 1000000
    wait_time_ms: 4.841
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.41393574412016604
    mean_inference_ms: 1.310043355056151
    mean_processing_ms: 0.27914870314717855
  time_since_restore: 185.5221974849701
  time_this_iter_s: 5.465212345123291
  time_total_s: 185.5221974849701
  timestamp: 1563415257
  timesteps_since_restore: 1000000
  timesteps_this_iter: 30000
  timesteps_total: 1000000
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 185 s, 34 iter, 1000000 ts, -69.2 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-01-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 9.016143719886177
  episode_reward_mean: -66.09078260386292
  episode_reward_min: -176.77027121528994
  episodes_this_iter: 205
  episodes_total: 6870
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.832
    dispatch_time_ms: 5.636
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -491.3446350097656
      policy_loss: -1.2286525722708303e+22
      var_gnorm: 23.996047973632812
      vf_explained_var: 0.0011792778968811035
      vf_loss: 10276.56640625
    num_steps_sampled: 1030000
    num_steps_trained: 1030000
    wait_time_ms: 3.536
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4140505878528485
    mean_inference_ms: 1.309930269022138
    mean_processing_ms: 0.27899508747974655
  time_since_restore: 191.0455539226532
  time_this_iter_s: 5.5233564376831055
  time_total_s: 191.0455539226532
  timestamp: 1563415263
  timesteps_since_restore: 1030000
  timesteps_this_iter: 30000
  timesteps_total: 1030000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 191 s, 35 iter, 1030000 ts, -66.1 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-01-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 5.579149029935719
  episode_reward_mean: -59.25975398657234
  episode_reward_min: -164.45822399918188
  episodes_this_iter: 197
  episodes_total: 7067
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 2.17
    dispatch_time_ms: 5.092
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -518.0835571289062
      policy_loss: -6.554117564819256e+19
      var_gnorm: 24.024433135986328
      vf_explained_var: -0.0034987926483154297
      vf_loss: 6613.1767578125
    num_steps_sampled: 1060000
    num_steps_trained: 1060000
    wait_time_ms: 4.497
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4140993410298648
    mean_inference_ms: 1.3095026307623747
    mean_processing_ms: 0.2791576416454108
  time_since_restore: 196.4667296409607
  time_this_iter_s: 5.421175718307495
  time_total_s: 196.4667296409607
  timestamp: 1563415268
  timesteps_since_restore: 1060000
  timesteps_this_iter: 30000
  timesteps_total: 1060000
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-a3c
Number of trials: 2 ({'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - A3C_RoboschoolReacher-v1_1_num_workers=7:	PENDING
RUNNING trials:
 - A3C_RoboschoolReacher-v1_0_num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=21333], 196 s, 36 iter, 1060000 ts, -59.3 rew

Result for A3C_RoboschoolReacher-v1_0_num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_04-01-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 3.337848647290523
  episode_reward_mean: -66.19407890477792
  episode_reward_min: -170.45885882836103
  episodes_this_iter: 198
  episodes_total: 7265
  experiment_id: be466ccc03234eec9ecbc7e57571149a
  hostname: navel-notebook-1
  info:
    apply_time_ms: 1.756
    dispatch_time_ms: 4.982
    learner:
      cur_lr: 9.999999747378752e-05
      grad_gnorm: 0.0
      model: {}
      policy_entropy: -513.4883422851562
      policy_loss: -2.6247924384237e+21
      var_gnorm: 24.052574157714844
      vf_explained_var: -0.0065495967864990234
      vf_loss: 7898.75146484375
    num_steps_sampled: 1090000
    num_steps_trained: 1090000
    wait_time_ms: 6.757
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 21333
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4142579851735515
    mean_inference_ms: 1.3097623200947606
    mean_processing_ms: 0.2790351510424707
  time_since_restore: 201.92417430877686
  time_this_iter_s: 5.457444667816162
  time_total_s: 201.92417430877686
  timestamp: 1563415273
  timesteps_since_restore: 1090000
  timesteps_this_iter: 30000
  timesteps_total: 1090000
  training_iteration: 37
  