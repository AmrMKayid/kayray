2019-07-18 01:30:44,947	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-18_01-30-44_947143_9164/logs.
2019-07-18 01:30:45,053	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-18 01:30:45,174	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-18 01:30:45,178	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-18 01:30:45,201	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-18_01-30-44_947143_9164/logs.
2019-07-18 01:30:45,201	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-18 01:30:45,322	INFO tune.py:65 -- Did not find checkpoint file in /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg.
2019-07-18 01:30:45,323	INFO tune.py:233 -- Starting a new experiment.
2019-07-18 01:30:45,424	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
[32m [     0.22188s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.22223s,  INFO] Experiment configs: 
 {
  "gym-reacher-apex-ddpg": {
    "env": "RoboschoolReacher-v1",
    "run": "APEX_DDPG",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 50,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 18,
      "training_iteration": 500
    },
    "config": {
      "use_huber": true,
      "clip_rewards": false,
      "num_gpus": 1,
      "num_workers": {
        "grid_search": [
          11,
          7
        ]
      },
      "num_envs_per_worker": {
        "grid_search": [
          16,
          8,
          4
        ]
      },
      "n_step": 3,
      "exploration_ou_noise_scale": 1.0,
      "target_network_update_freq": 50000,
      "tau": 1.0,
      "evaluation_interval": 5,
      "evaluation_num_episodes": 10
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 2.3/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 2.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING

[2m[36m(pid=9212)[0m 2019-07-18 01:30:47.382355: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9212)[0m [32m [     0.01608s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9212)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9212)[0m 2019-07-18 01:30:47,968	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f2b81e53e48>}
[2m[36m(pid=9212)[0m 2019-07-18 01:30:47,968	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f2b81eb3390>}
[2m[36m(pid=9212)[0m 2019-07-18 01:30:47,968	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f2b81e7fdd8>}
[2m[36m(pid=9212)[0m [32m [     0.60400s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60440s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60489s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60532s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60572s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60611s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60650s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60689s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60728s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60767s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60806s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60846s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60889s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60929s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     0.60972s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m 2019-07-18 01:30:47,977	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=9212)[0m 2019-07-18 01:30:49,213	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=9212)[0m [32m [     2.10728s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,718	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f2b7c4517b8>}
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,718	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f2b7c451470>}
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,718	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f2b7c446908>}
[2m[36m(pid=9212)[0m [32m [     3.35396s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.35484s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.35570s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.35663s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.35750s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.35838s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.35932s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36023s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36125s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36215s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36305s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36392s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36478s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36567s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m [32m [     3.36653s,  INFO] TimeLimit:
[2m[36m(pid=9212)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9212)[0m - action_space = Box(2,)
[2m[36m(pid=9212)[0m - observation_space = Box(9,)
[2m[36m(pid=9212)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9212)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9212)[0m - _max_episode_steps = 150
[2m[36m(pid=9212)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,735	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,777	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,927	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.95, max=0.044, mean=-0.24)},
[2m[36m(pid=9212)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.324, max=0.992, mean=0.2)},
[2m[36m(pid=9212)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.951, max=0.97, mean=-0.033)},
[2m[36m(pid=9212)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.961, max=0.53, mean=-0.1)},
[2m[36m(pid=9212)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.997, max=0.818, mean=-0.005)},
[2m[36m(pid=9212)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.229, max=0.999, mean=0.17)},
[2m[36m(pid=9212)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.189, max=0.961, mean=0.166)},
[2m[36m(pid=9212)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.034, max=0.187, mean=-0.203)},
[2m[36m(pid=9212)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.993, max=0.064, mean=-0.182)},
[2m[36m(pid=9212)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.998, max=0.991, mean=0.005)},
[2m[36m(pid=9212)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.997, max=0.116, mean=-0.199)},
[2m[36m(pid=9212)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.923, max=0.384, mean=-0.119)},
[2m[36m(pid=9212)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.576, max=0.818, mean=-0.01)},
[2m[36m(pid=9212)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.783, max=1.025, mean=0.095)},
[2m[36m(pid=9212)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.987, max=0.667, mean=-0.069)},
[2m[36m(pid=9212)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.391, max=0.921, mean=0.132)}}
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,928	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=9212)[0m   1: {'agent0': None},
[2m[36m(pid=9212)[0m   2: {'agent0': None},
[2m[36m(pid=9212)[0m   3: {'agent0': None},
[2m[36m(pid=9212)[0m   4: {'agent0': None},
[2m[36m(pid=9212)[0m   5: {'agent0': None},
[2m[36m(pid=9212)[0m   6: {'agent0': None},
[2m[36m(pid=9212)[0m   7: {'agent0': None},
[2m[36m(pid=9212)[0m   8: {'agent0': None},
[2m[36m(pid=9212)[0m   9: {'agent0': None},
[2m[36m(pid=9212)[0m   10: {'agent0': None},
[2m[36m(pid=9212)[0m   11: {'agent0': None},
[2m[36m(pid=9212)[0m   12: {'agent0': None},
[2m[36m(pid=9212)[0m   13: {'agent0': None},
[2m[36m(pid=9212)[0m   14: {'agent0': None},
[2m[36m(pid=9212)[0m   15: {'agent0': None}}
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,928	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.95, max=0.044, mean=-0.24)
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,929	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.95, max=0.044, mean=-0.24)
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,940	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 0,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.95, max=0.044, mean=-0.24),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 1,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.324, max=0.992, mean=0.2),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 2,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.951, max=0.97, mean=-0.033),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 3,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.961, max=0.53, mean=-0.1),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 4,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.997, max=0.818, mean=-0.005),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 5,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.229, max=0.999, mean=0.17),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 6,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.189, max=0.961, mean=0.166),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 7,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.034, max=0.187, mean=-0.203),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 8,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.993, max=0.064, mean=-0.182),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 9,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.998, max=0.991, mean=0.005),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 10,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.997, max=0.116, mean=-0.199),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 11,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.923, max=0.384, mean=-0.119),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 12,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.576, max=0.818, mean=-0.01),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 13,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.783, max=1.025, mean=0.095),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 14,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.987, max=0.667, mean=-0.069),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9212)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9212)[0m                                   'env_id': 15,
[2m[36m(pid=9212)[0m                                   'info': None,
[2m[36m(pid=9212)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.391, max=0.921, mean=0.132),
[2m[36m(pid=9212)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9212)[0m                                   'rnn_state': []},
[2m[36m(pid=9212)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,940	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=9212)[0m 2019-07-18 01:30:50,979	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-0.357, max=-0.011, mean=-0.182),
[2m[36m(pid=9212)[0m                       [],
[2m[36m(pid=9212)[0m                       {})}
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9213)[0m 2019-07-18 01:30:51,679	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9213)[0m 2019-07-18 01:30:51.679851: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9213)[0m [32m [     0.03800s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m 2019-07-18 01:30:51,746	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9209)[0m 2019-07-18 01:30:51.747296: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9218)[0m 2019-07-18 01:30:51,752	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9218)[0m 2019-07-18 01:30:51.752578: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9218)[0m [32m [     0.03777s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     0.03793s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     0.03218s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m 2019-07-18 01:30:51,733	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9409)[0m 2019-07-18 01:30:51.734224: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9207)[0m [32m [     0.02754s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9213)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9207)[0m 2019-07-18 01:30:51,806	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9207)[0m 2019-07-18 01:30:51.807493: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9209)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9209)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9218)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9218)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9211)[0m 2019-07-18 01:30:51,815	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9211)[0m 2019-07-18 01:30:51.815907: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9211)[0m [32m [     0.03924s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9409)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9215)[0m 2019-07-18 01:30:51,886	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9215)[0m 2019-07-18 01:30:51.886825: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9216)[0m [32m [     0.03938s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9207)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9215)[0m [32m [     0.03974s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m 2019-07-18 01:30:51,889	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9214)[0m 2019-07-18 01:30:51.889551: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9211)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9211)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9216)[0m 2019-07-18 01:30:51,871	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9216)[0m 2019-07-18 01:30:51.872231: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9214)[0m [32m [     0.03667s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m 2019-07-18 01:30:51,904	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9210)[0m 2019-07-18 01:30:51.905283: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9210)[0m [32m [     0.03926s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9216)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9215)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9215)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9212)[0m 2019-07-18 01:30:51,987	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.163),
[2m[36m(pid=9212)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=9212)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1651057419.0, max=1651057419.0, mean=1651057419.0),
[2m[36m(pid=9212)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=9212)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-10.0, max=10.0, mean=0.455),
[2m[36m(pid=9212)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-10.0, max=10.0, mean=0.447),
[2m[36m(pid=9212)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.164),
[2m[36m(pid=9212)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-11.728, max=11.814, mean=-0.632),
[2m[36m(pid=9212)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-26.117, max=17.28, mean=-2.005),
[2m[36m(pid=9212)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=9212)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.016, max=26.476, mean=6.094)},
[2m[36m(pid=9212)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9214)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9214)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9210)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9210)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9212)[0m 2019-07-18 01:30:52,166	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m { 'data': { 'actions': np.ndarray((900, 2), dtype=float32, min=-1.0, max=1.0, mean=0.171),
[2m[36m(pid=9212)[0m             'agent_index': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m             'dones': np.ndarray((900,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=9212)[0m             'eps_id': np.ndarray((900,), dtype=int64, min=112984700.0, max=1651057419.0, mean=790743375.667),
[2m[36m(pid=9212)[0m             'infos': np.ndarray((900,), dtype=object, head={}),
[2m[36m(pid=9212)[0m             'new_obs': np.ndarray((900, 9), dtype=float32, min=-10.0, max=10.0, mean=0.327),
[2m[36m(pid=9212)[0m             'obs': np.ndarray((900, 9), dtype=float32, min=-10.0, max=10.0, mean=0.322),
[2m[36m(pid=9212)[0m             'prev_actions': np.ndarray((900, 2), dtype=float32, min=-1.0, max=1.0, mean=0.173),
[2m[36m(pid=9212)[0m             'prev_rewards': np.ndarray((900,), dtype=float32, min=-28.16, max=28.102, mean=-0.374),
[2m[36m(pid=9212)[0m             'rewards': np.ndarray((900,), dtype=float32, min=-39.971, max=36.48, mean=-1.122),
[2m[36m(pid=9212)[0m             't': np.ndarray((900,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=9212)[0m             'unroll_id': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9212)[0m             'weights': np.ndarray((900,), dtype=float32, min=0.003, max=40.342, mean=6.052)},
[2m[36m(pid=9212)[0m   'type': 'SampleBatch'}
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9213)[0m [32m [     1.54246s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54347s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54424s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54537s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54645s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54761s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54878s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.54984s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55068s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55158s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55250s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55346s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55458s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m [32m [     1.48315s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48403s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48495s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48595s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48685s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48779s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48876s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.48963s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49112s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49217s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49310s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49415s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49503s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55549s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9213)[0m [32m [     1.55644s,  INFO] TimeLimit:
[2m[36m(pid=9213)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9213)[0m - action_space = Box(2,)
[2m[36m(pid=9213)[0m - observation_space = Box(9,)
[2m[36m(pid=9213)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9213)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9213)[0m - _max_episode_steps = 150
[2m[36m(pid=9213)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49603s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9409)[0m [32m [     1.49694s,  INFO] TimeLimit:
[2m[36m(pid=9409)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9409)[0m - action_space = Box(2,)
[2m[36m(pid=9409)[0m - observation_space = Box(9,)
[2m[36m(pid=9409)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9409)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9409)[0m - _max_episode_steps = 150
[2m[36m(pid=9409)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.53457s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.53561s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.53679s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.53791s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.53896s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.54030s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.54140s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.53581s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.53689s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.53798s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.53897s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.53990s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54102s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54221s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54344s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54443s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54562s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54681s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54809s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.54911s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m [32m [     1.54504s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.55009s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9209)[0m [32m [     1.55121s,  INFO] TimeLimit:
[2m[36m(pid=9209)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9209)[0m - action_space = Box(2,)
[2m[36m(pid=9209)[0m - observation_space = Box(9,)
[2m[36m(pid=9209)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9209)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9209)[0m - _max_episode_steps = 150
[2m[36m(pid=9209)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.54620s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.54720s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.54819s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.54931s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.55052s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.55160s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9218)[0m [32m [     1.55267s,  INFO] TimeLimit:
[2m[36m(pid=9218)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9218)[0m - action_space = Box(2,)
[2m[36m(pid=9218)[0m - observation_space = Box(9,)
[2m[36m(pid=9218)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9218)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9218)[0m - _max_episode_steps = 150
[2m[36m(pid=9218)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.52941s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53049s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53150s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53244s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53351s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53448s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53545s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53654s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53767s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.53888s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.53518s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.53604s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.53696s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.53791s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.53893s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.53989s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54102s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54225s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54339s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.54006s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.54117s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54448s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.54212s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54534s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54630s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.54313s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.54844s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9207)[0m [32m [     1.54622s,  INFO] TimeLimit:
[2m[36m(pid=9207)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9207)[0m - action_space = Box(2,)
[2m[36m(pid=9207)[0m - observation_space = Box(9,)
[2m[36m(pid=9207)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9207)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9207)[0m - _max_episode_steps = 150
[2m[36m(pid=9207)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.55130s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9211)[0m [32m [     1.55220s,  INFO] TimeLimit:
[2m[36m(pid=9211)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9211)[0m - action_space = Box(2,)
[2m[36m(pid=9211)[0m - observation_space = Box(9,)
[2m[36m(pid=9211)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9211)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9211)[0m - _max_episode_steps = 150
[2m[36m(pid=9211)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52310s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52424s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52523s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52623s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52728s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52833s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.52928s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53055s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53163s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53286s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53385s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53496s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.55592s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.55684s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53596s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53689s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9214)[0m [32m [     1.53787s,  INFO] TimeLimit:
[2m[36m(pid=9214)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9214)[0m - action_space = Box(2,)
[2m[36m(pid=9214)[0m - observation_space = Box(9,)
[2m[36m(pid=9214)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9214)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9214)[0m - _max_episode_steps = 150
[2m[36m(pid=9214)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.55781s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.55876s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.55969s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56063s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56161s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56266s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56357s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56470s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56558s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56647s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56741s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55363s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56830s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55475s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m [32m [     1.56939s,  INFO] TimeLimit:
[2m[36m(pid=9216)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9216)[0m - action_space = Box(2,)
[2m[36m(pid=9216)[0m - observation_space = Box(9,)
[2m[36m(pid=9216)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9216)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9216)[0m - _max_episode_steps = 150
[2m[36m(pid=9216)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55572s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55672s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55766s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55859s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.55964s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56069s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56167s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56255s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56345s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56438s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56549s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56648s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9215)[0m [32m [     1.56746s,  INFO] TimeLimit:
[2m[36m(pid=9215)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9215)[0m - action_space = Box(2,)
[2m[36m(pid=9215)[0m - observation_space = Box(9,)
[2m[36m(pid=9215)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9215)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9215)[0m - _max_episode_steps = 150
[2m[36m(pid=9215)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,440	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=9210)[0m [32m [     1.58080s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58178s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58285s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58399s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58519s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58623s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58723s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58817s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.58911s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.59007s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.59109s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.59220s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.59342s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.59459s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9210)[0m [32m [     1.59559s,  INFO] TimeLimit:
[2m[36m(pid=9210)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9210)[0m - action_space = Box(2,)
[2m[36m(pid=9210)[0m - observation_space = Box(9,)
[2m[36m(pid=9210)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9210)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9210)[0m - _max_episode_steps = 150
[2m[36m(pid=9210)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,594	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.67, max=0.73, mean=0.092)},
[2m[36m(pid=9216)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.695, max=0.719, mean=-0.046)},
[2m[36m(pid=9216)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.999, max=0.052, mean=-0.214)},
[2m[36m(pid=9216)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.071, max=0.93, mean=0.19)},
[2m[36m(pid=9216)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.891, max=0.454, mean=-0.054)},
[2m[36m(pid=9216)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.92, max=0.87, mean=0.03)},
[2m[36m(pid=9216)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.869, max=0.394, mean=-0.133)},
[2m[36m(pid=9216)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.706, max=0.708, mean=0.04)},
[2m[36m(pid=9216)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.132, max=0.974, mean=0.233)},
[2m[36m(pid=9216)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.99, max=0.519, mean=-0.06)},
[2m[36m(pid=9216)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.371, max=0.974, mean=0.12)},
[2m[36m(pid=9216)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.999, max=0.168, mean=-0.171)},
[2m[36m(pid=9216)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.793, max=0.85, mean=-0.04)},
[2m[36m(pid=9216)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.998, max=0.08, mean=-0.135)},
[2m[36m(pid=9216)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.516, max=0.777, mean=0.113)},
[2m[36m(pid=9216)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.409, max=0.913, mean=0.105)}}
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,595	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=9216)[0m   1: {'agent0': None},
[2m[36m(pid=9216)[0m   2: {'agent0': None},
[2m[36m(pid=9216)[0m   3: {'agent0': None},
[2m[36m(pid=9216)[0m   4: {'agent0': None},
[2m[36m(pid=9216)[0m   5: {'agent0': None},
[2m[36m(pid=9216)[0m   6: {'agent0': None},
[2m[36m(pid=9216)[0m   7: {'agent0': None},
[2m[36m(pid=9216)[0m   8: {'agent0': None},
[2m[36m(pid=9216)[0m   9: {'agent0': None},
[2m[36m(pid=9216)[0m   10: {'agent0': None},
[2m[36m(pid=9216)[0m   11: {'agent0': None},
[2m[36m(pid=9216)[0m   12: {'agent0': None},
[2m[36m(pid=9216)[0m   13: {'agent0': None},
[2m[36m(pid=9216)[0m   14: {'agent0': None},
[2m[36m(pid=9216)[0m   15: {'agent0': None}}
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,595	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.67, max=0.73, mean=0.092)
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,596	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.67, max=0.73, mean=0.092)
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,607	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 0,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.67, max=0.73, mean=0.092),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 1,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.695, max=0.719, mean=-0.046),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 2,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.999, max=0.052, mean=-0.214),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 3,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.071, max=0.93, mean=0.19),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 4,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.891, max=0.454, mean=-0.054),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 5,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.92, max=0.87, mean=0.03),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 6,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.869, max=0.394, mean=-0.133),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 7,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.706, max=0.708, mean=0.04),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 8,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.132, max=0.974, mean=0.233),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 9,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.99, max=0.519, mean=-0.06),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 10,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.371, max=0.974, mean=0.12),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 11,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.999, max=0.168, mean=-0.171),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 12,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.793, max=0.85, mean=-0.04),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 13,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.998, max=0.08, mean=-0.135),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 14,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.516, max=0.777, mean=0.113),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9216)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9216)[0m                                   'env_id': 15,
[2m[36m(pid=9216)[0m                                   'info': None,
[2m[36m(pid=9216)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.409, max=0.913, mean=0.105),
[2m[36m(pid=9216)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9216)[0m                                   'rnn_state': []},
[2m[36m(pid=9216)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,607	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=9216)[0m 2019-07-18 01:30:53,650	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=0.108, max=0.455, mean=0.301),
[2m[36m(pid=9216)[0m                       [],
[2m[36m(pid=9216)[0m                       {})}
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m 2019-07-18 01:30:54,021	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.127),
[2m[36m(pid=9216)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=591511507.0, max=591511507.0, mean=591511507.0),
[2m[36m(pid=9216)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=9216)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-0.894, max=2.177, mean=0.247),
[2m[36m(pid=9216)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-0.758, max=2.177, mean=0.246),
[2m[36m(pid=9216)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.129),
[2m[36m(pid=9216)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-2.488, max=3.065, mean=-0.249),
[2m[36m(pid=9216)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-6.949, max=8.868, mean=-0.801),
[2m[36m(pid=9216)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=9216)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.022, max=8.865, mean=2.464)},
[2m[36m(pid=9216)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m 2019-07-18 01:30:54,102	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9216)[0m { 'data': { 'actions': np.ndarray((800, 2), dtype=float32, min=-1.0, max=1.0, mean=0.125),
[2m[36m(pid=9216)[0m             'agent_index': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m             'dones': np.ndarray((800,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m             'eps_id': np.ndarray((800,), dtype=int64, min=282050063.0, max=1991093812.0, mean=1031177297.375),
[2m[36m(pid=9216)[0m             'infos': np.ndarray((800,), dtype=object, head={}),
[2m[36m(pid=9216)[0m             'new_obs': np.ndarray((800, 9), dtype=float32, min=-1.517, max=2.514, mean=0.122),
[2m[36m(pid=9216)[0m             'obs': np.ndarray((800, 9), dtype=float32, min=-1.517, max=2.514, mean=0.116),
[2m[36m(pid=9216)[0m             'prev_actions': np.ndarray((800, 2), dtype=float32, min=-1.0, max=1.0, mean=0.127),
[2m[36m(pid=9216)[0m             'prev_rewards': np.ndarray((800,), dtype=float32, min=-2.873, max=3.907, mean=0.042),
[2m[36m(pid=9216)[0m             'rewards': np.ndarray((800,), dtype=float32, min=-8.061, max=10.571, mean=0.098),
[2m[36m(pid=9216)[0m             't': np.ndarray((800,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=9216)[0m             'unroll_id': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9216)[0m             'weights': np.ndarray((800,), dtype=float32, min=0.004, max=10.572, mean=2.014)},
[2m[36m(pid=9216)[0m   'type': 'SampleBatch'}
[2m[36m(pid=9216)[0m 
[2m[36m(pid=9212)[0m 2019-07-18 01:30:59,380	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m { 'count': 512,
[2m[36m(pid=9212)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.011),
[2m[36m(pid=9212)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=13.0, max=12797.0, mean=6170.586),
[2m[36m(pid=9212)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.014),
[2m[36m(pid=9212)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-4.616, max=5.712, mean=0.013),
[2m[36m(pid=9212)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-5.021, max=5.598, mean=0.01),
[2m[36m(pid=9212)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-19.162, max=18.065, mean=-0.484),
[2m[36m(pid=9212)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.069, max=0.249, mean=0.114)},
[2m[36m(pid=9212)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=9212)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m 2019-07-18 01:31:00,096	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9212)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.029484706,
[2m[36m(pid=9212)[0m                                          'mean_q': -0.17461827,
[2m[36m(pid=9212)[0m                                          'min_q': -0.67049676},
[2m[36m(pid=9212)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-18.404, max=18.937, mean=0.469)}}
[2m[36m(pid=9212)[0m 
[2m[36m(pid=9581)[0m [32m [     0.09071s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m 2019-07-18 01:31:04,137	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9581)[0m 2019-07-18 01:31:04.138256: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9581)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9581)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9581)[0m [32m [     2.68452s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.68553s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.68654s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.68756s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.68879s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.68975s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.69072s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.69290s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.69771s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.70064s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.70492s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.70596s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.70689s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.70779s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9581)[0m [32m [     2.71285s,  INFO] TimeLimit:
[2m[36m(pid=9581)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9581)[0m - action_space = Box(2,)
[2m[36m(pid=9581)[0m - observation_space = Box(9,)
[2m[36m(pid=9581)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9581)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9581)[0m - _max_episode_steps = 150
[2m[36m(pid=9581)[0m - _elapsed_steps = None [0m
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-31-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.01164669448392
  episode_reward_mean: -11.641424572750251
  episode_reward_min: -53.30614287817243
  episodes_this_iter: 800
  episodes_total: 800
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 20.486539840698242
        mean_q: 2.43403959274292
        min_q: -11.076241493225098
    learner_queue:
      size_count: 2065
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 340000
    num_steps_trained: 162304
    num_target_updates: 3
    num_weight_syncs: 425
    replay_shard_0:
      add_batch_time_ms: 100.832
      policy_default_policy:
        added_count: 93600
        est_size_bytes: 31917600
        num_entries: 93600
        sampled_count: 37888
      replay_time_ms: 39.862
      update_priorities_time_ms: 101.845
    sample_throughput: 21933.438
    train_throughput: 0.0
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.30659500382837
    mean_inference_ms: 3.3925672116633847
    mean_processing_ms: 4.806889682907064
  time_since_restore: 31.077011823654175
  time_this_iter_s: 31.077011823654175
  time_total_s: 31.077011823654175
  timestamp: 1563406285
  timesteps_since_restore: 340000
  timesteps_this_iter: 340000
  timesteps_total: 340000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 31 s, 1 iter, 340000 ts, -11.6 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-31-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.171021791033397
  episode_reward_mean: -29.71662378776905
  episode_reward_min: -184.6035732189758
  episodes_this_iter: 960
  episodes_total: 1760
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 20.34907341003418
        mean_q: -1.7789058685302734
        min_q: -34.27204132080078
    learner_queue:
      size_count: 2463
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4004996878900157
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 727200
    num_steps_trained: 366592
    num_target_updates: 7
    num_weight_syncs: 909
    replay_shard_0:
      add_batch_time_ms: 105.899
      policy_default_policy:
        added_count: 180000
        est_size_bytes: 61380000
        num_entries: 180000
        sampled_count: 89088
      replay_time_ms: 34.494
      update_priorities_time_ms: 97.668
    sample_throughput: 23250.398
    train_throughput: 14880.255
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.258707318527244
    mean_inference_ms: 2.9907130701625984
    mean_processing_ms: 4.600279214889281
  time_since_restore: 62.03302621841431
  time_this_iter_s: 30.956014394760132
  time_total_s: 62.03302621841431
  timestamp: 1563406316
  timesteps_since_restore: 727200
  timesteps_this_iter: 387200
  timesteps_total: 727200
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 62 s, 2 iter, 727200 ts, -29.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-32-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.76075173483982
  episode_reward_mean: -1.8418581211599376
  episode_reward_min: -40.02479508474633
  episodes_this_iter: 928
  episodes_total: 2688
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.563495635986328
        mean_q: 0.19622598588466644
        min_q: -36.71669387817383
    learner_queue:
      size_count: 2846
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 1122400
    num_steps_trained: 562688
    num_target_updates: 11
    num_weight_syncs: 1403
    replay_shard_0:
      add_batch_time_ms: 118.629
      policy_default_policy:
        added_count: 275200
        est_size_bytes: 93843200
        num_entries: 275200
        sampled_count: 136704
      replay_time_ms: 45.112
      update_priorities_time_ms: 122.329
    sample_throughput: 41433.0
    train_throughput: 5303.424
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.194781021415341
    mean_inference_ms: 2.938924809664858
    mean_processing_ms: 4.598751629510311
  time_since_restore: 93.13594579696655
  time_this_iter_s: 31.102919578552246
  time_total_s: 93.13594579696655
  timestamp: 1563406348
  timesteps_since_restore: 1122400
  timesteps_this_iter: 395200
  timesteps_total: 1122400
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 93 s, 3 iter, 1122400 ts, -1.84 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-32-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.969017830182956
  episode_reward_mean: 5.0524750164353325
  episode_reward_min: -32.32672442711255
  episodes_this_iter: 992
  episodes_total: 3680
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 23.270442962646484
        mean_q: 1.6254193782806396
        min_q: -30.304521560668945
    learner_queue:
      size_count: 3202
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 1520000
    num_steps_trained: 744960
    num_target_updates: 14
    num_weight_syncs: 1900
    replay_shard_0:
      add_batch_time_ms: 80.583
      policy_default_policy:
        added_count: 367200
        est_size_bytes: 125215200
        num_entries: 367200
        sampled_count: 184320
      replay_time_ms: 40.39
      update_priorities_time_ms: 100.6
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.1428398175834165
    mean_inference_ms: 2.8831690082268637
    mean_processing_ms: 4.572069137362526
  time_since_restore: 124.05038595199585
  time_this_iter_s: 30.914440155029297
  time_total_s: 124.05038595199585
  timestamp: 1563406378
  timesteps_since_restore: 1520000
  timesteps_this_iter: 397600
  timesteps_total: 1520000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 124 s, 4 iter, 1520000 ts, 5.05 rew

[2m[36m(pid=9212)[0m 2019-07-18 01:33:29,905	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-33-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.60064314711113
  episode_reward_mean: 14.639279843988628
  episode_reward_min: -17.07521118468628
  episodes_this_iter: 928
  episodes_total: 4608
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 33.15758370047165
    episode_reward_mean: 12.604737271468213
    episode_reward_min: -40.574366884723645
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.692947379754645
      mean_inference_ms: 0.871061186905606
      mean_processing_ms: 1.435090461241812
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 29.398033142089844
        mean_q: 4.093468189239502
        min_q: -26.434179306030273
    learner_queue:
      size_count: 3573
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 1915200
    num_steps_trained: 934912
    num_target_updates: 18
    num_weight_syncs: 2394
    replay_shard_0:
      add_batch_time_ms: 108.736
      policy_default_policy:
        added_count: 465600
        est_size_bytes: 158769600
        num_entries: 465600
        sampled_count: 231936
      replay_time_ms: 43.583
      update_priorities_time_ms: 122.855
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.1381733692452976
    mean_inference_ms: 2.86760732167832
    mean_processing_ms: 4.57180770609865
  time_since_restore: 155.01243948936462
  time_this_iter_s: 30.962053537368774
  time_total_s: 155.01243948936462
  timestamp: 1563406409
  timesteps_since_restore: 1915200
  timesteps_this_iter: 395200
  timesteps_total: 1915200
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.3/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 155 s, 5 iter, 1915200 ts, 14.6 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-34-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.86843335454465
  episode_reward_mean: 15.85244822463932
  episode_reward_min: -12.656677577471333
  episodes_this_iter: 976
  episodes_total: 5584
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 29.915178298950195
        mean_q: 5.006956100463867
        min_q: -32.40678787231445
    learner_queue:
      size_count: 3973
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 2303200
    num_steps_trained: 1139200
    num_target_updates: 22
    num_weight_syncs: 2879
    replay_shard_0:
      add_batch_time_ms: 97.144
      policy_default_policy:
        added_count: 557600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 283136
      replay_time_ms: 46.19
      update_priorities_time_ms: 101.691
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.116845497758702
    mean_inference_ms: 2.838540608953468
    mean_processing_ms: 4.565649593923091
  time_since_restore: 185.8305220603943
  time_this_iter_s: 30.818082571029663
  time_total_s: 185.8305220603943
  timestamp: 1563406442
  timesteps_since_restore: 2303200
  timesteps_this_iter: 388000
  timesteps_total: 2303200
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.7/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 185 s, 6 iter, 2303200 ts, 15.9 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-34-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.926683254106685
  episode_reward_mean: 15.588389669690324
  episode_reward_min: -19.215801869194244
  episodes_this_iter: 960
  episodes_total: 6544
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.26968002319336
        mean_q: 6.616795539855957
        min_q: -4.586335182189941
    learner_queue:
      size_count: 4354
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.46
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 2697600
    num_steps_trained: 1334272
    num_target_updates: 26
    num_weight_syncs: 3372
    replay_shard_0:
      add_batch_time_ms: 77.603
      policy_default_policy:
        added_count: 649600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 332800
      replay_time_ms: 33.322
      update_priorities_time_ms: 101.509
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.117890169614437
    mean_inference_ms: 2.820893575244351
    mean_processing_ms: 4.550672621697042
  time_since_restore: 216.82989478111267
  time_this_iter_s: 30.999372720718384
  time_total_s: 216.82989478111267
  timestamp: 1563406473
  timesteps_since_restore: 2697600
  timesteps_this_iter: 394400
  timesteps_total: 2697600
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.7/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 216 s, 7 iter, 2697600 ts, 15.6 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-35-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.26909306414964
  episode_reward_mean: 14.783037525566623
  episode_reward_min: -19.920243083099006
  episodes_this_iter: 928
  episodes_total: 7472
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.39990997314453
        mean_q: 4.953753471374512
        min_q: -2.9469263553619385
    learner_queue:
      size_count: 4754
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4770744176750626
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 3087200
    num_steps_trained: 1539072
    num_target_updates: 30
    num_weight_syncs: 3859
    replay_shard_0:
      add_batch_time_ms: 76.116
      policy_default_policy:
        added_count: 744000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 384512
      replay_time_ms: 50.181
      update_priorities_time_ms: 113.073
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.114057491641489
    mean_inference_ms: 2.816190299943614
    mean_processing_ms: 4.558323981111114
  time_since_restore: 247.86284255981445
  time_this_iter_s: 31.032947778701782
  time_total_s: 247.86284255981445
  timestamp: 1563406504
  timesteps_since_restore: 3087200
  timesteps_this_iter: 389600
  timesteps_total: 3087200
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.7/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 247 s, 8 iter, 3087200 ts, 14.8 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-35-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.08342129831978
  episode_reward_mean: 16.917318754485855
  episode_reward_min: -13.583793906518245
  episodes_this_iter: 960
  episodes_total: 8432
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.39706039428711
        mean_q: 4.7595977783203125
        min_q: -1.6715941429138184
    learner_queue:
      size_count: 5149
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 2.0
      size_std: 0.36055512754639896
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 3477600
    num_steps_trained: 1741824
    num_target_updates: 34
    num_weight_syncs: 4347
    replay_shard_0:
      add_batch_time_ms: 76.9
      policy_default_policy:
        added_count: 830400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 438784
      replay_time_ms: 41.082
      update_priorities_time_ms: 100.207
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.114358272063799
    mean_inference_ms: 2.8103120352373243
    mean_processing_ms: 4.551798871215851
  time_since_restore: 278.81947565078735
  time_this_iter_s: 30.9566330909729
  time_total_s: 278.81947565078735
  timestamp: 1563406535
  timesteps_since_restore: 3477600
  timesteps_this_iter: 390400
  timesteps_total: 3477600
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.7/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 278 s, 9 iter, 3477600 ts, 16.9 rew

[2m[36m(pid=9212)[0m 2019-07-18 01:36:06,681	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-36-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.15926063375947
  episode_reward_mean: 16.78955242861813
  episode_reward_min: -15.724297053797807
  episodes_this_iter: 944
  episodes_total: 9376
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 35.94044409517012
    episode_reward_mean: 18.448585085407267
    episode_reward_min: -5.124296070614374
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.3609482533795387
      mean_inference_ms: 0.774964433719819
      mean_processing_ms: 1.2722404581649251
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.14429473876953
        mean_q: 4.266484260559082
        min_q: -1.9072147607803345
    learner_queue:
      size_count: 5540
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 3868800
    num_steps_trained: 1940992
    num_target_updates: 38
    num_weight_syncs: 4836
    replay_shard_0:
      add_batch_time_ms: 90.137
      policy_default_policy:
        added_count: 929600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 485888
      replay_time_ms: 40.887
      update_priorities_time_ms: 128.746
    sample_throughput: 20432.112
    train_throughput: 0.0
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.117864862408579
    mean_inference_ms: 2.8087997800326416
    mean_processing_ms: 4.556151738953728
  time_since_restore: 309.7848012447357
  time_this_iter_s: 30.965325593948364
  time_total_s: 309.7848012447357
  timestamp: 1563406566
  timesteps_since_restore: 3868800
  timesteps_this_iter: 391200
  timesteps_total: 3868800
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.8/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 309 s, 10 iter, 3868800 ts, 16.8 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-36-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.26748490809489
  episode_reward_mean: 17.867045487646177
  episode_reward_min: -14.062716931317356
  episodes_this_iter: 992
  episodes_total: 10368
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.97926139831543
        mean_q: 4.853418350219727
        min_q: -0.962140679359436
    learner_queue:
      size_count: 5932
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 2.0
      size_std: 0.3815756805667782
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 4260800
    num_steps_trained: 2142720
    num_target_updates: 42
    num_weight_syncs: 5326
    replay_shard_0:
      add_batch_time_ms: 71.506
      policy_default_policy:
        added_count: 1029600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 534016
      replay_time_ms: 43.121
      update_priorities_time_ms: 108.091
    sample_throughput: 21234.115
    train_throughput: 9059.889
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.103896435356856
    mean_inference_ms: 2.7957229173225593
    mean_processing_ms: 4.551449041929056
  time_since_restore: 340.87807869911194
  time_this_iter_s: 31.09327745437622
  time_total_s: 340.87807869911194
  timestamp: 1563406599
  timesteps_since_restore: 4260800
  timesteps_this_iter: 392000
  timesteps_total: 4260800
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 15.8/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'RUNNING': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9212], 340 s, 11 iter, 4260800 ts, 17.9 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-37-10
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.60998743561659
  episode_reward_mean: 18.19460694761693
  episode_reward_min: -5.497679573703288
  episodes_this_iter: 960
  episodes_total: 11328
  experiment_id: be23a57fbc0f4a9ab31644a20c5b5e89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 37.82170867919922
        mean_q: 4.718960762023926
        min_q: -0.7899306416511536
    learner_queue:
      size_count: 6314
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.41424630354415964
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 12
    num_steps_sampled: 4655200
    num_steps_trained: 2337792
    num_target_updates: 46
    num_weight_syncs: 5819
    replay_shard_0:
      add_batch_time_ms: 86.401
      policy_default_policy:
        added_count: 1114400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 587264
      replay_time_ms: 34.759
      update_priorities_time_ms: 112.718
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9212
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.101463203462008
    mean_inference_ms: 2.792138077608187
    mean_processing_ms: 4.535495325025838
  time_since_restore: 371.9095516204834
  time_this_iter_s: 31.03147292137146
  time_total_s: 371.9095516204834
  timestamp: 1563406630
  timesteps_since_restore: 4655200
  timesteps_this_iter: 394400
  timesteps_total: 4655200
  training_iteration: 12
  2019-07-18 01:37:10,786	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 01:37:10,794	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 15.8/16.7 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'PENDING': 5})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

[2m[36m(pid=9736)[0m [32m [     0.04617s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m 2019-07-18 01:37:11.046248: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9736)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9736)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9736)[0m [32m [     1.36536s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     1.36578s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     1.36617s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     1.36655s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     1.36693s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     1.36739s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     1.36797s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m 2019-07-18 01:37:12,361	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f7dbdabda58>}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:12,361	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7dbda92f98>}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:12,361	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f7dbda92748>}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:12,369	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=9736)[0m 2019-07-18 01:37:12,376	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=9584)[0m [32m [     0.02852s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m 2019-07-18 01:37:12,448	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9577)[0m 2019-07-18 01:37:12.449338: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9577)[0m [32m [     0.02821s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m 2019-07-18 01:37:12,445	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9584)[0m 2019-07-18 01:37:12.445894: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9578)[0m 2019-07-18 01:37:12,508	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9578)[0m 2019-07-18 01:37:12.509359: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9578)[0m [32m [     0.03862s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     0.03790s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     0.03800s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m 2019-07-18 01:37:12,486	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9579)[0m 2019-07-18 01:37:12.486925: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9587)[0m 2019-07-18 01:37:12,489	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9587)[0m 2019-07-18 01:37:12.489797: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9580)[0m 2019-07-18 01:37:12,501	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9580)[0m 2019-07-18 01:37:12.502350: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9580)[0m [32m [     0.03978s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m 2019-07-18 01:37:12,479	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9586)[0m 2019-07-18 01:37:12.480046: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9584)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9584)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9586)[0m [32m [     0.03782s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     0.03915s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m 2019-07-18 01:37:12,496	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9588)[0m 2019-07-18 01:37:12.497044: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9577)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9577)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9586)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9586)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9587)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9587)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9588)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9588)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9578)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9578)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9579)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9579)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9580)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9580)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9736)[0m [32m [     1.76090s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.52893s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.52993s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.53092s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.53187s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.53280s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.53374s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9577)[0m [32m [     2.54505s,  INFO] TimeLimit:
[2m[36m(pid=9577)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9577)[0m - action_space = Box(2,)
[2m[36m(pid=9577)[0m - observation_space = Box(9,)
[2m[36m(pid=9577)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9577)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9577)[0m - _max_episode_steps = 150
[2m[36m(pid=9577)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.61803s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.61898s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.61992s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.62083s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.62173s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.62266s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9587)[0m [32m [     2.62360s,  INFO] TimeLimit:
[2m[36m(pid=9587)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9587)[0m - action_space = Box(2,)
[2m[36m(pid=9587)[0m - observation_space = Box(9,)
[2m[36m(pid=9587)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9587)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9587)[0m - _max_episode_steps = 150
[2m[36m(pid=9587)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.66081s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.66556s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.66652s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.66751s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.66840s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.66927s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m [32m [     2.67018s,  INFO] TimeLimit:
[2m[36m(pid=9584)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9584)[0m - action_space = Box(2,)
[2m[36m(pid=9584)[0m - observation_space = Box(9,)
[2m[36m(pid=9584)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9584)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9584)[0m - _max_episode_steps = 150
[2m[36m(pid=9584)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.64848s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.64953s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.65558s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.66521s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.66638s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.66732s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9586)[0m [32m [     2.66825s,  INFO] TimeLimit:
[2m[36m(pid=9586)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9586)[0m - action_space = Box(2,)
[2m[36m(pid=9586)[0m - observation_space = Box(9,)
[2m[36m(pid=9586)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9586)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9586)[0m - _max_episode_steps = 150
[2m[36m(pid=9586)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,175	INFO rollout_worker.py:428 -- Generating sample batch of size 400
[2m[36m(pid=9588)[0m [32m [     2.70992s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     2.71100s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     2.71200s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     2.71300s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     2.71401s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     2.71491s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9588)[0m [32m [     2.71580s,  INFO] TimeLimit:
[2m[36m(pid=9588)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9588)[0m - action_space = Box(2,)
[2m[36m(pid=9588)[0m - observation_space = Box(9,)
[2m[36m(pid=9588)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9588)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9588)[0m - _max_episode_steps = 150
[2m[36m(pid=9588)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.74396s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.74494s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.74587s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.74675s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.74767s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.74858s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9579)[0m [32m [     2.75992s,  INFO] TimeLimit:
[2m[36m(pid=9579)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9579)[0m - action_space = Box(2,)
[2m[36m(pid=9579)[0m - observation_space = Box(9,)
[2m[36m(pid=9579)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9579)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9579)[0m - _max_episode_steps = 150
[2m[36m(pid=9579)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9580)[0m [32m [     2.92783s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9580)[0m [32m [     2.92889s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9580)[0m [32m [     2.92990s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     4.38905s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     4.39009s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     4.39111s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     4.39212s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     4.39306s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,386	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f7db60e1978>}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,387	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7db60e1630>}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,387	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f7db60d5e80>}
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,399	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.978, max=0.675, mean=-0.002)},
[2m[36m(pid=9584)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.137, max=1.0, mean=0.228)},
[2m[36m(pid=9584)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.222, max=0.961, mean=0.258)},
[2m[36m(pid=9584)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.882, max=0.771, mean=-0.075)},
[2m[36m(pid=9584)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.549, max=0.836, mean=0.026)},
[2m[36m(pid=9584)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.981, max=0.196, mean=-0.088)},
[2m[36m(pid=9584)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.523, max=0.853, mean=0.08)},
[2m[36m(pid=9584)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.256, max=0.999, mean=0.211)}}
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,400	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=9584)[0m   1: {'agent0': None},
[2m[36m(pid=9584)[0m   2: {'agent0': None},
[2m[36m(pid=9584)[0m   3: {'agent0': None},
[2m[36m(pid=9584)[0m   4: {'agent0': None},
[2m[36m(pid=9584)[0m   5: {'agent0': None},
[2m[36m(pid=9584)[0m   6: {'agent0': None},
[2m[36m(pid=9584)[0m   7: {'agent0': None}}
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,400	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.978, max=0.675, mean=-0.002)
[2m[36m(pid=9736)[0m [32m [     4.40441s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m [32m [     4.40538s,  INFO] TimeLimit:
[2m[36m(pid=9736)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9736)[0m - action_space = Box(2,)
[2m[36m(pid=9736)[0m - observation_space = Box(9,)
[2m[36m(pid=9736)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9736)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9736)[0m - _max_episode_steps = 150
[2m[36m(pid=9736)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9580)[0m [32m [     2.94521s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9580)[0m [32m [     2.94617s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,408	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=9580)[0m [32m [     2.94705s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9580)[0m [32m [     2.94793s,  INFO] TimeLimit:
[2m[36m(pid=9580)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9580)[0m - action_space = Box(2,)
[2m[36m(pid=9580)[0m - observation_space = Box(9,)
[2m[36m(pid=9580)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9580)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9580)[0m - _max_episode_steps = 150
[2m[36m(pid=9580)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,410	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.978, max=0.675, mean=-0.002)
[2m[36m(pid=9578)[0m [32m [     2.97497s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9578)[0m [32m [     2.97590s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9578)[0m [32m [     2.97688s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9578)[0m [32m [     2.97790s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9578)[0m [32m [     2.97902s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9578)[0m [32m [     2.98010s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9578)[0m [32m [     2.98215s,  INFO] TimeLimit:
[2m[36m(pid=9578)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9578)[0m - action_space = Box(2,)
[2m[36m(pid=9578)[0m - observation_space = Box(9,)
[2m[36m(pid=9578)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9578)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9578)[0m - _max_episode_steps = 150
[2m[36m(pid=9578)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,425	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9584)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 0,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.978, max=0.675, mean=-0.002),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 1,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.137, max=1.0, mean=0.228),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 2,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.222, max=0.961, mean=0.258),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 3,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.882, max=0.771, mean=-0.075),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 4,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.549, max=0.836, mean=0.026),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 5,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.981, max=0.196, mean=-0.088),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 6,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.523, max=0.853, mean=0.08),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9584)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9584)[0m                                   'env_id': 7,
[2m[36m(pid=9584)[0m                                   'info': None,
[2m[36m(pid=9584)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.256, max=0.999, mean=0.211),
[2m[36m(pid=9584)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9584)[0m                                   'rnn_state': []},
[2m[36m(pid=9584)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,426	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,495	INFO rollout_worker.py:428 -- Generating sample batch of size 400
[2m[36m(pid=9584)[0m 2019-07-18 01:37:15,550	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9584)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=0.017, max=0.399, mean=0.195),
[2m[36m(pid=9584)[0m                       [],
[2m[36m(pid=9584)[0m                       {})}
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,613	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.625, max=0.946, mean=-0.011)},
[2m[36m(pid=9736)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.706, max=0.708, mean=-0.028)},
[2m[36m(pid=9736)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.519, max=0.776, mean=0.116)},
[2m[36m(pid=9736)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.978, max=0.209, mean=-0.115)},
[2m[36m(pid=9736)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.211, max=0.903, mean=0.188)},
[2m[36m(pid=9736)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.733, max=0.789, mean=-0.073)},
[2m[36m(pid=9736)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.733, max=0.992, mean=-0.045)},
[2m[36m(pid=9736)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.993, max=0.299, mean=-0.182)}}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,613	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=9736)[0m   1: {'agent0': None},
[2m[36m(pid=9736)[0m   2: {'agent0': None},
[2m[36m(pid=9736)[0m   3: {'agent0': None},
[2m[36m(pid=9736)[0m   4: {'agent0': None},
[2m[36m(pid=9736)[0m   5: {'agent0': None},
[2m[36m(pid=9736)[0m   6: {'agent0': None},
[2m[36m(pid=9736)[0m   7: {'agent0': None}}
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,614	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.625, max=0.946, mean=-0.011)
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,614	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.625, max=0.946, mean=-0.011)
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,627	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 0,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.625, max=0.946, mean=-0.011),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 1,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.706, max=0.708, mean=-0.028),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 2,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.519, max=0.776, mean=0.116),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 3,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.978, max=0.209, mean=-0.115),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 4,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.211, max=0.903, mean=0.188),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 5,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.733, max=0.789, mean=-0.073),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 6,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.733, max=0.992, mean=-0.045),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9736)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9736)[0m                                   'env_id': 7,
[2m[36m(pid=9736)[0m                                   'info': None,
[2m[36m(pid=9736)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.993, max=0.299, mean=-0.182),
[2m[36m(pid=9736)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9736)[0m                                   'rnn_state': []},
[2m[36m(pid=9736)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,627	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=9736)[0m 2019-07-18 01:37:15,707	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-0.027, max=0.093, mean=0.03),
[2m[36m(pid=9736)[0m                       [],
[2m[36m(pid=9736)[0m                       {})}
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9584)[0m 2019-07-18 01:37:16,018	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9584)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.462),
[2m[36m(pid=9584)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=765708738.0, max=765708738.0, mean=765708738.0),
[2m[36m(pid=9584)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=9584)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-8.008, max=7.057, mean=0.257),
[2m[36m(pid=9584)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-8.008, max=7.057, mean=0.207),
[2m[36m(pid=9584)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.468),
[2m[36m(pid=9584)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-11.333, max=11.07, mean=-0.362),
[2m[36m(pid=9584)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-31.922, max=24.99, mean=-0.431),
[2m[36m(pid=9584)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=9584)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.361, max=31.679, mean=9.049)},
[2m[36m(pid=9584)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9584)[0m 2019-07-18 01:37:16,097	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9584)[0m { 'data': { 'actions': np.ndarray((400, 2), dtype=float32, min=-1.0, max=1.0, mean=0.459),
[2m[36m(pid=9584)[0m             'agent_index': np.ndarray((400,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m             'dones': np.ndarray((400,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m             'eps_id': np.ndarray((400,), dtype=int64, min=765708738.0, max=1821924860.0, mean=1142409680.625),
[2m[36m(pid=9584)[0m             'infos': np.ndarray((400,), dtype=object, head={}),
[2m[36m(pid=9584)[0m             'new_obs': np.ndarray((400, 9), dtype=float32, min=-9.993, max=8.451, mean=0.252),
[2m[36m(pid=9584)[0m             'obs': np.ndarray((400, 9), dtype=float32, min=-9.993, max=8.451, mean=0.219),
[2m[36m(pid=9584)[0m             'prev_actions': np.ndarray((400, 2), dtype=float32, min=-1.0, max=1.0, mean=0.464),
[2m[36m(pid=9584)[0m             'prev_rewards': np.ndarray((400,), dtype=float32, min=-14.516, max=19.794, mean=-0.195),
[2m[36m(pid=9584)[0m             'rewards': np.ndarray((400,), dtype=float32, min=-36.765, max=37.599, mean=-0.979),
[2m[36m(pid=9584)[0m             't': np.ndarray((400,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=9584)[0m             'unroll_id': np.ndarray((400,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9584)[0m             'weights': np.ndarray((400,), dtype=float32, min=0.001, max=37.222, mean=7.782)},
[2m[36m(pid=9584)[0m   'type': 'SampleBatch'}
[2m[36m(pid=9584)[0m 
[2m[36m(pid=9736)[0m 2019-07-18 01:37:16,764	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.095),
[2m[36m(pid=9736)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=9736)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=827776526.0, max=827776526.0, mean=827776526.0),
[2m[36m(pid=9736)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=9736)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.258),
[2m[36m(pid=9736)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.26),
[2m[36m(pid=9736)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.094),
[2m[36m(pid=9736)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-30.356, max=26.204, mean=-0.255),
[2m[36m(pid=9736)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-40.425, max=35.426, mean=-0.383),
[2m[36m(pid=9736)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=9736)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.011, max=40.16, mean=11.16)},
[2m[36m(pid=9736)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m 2019-07-18 01:37:16,860	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m { 'data': { 'actions': np.ndarray((450, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.097),
[2m[36m(pid=9736)[0m             'agent_index': np.ndarray((450,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m             'dones': np.ndarray((450,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=9736)[0m             'eps_id': np.ndarray((450,), dtype=int64, min=370112582.0, max=1499845278.0, mean=899244795.333),
[2m[36m(pid=9736)[0m             'infos': np.ndarray((450,), dtype=object, head={}),
[2m[36m(pid=9736)[0m             'new_obs': np.ndarray((450, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.209),
[2m[36m(pid=9736)[0m             'obs': np.ndarray((450, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.201),
[2m[36m(pid=9736)[0m             'prev_actions': np.ndarray((450, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.096),
[2m[36m(pid=9736)[0m             'prev_rewards': np.ndarray((450,), dtype=float32, min=-30.356, max=30.603, mean=-0.321),
[2m[36m(pid=9736)[0m             'rewards': np.ndarray((450,), dtype=float32, min=-40.425, max=36.748, mean=-0.7),
[2m[36m(pid=9736)[0m             't': np.ndarray((450,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=9736)[0m             'unroll_id': np.ndarray((450,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9736)[0m             'weights': np.ndarray((450,), dtype=float32, min=0.011, max=40.802, mean=10.106)},
[2m[36m(pid=9736)[0m   'type': 'SampleBatch'}
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9845)[0m [32m [     0.04573s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m 2019-07-18 01:37:18,510	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9845)[0m 2019-07-18 01:37:18.510769: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9844)[0m [32m [     0.02885s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m 2019-07-18 01:37:18,581	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9844)[0m 2019-07-18 01:37:18.582397: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9845)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9845)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9844)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9844)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9938)[0m 2019-07-18 01:37:19,085	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9938)[0m 2019-07-18 01:37:19.085886: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9938)[0m [32m [     0.04065s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9938)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9845)[0m [32m [     1.21314s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m [32m [     1.21422s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m [32m [     1.21526s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m [32m [     1.21623s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m [32m [     1.21716s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m [32m [     1.21815s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9845)[0m [32m [     1.21922s,  INFO] TimeLimit:
[2m[36m(pid=9845)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9845)[0m - action_space = Box(2,)
[2m[36m(pid=9845)[0m - observation_space = Box(9,)
[2m[36m(pid=9845)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9845)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9845)[0m - _max_episode_steps = 150
[2m[36m(pid=9845)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.29323s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.29478s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.29578s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.29688s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.29816s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.29967s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9844)[0m [32m [     1.30069s,  INFO] TimeLimit:
[2m[36m(pid=9844)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9844)[0m - action_space = Box(2,)
[2m[36m(pid=9844)[0m - observation_space = Box(9,)
[2m[36m(pid=9844)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9844)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9844)[0m - _max_episode_steps = 150
[2m[36m(pid=9844)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.82503s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.82606s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.82710s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.82806s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.82904s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.83016s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9938)[0m [32m [     1.83119s,  INFO] TimeLimit:
[2m[36m(pid=9938)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9938)[0m - action_space = Box(2,)
[2m[36m(pid=9938)[0m - observation_space = Box(9,)
[2m[36m(pid=9938)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9938)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9938)[0m - _max_episode_steps = 150
[2m[36m(pid=9938)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9736)[0m 2019-07-18 01:37:22,866	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m { 'count': 512,
[2m[36m(pid=9736)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=0.049),
[2m[36m(pid=9736)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=3.0, max=13182.0, mean=6225.033),
[2m[36m(pid=9736)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.004),
[2m[36m(pid=9736)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-8.33, max=8.287, mean=0.046),
[2m[36m(pid=9736)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-8.33, max=6.985, mean=0.037),
[2m[36m(pid=9736)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-36.765, max=31.061, mean=-0.789),
[2m[36m(pid=9736)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.044, max=0.292, mean=0.085)},
[2m[36m(pid=9736)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=9736)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m 2019-07-18 01:37:23,159	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=9736)[0m 
[2m[36m(pid=9736)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.18649158,
[2m[36m(pid=9736)[0m                                          'mean_q': 0.0040280162,
[2m[36m(pid=9736)[0m                                          'min_q': -0.22909774},
[2m[36m(pid=9736)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-31.25, max=36.894, mean=0.807)}}
[2m[36m(pid=9736)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-37-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.61281404694942
  episode_reward_mean: -11.52376563176303
  episode_reward_min: -95.38790541024339
  episodes_this_iter: 736
  episodes_total: 736
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 14.404412269592285
        mean_q: 2.225229501724243
        min_q: -10.951786994934082
    learner_queue:
      size_count: 1789
      size_mean: 0.02
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.13999999999999999
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 316000
    num_steps_trained: 182784
    num_target_updates: 3
    num_weight_syncs: 790
    replay_shard_0:
      add_batch_time_ms: 21.474
      policy_default_policy:
        added_count: 78400
        est_size_bytes: 26734400
        num_entries: 78400
        sampled_count: 43520
      replay_time_ms: 35.069
      update_priorities_time_ms: 108.43
    sample_throughput: 12604.649
    train_throughput: 3226.79
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0655913224293916
    mean_inference_ms: 2.750894419478905
    mean_processing_ms: 2.2799185865977476
  time_since_restore: 30.766132354736328
  time_this_iter_s: 30.766132354736328
  time_total_s: 30.766132354736328
  timestamp: 1563406669
  timesteps_since_restore: 316000
  timesteps_this_iter: 316000
  timesteps_total: 316000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 30 s, 1 iter, 316000 ts, -11.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-38-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.173023087885145
  episode_reward_mean: -17.936456430743448
  episode_reward_min: -102.17489645082259
  episodes_this_iter: 744
  episodes_total: 1480
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 20.696680068969727
        mean_q: 1.3573691844940186
        min_q: -23.054059982299805
    learner_queue:
      size_count: 2212
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 622400
    num_steps_trained: 399360
    num_target_updates: 7
    num_weight_syncs: 1556
    replay_shard_0:
      add_batch_time_ms: 22.688
      policy_default_policy:
        added_count: 163600
        est_size_bytes: 55787600
        num_entries: 163600
        sampled_count: 93696
      replay_time_ms: 30.768
      update_priorities_time_ms: 98.817
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.11603989914207
    mean_inference_ms: 2.834914814932214
    mean_processing_ms: 2.3128268981442623
  time_since_restore: 61.45977067947388
  time_this_iter_s: 30.69363832473755
  time_total_s: 61.45977067947388
  timestamp: 1563406700
  timesteps_since_restore: 622400
  timesteps_this_iter: 306400
  timesteps_total: 622400
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 61 s, 2 iter, 622400 ts, -17.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-38-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 30.40497784649671
  episode_reward_mean: 0.2967558016896635
  episode_reward_min: -42.61187834544204
  episodes_this_iter: 752
  episodes_total: 2232
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 23.979001998901367
        mean_q: 4.125753402709961
        min_q: -16.304384231567383
    learner_queue:
      size_count: 2625
      size_mean: 0.24
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.47159304490206383
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 930400
    num_steps_trained: 611328
    num_target_updates: 12
    num_weight_syncs: 2326
    replay_shard_0:
      add_batch_time_ms: 17.189
      policy_default_policy:
        added_count: 234400
        est_size_bytes: 79930400
        num_entries: 234400
        sampled_count: 147456
      replay_time_ms: 41.353
      update_priorities_time_ms: 112.393
    sample_throughput: 21459.454
    train_throughput: 0.0
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.1139602768214782
    mean_inference_ms: 2.8410406987703962
    mean_processing_ms: 2.3218023866667665
  time_since_restore: 92.21673250198364
  time_this_iter_s: 30.756961822509766
  time_total_s: 92.21673250198364
  timestamp: 1563406731
  timesteps_since_restore: 930400
  timesteps_this_iter: 308000
  timesteps_total: 930400
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 92 s, 3 iter, 930400 ts, 0.297 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-39-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.462040705376005
  episode_reward_mean: 13.317651784169
  episode_reward_min: -15.990310368801179
  episodes_this_iter: 776
  episodes_total: 3008
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.36928367614746
        mean_q: 5.32193660736084
        min_q: -24.147985458374023
    learner_queue:
      size_count: 3016
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4386342439892262
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 1242800
    num_steps_trained: 811520
    num_target_updates: 16
    num_weight_syncs: 3107
    replay_shard_0:
      add_batch_time_ms: 30.619
      policy_default_policy:
        added_count: 310400
        est_size_bytes: 105846400
        num_entries: 310400
        sampled_count: 195584
      replay_time_ms: 42.176
      update_priorities_time_ms: 102.663
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.094936229360767
    mean_inference_ms: 2.7993080479618935
    mean_processing_ms: 2.309931657358768
  time_since_restore: 122.8139636516571
  time_this_iter_s: 30.597231149673462
  time_total_s: 122.8139636516571
  timestamp: 1563406761
  timesteps_since_restore: 1242800
  timesteps_this_iter: 312400
  timesteps_total: 1242800
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 122 s, 4 iter, 1242800 ts, 13.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

[2m[36m(pid=9736)[0m 2019-07-18 01:39:52,477	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-39-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.682692584741794
  episode_reward_mean: 15.077669150540343
  episode_reward_min: -11.163478502099302
  episodes_this_iter: 760
  episodes_total: 3768
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 35.24147518939337
    episode_reward_mean: 12.804467854455947
    episode_reward_min: -47.67085419672228
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.722742973219644
      mean_inference_ms: 1.3011784279574756
      mean_processing_ms: 0.9058008186029057
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.410520553588867
        mean_q: 8.08314323425293
        min_q: -18.751937866210938
    learner_queue:
      size_count: 3413
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 1554000
    num_steps_trained: 1014784
    num_target_updates: 20
    num_weight_syncs: 3885
    replay_shard_0:
      add_batch_time_ms: 58.479
      policy_default_policy:
        added_count: 386000
        est_size_bytes: 131626000
        num_entries: 386000
        sampled_count: 246272
      replay_time_ms: 36.515
      update_priorities_time_ms: 111.039
    sample_throughput: 15627.501
    train_throughput: 0.0
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0941959790974076
    mean_inference_ms: 2.808288173966815
    mean_processing_ms: 2.3064195461544776
  time_since_restore: 153.45005774497986
  time_this_iter_s: 30.636094093322754
  time_total_s: 153.45005774497986
  timestamp: 1563406792
  timesteps_since_restore: 1554000
  timesteps_this_iter: 311200
  timesteps_total: 1554000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 153 s, 5 iter, 1554000 ts, 15.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-40-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.72800803471033
  episode_reward_mean: 14.06299461115243
  episode_reward_min: -12.957482533463423
  episodes_this_iter: 752
  episodes_total: 4520
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.58009719848633
        mean_q: 8.74729061126709
        min_q: -12.698301315307617
    learner_queue:
      size_count: 3821
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 1862000
    num_steps_trained: 1223680
    num_target_updates: 24
    num_weight_syncs: 4655
    replay_shard_0:
      add_batch_time_ms: 29.57
      policy_default_policy:
        added_count: 473200
        est_size_bytes: 161361200
        num_entries: 473200
        sampled_count: 294400
      replay_time_ms: 39.62
      update_priorities_time_ms: 117.798
    sample_throughput: 0.0
    train_throughput: 35489.731
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.100772734096716
    mean_inference_ms: 2.8124306747962695
    mean_processing_ms: 2.3077107935547954
  time_since_restore: 184.2092695236206
  time_this_iter_s: 30.759211778640747
  time_total_s: 184.2092695236206
  timestamp: 1563406824
  timesteps_since_restore: 1862000
  timesteps_this_iter: 308000
  timesteps_total: 1862000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 184 s, 6 iter, 1862000 ts, 14.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-40-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.57333155406922
  episode_reward_mean: 15.364048075718465
  episode_reward_min: -11.435497552478576
  episodes_this_iter: 768
  episodes_total: 5288
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.48383712768555
        mean_q: 8.063718795776367
        min_q: -9.028610229492188
    learner_queue:
      size_count: 4221
      size_mean: 0.28
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.530659966456864
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 2173200
    num_steps_trained: 1427968
    num_target_updates: 28
    num_weight_syncs: 5433
    replay_shard_0:
      add_batch_time_ms: 50.978
      policy_default_policy:
        added_count: 550400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 345600
      replay_time_ms: 38.51
      update_priorities_time_ms: 99.982
    sample_throughput: 13206.247
    train_throughput: 0.0
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0890161861067806
    mean_inference_ms: 2.822724746731931
    mean_processing_ms: 2.305889744770058
  time_since_restore: 214.92256021499634
  time_this_iter_s: 30.713290691375732
  time_total_s: 214.92256021499634
  timestamp: 1563406855
  timesteps_since_restore: 2173200
  timesteps_this_iter: 311200
  timesteps_total: 2173200
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 214 s, 7 iter, 2173200 ts, 15.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-41-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.229854938583024
  episode_reward_mean: 14.971684414328994
  episode_reward_min: -15.501750419894393
  episodes_this_iter: 744
  episodes_total: 6032
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 38.3409538269043
        mean_q: 8.218952178955078
        min_q: -1.3509432077407837
    learner_queue:
      size_count: 4623
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4004996878900157
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 2485600
    num_steps_trained: 1634304
    num_target_updates: 32
    num_weight_syncs: 6214
    replay_shard_0:
      add_batch_time_ms: 43.814
      policy_default_policy:
        added_count: 622400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 398336
      replay_time_ms: 42.612
      update_priorities_time_ms: 122.517
    sample_throughput: 16946.797
    train_throughput: 7230.633
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.095442882152085
    mean_inference_ms: 2.827643280734152
    mean_processing_ms: 2.3076456928944147
  time_since_restore: 245.55891823768616
  time_this_iter_s: 30.63635802268982
  time_total_s: 245.55891823768616
  timestamp: 1563406885
  timesteps_since_restore: 2485600
  timesteps_this_iter: 312400
  timesteps_total: 2485600
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 245 s, 8 iter, 2485600 ts, 15 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-41-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.22307409480285
  episode_reward_mean: 15.107841881982099
  episode_reward_min: -18.79001263368035
  episodes_this_iter: 744
  episodes_total: 6776
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.655235290527344
        mean_q: 7.33397102355957
        min_q: -0.927620530128479
    learner_queue:
      size_count: 5036
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 2793600
    num_steps_trained: 1845760
    num_target_updates: 36
    num_weight_syncs: 6984
    replay_shard_0:
      add_batch_time_ms: 20.499
      policy_default_policy:
        added_count: 698400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 455680
      replay_time_ms: 46.735
      update_priorities_time_ms: 112.401
    sample_throughput: 0.0
    train_throughput: 17026.225
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0959641615138023
    mean_inference_ms: 2.8329541201712005
    mean_processing_ms: 2.305074696532975
  time_since_restore: 276.2273950576782
  time_this_iter_s: 30.668476819992065
  time_total_s: 276.2273950576782
  timestamp: 1563406916
  timesteps_since_restore: 2793600
  timesteps_this_iter: 308000
  timesteps_total: 2793600
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 276 s, 9 iter, 2793600 ts, 15.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

[2m[36m(pid=9736)[0m 2019-07-18 01:42:27,351	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-42-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.049595064076364
  episode_reward_mean: 16.074796311290957
  episode_reward_min: -11.266876892938898
  episodes_this_iter: 752
  episodes_total: 7528
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 33.74363100215082
    episode_reward_mean: 13.491382077815155
    episode_reward_min: -0.8551253370388813
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.4731854878816921
      mean_inference_ms: 1.0575056737955915
      mean_processing_ms: 0.7884960449913484
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.24900817871094
        mean_q: 6.548273086547852
        min_q: -0.8267830610275269
    learner_queue:
      size_count: 5454
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.417612260356422
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 3102800
    num_steps_trained: 2059264
    num_target_updates: 41
    num_weight_syncs: 7757
    replay_shard_0:
      add_batch_time_ms: 20.47
      policy_default_policy:
        added_count: 780400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 511488
      replay_time_ms: 31.705
      update_priorities_time_ms: 102.337
    sample_throughput: 18393.045
    train_throughput: 5885.774
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.09341999302819
    mean_inference_ms: 2.834259096648312
    mean_processing_ms: 2.306354140435087
  time_since_restore: 307.0173490047455
  time_this_iter_s: 30.78995394706726
  time_total_s: 307.0173490047455
  timestamp: 1563406947
  timesteps_since_restore: 3102800
  timesteps_this_iter: 309200
  timesteps_total: 3102800
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 307 s, 10 iter, 3102800 ts, 16.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-42-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.17723687493864
  episode_reward_mean: 17.29455810379779
  episode_reward_min: -5.398310527260818
  episodes_this_iter: 752
  episodes_total: 8280
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.712284088134766
        mean_q: 6.529592514038086
        min_q: -0.41834598779678345
    learner_queue:
      size_count: 5883
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 3410000
    num_steps_trained: 2279424
    num_target_updates: 45
    num_weight_syncs: 8525
    replay_shard_0:
      add_batch_time_ms: 42.576
      policy_default_policy:
        added_count: 862000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 563200
      replay_time_ms: 39.028
      update_priorities_time_ms: 101.294
    sample_throughput: 21430.398
    train_throughput: 9143.637
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.098883819379648
    mean_inference_ms: 2.8420309177224077
    mean_processing_ms: 2.3072891366499864
  time_since_restore: 337.725301027298
  time_this_iter_s: 30.70795202255249
  time_total_s: 337.725301027298
  timestamp: 1563406979
  timesteps_since_restore: 3410000
  timesteps_this_iter: 307200
  timesteps_total: 3410000
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 337 s, 11 iter, 3410000 ts, 17.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-43-30
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.24973252167221
  episode_reward_mean: 17.52409719523903
  episode_reward_min: -16.058793166619925
  episodes_this_iter: 752
  episodes_total: 9032
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.29059600830078
        mean_q: 6.4242048263549805
        min_q: -1.347670316696167
    learner_queue:
      size_count: 6298
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 3718400
    num_steps_trained: 2491904
    num_target_updates: 49
    num_weight_syncs: 9296
    replay_shard_0:
      add_batch_time_ms: 47.053
      policy_default_policy:
        added_count: 943600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 614400
      replay_time_ms: 36.731
      update_priorities_time_ms: 127.182
    sample_throughput: 7434.501
    train_throughput: 9516.162
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0994980731865
    mean_inference_ms: 2.8475253339806743
    mean_processing_ms: 2.311069159106536
  time_since_restore: 368.3880536556244
  time_this_iter_s: 30.662752628326416
  time_total_s: 368.3880536556244
  timestamp: 1563407010
  timesteps_since_restore: 3718400
  timesteps_this_iter: 308400
  timesteps_total: 3718400
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 368 s, 12 iter, 3718400 ts, 17.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-44-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.73748047829895
  episode_reward_mean: 17.520372808903385
  episode_reward_min: -12.29686088441047
  episodes_this_iter: 760
  episodes_total: 9792
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.46434783935547
        mean_q: 6.308156967163086
        min_q: -1.203294038772583
    learner_queue:
      size_count: 6710
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 4028800
    num_steps_trained: 2702336
    num_target_updates: 53
    num_weight_syncs: 10072
    replay_shard_0:
      add_batch_time_ms: 37.613
      policy_default_policy:
        added_count: 1014800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 670208
      replay_time_ms: 40.195
      update_priorities_time_ms: 99.939
    sample_throughput: 14250.369
    train_throughput: 7296.189
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.1009554249736353
    mean_inference_ms: 2.8459037174191075
    mean_processing_ms: 2.307387658437136
  time_since_restore: 399.15794491767883
  time_this_iter_s: 30.769891262054443
  time_total_s: 399.15794491767883
  timestamp: 1563407040
  timesteps_since_restore: 4028800
  timesteps_this_iter: 310400
  timesteps_total: 4028800
  training_iteration: 13
  2019-07-18 01:44:31,497	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 01:44:31,506	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9736], 399 s, 13 iter, 4028800 ts, 17.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-44-31
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.05308786826469
  episode_reward_mean: 18.197847307349463
  episode_reward_min: -13.475940384037738
  episodes_this_iter: 752
  episodes_total: 10544
  experiment_id: b9aba360c593421eb5400245c8c62855
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.642913818359375
        mean_q: 5.838855743408203
        min_q: -0.9513905644416809
    learner_queue:
      size_count: 7115
      size_mean: 0.46
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 2.0
      - 5.0
      size_std: 1.0992724866929036
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 0
    num_steps_sampled: 4339200
    num_steps_trained: 2910208
    num_target_updates: 57
    num_weight_syncs: 10848
    replay_shard_0:
      add_batch_time_ms: 38.203
      policy_default_policy:
        added_count: 1090800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 723456
      replay_time_ms: 36.28
      update_priorities_time_ms: 104.165
    sample_throughput: 10488.546
    train_throughput: 13425.339
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9736
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.099677626517044
    mean_inference_ms: 2.847700468847879
    mean_processing_ms: 2.3088543713867318
  time_since_restore: 429.8428120613098
  time_this_iter_s: 30.68486714363098
  time_total_s: 429.8428120613098
  timestamp: 1563407071
  timesteps_since_restore: 4339200
  timesteps_this_iter: 310400
  timesteps_total: 4339200
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 14.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'PENDING': 4})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

[2m[36m(pid=9931)[0m [32m [     0.07806s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m 2019-07-18 01:44:31.781166: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9931)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9931)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9931)[0m [32m [     1.42662s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m [32m [     1.42713s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m [32m [     1.42759s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m 2019-07-18 01:44:33,127	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fec9f13dac8>}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:33,127	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fec9f128080>}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:33,127	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fec9f166cc0>}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:33,131	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=9931)[0m 2019-07-18 01:44:33,139	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=9935)[0m 2019-07-18 01:44:33,213	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9935)[0m 2019-07-18 01:44:33.213629: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9935)[0m [32m [     0.03165s,  INFO] TimeLimit:
[2m[36m(pid=9935)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9935)[0m - action_space = Box(2,)
[2m[36m(pid=9935)[0m - observation_space = Box(9,)
[2m[36m(pid=9935)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9935)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9935)[0m - _max_episode_steps = 150
[2m[36m(pid=9935)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9843)[0m 2019-07-18 01:44:33,255	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9843)[0m 2019-07-18 01:44:33.255906: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9843)[0m [32m [     0.03928s,  INFO] TimeLimit:
[2m[36m(pid=9843)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9843)[0m - action_space = Box(2,)
[2m[36m(pid=9843)[0m - observation_space = Box(9,)
[2m[36m(pid=9843)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9843)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9843)[0m - _max_episode_steps = 150
[2m[36m(pid=9843)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9941)[0m 2019-07-18 01:44:33,270	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9941)[0m 2019-07-18 01:44:33.271734: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9941)[0m [32m [     0.05114s,  INFO] TimeLimit:
[2m[36m(pid=9941)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9941)[0m - action_space = Box(2,)
[2m[36m(pid=9941)[0m - observation_space = Box(9,)
[2m[36m(pid=9941)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9941)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9941)[0m - _max_episode_steps = 150
[2m[36m(pid=9941)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9937)[0m [32m [     0.03859s,  INFO] TimeLimit:
[2m[36m(pid=9937)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9937)[0m - action_space = Box(2,)
[2m[36m(pid=9937)[0m - observation_space = Box(9,)
[2m[36m(pid=9937)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9937)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9937)[0m - _max_episode_steps = 150
[2m[36m(pid=9937)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9933)[0m 2019-07-18 01:44:33,248	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9933)[0m 2019-07-18 01:44:33.249109: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9937)[0m 2019-07-18 01:44:33,241	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9937)[0m 2019-07-18 01:44:33.241814: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9933)[0m [32m [     0.04001s,  INFO] TimeLimit:
[2m[36m(pid=9933)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9933)[0m - action_space = Box(2,)
[2m[36m(pid=9933)[0m - observation_space = Box(9,)
[2m[36m(pid=9933)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9933)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9933)[0m - _max_episode_steps = 150
[2m[36m(pid=9933)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9929)[0m [32m [     0.03986s,  INFO] TimeLimit:
[2m[36m(pid=9929)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9929)[0m - action_space = Box(2,)
[2m[36m(pid=9929)[0m - observation_space = Box(9,)
[2m[36m(pid=9929)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9929)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9929)[0m - _max_episode_steps = 150
[2m[36m(pid=9929)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9929)[0m 2019-07-18 01:44:33,241	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=9929)[0m 2019-07-18 01:44:33.241843: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=9935)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9935)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9843)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9843)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9933)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9933)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9937)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9937)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9929)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9929)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9941)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9941)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=9931)[0m [32m [     1.83521s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9937)[0m [32m [     2.54936s,  INFO] TimeLimit:
[2m[36m(pid=9937)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9937)[0m - action_space = Box(2,)
[2m[36m(pid=9937)[0m - observation_space = Box(9,)
[2m[36m(pid=9937)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9937)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9937)[0m - _max_episode_steps = 150
[2m[36m(pid=9937)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9937)[0m [32m [     2.55031s,  INFO] TimeLimit:
[2m[36m(pid=9937)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9937)[0m - action_space = Box(2,)
[2m[36m(pid=9937)[0m - observation_space = Box(9,)
[2m[36m(pid=9937)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9937)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9937)[0m - _max_episode_steps = 150
[2m[36m(pid=9937)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9937)[0m [32m [     2.55124s,  INFO] TimeLimit:
[2m[36m(pid=9937)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9937)[0m - action_space = Box(2,)
[2m[36m(pid=9937)[0m - observation_space = Box(9,)
[2m[36m(pid=9937)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9937)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9937)[0m - _max_episode_steps = 150
[2m[36m(pid=9937)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9933)[0m [32m [     2.77298s,  INFO] TimeLimit:
[2m[36m(pid=9933)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9933)[0m - action_space = Box(2,)
[2m[36m(pid=9933)[0m - observation_space = Box(9,)
[2m[36m(pid=9933)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9933)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9933)[0m - _max_episode_steps = 150
[2m[36m(pid=9933)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9933)[0m [32m [     2.77387s,  INFO] TimeLimit:
[2m[36m(pid=9933)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9933)[0m - action_space = Box(2,)
[2m[36m(pid=9933)[0m - observation_space = Box(9,)
[2m[36m(pid=9933)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9933)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9933)[0m - _max_episode_steps = 150
[2m[36m(pid=9933)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9933)[0m [32m [     2.77479s,  INFO] TimeLimit:
[2m[36m(pid=9933)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9933)[0m - action_space = Box(2,)
[2m[36m(pid=9933)[0m - observation_space = Box(9,)
[2m[36m(pid=9933)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9933)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9933)[0m - _max_episode_steps = 150
[2m[36m(pid=9933)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9929)[0m [32m [     3.01448s,  INFO] TimeLimit:
[2m[36m(pid=9929)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9929)[0m - action_space = Box(2,)
[2m[36m(pid=9929)[0m - observation_space = Box(9,)
[2m[36m(pid=9929)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9929)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9929)[0m - _max_episode_steps = 150
[2m[36m(pid=9929)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9929)[0m [32m [     3.01545s,  INFO] TimeLimit:
[2m[36m(pid=9929)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9929)[0m - action_space = Box(2,)
[2m[36m(pid=9929)[0m - observation_space = Box(9,)
[2m[36m(pid=9929)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9929)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9929)[0m - _max_episode_steps = 150
[2m[36m(pid=9929)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9929)[0m [32m [     3.01656s,  INFO] TimeLimit:
[2m[36m(pid=9929)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9929)[0m - action_space = Box(2,)
[2m[36m(pid=9929)[0m - observation_space = Box(9,)
[2m[36m(pid=9929)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9929)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9929)[0m - _max_episode_steps = 150
[2m[36m(pid=9929)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9843)[0m [32m [     3.05255s,  INFO] TimeLimit:
[2m[36m(pid=9843)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9843)[0m - action_space = Box(2,)
[2m[36m(pid=9843)[0m - observation_space = Box(9,)
[2m[36m(pid=9843)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9843)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9843)[0m - _max_episode_steps = 150
[2m[36m(pid=9843)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9843)[0m [32m [     3.05350s,  INFO] TimeLimit:
[2m[36m(pid=9843)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9843)[0m - action_space = Box(2,)
[2m[36m(pid=9843)[0m - observation_space = Box(9,)
[2m[36m(pid=9843)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9843)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9843)[0m - _max_episode_steps = 150
[2m[36m(pid=9843)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9843)[0m [32m [     3.05448s,  INFO] TimeLimit:
[2m[36m(pid=9843)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9843)[0m - action_space = Box(2,)
[2m[36m(pid=9843)[0m - observation_space = Box(9,)
[2m[36m(pid=9843)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9843)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9843)[0m - _max_episode_steps = 150
[2m[36m(pid=9843)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9941)[0m [32m [     3.09331s,  INFO] TimeLimit:
[2m[36m(pid=9941)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9941)[0m - action_space = Box(2,)
[2m[36m(pid=9941)[0m - observation_space = Box(9,)
[2m[36m(pid=9941)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9941)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9941)[0m - _max_episode_steps = 150
[2m[36m(pid=9941)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9941)[0m [32m [     3.09433s,  INFO] TimeLimit:
[2m[36m(pid=9941)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9941)[0m - action_space = Box(2,)
[2m[36m(pid=9941)[0m - observation_space = Box(9,)
[2m[36m(pid=9941)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9941)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9941)[0m - _max_episode_steps = 150
[2m[36m(pid=9941)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9941)[0m [32m [     3.09536s,  INFO] TimeLimit:
[2m[36m(pid=9941)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9941)[0m - action_space = Box(2,)
[2m[36m(pid=9941)[0m - observation_space = Box(9,)
[2m[36m(pid=9941)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9941)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9941)[0m - _max_episode_steps = 150
[2m[36m(pid=9941)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9935)[0m [32m [     3.09242s,  INFO] TimeLimit:
[2m[36m(pid=9935)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9935)[0m - action_space = Box(2,)
[2m[36m(pid=9935)[0m - observation_space = Box(9,)
[2m[36m(pid=9935)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9935)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9935)[0m - _max_episode_steps = 150
[2m[36m(pid=9935)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9935)[0m [32m [     3.09329s,  INFO] TimeLimit:
[2m[36m(pid=9935)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9935)[0m - action_space = Box(2,)
[2m[36m(pid=9935)[0m - observation_space = Box(9,)
[2m[36m(pid=9935)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9935)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9935)[0m - _max_episode_steps = 150
[2m[36m(pid=9935)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9935)[0m [32m [     3.09413s,  INFO] TimeLimit:
[2m[36m(pid=9935)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9935)[0m - action_space = Box(2,)
[2m[36m(pid=9935)[0m - observation_space = Box(9,)
[2m[36m(pid=9935)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9935)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9935)[0m - _max_episode_steps = 150
[2m[36m(pid=9935)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,353	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,467	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.0, max=0.198, mean=-0.199)},
[2m[36m(pid=9935)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.325, max=0.959, mean=0.039)},
[2m[36m(pid=9935)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.21, max=0.977, mean=0.227)},
[2m[36m(pid=9935)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.836, max=0.603, mean=0.049)}}
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,468	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=9935)[0m   1: {'agent0': None},
[2m[36m(pid=9935)[0m   2: {'agent0': None},
[2m[36m(pid=9935)[0m   3: {'agent0': None}}
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,468	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-1.0, max=0.198, mean=-0.199)
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,468	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-1.0, max=0.198, mean=-0.199)
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,472	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9935)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9935)[0m                                   'env_id': 0,
[2m[36m(pid=9935)[0m                                   'info': None,
[2m[36m(pid=9935)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.0, max=0.198, mean=-0.199),
[2m[36m(pid=9935)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9935)[0m                                   'rnn_state': []},
[2m[36m(pid=9935)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9935)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9935)[0m                                   'env_id': 1,
[2m[36m(pid=9935)[0m                                   'info': None,
[2m[36m(pid=9935)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.325, max=0.959, mean=0.039),
[2m[36m(pid=9935)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9935)[0m                                   'rnn_state': []},
[2m[36m(pid=9935)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9935)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9935)[0m                                   'env_id': 2,
[2m[36m(pid=9935)[0m                                   'info': None,
[2m[36m(pid=9935)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.21, max=0.977, mean=0.227),
[2m[36m(pid=9935)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9935)[0m                                   'rnn_state': []},
[2m[36m(pid=9935)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9935)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9935)[0m                                   'env_id': 3,
[2m[36m(pid=9935)[0m                                   'info': None,
[2m[36m(pid=9935)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.836, max=0.603, mean=0.049),
[2m[36m(pid=9935)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9935)[0m                                   'rnn_state': []},
[2m[36m(pid=9935)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,472	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,530	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9935)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=0.072, max=0.161, mean=0.119),
[2m[36m(pid=9935)[0m                       [],
[2m[36m(pid=9935)[0m                       {})}
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9931)[0m [32m [     4.84706s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m [32m [     4.84803s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m [32m [     4.84898s,  INFO] TimeLimit:
[2m[36m(pid=9931)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=9931)[0m - action_space = Box(2,)
[2m[36m(pid=9931)[0m - observation_space = Box(9,)
[2m[36m(pid=9931)[0m - reward_range = (-inf, inf)
[2m[36m(pid=9931)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=9931)[0m - _max_episode_steps = 150
[2m[36m(pid=9931)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,547	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fec97762390>}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,548	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fec977620b8>}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,548	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fec9775ad30>}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,561	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,632	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,678	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.781, max=0.625, mean=-0.039)},
[2m[36m(pid=9931)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.982, max=0.223, mean=-0.204)},
[2m[36m(pid=9931)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.815, max=0.579, mean=-0.117)},
[2m[36m(pid=9931)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.949, max=0.889, mean=-0.062)}}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,678	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=9931)[0m   1: {'agent0': None},
[2m[36m(pid=9931)[0m   2: {'agent0': None},
[2m[36m(pid=9931)[0m   3: {'agent0': None}}
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,679	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.781, max=0.625, mean=-0.039)
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,679	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.781, max=0.625, mean=-0.039)
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,682	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9931)[0m                                   'env_id': 0,
[2m[36m(pid=9931)[0m                                   'info': None,
[2m[36m(pid=9931)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.781, max=0.625, mean=-0.039),
[2m[36m(pid=9931)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9931)[0m                                   'rnn_state': []},
[2m[36m(pid=9931)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9931)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9931)[0m                                   'env_id': 1,
[2m[36m(pid=9931)[0m                                   'info': None,
[2m[36m(pid=9931)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.982, max=0.223, mean=-0.204),
[2m[36m(pid=9931)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9931)[0m                                   'rnn_state': []},
[2m[36m(pid=9931)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9931)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9931)[0m                                   'env_id': 2,
[2m[36m(pid=9931)[0m                                   'info': None,
[2m[36m(pid=9931)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.815, max=0.579, mean=-0.117),
[2m[36m(pid=9931)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9931)[0m                                   'rnn_state': []},
[2m[36m(pid=9931)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=9931)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=9931)[0m                                   'env_id': 3,
[2m[36m(pid=9931)[0m                                   'info': None,
[2m[36m(pid=9931)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.949, max=0.889, mean=-0.062),
[2m[36m(pid=9931)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=9931)[0m                                   'rnn_state': []},
[2m[36m(pid=9931)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,682	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=9931)[0m 2019-07-18 01:44:36,785	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-0.407, max=0.288, mean=-0.058),
[2m[36m(pid=9931)[0m                       [],
[2m[36m(pid=9931)[0m                       {})}
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,877	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9935)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.186),
[2m[36m(pid=9935)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=642208670.0, max=642208670.0, mean=642208670.0),
[2m[36m(pid=9935)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=9935)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-3.305, max=3.708, mean=-0.078),
[2m[36m(pid=9935)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-3.305, max=3.708, mean=-0.094),
[2m[36m(pid=9935)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.196),
[2m[36m(pid=9935)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-3.779, max=3.031, mean=0.004),
[2m[36m(pid=9935)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-10.676, max=8.954, mean=0.176),
[2m[36m(pid=9935)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=9935)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.049, max=10.705, mean=3.694)},
[2m[36m(pid=9935)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9935)[0m 2019-07-18 01:44:36,910	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9935)[0m { 'data': { 'actions': np.ndarray((200, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.189),
[2m[36m(pid=9935)[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m             'eps_id': np.ndarray((200,), dtype=int64, min=642208670.0, max=1640639007.0, mean=1019490030.0),
[2m[36m(pid=9935)[0m             'infos': np.ndarray((200,), dtype=object, head={}),
[2m[36m(pid=9935)[0m             'new_obs': np.ndarray((200, 9), dtype=float32, min=-4.789, max=3.708, mean=-0.131),
[2m[36m(pid=9935)[0m             'obs': np.ndarray((200, 9), dtype=float32, min=-4.789, max=3.708, mean=-0.125),
[2m[36m(pid=9935)[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.2),
[2m[36m(pid=9935)[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-4.585, max=5.402, mean=-0.038),
[2m[36m(pid=9935)[0m             'rewards': np.ndarray((200,), dtype=float32, min=-12.293, max=14.328, mean=-0.042),
[2m[36m(pid=9935)[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=9935)[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9935)[0m             'weights': np.ndarray((200,), dtype=float32, min=0.007, max=14.212, mean=2.992)},
[2m[36m(pid=9935)[0m   'type': 'SampleBatch'}
[2m[36m(pid=9935)[0m 
[2m[36m(pid=9931)[0m 2019-07-18 01:44:37,456	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.031),
[2m[36m(pid=9931)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=9931)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=290083456.0, max=290083456.0, mean=290083456.0),
[2m[36m(pid=9931)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=9931)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-8.087, max=8.345, mean=0.114),
[2m[36m(pid=9931)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-8.087, max=8.345, mean=0.117),
[2m[36m(pid=9931)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.026),
[2m[36m(pid=9931)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-10.622, max=6.884, mean=-0.22),
[2m[36m(pid=9931)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-21.396, max=11.716, mean=-0.637),
[2m[36m(pid=9931)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=9931)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.012, max=21.328, mean=2.767)},
[2m[36m(pid=9931)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m 2019-07-18 01:44:37,503	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m { 'data': { 'actions': np.ndarray((300, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.041),
[2m[36m(pid=9931)[0m             'agent_index': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m             'dones': np.ndarray((300,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=9931)[0m             'eps_id': np.ndarray((300,), dtype=int64, min=107506051.0, max=290083456.0, mean=198794753.5),
[2m[36m(pid=9931)[0m             'infos': np.ndarray((300,), dtype=object, head={}),
[2m[36m(pid=9931)[0m             'new_obs': np.ndarray((300, 9), dtype=float32, min=-8.087, max=8.345, mean=0.11),
[2m[36m(pid=9931)[0m             'obs': np.ndarray((300, 9), dtype=float32, min=-8.087, max=8.345, mean=0.11),
[2m[36m(pid=9931)[0m             'prev_actions': np.ndarray((300, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.035),
[2m[36m(pid=9931)[0m             'prev_rewards': np.ndarray((300,), dtype=float32, min=-10.622, max=6.884, mean=-0.114),
[2m[36m(pid=9931)[0m             'rewards': np.ndarray((300,), dtype=float32, min=-21.396, max=17.215, mean=-0.304),
[2m[36m(pid=9931)[0m             't': np.ndarray((300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=9931)[0m             'unroll_id': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=9931)[0m             'weights': np.ndarray((300,), dtype=float32, min=0.005, max=21.328, mean=5.02)},
[2m[36m(pid=9931)[0m   'type': 'SampleBatch'}
[2m[36m(pid=9931)[0m 
[2m[36m(pid=10681)[0m 2019-07-18 01:44:39,023	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10681)[0m 2019-07-18 01:44:39.023630: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10681)[0m [32m [     0.09018s,  INFO] TimeLimit:
[2m[36m(pid=10681)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10681)[0m - action_space = Box(2,)
[2m[36m(pid=10681)[0m - observation_space = Box(9,)
[2m[36m(pid=10681)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10681)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10681)[0m - _max_episode_steps = 150
[2m[36m(pid=10681)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10684)[0m 2019-07-18 01:44:39,089	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10684)[0m 2019-07-18 01:44:39.089637: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10684)[0m [32m [     0.05623s,  INFO] TimeLimit:
[2m[36m(pid=10684)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10684)[0m - action_space = Box(2,)
[2m[36m(pid=10684)[0m - observation_space = Box(9,)
[2m[36m(pid=10684)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10684)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10684)[0m - _max_episode_steps = 150
[2m[36m(pid=10684)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10681)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10681)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10682)[0m 2019-07-18 01:44:39,183	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10682)[0m [32m [     0.03986s,  INFO] TimeLimit:
[2m[36m(pid=10682)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10682)[0m - action_space = Box(2,)
[2m[36m(pid=10682)[0m - observation_space = Box(9,)
[2m[36m(pid=10682)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10682)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10682)[0m - _max_episode_steps = 150
[2m[36m(pid=10682)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10682)[0m 2019-07-18 01:44:39.184595: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10684)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10684)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10683)[0m 2019-07-18 01:44:39,329	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10683)[0m 2019-07-18 01:44:39.330011: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10683)[0m [32m [     0.10447s,  INFO] TimeLimit:
[2m[36m(pid=10683)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10683)[0m - action_space = Box(2,)
[2m[36m(pid=10683)[0m - observation_space = Box(9,)
[2m[36m(pid=10683)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10683)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10683)[0m - _max_episode_steps = 150
[2m[36m(pid=10683)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10682)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10682)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10683)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10683)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10792)[0m [32m [     0.04035s,  INFO] TimeLimit:
[2m[36m(pid=10792)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10792)[0m - action_space = Box(2,)
[2m[36m(pid=10792)[0m - observation_space = Box(9,)
[2m[36m(pid=10792)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10792)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10792)[0m - _max_episode_steps = 150
[2m[36m(pid=10792)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10792)[0m 2019-07-18 01:44:40,204	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10792)[0m 2019-07-18 01:44:40.204795: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10792)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10792)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10684)[0m [32m [     1.58274s,  INFO] TimeLimit:
[2m[36m(pid=10684)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10684)[0m - action_space = Box(2,)
[2m[36m(pid=10684)[0m - observation_space = Box(9,)
[2m[36m(pid=10684)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10684)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10684)[0m - _max_episode_steps = 150
[2m[36m(pid=10684)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10684)[0m [32m [     1.58372s,  INFO] TimeLimit:
[2m[36m(pid=10684)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10684)[0m - action_space = Box(2,)
[2m[36m(pid=10684)[0m - observation_space = Box(9,)
[2m[36m(pid=10684)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10684)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10684)[0m - _max_episode_steps = 150
[2m[36m(pid=10684)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10684)[0m [32m [     1.58471s,  INFO] TimeLimit:
[2m[36m(pid=10684)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10684)[0m - action_space = Box(2,)
[2m[36m(pid=10684)[0m - observation_space = Box(9,)
[2m[36m(pid=10684)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10684)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10684)[0m - _max_episode_steps = 150
[2m[36m(pid=10684)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10681)[0m [32m [     1.71744s,  INFO] TimeLimit:
[2m[36m(pid=10681)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10681)[0m - action_space = Box(2,)
[2m[36m(pid=10681)[0m - observation_space = Box(9,)
[2m[36m(pid=10681)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10681)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10681)[0m - _max_episode_steps = 150
[2m[36m(pid=10681)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10681)[0m [32m [     1.71836s,  INFO] TimeLimit:
[2m[36m(pid=10681)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10681)[0m - action_space = Box(2,)
[2m[36m(pid=10681)[0m - observation_space = Box(9,)
[2m[36m(pid=10681)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10681)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10681)[0m - _max_episode_steps = 150
[2m[36m(pid=10681)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10681)[0m [32m [     1.71931s,  INFO] TimeLimit:
[2m[36m(pid=10681)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10681)[0m - action_space = Box(2,)
[2m[36m(pid=10681)[0m - observation_space = Box(9,)
[2m[36m(pid=10681)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10681)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10681)[0m - _max_episode_steps = 150
[2m[36m(pid=10681)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10683)[0m [32m [     1.53770s,  INFO] TimeLimit:
[2m[36m(pid=10683)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10683)[0m - action_space = Box(2,)
[2m[36m(pid=10683)[0m - observation_space = Box(9,)
[2m[36m(pid=10683)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10683)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10683)[0m - _max_episode_steps = 150
[2m[36m(pid=10683)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10683)[0m [32m [     1.54002s,  INFO] TimeLimit:
[2m[36m(pid=10683)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10683)[0m - action_space = Box(2,)
[2m[36m(pid=10683)[0m - observation_space = Box(9,)
[2m[36m(pid=10683)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10683)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10683)[0m - _max_episode_steps = 150
[2m[36m(pid=10683)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10683)[0m [32m [     1.54105s,  INFO] TimeLimit:
[2m[36m(pid=10683)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10683)[0m - action_space = Box(2,)
[2m[36m(pid=10683)[0m - observation_space = Box(9,)
[2m[36m(pid=10683)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10683)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10683)[0m - _max_episode_steps = 150
[2m[36m(pid=10683)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10682)[0m [32m [     1.75026s,  INFO] TimeLimit:
[2m[36m(pid=10682)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10682)[0m - action_space = Box(2,)
[2m[36m(pid=10682)[0m - observation_space = Box(9,)
[2m[36m(pid=10682)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10682)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10682)[0m - _max_episode_steps = 150
[2m[36m(pid=10682)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10682)[0m [32m [     1.75133s,  INFO] TimeLimit:
[2m[36m(pid=10682)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10682)[0m - action_space = Box(2,)
[2m[36m(pid=10682)[0m - observation_space = Box(9,)
[2m[36m(pid=10682)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10682)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10682)[0m - _max_episode_steps = 150
[2m[36m(pid=10682)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10682)[0m [32m [     1.75233s,  INFO] TimeLimit:
[2m[36m(pid=10682)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10682)[0m - action_space = Box(2,)
[2m[36m(pid=10682)[0m - observation_space = Box(9,)
[2m[36m(pid=10682)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10682)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10682)[0m - _max_episode_steps = 150
[2m[36m(pid=10682)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10792)[0m [32m [     1.94916s,  INFO] TimeLimit:
[2m[36m(pid=10792)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10792)[0m - action_space = Box(2,)
[2m[36m(pid=10792)[0m - observation_space = Box(9,)
[2m[36m(pid=10792)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10792)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10792)[0m - _max_episode_steps = 150
[2m[36m(pid=10792)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10792)[0m [32m [     1.95026s,  INFO] TimeLimit:
[2m[36m(pid=10792)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10792)[0m - action_space = Box(2,)
[2m[36m(pid=10792)[0m - observation_space = Box(9,)
[2m[36m(pid=10792)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10792)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10792)[0m - _max_episode_steps = 150
[2m[36m(pid=10792)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10792)[0m [32m [     1.95128s,  INFO] TimeLimit:
[2m[36m(pid=10792)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10792)[0m - action_space = Box(2,)
[2m[36m(pid=10792)[0m - observation_space = Box(9,)
[2m[36m(pid=10792)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10792)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10792)[0m - _max_episode_steps = 150
[2m[36m(pid=10792)[0m - _elapsed_steps = None [0m
[2m[36m(pid=9931)[0m 2019-07-18 01:44:45,134	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m { 'count': 512,
[2m[36m(pid=9931)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.006),
[2m[36m(pid=9931)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=17.0, max=12567.0, mean=6479.945),
[2m[36m(pid=9931)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.021),
[2m[36m(pid=9931)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-4.24, max=4.499, mean=0.011),
[2m[36m(pid=9931)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-3.478, max=3.925, mean=0.006),
[2m[36m(pid=9931)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-12.169, max=13.156, mean=-0.019),
[2m[36m(pid=9931)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.043, max=0.17, mean=0.065)},
[2m[36m(pid=9931)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=9931)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m 2019-07-18 01:44:45,444	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=9931)[0m 
[2m[36m(pid=9931)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.065692596,
[2m[36m(pid=9931)[0m                                          'mean_q': -0.05888705,
[2m[36m(pid=9931)[0m                                          'min_q': -0.21904972},
[2m[36m(pid=9931)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-13.229, max=12.082, mean=0.016)}}
[2m[36m(pid=9931)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-45-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.98252175803608
  episode_reward_mean: -11.344960959658975
  episode_reward_min: -53.08657252795541
  episodes_this_iter: 604
  episodes_total: 604
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 20.378326416015625
        mean_q: 4.252931118011475
        min_q: -9.131189346313477
    learner_queue:
      size_count: 2348
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 254600
    num_steps_trained: 202752
    num_target_updates: 4
    num_weight_syncs: 634
    replay_shard_0:
      add_batch_time_ms: 8.089
      policy_default_policy:
        added_count: 60000
        est_size_bytes: 20460000
        num_entries: 60000
        sampled_count: 51200
      replay_time_ms: 34.156
      update_priorities_time_ms: 97.357
    sample_throughput: 8870.263
    train_throughput: 0.0
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.583587113557856
    mean_inference_ms: 2.1051279649443098
    mean_processing_ms: 1.1859312213800537
  time_since_restore: 30.545530557632446
  time_this_iter_s: 30.545530557632446
  time_total_s: 30.545530557632446
  timestamp: 1563407110
  timesteps_since_restore: 254600
  timesteps_this_iter: 254600
  timesteps_total: 254600
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 30 s, 1 iter, 254600 ts, -11.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-45-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.742656080388485
  episode_reward_mean: -8.757389645107867
  episode_reward_min: -52.45292909527256
  episodes_this_iter: 608
  episodes_total: 1212
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.308069229125977
        mean_q: 1.2522565126419067
        min_q: -16.442508697509766
    learner_queue:
      size_count: 2790
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.49999999999999994
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 508200
    num_steps_trained: 429568
    num_target_updates: 8
    num_weight_syncs: 1268
    replay_shard_0:
      add_batch_time_ms: 7.031
      policy_default_policy:
        added_count: 125400
        est_size_bytes: 42761400
        num_entries: 125400
        sampled_count: 104960
      replay_time_ms: 36.844
      update_priorities_time_ms: 91.677
    sample_throughput: 0.0
    train_throughput: 22610.565
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5950516816250218
    mean_inference_ms: 2.1611408884058214
    mean_processing_ms: 1.191382771095461
  time_since_restore: 61.12142252922058
  time_this_iter_s: 30.575891971588135
  time_total_s: 61.12142252922058
  timestamp: 1563407141
  timesteps_since_restore: 508200
  timesteps_this_iter: 253600
  timesteps_total: 508200
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 61 s, 2 iter, 508200 ts, -8.76 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-46-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 32.66460247673069
  episode_reward_mean: -2.2376389037295343
  episode_reward_min: -56.528703064948395
  episodes_this_iter: 612
  episodes_total: 1824
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 27.83976173400879
        mean_q: 3.5866494178771973
        min_q: -15.029388427734375
    learner_queue:
      size_count: 3234
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 761200
    num_steps_trained: 656896
    num_target_updates: 13
    num_weight_syncs: 1900
    replay_shard_0:
      add_batch_time_ms: 9.377
      policy_default_policy:
        added_count: 188600
        est_size_bytes: 64312600
        num_entries: 188600
        sampled_count: 161280
      replay_time_ms: 41.563
      update_priorities_time_ms: 103.862
    sample_throughput: 7358.751
    train_throughput: 15070.722
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5920419495111777
    mean_inference_ms: 2.183218210139511
    mean_processing_ms: 1.1983256602453218
  time_since_restore: 91.67578387260437
  time_this_iter_s: 30.55436134338379
  time_total_s: 91.67578387260437
  timestamp: 1563407171
  timesteps_since_restore: 761200
  timesteps_this_iter: 253000
  timesteps_total: 761200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 91 s, 3 iter, 761200 ts, -2.24 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-46-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.84565974203542
  episode_reward_mean: 13.635874473865771
  episode_reward_min: -9.430000184942083
  episodes_this_iter: 624
  episodes_total: 2448
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.446155548095703
        mean_q: 5.42909574508667
        min_q: -12.350436210632324
    learner_queue:
      size_count: 3693
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4472135954999579
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 1012400
    num_steps_trained: 891904
    num_target_updates: 17
    num_weight_syncs: 2529
    replay_shard_0:
      add_batch_time_ms: 18.405
      policy_default_policy:
        added_count: 250800
        est_size_bytes: 85522800
        num_entries: 250800
        sampled_count: 216576
      replay_time_ms: 37.33
      update_priorities_time_ms: 104.261
    sample_throughput: 11827.935
    train_throughput: 10093.171
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.581016492606833
    mean_inference_ms: 2.185027519442158
    mean_processing_ms: 1.1907855489855852
  time_since_restore: 122.15199089050293
  time_this_iter_s: 30.47620701789856
  time_total_s: 122.15199089050293
  timestamp: 1563407202
  timesteps_since_restore: 1012400
  timesteps_this_iter: 251200
  timesteps_total: 1012400
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 122 s, 4 iter, 1012400 ts, 13.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

[2m[36m(pid=9931)[0m 2019-07-18 01:47:12,880	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-47-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.69634933359757
  episode_reward_mean: 14.435245074424929
  episode_reward_min: -16.149394556534137
  episodes_this_iter: 620
  episodes_total: 3068
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 27.840766559184303
    episode_reward_mean: 9.640371836379881
    episode_reward_min: -6.559070008945281
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.9333717275031163
      mean_inference_ms: 1.3974243446162036
      mean_processing_ms: 0.594954265109068
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.70978927612305
        mean_q: 6.814935207366943
        min_q: -12.700950622558594
    learner_queue:
      size_count: 4135
      size_mean: 0.34
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.1000000000000014
      - 3.0
      size_std: 0.7102112361825882
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 1265000
    num_steps_trained: 1118208
    num_target_updates: 22
    num_weight_syncs: 3161
    replay_shard_0:
      add_batch_time_ms: 7.263
      policy_default_policy:
        added_count: 315600
        est_size_bytes: 107619600
        num_entries: 315600
        sampled_count: 272384
      replay_time_ms: 37.642
      update_priorities_time_ms: 94.899
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5779536833755832
    mean_inference_ms: 2.189177236973704
    mean_processing_ms: 1.188182365307094
  time_since_restore: 152.73605155944824
  time_this_iter_s: 30.584060668945312
  time_total_s: 152.73605155944824
  timestamp: 1563407232
  timesteps_since_restore: 1265000
  timesteps_this_iter: 252600
  timesteps_total: 1265000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 152 s, 5 iter, 1265000 ts, 14.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-47-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.48087979224912
  episode_reward_mean: 15.484478391303668
  episode_reward_min: -12.688055754749366
  episodes_this_iter: 616
  episodes_total: 3684
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.746660232543945
        mean_q: 6.744966506958008
        min_q: -10.905569076538086
    learner_queue:
      size_count: 4577
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.45999999999999996
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 1519000
    num_steps_trained: 1344000
    num_target_updates: 26
    num_weight_syncs: 3795
    replay_shard_0:
      add_batch_time_ms: 7.467
      policy_default_policy:
        added_count: 381600
        est_size_bytes: 130125600
        num_entries: 381600
        sampled_count: 325120
      replay_time_ms: 39.843
      update_priorities_time_ms: 104.876
    sample_throughput: 13543.119
    train_throughput: 13868.154
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5758691848423754
    mean_inference_ms: 2.1896708567338146
    mean_processing_ms: 1.1866681989222359
  time_since_restore: 183.29776287078857
  time_this_iter_s: 30.561711311340332
  time_total_s: 183.29776287078857
  timestamp: 1563407264
  timesteps_since_restore: 1519000
  timesteps_this_iter: 254000
  timesteps_total: 1519000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 183 s, 6 iter, 1519000 ts, 15.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-48-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.18670786353784
  episode_reward_mean: 16.173750502143992
  episode_reward_min: -16.86143062309315
  episodes_this_iter: 620
  episodes_total: 4304
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.65929412841797
        mean_q: 7.807070732116699
        min_q: -6.982769012451172
    learner_queue:
      size_count: 5005
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4386342439892262
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 1774600
    num_steps_trained: 1563136
    num_target_updates: 31
    num_weight_syncs: 4434
    replay_shard_0:
      add_batch_time_ms: 17.7
      policy_default_policy:
        added_count: 442400
        est_size_bytes: 150858400
        num_entries: 442400
        sampled_count: 378880
      replay_time_ms: 41.835
      update_priorities_time_ms: 119.898
    sample_throughput: 4810.533
    train_throughput: 24629.931
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5766698597418762
    mean_inference_ms: 2.1911451372766
    mean_processing_ms: 1.1885627181762042
  time_since_restore: 213.8527307510376
  time_this_iter_s: 30.554967880249023
  time_total_s: 213.8527307510376
  timestamp: 1563407294
  timesteps_since_restore: 1774600
  timesteps_this_iter: 255600
  timesteps_total: 1774600
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 213 s, 7 iter, 1774600 ts, 16.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-48-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.07408149912919
  episode_reward_mean: 16.79457009706199
  episode_reward_min: -14.633162887693546
  episodes_this_iter: 632
  episodes_total: 4936
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.884090423583984
        mean_q: 7.102789878845215
        min_q: -4.764046669006348
    learner_queue:
      size_count: 5435
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 2029800
    num_steps_trained: 1783808
    num_target_updates: 35
    num_weight_syncs: 5071
    replay_shard_0:
      add_batch_time_ms: 15.776
      policy_default_policy:
        added_count: 507600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 433664
      replay_time_ms: 38.571
      update_priorities_time_ms: 104.692
    sample_throughput: 14684.242
    train_throughput: 7518.332
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5711073929520472
    mean_inference_ms: 2.1848246564456857
    mean_processing_ms: 1.1855342879858972
  time_since_restore: 244.3753936290741
  time_this_iter_s: 30.5226628780365
  time_total_s: 244.3753936290741
  timestamp: 1563407325
  timesteps_since_restore: 2029800
  timesteps_this_iter: 255200
  timesteps_total: 2029800
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 244 s, 8 iter, 2029800 ts, 16.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-49-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.37987158124182
  episode_reward_mean: 15.498892608330033
  episode_reward_min: -10.501928410236623
  episodes_this_iter: 612
  episodes_total: 5548
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.69511032104492
        mean_q: 7.265905857086182
        min_q: -7.090865135192871
    learner_queue:
      size_count: 5886
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.45999999999999996
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 2281600
    num_steps_trained: 2014720
    num_target_updates: 40
    num_weight_syncs: 5702
    replay_shard_0:
      add_batch_time_ms: 10.762
      policy_default_policy:
        added_count: 567800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 491008
      replay_time_ms: 38.053
      update_priorities_time_ms: 112.245
    sample_throughput: 11754.183
    train_throughput: 12036.284
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.574553041571549
    mean_inference_ms: 2.1864090415159727
    mean_processing_ms: 1.185443051290302
  time_since_restore: 274.904212474823
  time_this_iter_s: 30.5288188457489
  time_total_s: 274.904212474823
  timestamp: 1563407356
  timesteps_since_restore: 2281600
  timesteps_this_iter: 251800
  timesteps_total: 2281600
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 274 s, 9 iter, 2281600 ts, 15.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

[2m[36m(pid=9931)[0m 2019-07-18 01:49:46,648	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-49-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.34405160462927
  episode_reward_mean: 16.696445088379168
  episode_reward_min: -11.44582680259631
  episodes_this_iter: 620
  episodes_total: 6168
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 31.082800362663175
    episode_reward_mean: 18.24494146179286
    episode_reward_min: 1.473144642615561
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.7786153423050042
      mean_inference_ms: 1.0898720153645907
      mean_processing_ms: 0.48667131556981624
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.77340316772461
        mean_q: 5.5729265213012695
        min_q: -6.0256171226501465
    learner_queue:
      size_count: 6330
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4000000000000001
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 2534200
    num_steps_trained: 2242048
    num_target_updates: 44
    num_weight_syncs: 6333
    replay_shard_0:
      add_batch_time_ms: 5.997
      policy_default_policy:
        added_count: 630800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 546304
      replay_time_ms: 38.123
      update_priorities_time_ms: 108.25
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5746510005717111
    mean_inference_ms: 2.183318498594833
    mean_processing_ms: 1.187791189952327
  time_since_restore: 305.5090363025665
  time_this_iter_s: 30.60482382774353
  time_total_s: 305.5090363025665
  timestamp: 1563407386
  timesteps_since_restore: 2534200
  timesteps_this_iter: 252600
  timesteps_total: 2534200
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 305 s, 10 iter, 2534200 ts, 16.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-50-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.85586016253203
  episode_reward_mean: 16.93776332492556
  episode_reward_min: -9.786979020743791
  episodes_this_iter: 612
  episodes_total: 6780
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.36491394042969
        mean_q: 5.447107791900635
        min_q: -1.4673025608062744
    learner_queue:
      size_count: 6785
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4472135954999579
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 2785400
    num_steps_trained: 2475008
    num_target_updates: 49
    num_weight_syncs: 6959
    replay_shard_0:
      add_batch_time_ms: 12.208
      policy_default_policy:
        added_count: 694400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 600064
      replay_time_ms: 28.594
      update_priorities_time_ms: 94.284
    sample_throughput: 8415.622
    train_throughput: 10771.996
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5779366671276196
    mean_inference_ms: 2.1870988410291816
    mean_processing_ms: 1.1870900843826633
  time_since_restore: 336.0751874446869
  time_this_iter_s: 30.56615114212036
  time_total_s: 336.0751874446869
  timestamp: 1563407418
  timesteps_since_restore: 2785400
  timesteps_this_iter: 251200
  timesteps_total: 2785400
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 2, 'RUNNING': 1, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	RUNNING, [12 CPUs, 1 GPUs], [pid=9931], 336 s, 11 iter, 2785400 ts, 16.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:
  custom_metrics: {}
  date: 2019-07-18_01-50-48
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.52540636801773
  episode_reward_mean: 18.10038978385547
  episode_reward_min: -13.245783014379182
  episodes_this_iter: 628
  episodes_total: 7408
  experiment_id: 83a1a77368be46c880c82370aa3b4c33
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.178659439086914
        mean_q: 5.376034736633301
        min_q: -1.346014142036438
    learner_queue:
      size_count: 7225
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 10
    num_steps_sampled: 3039000
    num_steps_trained: 2700288
    num_target_updates: 53
    num_weight_syncs: 7594
    replay_shard_0:
      add_batch_time_ms: 6.156
      policy_default_policy:
        added_count: 751400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 659968
      replay_time_ms: 44.491
      update_priorities_time_ms: 96.918
    sample_throughput: 16907.13
    train_throughput: 8656.451
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 9931
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5752828753374248
    mean_inference_ms: 2.1834090676399263
    mean_processing_ms: 1.1874392873563757
  time_since_restore: 366.67223930358887
  time_this_iter_s: 30.597051858901978
  time_total_s: 366.67223930358887
  timestamp: 1563407448
  timesteps_since_restore: 3039000
  timesteps_this_iter: 253600
  timesteps_total: 3039000
  training_iteration: 12
  2019-07-18 01:50:48,763	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 01:50:48,772	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 14.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'PENDING': 3})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

[2m[36m(pid=10782)[0m 2019-07-18 01:50:49.059487: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10782)[0m [32m [     0.07661s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10782)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10782)[0m 2019-07-18 01:50:50,359	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f43d62bca20>}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:50,359	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f43d62e5cf8>}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:50,359	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f43d68d6518>}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:50,369	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=10782)[0m 2019-07-18 01:50:50,375	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=10782)[0m [32m [     1.38245s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38292s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38341s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38388s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38432s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38475s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38517s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38558s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38599s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38640s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38682s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38723s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38769s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38811s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     1.38854s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     0.02431s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m 2019-07-18 01:50:50,461	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10802)[0m 2019-07-18 01:50:50.461485: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10786)[0m [32m [     0.02205s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m 2019-07-18 01:50:50,440	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10786)[0m 2019-07-18 01:50:50.441310: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10789)[0m 2019-07-18 01:50:50,445	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10789)[0m 2019-07-18 01:50:50.445948: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10783)[0m 2019-07-18 01:50:50,481	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10783)[0m 2019-07-18 01:50:50.481592: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10789)[0m [32m [     0.03105s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     0.03699s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     0.03487s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m 2019-07-18 01:50:50,465	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10799)[0m 2019-07-18 01:50:50.465845: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10785)[0m [32m [     0.03710s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m 2019-07-18 01:50:50,487	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10785)[0m 2019-07-18 01:50:50.487898: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10685)[0m [32m [     0.03722s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m 2019-07-18 01:50:50,498	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=10685)[0m 2019-07-18 01:50:50.499334: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=10802)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10802)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10786)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10786)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10789)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10789)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10799)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10799)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10685)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10685)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10785)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10785)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10783)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10783)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=10782)[0m [32m [     1.58621s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.91855s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.91926s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.91990s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92050s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92108s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92169s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92230s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92292s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92347s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92414s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92472s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92535s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92592s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m [32m [     0.94873s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.94974s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95065s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95154s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95243s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95336s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95428s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95521s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95611s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95700s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95788s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95878s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.95978s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92647s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10786)[0m [32m [     0.92706s,  INFO] TimeLimit:
[2m[36m(pid=10786)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10786)[0m - action_space = Box(2,)
[2m[36m(pid=10786)[0m - observation_space = Box(9,)
[2m[36m(pid=10786)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10786)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10786)[0m - _max_episode_steps = 150
[2m[36m(pid=10786)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.96070s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m [32m [     0.96160s,  INFO] TimeLimit:
[2m[36m(pid=10789)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10789)[0m - action_space = Box(2,)
[2m[36m(pid=10789)[0m - observation_space = Box(9,)
[2m[36m(pid=10789)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10789)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10789)[0m - _max_episode_steps = 150
[2m[36m(pid=10789)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,409	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=10802)[0m [32m [     1.10043s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10130s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10217s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10296s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10380s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10466s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10546s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10624s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10704s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10797s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10880s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.10962s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.11050s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.11135s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10802)[0m [32m [     1.11219s,  INFO] TimeLimit:
[2m[36m(pid=10802)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10802)[0m - action_space = Box(2,)
[2m[36m(pid=10802)[0m - observation_space = Box(9,)
[2m[36m(pid=10802)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10802)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10802)[0m - _max_episode_steps = 150
[2m[36m(pid=10802)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,553	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.774, max=0.997, mean=0.01)},
[2m[36m(pid=10789)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.844, max=0.584, mean=0.011)},
[2m[36m(pid=10789)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.26, max=0.955, mean=0.129)},
[2m[36m(pid=10789)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.248, max=0.969, mean=0.12)},
[2m[36m(pid=10789)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.802, max=0.687, mean=-0.084)},
[2m[36m(pid=10789)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.142, max=0.842, mean=0.205)},
[2m[36m(pid=10789)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.341, max=0.942, mean=0.12)},
[2m[36m(pid=10789)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.677, max=0.949, mean=0.013)},
[2m[36m(pid=10789)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.812, max=0.583, mean=-0.05)},
[2m[36m(pid=10789)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.19, max=0.999, mean=0.169)},
[2m[36m(pid=10789)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.98, max=0.55, mean=-0.137)},
[2m[36m(pid=10789)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.253, max=0.931, mean=0.201)},
[2m[36m(pid=10789)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.647, max=0.802, mean=0.114)},
[2m[36m(pid=10789)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.966, max=0.26, mean=-0.151)},
[2m[36m(pid=10789)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.964, max=0.178, mean=-0.207)},
[2m[36m(pid=10789)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.854, max=0.0, mean=-0.217)}}
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,554	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=10789)[0m   1: {'agent0': None},
[2m[36m(pid=10789)[0m   2: {'agent0': None},
[2m[36m(pid=10789)[0m   3: {'agent0': None},
[2m[36m(pid=10789)[0m   4: {'agent0': None},
[2m[36m(pid=10789)[0m   5: {'agent0': None},
[2m[36m(pid=10789)[0m   6: {'agent0': None},
[2m[36m(pid=10789)[0m   7: {'agent0': None},
[2m[36m(pid=10789)[0m   8: {'agent0': None},
[2m[36m(pid=10789)[0m   9: {'agent0': None},
[2m[36m(pid=10789)[0m   10: {'agent0': None},
[2m[36m(pid=10789)[0m   11: {'agent0': None},
[2m[36m(pid=10789)[0m   12: {'agent0': None},
[2m[36m(pid=10789)[0m   13: {'agent0': None},
[2m[36m(pid=10789)[0m   14: {'agent0': None},
[2m[36m(pid=10789)[0m   15: {'agent0': None}}
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,554	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.774, max=0.997, mean=0.01)
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,555	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.774, max=0.997, mean=0.01)
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,565	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10789)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 0,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.774, max=0.997, mean=0.01),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 1,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.844, max=0.584, mean=0.011),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 2,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.26, max=0.955, mean=0.129),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 3,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.248, max=0.969, mean=0.12),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 4,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.802, max=0.687, mean=-0.084),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 5,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.142, max=0.842, mean=0.205),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 6,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.341, max=0.942, mean=0.12),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 7,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.677, max=0.949, mean=0.013),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 8,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.812, max=0.583, mean=-0.05),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 9,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.19, max=0.999, mean=0.169),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 10,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.98, max=0.55, mean=-0.137),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 11,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.253, max=0.931, mean=0.201),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 12,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.647, max=0.802, mean=0.114),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 13,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.966, max=0.26, mean=-0.151),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 14,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.964, max=0.178, mean=-0.207),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10789)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10789)[0m                                   'env_id': 15,
[2m[36m(pid=10789)[0m                                   'info': None,
[2m[36m(pid=10789)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.854, max=0.0, mean=-0.217),
[2m[36m(pid=10789)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10789)[0m                                   'rnn_state': []},
[2m[36m(pid=10789)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,565	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,600	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10789)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=0.008, max=0.662, mean=0.333),
[2m[36m(pid=10789)[0m                       [],
[2m[36m(pid=10789)[0m                       {})}
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10685)[0m [32m [     1.20322s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20392s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20451s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20520s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20584s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20645s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20704s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20764s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20822s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20878s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20937s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.20998s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.21068s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,699	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f43ce8dea90>}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,699	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f43ce8de748>}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,699	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f43ce8ce6a0>}
[2m[36m(pid=10782)[0m [32m [     2.72188s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72276s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72351s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72451s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72542s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72631s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72718s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72806s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72889s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.72967s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.73054s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.73141s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.73227s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.21129s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10685)[0m [32m [     1.21188s,  INFO] TimeLimit:
[2m[36m(pid=10685)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10685)[0m - action_space = Box(2,)
[2m[36m(pid=10685)[0m - observation_space = Box(9,)
[2m[36m(pid=10685)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10685)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10685)[0m - _max_episode_steps = 150
[2m[36m(pid=10685)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.73313s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m [32m [     2.73398s,  INFO] TimeLimit:
[2m[36m(pid=10782)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10782)[0m - action_space = Box(2,)
[2m[36m(pid=10782)[0m - observation_space = Box(9,)
[2m[36m(pid=10782)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10782)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10782)[0m - _max_episode_steps = 150
[2m[36m(pid=10782)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,716	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=10785)[0m [32m [     1.29100s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29197s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29288s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29377s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29463s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29550s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29642s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29737s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29834s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.29926s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.30013s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.30101s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.30190s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m [32m [     1.32173s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32271s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32358s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32426s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32493s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32554s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32612s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32672s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32730s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32786s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32864s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32927s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,747	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.30277s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10785)[0m [32m [     1.30366s,  INFO] TimeLimit:
[2m[36m(pid=10785)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10785)[0m - action_space = Box(2,)
[2m[36m(pid=10785)[0m - observation_space = Box(9,)
[2m[36m(pid=10785)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10785)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10785)[0m - _max_episode_steps = 150
[2m[36m(pid=10785)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.32988s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.33055s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10783)[0m [32m [     1.33130s,  INFO] TimeLimit:
[2m[36m(pid=10783)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10783)[0m - action_space = Box(2,)
[2m[36m(pid=10783)[0m - observation_space = Box(9,)
[2m[36m(pid=10783)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10783)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10783)[0m - _max_episode_steps = 150
[2m[36m(pid=10783)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.34541s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.34640s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.34741s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.34833s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.34924s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35011s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35102s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35195s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35292s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35388s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35481s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35578s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35663s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35746s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10799)[0m [32m [     1.35835s,  INFO] TimeLimit:
[2m[36m(pid=10799)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=10799)[0m - action_space = Box(2,)
[2m[36m(pid=10799)[0m - observation_space = Box(9,)
[2m[36m(pid=10799)[0m - reward_range = (-inf, inf)
[2m[36m(pid=10799)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=10799)[0m - _max_episode_steps = 150
[2m[36m(pid=10799)[0m - _elapsed_steps = None [0m
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,868	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.964, max=0.302, mean=-0.227)},
[2m[36m(pid=10782)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.281, max=0.96, mean=0.098)},
[2m[36m(pid=10782)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.982, max=0.967, mean=0.024)},
[2m[36m(pid=10782)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.016, max=0.97, mean=0.02)},
[2m[36m(pid=10782)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.976, max=0.159, mean=-0.205)},
[2m[36m(pid=10782)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.371, max=0.958, mean=0.127)},
[2m[36m(pid=10782)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.686, max=0.728, mean=-0.045)},
[2m[36m(pid=10782)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.576, max=0.817, mean=0.06)},
[2m[36m(pid=10782)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.849, max=0.796, mean=0.041)},
[2m[36m(pid=10782)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.942, max=0.334, mean=-0.107)},
[2m[36m(pid=10782)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.937, max=0.349, mean=-0.167)},
[2m[36m(pid=10782)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.852, max=0.238, mean=-0.157)},
[2m[36m(pid=10782)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.256, max=0.992, mean=0.201)},
[2m[36m(pid=10782)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.975, max=0.252, mean=-0.118)},
[2m[36m(pid=10782)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.636, max=0.771, mean=0.082)},
[2m[36m(pid=10782)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.799, max=0.601, mean=0.002)}}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,868	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=10782)[0m   1: {'agent0': None},
[2m[36m(pid=10782)[0m   2: {'agent0': None},
[2m[36m(pid=10782)[0m   3: {'agent0': None},
[2m[36m(pid=10782)[0m   4: {'agent0': None},
[2m[36m(pid=10782)[0m   5: {'agent0': None},
[2m[36m(pid=10782)[0m   6: {'agent0': None},
[2m[36m(pid=10782)[0m   7: {'agent0': None},
[2m[36m(pid=10782)[0m   8: {'agent0': None},
[2m[36m(pid=10782)[0m   9: {'agent0': None},
[2m[36m(pid=10782)[0m   10: {'agent0': None},
[2m[36m(pid=10782)[0m   11: {'agent0': None},
[2m[36m(pid=10782)[0m   12: {'agent0': None},
[2m[36m(pid=10782)[0m   13: {'agent0': None},
[2m[36m(pid=10782)[0m   14: {'agent0': None},
[2m[36m(pid=10782)[0m   15: {'agent0': None}}
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,869	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.964, max=0.302, mean=-0.227)
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,869	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.964, max=0.302, mean=-0.227)
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,875	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 0,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.964, max=0.302, mean=-0.227),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 1,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.281, max=0.96, mean=0.098),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 2,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.982, max=0.967, mean=0.024),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 3,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.016, max=0.97, mean=0.02),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 4,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.976, max=0.159, mean=-0.205),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 5,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.371, max=0.958, mean=0.127),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 6,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.686, max=0.728, mean=-0.045),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 7,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.576, max=0.817, mean=0.06),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 8,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.849, max=0.796, mean=0.041),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 9,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.942, max=0.334, mean=-0.107),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 10,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.937, max=0.349, mean=-0.167),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 11,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.852, max=0.238, mean=-0.157),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 12,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.256, max=0.992, mean=0.201),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 13,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.975, max=0.252, mean=-0.118),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 14,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.636, max=0.771, mean=0.082),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=10782)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=10782)[0m                                   'env_id': 15,
[2m[36m(pid=10782)[0m                                   'info': None,
[2m[36m(pid=10782)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.799, max=0.601, mean=0.002),
[2m[36m(pid=10782)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=10782)[0m                                   'rnn_state': []},
[2m[36m(pid=10782)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,875	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,904	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10789)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.003),
[2m[36m(pid=10789)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=1336272324.0, max=1336272324.0, mean=1336272324.0),
[2m[36m(pid=10789)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=10789)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-4.988, max=3.707, mean=-0.066),
[2m[36m(pid=10789)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-4.988, max=3.707, mean=-0.078),
[2m[36m(pid=10789)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.008),
[2m[36m(pid=10789)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-3.015, max=5.432, mean=-0.019),
[2m[36m(pid=10789)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-8.503, max=13.405, mean=-0.087),
[2m[36m(pid=10789)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=10789)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.056, max=13.494, mean=3.229)},
[2m[36m(pid=10789)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10782)[0m 2019-07-18 01:50:51,904	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-0.324, max=0.045, mean=-0.166),
[2m[36m(pid=10782)[0m                       [],
[2m[36m(pid=10782)[0m                       {})}
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10789)[0m 2019-07-18 01:50:51,967	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10789)[0m { 'data': { 'actions': np.ndarray((800, 2), dtype=float32, min=-1.0, max=1.0, mean=0.02),
[2m[36m(pid=10789)[0m             'agent_index': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m             'dones': np.ndarray((800,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m             'eps_id': np.ndarray((800,), dtype=int64, min=52844776.0, max=1884389967.0, mean=771764550.25),
[2m[36m(pid=10789)[0m             'infos': np.ndarray((800,), dtype=object, head={}),
[2m[36m(pid=10789)[0m             'new_obs': np.ndarray((800, 9), dtype=float32, min=-6.039, max=4.218, mean=-0.041),
[2m[36m(pid=10789)[0m             'obs': np.ndarray((800, 9), dtype=float32, min=-6.039, max=4.218, mean=-0.045),
[2m[36m(pid=10789)[0m             'prev_actions': np.ndarray((800, 2), dtype=float32, min=-1.0, max=1.0, mean=0.008),
[2m[36m(pid=10789)[0m             'prev_rewards': np.ndarray((800,), dtype=float32, min=-7.894, max=7.973, mean=-0.082),
[2m[36m(pid=10789)[0m             'rewards': np.ndarray((800,), dtype=float32, min=-14.598, max=19.334, mean=-0.27),
[2m[36m(pid=10789)[0m             't': np.ndarray((800,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=10789)[0m             'unroll_id': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10789)[0m             'weights': np.ndarray((800,), dtype=float32, min=0.007, max=19.528, mean=3.433)},
[2m[36m(pid=10789)[0m   'type': 'SampleBatch'}
[2m[36m(pid=10789)[0m 
[2m[36m(pid=10782)[0m 2019-07-18 01:50:52,622	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.046),
[2m[36m(pid=10782)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=10782)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1445000664.0, max=1445000664.0, mean=1445000664.0),
[2m[36m(pid=10782)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=10782)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-3.717, max=4.962, mean=0.099),
[2m[36m(pid=10782)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-3.717, max=4.669, mean=0.081),
[2m[36m(pid=10782)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.047),
[2m[36m(pid=10782)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-6.497, max=3.464, mean=-0.077),
[2m[36m(pid=10782)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-15.808, max=9.525, mean=-0.377),
[2m[36m(pid=10782)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=10782)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.033, max=15.63, mean=2.987)},
[2m[36m(pid=10782)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m 2019-07-18 01:50:52,710	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m { 'data': { 'actions': np.ndarray((900, 2), dtype=float32, min=-1.0, max=1.0, mean=0.056),
[2m[36m(pid=10782)[0m             'agent_index': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m             'dones': np.ndarray((900,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=10782)[0m             'eps_id': np.ndarray((900,), dtype=int64, min=144488613.0, max=1445000664.0, mean=686304893.167),
[2m[36m(pid=10782)[0m             'infos': np.ndarray((900,), dtype=object, head={}),
[2m[36m(pid=10782)[0m             'new_obs': np.ndarray((900, 9), dtype=float32, min=-4.324, max=4.962, mean=0.072),
[2m[36m(pid=10782)[0m             'obs': np.ndarray((900, 9), dtype=float32, min=-4.324, max=4.669, mean=0.063),
[2m[36m(pid=10782)[0m             'prev_actions': np.ndarray((900, 2), dtype=float32, min=-1.0, max=1.0, mean=0.056),
[2m[36m(pid=10782)[0m             'prev_rewards': np.ndarray((900,), dtype=float32, min=-6.497, max=4.383, mean=-0.038),
[2m[36m(pid=10782)[0m             'rewards': np.ndarray((900,), dtype=float32, min=-15.808, max=12.077, mean=-0.157),
[2m[36m(pid=10782)[0m             't': np.ndarray((900,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=10782)[0m             'unroll_id': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=10782)[0m             'weights': np.ndarray((900,), dtype=float32, min=0.0, max=15.63, mean=2.684)},
[2m[36m(pid=10782)[0m   'type': 'SampleBatch'}
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m 2019-07-18 01:50:56,865	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m { 'count': 512,
[2m[36m(pid=10782)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.053),
[2m[36m(pid=10782)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=37.0, max=12761.0, mean=6112.584),
[2m[36m(pid=10782)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.016),
[2m[36m(pid=10782)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-8.181, max=7.297, mean=0.02),
[2m[36m(pid=10782)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-8.043, max=6.064, mean=0.014),
[2m[36m(pid=10782)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-33.966, max=25.412, mean=-1.005),
[2m[36m(pid=10782)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.032, max=0.168, mean=0.062)},
[2m[36m(pid=10782)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=10782)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m 2019-07-18 01:50:57,108	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=10782)[0m 
[2m[36m(pid=10782)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.049035136,
[2m[36m(pid=10782)[0m                                          'mean_q': -0.12568828,
[2m[36m(pid=10782)[0m                                          'min_q': -0.59204626},
[2m[36m(pid=10782)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-25.39, max=33.773, mean=0.995)}}
[2m[36m(pid=10782)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-51-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.477887034429052
  episode_reward_mean: -11.30403800741106
  episode_reward_min: -89.65337660446582
  episodes_this_iter: 928
  episodes_total: 928
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 19.378440856933594
        mean_q: 3.418339729309082
        min_q: -13.09343433380127
    learner_queue:
      size_count: 3881
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 258
    num_steps_sampled: 319200
    num_steps_trained: 328704
    num_target_updates: 6
    num_weight_syncs: 399
    replay_shard_0:
      add_batch_time_ms: 79.281
      policy_default_policy:
        added_count: 88000
        est_size_bytes: 30008000
        num_entries: 88000
        sampled_count: 90624
      replay_time_ms: 30.434
      update_priorities_time_ms: 78.731
    sample_throughput: 36847.492
    train_throughput: 0.0
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.105451118776281
    mean_inference_ms: 1.6854347110867738
    mean_processing_ms: 3.572904934430451
  time_since_restore: 30.672906160354614
  time_this_iter_s: 30.672906160354614
  time_total_s: 30.672906160354614
  timestamp: 1563407484
  timesteps_since_restore: 319200
  timesteps_this_iter: 319200
  timesteps_total: 319200
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=10782], 30 s, 1 iter, 319200 ts, -11.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-51-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.91776877172199
  episode_reward_mean: 4.772267524560968
  episode_reward_min: -58.85824431563788
  episodes_this_iter: 880
  episodes_total: 1808
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 28.259641647338867
        mean_q: 5.16040563583374
        min_q: -13.55829906463623
    learner_queue:
      size_count: 4593
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 3.0
      size_std: 0.565685424949238
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 258
    num_steps_sampled: 628000
    num_steps_trained: 693248
    num_target_updates: 13
    num_weight_syncs: 785
    replay_shard_0:
      add_batch_time_ms: 75.016
      policy_default_policy:
        added_count: 157600
        est_size_bytes: 53741600
        num_entries: 157600
        sampled_count: 185344
      replay_time_ms: 29.696
      update_priorities_time_ms: 75.86
    sample_throughput: 20221.431
    train_throughput: 12941.716
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.194893875999311
    mean_inference_ms: 1.7419318832645785
    mean_processing_ms: 3.6523295407317655
  time_since_restore: 61.36098265647888
  time_this_iter_s: 30.688076496124268
  time_total_s: 61.36098265647888
  timestamp: 1563407515
  timesteps_since_restore: 628000
  timesteps_this_iter: 308800
  timesteps_total: 628000
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=10782], 61 s, 2 iter, 628000 ts, 4.77 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-52-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.511104327277714
  episode_reward_mean: 15.131377280085173
  episode_reward_min: -15.584964079134942
  episodes_this_iter: 896
  episodes_total: 2704
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.821285247802734
        mean_q: 9.046603202819824
        min_q: -14.031439781188965
    learner_queue:
      size_count: 5296
      size_mean: 0.28
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.567097875150313
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 258
    num_steps_sampled: 939200
    num_steps_trained: 1053184
    num_target_updates: 20
    num_weight_syncs: 1174
    replay_shard_0:
      add_batch_time_ms: 65.526
      policy_default_policy:
        added_count: 247200
        est_size_bytes: 84295200
        num_entries: 247200
        sampled_count: 275456
      replay_time_ms: 26.457
      update_priorities_time_ms: 84.982
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.207643363475691
    mean_inference_ms: 1.755501298047401
    mean_processing_ms: 3.668843377659562
  time_since_restore: 92.07049179077148
  time_this_iter_s: 30.709509134292603
  time_total_s: 92.07049179077148
  timestamp: 1563407546
  timesteps_since_restore: 939200
  timesteps_this_iter: 311200
  timesteps_total: 939200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=10782], 92 s, 3 iter, 939200 ts, 15.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-52-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.466523858680894
  episode_reward_mean: 15.11121900185016
  episode_reward_min: -16.36538935150221
  episodes_this_iter: 896
  episodes_total: 3600
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.2400016784668
        mean_q: 10.371391296386719
        min_q: -3.1080477237701416
    learner_queue:
      size_count: 6006
      size_mean: 0.38
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5617828762075255
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 258
    num_steps_sampled: 1247200
    num_steps_trained: 1416704
    num_target_updates: 28
    num_weight_syncs: 1559
    replay_shard_0:
      add_batch_time_ms: 77.82
      policy_default_policy:
        added_count: 316000
        est_size_bytes: 107756000
        num_entries: 316000
        sampled_count: 371712
      replay_time_ms: 29.026
      update_priorities_time_ms: 97.985
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.205427883087815
    mean_inference_ms: 1.7649229596039464
    mean_processing_ms: 3.6747396897871645
  time_since_restore: 122.6927375793457
  time_this_iter_s: 30.62224578857422
  time_total_s: 122.6927375793457
  timestamp: 1563407576
  timesteps_since_restore: 1247200
  timesteps_this_iter: 308000
  timesteps_total: 1247200
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=10782], 122 s, 4 iter, 1247200 ts, 15.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

[2m[36m(pid=10782)[0m 2019-07-18 01:53:27,648	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-53-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.662837488935125
  episode_reward_mean: 16.370140320907208
  episode_reward_min: -14.35849139283288
  episodes_this_iter: 896
  episodes_total: 4496
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 36.240981515847544
    episode_reward_mean: 13.274672114372342
    episode_reward_min: -27.41222857808276
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.9600766584537708
      mean_inference_ms: 0.6532883465438957
      mean_processing_ms: 0.9989003952496457
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.631473541259766
        mean_q: 9.863333702087402
        min_q: -0.15049009025096893
    learner_queue:
      size_count: 6702
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4472135954999579
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 258
    num_steps_sampled: 1559200
    num_steps_trained: 1773056
    num_target_updates: 35
    num_weight_syncs: 1949
    replay_shard_0:
      add_batch_time_ms: 70.004
      policy_default_policy:
        added_count: 402400
        est_size_bytes: 137218400
        num_entries: 402400
        sampled_count: 459776
      replay_time_ms: 26.907
      update_priorities_time_ms: 72.468
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.190868237774782
    mean_inference_ms: 1.7545335169598693
    mean_processing_ms: 3.687632642636952
  time_since_restore: 153.44258165359497
  time_this_iter_s: 30.749844074249268
  time_total_s: 153.44258165359497
  timestamp: 1563407607
  timesteps_since_restore: 1559200
  timesteps_this_iter: 312000
  timesteps_total: 1559200
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=10782], 153 s, 5 iter, 1559200 ts, 16.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-54-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.06075672894702
  episode_reward_mean: 17.93910513510743
  episode_reward_min: -6.836373344276618
  episodes_this_iter: 880
  episodes_total: 5376
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.87957763671875
        mean_q: 8.897796630859375
        min_q: -5.1985554695129395
    learner_queue:
      size_count: 7405
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 265
    num_steps_sampled: 1869600
    num_steps_trained: 2131968
    num_target_updates: 42
    num_weight_syncs: 2337
    replay_shard_0:
      add_batch_time_ms: 69.117
      policy_default_policy:
        added_count: 477600
        est_size_bytes: 162861600
        num_entries: 477600
        sampled_count: 551424
      replay_time_ms: 38.233
      update_priorities_time_ms: 84.61
    sample_throughput: 0.0
    train_throughput: 11450.925
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.18877902559204
    mean_inference_ms: 1.7585938527787468
    mean_processing_ms: 3.689308549044894
  time_since_restore: 184.1420168876648
  time_this_iter_s: 30.699435234069824
  time_total_s: 184.1420168876648
  timestamp: 1563407640
  timesteps_since_restore: 1869600
  timesteps_this_iter: 310400
  timesteps_total: 1869600
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 3, 'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=10782], 184 s, 6 iter, 1869600 ts, 17.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-54-30
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.352935883261296
  episode_reward_mean: 18.076601393994995
  episode_reward_min: -10.031119449704349
  episodes_this_iter: 896
  episodes_total: 6272
  experiment_id: 5205befd606e4703898bc270fd0acd34
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.679019927978516
        mean_q: 8.262700080871582
        min_q: -8.347349166870117
    learner_queue:
      size_count: 8123
      size_mean: 0.34
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4737087712930804
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 265
    num_steps_sampled: 2176000
    num_steps_trained: 2499584
    num_target_updates: 49
    num_weight_syncs: 2720
    replay_shard_0:
      add_batch_time_ms: 66.626
      policy_default_policy:
        added_count: 562400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 642560
      replay_time_ms: 25.361
      update_priorities_time_ms: 92.801
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 10782
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.196733091946874
    mean_inference_ms: 1.7614542517782301
    mean_processing_ms: 3.7032904649757796
  time_since_restore: 214.81573176383972
  time_this_iter_s: 30.673714876174927
  time_total_s: 214.81573176383972
  timestamp: 1563407670
  timesteps_since_restore: 2176000
  timesteps_this_iter: 306400
  timesteps_total: 2176000
  training_iteration: 7
  2019-07-18 01:54:30,992	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 01:54:31,004	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 4, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew

[2m[36m(pid=11103)[0m [32m [     0.01784s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m 2019-07-18 01:54:34.408677: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11103)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11103)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11103)[0m [32m [     0.59583s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     0.59628s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     0.59667s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     0.59705s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     0.59744s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     0.59793s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     0.59833s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m 2019-07-18 01:54:34,984	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f0909e81978>}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:34,984	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0909e53748>}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:34,985	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f090a4964a8>}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:34,990	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=11103)[0m 2019-07-18 01:54:34,997	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=11101)[0m 2019-07-18 01:54:35,068	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11101)[0m 2019-07-18 01:54:35.069284: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11101)[0m [32m [     0.02761s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     0.03537s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m 2019-07-18 01:54:35,076	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11115)[0m 2019-07-18 01:54:35.077522: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11102)[0m 2019-07-18 01:54:35,155	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11102)[0m 2019-07-18 01:54:35.156351: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11102)[0m [32m [     0.09591s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11102)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11101)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11101)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11115)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11115)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11103)[0m [32m [     0.92279s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30301s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30389s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30477s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30562s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30650s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30737s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11101)[0m [32m [     2.30827s,  INFO] TimeLimit:
[2m[36m(pid=11101)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11101)[0m - action_space = Box(2,)
[2m[36m(pid=11101)[0m - observation_space = Box(9,)
[2m[36m(pid=11101)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11101)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11101)[0m - _max_episode_steps = 150
[2m[36m(pid=11101)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.38753s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.38847s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.38937s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.39025s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.39118s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.39225s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11102)[0m [32m [     2.39331s,  INFO] TimeLimit:
[2m[36m(pid=11102)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11102)[0m - action_space = Box(2,)
[2m[36m(pid=11102)[0m - observation_space = Box(9,)
[2m[36m(pid=11102)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11102)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11102)[0m - _max_episode_steps = 150
[2m[36m(pid=11102)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57432s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57520s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57605s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57690s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57779s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57869s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m [32m [     2.57960s,  INFO] TimeLimit:
[2m[36m(pid=11115)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11115)[0m - action_space = Box(2,)
[2m[36m(pid=11115)[0m - observation_space = Box(9,)
[2m[36m(pid=11115)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11115)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11115)[0m - _max_episode_steps = 150
[2m[36m(pid=11115)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,658	INFO rollout_worker.py:428 -- Generating sample batch of size 400
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,750	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.995, max=0.866, mean=-0.02)},
[2m[36m(pid=11115)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.991, max=0.175, mean=-0.19)},
[2m[36m(pid=11115)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.722, max=0.84, mean=0.082)},
[2m[36m(pid=11115)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.966, max=0.463, mean=-0.093)},
[2m[36m(pid=11115)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.832, max=0.295, mean=-0.15)},
[2m[36m(pid=11115)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.0, max=0.392, mean=-0.098)},
[2m[36m(pid=11115)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.891, max=0.455, mean=-0.153)},
[2m[36m(pid=11115)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.906, max=0.463, mean=-0.108)}}
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,751	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=11115)[0m   1: {'agent0': None},
[2m[36m(pid=11115)[0m   2: {'agent0': None},
[2m[36m(pid=11115)[0m   3: {'agent0': None},
[2m[36m(pid=11115)[0m   4: {'agent0': None},
[2m[36m(pid=11115)[0m   5: {'agent0': None},
[2m[36m(pid=11115)[0m   6: {'agent0': None},
[2m[36m(pid=11115)[0m   7: {'agent0': None}}
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,751	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.995, max=0.866, mean=-0.02)
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,752	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.995, max=0.866, mean=-0.02)
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,768	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11115)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 0,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.995, max=0.866, mean=-0.02),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 1,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.991, max=0.175, mean=-0.19),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 2,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.722, max=0.84, mean=0.082),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 3,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.966, max=0.463, mean=-0.093),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 4,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.832, max=0.295, mean=-0.15),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 5,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.0, max=0.392, mean=-0.098),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 6,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.891, max=0.455, mean=-0.153),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11115)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11115)[0m                                   'env_id': 7,
[2m[36m(pid=11115)[0m                                   'info': None,
[2m[36m(pid=11115)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.906, max=0.463, mean=-0.108),
[2m[36m(pid=11115)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11115)[0m                                   'rnn_state': []},
[2m[36m(pid=11115)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,768	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=11115)[0m 2019-07-18 01:54:37,856	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11115)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-0.216, max=0.246, mean=0.012),
[2m[36m(pid=11115)[0m                       [],
[2m[36m(pid=11115)[0m                       {})}
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11115)[0m 2019-07-18 01:54:38,145	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11115)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.121),
[2m[36m(pid=11115)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=1290609034.0, max=1290609034.0, mean=1290609034.0),
[2m[36m(pid=11115)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=11115)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-5.146, max=1.939, mean=-0.125),
[2m[36m(pid=11115)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-5.146, max=1.939, mean=-0.132),
[2m[36m(pid=11115)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.127),
[2m[36m(pid=11115)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-3.426, max=2.397, mean=-0.002),
[2m[36m(pid=11115)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-8.84, max=6.678, mean=-0.01),
[2m[36m(pid=11115)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=11115)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.035, max=8.857, mean=1.863)},
[2m[36m(pid=11115)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11103)[0m [32m [     3.80524s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     3.80628s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     3.80744s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     3.80834s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     3.80921s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     3.81008s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m [32m [     3.81093s,  INFO] TimeLimit:
[2m[36m(pid=11103)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11103)[0m - action_space = Box(2,)
[2m[36m(pid=11103)[0m - observation_space = Box(9,)
[2m[36m(pid=11103)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11103)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11103)[0m - _max_episode_steps = 150
[2m[36m(pid=11103)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,194	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f09004a6160>}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,194	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0900498dd8>}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,194	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0900498ac8>}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,205	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=11115)[0m 2019-07-18 01:54:38,195	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11115)[0m { 'data': { 'actions': np.ndarray((400, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.122),
[2m[36m(pid=11115)[0m             'agent_index': np.ndarray((400,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m             'dones': np.ndarray((400,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m             'eps_id': np.ndarray((400,), dtype=int64, min=102068054.0, max=1896845676.0, mean=882967681.0),
[2m[36m(pid=11115)[0m             'infos': np.ndarray((400,), dtype=object, head={}),
[2m[36m(pid=11115)[0m             'new_obs': np.ndarray((400, 9), dtype=float32, min=-5.491, max=3.987, mean=-0.092),
[2m[36m(pid=11115)[0m             'obs': np.ndarray((400, 9), dtype=float32, min=-5.491, max=3.987, mean=-0.098),
[2m[36m(pid=11115)[0m             'prev_actions': np.ndarray((400, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.128),
[2m[36m(pid=11115)[0m             'prev_rewards': np.ndarray((400,), dtype=float32, min=-7.552, max=7.582, mean=-0.07),
[2m[36m(pid=11115)[0m             'rewards': np.ndarray((400,), dtype=float32, min=-18.765, max=18.671, mean=-0.212),
[2m[36m(pid=11115)[0m             't': np.ndarray((400,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=11115)[0m             'unroll_id': np.ndarray((400,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11115)[0m             'weights': np.ndarray((400,), dtype=float32, min=0.007, max=19.041, mean=2.442)},
[2m[36m(pid=11115)[0m   'type': 'SampleBatch'}
[2m[36m(pid=11115)[0m 
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,249	INFO rollout_worker.py:428 -- Generating sample batch of size 400
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,358	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.215, max=1.015, mean=0.202)},
[2m[36m(pid=11103)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.741, max=0.061, mean=-0.221)},
[2m[36m(pid=11103)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.985, max=0.173, mean=-0.128)},
[2m[36m(pid=11103)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.736, max=0.15, mean=-0.236)},
[2m[36m(pid=11103)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.877, max=0.955, mean=-0.016)},
[2m[36m(pid=11103)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.239, max=0.903, mean=0.235)},
[2m[36m(pid=11103)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.31, max=0.986, mean=0.141)},
[2m[36m(pid=11103)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-1.011, max=0.124, mean=-0.219)}}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,358	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=11103)[0m   1: {'agent0': None},
[2m[36m(pid=11103)[0m   2: {'agent0': None},
[2m[36m(pid=11103)[0m   3: {'agent0': None},
[2m[36m(pid=11103)[0m   4: {'agent0': None},
[2m[36m(pid=11103)[0m   5: {'agent0': None},
[2m[36m(pid=11103)[0m   6: {'agent0': None},
[2m[36m(pid=11103)[0m   7: {'agent0': None}}
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,359	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.215, max=1.015, mean=0.202)
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,359	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.215, max=1.015, mean=0.202)
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,364	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 0,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.215, max=1.015, mean=0.202),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 1,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.741, max=0.061, mean=-0.221),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 2,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.985, max=0.173, mean=-0.128),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 3,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.736, max=0.15, mean=-0.236),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 4,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.877, max=0.955, mean=-0.016),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 5,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.239, max=0.903, mean=0.235),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 6,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.31, max=0.986, mean=0.141),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11103)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11103)[0m                                   'env_id': 7,
[2m[36m(pid=11103)[0m                                   'info': None,
[2m[36m(pid=11103)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-1.011, max=0.124, mean=-0.219),
[2m[36m(pid=11103)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11103)[0m                                   'rnn_state': []},
[2m[36m(pid=11103)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,364	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=11103)[0m 2019-07-18 01:54:38,413	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-0.115, max=0.463, mean=0.176),
[2m[36m(pid=11103)[0m                       [],
[2m[36m(pid=11103)[0m                       {})}
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m 2019-07-18 01:54:39,449	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.129),
[2m[36m(pid=11103)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=11103)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=680863557.0, max=680863557.0, mean=680863557.0),
[2m[36m(pid=11103)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=11103)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-7.072, max=9.079, mean=0.257),
[2m[36m(pid=11103)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-7.072, max=9.079, mean=0.259),
[2m[36m(pid=11103)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.131),
[2m[36m(pid=11103)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-6.235, max=9.894, mean=-0.281),
[2m[36m(pid=11103)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-16.191, max=18.512, mean=-0.814),
[2m[36m(pid=11103)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=11103)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.015, max=18.423, mean=3.521)},
[2m[36m(pid=11103)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m 2019-07-18 01:54:39,605	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m { 'data': { 'actions': np.ndarray((450, 2), dtype=float32, min=-1.0, max=1.0, mean=0.131),
[2m[36m(pid=11103)[0m             'agent_index': np.ndarray((450,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m             'dones': np.ndarray((450,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=11103)[0m             'eps_id': np.ndarray((450,), dtype=int64, min=680863557.0, max=1989929230.0, mean=1466424097.333),
[2m[36m(pid=11103)[0m             'infos': np.ndarray((450,), dtype=object, head={}),
[2m[36m(pid=11103)[0m             'new_obs': np.ndarray((450, 9), dtype=float32, min=-7.235, max=9.079, mean=0.224),
[2m[36m(pid=11103)[0m             'obs': np.ndarray((450, 9), dtype=float32, min=-7.235, max=9.079, mean=0.221),
[2m[36m(pid=11103)[0m             'prev_actions': np.ndarray((450, 2), dtype=float32, min=-1.0, max=1.0, mean=0.134),
[2m[36m(pid=11103)[0m             'prev_rewards': np.ndarray((450,), dtype=float32, min=-8.455, max=9.894, mean=-0.249),
[2m[36m(pid=11103)[0m             'rewards': np.ndarray((450,), dtype=float32, min=-21.616, max=18.512, mean=-0.74),
[2m[36m(pid=11103)[0m             't': np.ndarray((450,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=11103)[0m             'unroll_id': np.ndarray((450,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11103)[0m             'weights': np.ndarray((450,), dtype=float32, min=0.0, max=21.324, mean=3.697)},
[2m[36m(pid=11103)[0m   'type': 'SampleBatch'}
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11253)[0m 2019-07-18 01:54:39,674	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11253)[0m 2019-07-18 01:54:39.674825: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11253)[0m [32m [     0.03877s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11253)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11256)[0m [32m [     0.06162s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m 2019-07-18 01:54:39,945	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11256)[0m 2019-07-18 01:54:39.945793: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11255)[0m 2019-07-18 01:54:40,027	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11255)[0m 2019-07-18 01:54:40.027799: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11255)[0m [32m [     0.04124s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11256)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11255)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11255)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11254)[0m [32m [     0.07990s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m 2019-07-18 01:54:40,735	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11254)[0m 2019-07-18 01:54:40.736068: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11254)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11254)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11253)[0m [32m [     1.71565s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m [32m [     1.71620s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m [32m [     1.71679s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m [32m [     1.71732s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m [32m [     1.71784s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m [32m [     1.71838s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11253)[0m [32m [     1.71900s,  INFO] TimeLimit:
[2m[36m(pid=11253)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11253)[0m - action_space = Box(2,)
[2m[36m(pid=11253)[0m - observation_space = Box(9,)
[2m[36m(pid=11253)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11253)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11253)[0m - _max_episode_steps = 150
[2m[36m(pid=11253)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55173s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55233s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55288s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55341s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55397s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55450s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11256)[0m [32m [     1.55513s,  INFO] TimeLimit:
[2m[36m(pid=11256)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11256)[0m - action_space = Box(2,)
[2m[36m(pid=11256)[0m - observation_space = Box(9,)
[2m[36m(pid=11256)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11256)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11256)[0m - _max_episode_steps = 150
[2m[36m(pid=11256)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.51825s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.51878s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.51935s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.51989s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.52037s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.52088s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11255)[0m [32m [     1.52145s,  INFO] TimeLimit:
[2m[36m(pid=11255)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11255)[0m - action_space = Box(2,)
[2m[36m(pid=11255)[0m - observation_space = Box(9,)
[2m[36m(pid=11255)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11255)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11255)[0m - _max_episode_steps = 150
[2m[36m(pid=11255)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.16979s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.17042s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.17092s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.17146s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.17196s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.17253s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11254)[0m [32m [     1.17313s,  INFO] TimeLimit:
[2m[36m(pid=11254)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11254)[0m - action_space = Box(2,)
[2m[36m(pid=11254)[0m - observation_space = Box(9,)
[2m[36m(pid=11254)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11254)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11254)[0m - _max_episode_steps = 150
[2m[36m(pid=11254)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11103)[0m 2019-07-18 01:54:45,104	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m { 'count': 512,
[2m[36m(pid=11103)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=0.053),
[2m[36m(pid=11103)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=24.0, max=13163.0, mean=5818.561),
[2m[36m(pid=11103)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.008),
[2m[36m(pid=11103)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.077),
[2m[36m(pid=11103)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.078),
[2m[36m(pid=11103)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-37.338, max=32.945, mean=0.185),
[2m[36m(pid=11103)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.04, max=0.267, mean=0.072)},
[2m[36m(pid=11103)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=11103)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m 2019-07-18 01:54:45,339	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=11103)[0m 
[2m[36m(pid=11103)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.8318051,
[2m[36m(pid=11103)[0m                                          'mean_q': 0.08354458,
[2m[36m(pid=11103)[0m                                          'min_q': -0.3268159},
[2m[36m(pid=11103)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-32.451, max=36.725, mean=-0.161)}}
[2m[36m(pid=11103)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-55-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 25.153965152441288
  episode_reward_mean: -10.79278218685187
  episode_reward_min: -51.315860374112184
  episodes_this_iter: 728
  episodes_total: 728
  experiment_id: 3da60c0f8b4843758fcac0831f35ee84
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 24.047117233276367
        mean_q: 3.2326512336730957
        min_q: -13.310821533203125
    learner_queue:
      size_count: 4449
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5385164807134504
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 38
    num_steps_sampled: 256000
    num_steps_trained: 322560
    num_target_updates: 6
    num_weight_syncs: 640
    replay_shard_0:
      add_batch_time_ms: 33.354
      policy_default_policy:
        added_count: 62800
        est_size_bytes: 21414800
        num_entries: 62800
        sampled_count: 83456
      replay_time_ms: 26.373
      update_priorities_time_ms: 79.757
    sample_throughput: 14181.085
    train_throughput: 9075.894
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11103
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.6675871959603694
    mean_inference_ms: 1.7692910286670047
    mean_processing_ms: 1.8964619254307338
  time_since_restore: 30.522696018218994
  time_this_iter_s: 30.522696018218994
  time_total_s: 30.522696018218994
  timestamp: 1563407712
  timesteps_since_restore: 256000
  timesteps_this_iter: 256000
  timesteps_total: 256000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11103], 30 s, 1 iter, 256000 ts, -10.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-55-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 35.147271821599254
  episode_reward_mean: 4.176342468371606
  episode_reward_min: -43.45320314905257
  episodes_this_iter: 704
  episodes_total: 1432
  experiment_id: 3da60c0f8b4843758fcac0831f35ee84
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.76943588256836
        mean_q: 6.179351806640625
        min_q: -19.071712493896484
    learner_queue:
      size_count: 5175
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4004996878900157
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 38
    num_steps_sampled: 499200
    num_steps_trained: 694784
    num_target_updates: 13
    num_weight_syncs: 1248
    replay_shard_0:
      add_batch_time_ms: 27.891
      policy_default_policy:
        added_count: 122000
        est_size_bytes: 41602000
        num_entries: 122000
        sampled_count: 175104
      replay_time_ms: 27.712
      update_priorities_time_ms: 90.881
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11103
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.708647455308942
    mean_inference_ms: 1.84715954088347
    mean_processing_ms: 1.9239244812160918
  time_since_restore: 60.977256536483765
  time_this_iter_s: 30.45456051826477
  time_total_s: 60.977256536483765
  timestamp: 1563407742
  timesteps_since_restore: 499200
  timesteps_this_iter: 243200
  timesteps_total: 499200
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11103], 60 s, 2 iter, 499200 ts, 4.18 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-56-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.81775503598655
  episode_reward_mean: 15.69433883395947
  episode_reward_min: -11.380855959476628
  episodes_this_iter: 704
  episodes_total: 2136
  experiment_id: 3da60c0f8b4843758fcac0831f35ee84
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.502927780151367
        mean_q: 7.873299598693848
        min_q: -8.503129005432129
    learner_queue:
      size_count: 5904
      size_mean: 0.34
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5517245689653488
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 41
    num_steps_sampled: 742000
    num_steps_trained: 1067520
    num_target_updates: 21
    num_weight_syncs: 1855
    replay_shard_0:
      add_batch_time_ms: 30.166
      policy_default_policy:
        added_count: 180000
        est_size_bytes: 61380000
        num_entries: 180000
        sampled_count: 266240
      replay_time_ms: 28.748
      update_priorities_time_ms: 100.169
    sample_throughput: 10382.455
    train_throughput: 13289.542
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11103
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.727306313055323
    mean_inference_ms: 1.8758052736199318
    mean_processing_ms: 1.94387115543518
  time_since_restore: 91.52513813972473
  time_this_iter_s: 30.547881603240967
  time_total_s: 91.52513813972473
  timestamp: 1563407773
  timesteps_since_restore: 742000
  timesteps_this_iter: 242800
  timesteps_total: 742000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11103], 91 s, 3 iter, 742000 ts, 15.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-56-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.55888411219254
  episode_reward_mean: 16.002843093762348
  episode_reward_min: -9.050504279014953
  episodes_this_iter: 696
  episodes_total: 2832
  experiment_id: 3da60c0f8b4843758fcac0831f35ee84
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.492395401000977
        mean_q: 8.01785945892334
        min_q: -8.604026794433594
    learner_queue:
      size_count: 6613
      size_mean: 0.28
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4915282290977803
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 41
    num_steps_sampled: 988000
    num_steps_trained: 1430528
    num_target_updates: 28
    num_weight_syncs: 2470
    replay_shard_0:
      add_batch_time_ms: 34.436
      policy_default_policy:
        added_count: 234800
        est_size_bytes: 80066800
        num_entries: 234800
        sampled_count: 358400
      replay_time_ms: 32.717
      update_priorities_time_ms: 86.938
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11103
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.736629721148323
    mean_inference_ms: 1.894601292550904
    mean_processing_ms: 1.9498665469169731
  time_since_restore: 122.01818013191223
  time_this_iter_s: 30.4930419921875
  time_total_s: 122.01818013191223
  timestamp: 1563407803
  timesteps_since_restore: 988000
  timesteps_this_iter: 246000
  timesteps_total: 988000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11103], 122 s, 4 iter, 988000 ts, 16 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew

[2m[36m(pid=11103)[0m 2019-07-18 01:57:14,432	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-57-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.679932125216226
  episode_reward_mean: 17.05804246944417
  episode_reward_min: -17.47880959815251
  episodes_this_iter: 712
  episodes_total: 3544
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 37.63292757360631
    episode_reward_mean: 12.781532544578608
    episode_reward_min: -27.255308157535342
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.7111484057500301
      mean_inference_ms: 1.400015733323427
      mean_processing_ms: 0.9391899410632129
  experiment_id: 3da60c0f8b4843758fcac0831f35ee84
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 38.109031677246094
        mean_q: 7.7199273109436035
        min_q: -37.374847412109375
    learner_queue:
      size_count: 7322
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 41
    num_steps_sampled: 1233200
    num_steps_trained: 1793536
    num_target_updates: 35
    num_weight_syncs: 3083
    replay_shard_0:
      add_batch_time_ms: 35.387
      policy_default_policy:
        added_count: 292800
        est_size_bytes: 99844800
        num_entries: 292800
        sampled_count: 447488
      replay_time_ms: 31.434
      update_priorities_time_ms: 83.384
    sample_throughput: 9319.381
    train_throughput: 11928.808
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11103
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.734649268640862
    mean_inference_ms: 1.89998951477438
    mean_processing_ms: 1.9524999420394877
  time_since_restore: 152.51810121536255
  time_this_iter_s: 30.499921083450317
  time_total_s: 152.51810121536255
  timestamp: 1563407834
  timesteps_since_restore: 1233200
  timesteps_this_iter: 245200
  timesteps_total: 1233200
  training_iteration: 5
  2019-07-18 01:57:46,254	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-18 01:57:46,268	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 4, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11103], 152 s, 5 iter, 1233200 ts, 17.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew

Result for APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-57-46
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.67739224865872
  episode_reward_mean: 18.294224238599934
  episode_reward_min: -4.876910759971476
  episodes_this_iter: 696
  episodes_total: 4240
  experiment_id: 3da60c0f8b4843758fcac0831f35ee84
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.43714904785156
        mean_q: 8.109750747680664
        min_q: -4.748051643371582
    learner_queue:
      size_count: 8046
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4004996878900157
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 41
    num_steps_sampled: 1476800
    num_steps_trained: 2164224
    num_target_updates: 43
    num_weight_syncs: 3692
    replay_shard_0:
      add_batch_time_ms: 31.846
      policy_default_policy:
        added_count: 355200
        est_size_bytes: 121123200
        num_entries: 355200
        sampled_count: 539136
      replay_time_ms: 27.535
      update_priorities_time_ms: 87.041
    sample_throughput: 17595.156
    train_throughput: 7507.267
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11103
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 2.7375184763431184
    mean_inference_ms: 1.9122111194173264
    mean_processing_ms: 1.9583185660579607
  time_since_restore: 183.03767490386963
  time_this_iter_s: 30.51957368850708
  time_total_s: 183.03767490386963
  timestamp: 1563407866
  timesteps_since_restore: 1476800
  timesteps_this_iter: 243600
  timesteps_total: 1476800
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

[2m[36m(pid=11457)[0m [32m [     0.06948s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m 2019-07-18 01:57:46.476092: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11457)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11457)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11457)[0m [32m [     1.10447s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m [32m [     1.10495s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m [32m [     1.10535s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m 2019-07-18 01:57:47,508	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fe35a93db00>}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:47,508	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe35a9674e0>}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:47,508	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fe35af56668>}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:47,512	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=11457)[0m 2019-07-18 01:57:47,519	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=11332)[0m [32m [     0.02181s,  INFO] TimeLimit:
[2m[36m(pid=11332)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11332)[0m - action_space = Box(2,)
[2m[36m(pid=11332)[0m - observation_space = Box(9,)
[2m[36m(pid=11332)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11332)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11332)[0m - _max_episode_steps = 150
[2m[36m(pid=11332)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11332)[0m 2019-07-18 01:57:47,623	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11332)[0m 2019-07-18 01:57:47.623793: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11437)[0m [32m [     0.02429s,  INFO] TimeLimit:
[2m[36m(pid=11437)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11437)[0m - action_space = Box(2,)
[2m[36m(pid=11437)[0m - observation_space = Box(9,)
[2m[36m(pid=11437)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11437)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11437)[0m - _max_episode_steps = 150
[2m[36m(pid=11437)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11437)[0m 2019-07-18 01:57:47,613	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11437)[0m 2019-07-18 01:57:47.613747: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11337)[0m 2019-07-18 01:57:47,586	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11337)[0m 2019-07-18 01:57:47.586510: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11337)[0m [32m [     0.02246s,  INFO] TimeLimit:
[2m[36m(pid=11337)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11337)[0m - action_space = Box(2,)
[2m[36m(pid=11337)[0m - observation_space = Box(9,)
[2m[36m(pid=11337)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11337)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11337)[0m - _max_episode_steps = 150
[2m[36m(pid=11337)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11339)[0m 2019-07-18 01:57:47,583	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11339)[0m 2019-07-18 01:57:47.583946: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11339)[0m [32m [     0.02386s,  INFO] TimeLimit:
[2m[36m(pid=11339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11339)[0m - action_space = Box(2,)
[2m[36m(pid=11339)[0m - observation_space = Box(9,)
[2m[36m(pid=11339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11339)[0m - _max_episode_steps = 150
[2m[36m(pid=11339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11338)[0m [32m [     0.03279s,  INFO] TimeLimit:
[2m[36m(pid=11338)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11338)[0m - action_space = Box(2,)
[2m[36m(pid=11338)[0m - observation_space = Box(9,)
[2m[36m(pid=11338)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11338)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11338)[0m - _max_episode_steps = 150
[2m[36m(pid=11338)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11338)[0m 2019-07-18 01:57:47,617	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11338)[0m 2019-07-18 01:57:47.618039: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11454)[0m 2019-07-18 01:57:47,597	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11454)[0m 2019-07-18 01:57:47.598085: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11454)[0m [32m [     0.02900s,  INFO] TimeLimit:
[2m[36m(pid=11454)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11454)[0m - action_space = Box(2,)
[2m[36m(pid=11454)[0m - observation_space = Box(9,)
[2m[36m(pid=11454)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11454)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11454)[0m - _max_episode_steps = 150
[2m[36m(pid=11454)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11331)[0m [32m [     0.03442s,  INFO] TimeLimit:
[2m[36m(pid=11331)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11331)[0m - action_space = Box(2,)
[2m[36m(pid=11331)[0m - observation_space = Box(9,)
[2m[36m(pid=11331)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11331)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11331)[0m - _max_episode_steps = 150
[2m[36m(pid=11331)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11331)[0m 2019-07-18 01:57:47,635	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=11331)[0m 2019-07-18 01:57:47.636081: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=11332)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11332)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11337)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11337)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11339)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11339)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11338)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11338)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11454)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11454)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11437)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11437)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11331)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11331)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=11457)[0m [32m [     1.35665s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11339)[0m [32m [     0.98789s,  INFO] TimeLimit:
[2m[36m(pid=11339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11339)[0m - action_space = Box(2,)
[2m[36m(pid=11339)[0m - observation_space = Box(9,)
[2m[36m(pid=11339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11339)[0m - _max_episode_steps = 150
[2m[36m(pid=11339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11339)[0m [32m [     0.98851s,  INFO] TimeLimit:
[2m[36m(pid=11339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11339)[0m - action_space = Box(2,)
[2m[36m(pid=11339)[0m - observation_space = Box(9,)
[2m[36m(pid=11339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11339)[0m - _max_episode_steps = 150
[2m[36m(pid=11339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11339)[0m [32m [     0.98922s,  INFO] TimeLimit:
[2m[36m(pid=11339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11339)[0m - action_space = Box(2,)
[2m[36m(pid=11339)[0m - observation_space = Box(9,)
[2m[36m(pid=11339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11339)[0m - _max_episode_steps = 150
[2m[36m(pid=11339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11454)[0m [32m [     0.97928s,  INFO] TimeLimit:
[2m[36m(pid=11454)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11454)[0m - action_space = Box(2,)
[2m[36m(pid=11454)[0m - observation_space = Box(9,)
[2m[36m(pid=11454)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11454)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11454)[0m - _max_episode_steps = 150
[2m[36m(pid=11454)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11454)[0m [32m [     0.98020s,  INFO] TimeLimit:
[2m[36m(pid=11454)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11454)[0m - action_space = Box(2,)
[2m[36m(pid=11454)[0m - observation_space = Box(9,)
[2m[36m(pid=11454)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11454)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11454)[0m - _max_episode_steps = 150
[2m[36m(pid=11454)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11454)[0m [32m [     0.98108s,  INFO] TimeLimit:
[2m[36m(pid=11454)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11454)[0m - action_space = Box(2,)
[2m[36m(pid=11454)[0m - observation_space = Box(9,)
[2m[36m(pid=11454)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11454)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11454)[0m - _max_episode_steps = 150
[2m[36m(pid=11454)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11332)[0m [32m [     0.96053s,  INFO] TimeLimit:
[2m[36m(pid=11332)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11332)[0m - action_space = Box(2,)
[2m[36m(pid=11332)[0m - observation_space = Box(9,)
[2m[36m(pid=11332)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11332)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11332)[0m - _max_episode_steps = 150
[2m[36m(pid=11332)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11332)[0m [32m [     0.96112s,  INFO] TimeLimit:
[2m[36m(pid=11332)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11332)[0m - action_space = Box(2,)
[2m[36m(pid=11332)[0m - observation_space = Box(9,)
[2m[36m(pid=11332)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11332)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11332)[0m - _max_episode_steps = 150
[2m[36m(pid=11332)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11332)[0m [32m [     0.96171s,  INFO] TimeLimit:
[2m[36m(pid=11332)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11332)[0m - action_space = Box(2,)
[2m[36m(pid=11332)[0m - observation_space = Box(9,)
[2m[36m(pid=11332)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11332)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11332)[0m - _max_episode_steps = 150
[2m[36m(pid=11332)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,571	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,601	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.753, max=0.999, mean=0.028)},
[2m[36m(pid=11339)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.771, max=0.106, mean=-0.247)},
[2m[36m(pid=11339)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.966, max=0.382, mean=-0.062)},
[2m[36m(pid=11339)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.826, max=0.867, mean=0.076)}}
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,602	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=11339)[0m   1: {'agent0': None},
[2m[36m(pid=11339)[0m   2: {'agent0': None},
[2m[36m(pid=11339)[0m   3: {'agent0': None}}
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,602	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.753, max=0.999, mean=0.028)
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,602	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.753, max=0.999, mean=0.028)
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,604	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11339)[0m                                   'env_id': 0,
[2m[36m(pid=11339)[0m                                   'info': None,
[2m[36m(pid=11339)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.753, max=0.999, mean=0.028),
[2m[36m(pid=11339)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11339)[0m                                   'rnn_state': []},
[2m[36m(pid=11339)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11339)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11339)[0m                                   'env_id': 1,
[2m[36m(pid=11339)[0m                                   'info': None,
[2m[36m(pid=11339)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.771, max=0.106, mean=-0.247),
[2m[36m(pid=11339)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11339)[0m                                   'rnn_state': []},
[2m[36m(pid=11339)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11339)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11339)[0m                                   'env_id': 2,
[2m[36m(pid=11339)[0m                                   'info': None,
[2m[36m(pid=11339)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.966, max=0.382, mean=-0.062),
[2m[36m(pid=11339)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11339)[0m                                   'rnn_state': []},
[2m[36m(pid=11339)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11339)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11339)[0m                                   'env_id': 3,
[2m[36m(pid=11339)[0m                                   'info': None,
[2m[36m(pid=11339)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.826, max=0.867, mean=0.076),
[2m[36m(pid=11339)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11339)[0m                                   'rnn_state': []},
[2m[36m(pid=11339)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,604	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=11331)[0m [32m [     1.05816s,  INFO] TimeLimit:
[2m[36m(pid=11331)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11331)[0m - action_space = Box(2,)
[2m[36m(pid=11331)[0m - observation_space = Box(9,)
[2m[36m(pid=11331)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11331)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11331)[0m - _max_episode_steps = 150
[2m[36m(pid=11331)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11331)[0m [32m [     1.05898s,  INFO] TimeLimit:
[2m[36m(pid=11331)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11331)[0m - action_space = Box(2,)
[2m[36m(pid=11331)[0m - observation_space = Box(9,)
[2m[36m(pid=11331)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11331)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11331)[0m - _max_episode_steps = 150
[2m[36m(pid=11331)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11331)[0m [32m [     1.05972s,  INFO] TimeLimit:
[2m[36m(pid=11331)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11331)[0m - action_space = Box(2,)
[2m[36m(pid=11331)[0m - observation_space = Box(9,)
[2m[36m(pid=11331)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11331)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11331)[0m - _max_episode_steps = 150
[2m[36m(pid=11331)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,627	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-0.456, max=0.41, mean=-0.002),
[2m[36m(pid=11339)[0m                       [],
[2m[36m(pid=11339)[0m                       {})}
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,765	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-0.891, max=1.0, mean=0.356),
[2m[36m(pid=11339)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=194630020.0, max=194630020.0, mean=194630020.0),
[2m[36m(pid=11339)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=11339)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-5.902, max=7.004, mean=0.23),
[2m[36m(pid=11339)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-5.902, max=4.174, mean=0.194),
[2m[36m(pid=11339)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-0.891, max=1.0, mean=0.351),
[2m[36m(pid=11339)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-7.354, max=2.371, mean=-0.272),
[2m[36m(pid=11339)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-19.776, max=5.293, mean=-1.27),
[2m[36m(pid=11339)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=11339)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.079, max=19.864, mean=4.576)},
[2m[36m(pid=11339)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m 2019-07-18 01:57:48,780	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11339)[0m { 'data': { 'actions': np.ndarray((200, 2), dtype=float32, min=-1.0, max=1.0, mean=0.334),
[2m[36m(pid=11339)[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m             'eps_id': np.ndarray((200,), dtype=int64, min=52917066.0, max=1158559275.0, mean=444070538.75),
[2m[36m(pid=11339)[0m             'infos': np.ndarray((200,), dtype=object, head={}),
[2m[36m(pid=11339)[0m             'new_obs': np.ndarray((200, 9), dtype=float32, min=-6.098, max=7.004, mean=0.146),
[2m[36m(pid=11339)[0m             'obs': np.ndarray((200, 9), dtype=float32, min=-6.098, max=5.782, mean=0.104),
[2m[36m(pid=11339)[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=-1.0, max=1.0, mean=0.33),
[2m[36m(pid=11339)[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-7.354, max=7.192, mean=-0.151),
[2m[36m(pid=11339)[0m             'rewards': np.ndarray((200,), dtype=float32, min=-19.776, max=21.03, mean=-0.348),
[2m[36m(pid=11339)[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=11339)[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11339)[0m             'weights': np.ndarray((200,), dtype=float32, min=0.035, max=21.052, mean=4.303)},
[2m[36m(pid=11339)[0m   'type': 'SampleBatch'}
[2m[36m(pid=11339)[0m 
[2m[36m(pid=11337)[0m [32m [     1.29490s,  INFO] TimeLimit:
[2m[36m(pid=11337)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11337)[0m - action_space = Box(2,)
[2m[36m(pid=11337)[0m - observation_space = Box(9,)
[2m[36m(pid=11337)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11337)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11337)[0m - _max_episode_steps = 150
[2m[36m(pid=11337)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11337)[0m [32m [     1.29559s,  INFO] TimeLimit:
[2m[36m(pid=11337)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11337)[0m - action_space = Box(2,)
[2m[36m(pid=11337)[0m - observation_space = Box(9,)
[2m[36m(pid=11337)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11337)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11337)[0m - _max_episode_steps = 150
[2m[36m(pid=11337)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11337)[0m [32m [     1.29621s,  INFO] TimeLimit:
[2m[36m(pid=11337)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11337)[0m - action_space = Box(2,)
[2m[36m(pid=11337)[0m - observation_space = Box(9,)
[2m[36m(pid=11337)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11337)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11337)[0m - _max_episode_steps = 150
[2m[36m(pid=11337)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11338)[0m [32m [     1.25813s,  INFO] TimeLimit:
[2m[36m(pid=11338)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11338)[0m - action_space = Box(2,)
[2m[36m(pid=11338)[0m - observation_space = Box(9,)
[2m[36m(pid=11338)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11338)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11338)[0m - _max_episode_steps = 150
[2m[36m(pid=11338)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11338)[0m [32m [     1.25885s,  INFO] TimeLimit:
[2m[36m(pid=11338)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11338)[0m - action_space = Box(2,)
[2m[36m(pid=11338)[0m - observation_space = Box(9,)
[2m[36m(pid=11338)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11338)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11338)[0m - _max_episode_steps = 150
[2m[36m(pid=11338)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11338)[0m [32m [     1.25954s,  INFO] TimeLimit:
[2m[36m(pid=11338)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11338)[0m - action_space = Box(2,)
[2m[36m(pid=11338)[0m - observation_space = Box(9,)
[2m[36m(pid=11338)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11338)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11338)[0m - _max_episode_steps = 150
[2m[36m(pid=11338)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11437)[0m [32m [     1.35684s,  INFO] TimeLimit:
[2m[36m(pid=11437)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11437)[0m - action_space = Box(2,)
[2m[36m(pid=11437)[0m - observation_space = Box(9,)
[2m[36m(pid=11437)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11437)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11437)[0m - _max_episode_steps = 150
[2m[36m(pid=11437)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11437)[0m [32m [     1.35751s,  INFO] TimeLimit:
[2m[36m(pid=11437)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11437)[0m - action_space = Box(2,)
[2m[36m(pid=11437)[0m - observation_space = Box(9,)
[2m[36m(pid=11437)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11437)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11437)[0m - _max_episode_steps = 150
[2m[36m(pid=11437)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11437)[0m [32m [     1.35823s,  INFO] TimeLimit:
[2m[36m(pid=11437)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11437)[0m - action_space = Box(2,)
[2m[36m(pid=11437)[0m - observation_space = Box(9,)
[2m[36m(pid=11437)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11437)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11437)[0m - _max_episode_steps = 150
[2m[36m(pid=11437)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m [32m [     2.62104s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m [32m [     2.62152s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m [32m [     2.62199s,  INFO] TimeLimit:
[2m[36m(pid=11457)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=11457)[0m - action_space = Box(2,)
[2m[36m(pid=11457)[0m - observation_space = Box(9,)
[2m[36m(pid=11457)[0m - reward_range = (-inf, inf)
[2m[36m(pid=11457)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=11457)[0m - _max_episode_steps = 150
[2m[36m(pid=11457)[0m - _elapsed_steps = None [0m
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,025	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fe352f59ba8>}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,026	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe352f59898>}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,026	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fe352f4ce10>}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,029	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,051	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,077	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.744, max=0.983, mean=0.043)},
[2m[36m(pid=11457)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.464, max=0.886, mean=0.089)},
[2m[36m(pid=11457)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.565, max=0.825, mean=0.024)},
[2m[36m(pid=11457)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.733, max=0.826, mean=-0.068)}}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,077	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=11457)[0m   1: {'agent0': None},
[2m[36m(pid=11457)[0m   2: {'agent0': None},
[2m[36m(pid=11457)[0m   3: {'agent0': None}}
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,077	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.744, max=0.983, mean=0.043)
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,077	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.744, max=0.983, mean=0.043)
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,079	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11457)[0m                                   'env_id': 0,
[2m[36m(pid=11457)[0m                                   'info': None,
[2m[36m(pid=11457)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.744, max=0.983, mean=0.043),
[2m[36m(pid=11457)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11457)[0m                                   'rnn_state': []},
[2m[36m(pid=11457)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11457)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11457)[0m                                   'env_id': 1,
[2m[36m(pid=11457)[0m                                   'info': None,
[2m[36m(pid=11457)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.464, max=0.886, mean=0.089),
[2m[36m(pid=11457)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11457)[0m                                   'rnn_state': []},
[2m[36m(pid=11457)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11457)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11457)[0m                                   'env_id': 2,
[2m[36m(pid=11457)[0m                                   'info': None,
[2m[36m(pid=11457)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.565, max=0.825, mean=0.024),
[2m[36m(pid=11457)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11457)[0m                                   'rnn_state': []},
[2m[36m(pid=11457)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=11457)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=11457)[0m                                   'env_id': 3,
[2m[36m(pid=11457)[0m                                   'info': None,
[2m[36m(pid=11457)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.733, max=0.826, mean=-0.068),
[2m[36m(pid=11457)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=11457)[0m                                   'rnn_state': []},
[2m[36m(pid=11457)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,079	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,098	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-0.682, max=0.308, mean=-0.207),
[2m[36m(pid=11457)[0m                       [],
[2m[36m(pid=11457)[0m                       {})}
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,307	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.165),
[2m[36m(pid=11457)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=11457)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=836271208.0, max=836271208.0, mean=836271208.0),
[2m[36m(pid=11457)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=11457)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-5.523, max=4.984, mean=0.067),
[2m[36m(pid=11457)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-5.523, max=4.984, mean=0.071),
[2m[36m(pid=11457)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.166),
[2m[36m(pid=11457)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-5.707, max=6.185, mean=-0.11),
[2m[36m(pid=11457)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-12.046, max=15.745, mean=-0.332),
[2m[36m(pid=11457)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=11457)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.006, max=15.792, mean=2.001)},
[2m[36m(pid=11457)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m 2019-07-18 01:57:49,327	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m { 'data': { 'actions': np.ndarray((300, 2), dtype=float32, min=-1.0, max=1.0, mean=0.162),
[2m[36m(pid=11457)[0m             'agent_index': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m             'dones': np.ndarray((300,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=11457)[0m             'eps_id': np.ndarray((300,), dtype=int64, min=836271208.0, max=1129260335.0, mean=982765771.5),
[2m[36m(pid=11457)[0m             'infos': np.ndarray((300,), dtype=object, head={}),
[2m[36m(pid=11457)[0m             'new_obs': np.ndarray((300, 9), dtype=float32, min=-5.678, max=4.984, mean=0.082),
[2m[36m(pid=11457)[0m             'obs': np.ndarray((300, 9), dtype=float32, min=-5.678, max=4.984, mean=0.085),
[2m[36m(pid=11457)[0m             'prev_actions': np.ndarray((300, 2), dtype=float32, min=-1.0, max=1.0, mean=0.163),
[2m[36m(pid=11457)[0m             'prev_rewards': np.ndarray((300,), dtype=float32, min=-6.373, max=6.393, mean=-0.05),
[2m[36m(pid=11457)[0m             'rewards': np.ndarray((300,), dtype=float32, min=-14.614, max=17.936, mean=-0.154),
[2m[36m(pid=11457)[0m             't': np.ndarray((300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=11457)[0m             'unroll_id': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=11457)[0m             'weights': np.ndarray((300,), dtype=float32, min=0.002, max=17.882, mean=2.251)},
[2m[36m(pid=11457)[0m   'type': 'SampleBatch'}
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m 2019-07-18 01:57:54,640	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m { 'count': 512,
[2m[36m(pid=11457)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=0.008),
[2m[36m(pid=11457)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=5.0, max=12596.0, mean=6117.857),
[2m[36m(pid=11457)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=11457)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.042),
[2m[36m(pid=11457)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.044),
[2m[36m(pid=11457)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-38.227, max=26.038, mean=-0.367),
[2m[36m(pid=11457)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.056, max=0.29, mean=0.11)},
[2m[36m(pid=11457)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=11457)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m 2019-07-18 01:57:54,893	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=11457)[0m 
[2m[36m(pid=11457)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.47726446,
[2m[36m(pid=11457)[0m                                          'mean_q': 0.058830783,
[2m[36m(pid=11457)[0m                                          'min_q': -0.19967207},
[2m[36m(pid=11457)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-26.038, max=38.206, mean=0.374)}}
[2m[36m(pid=11457)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-58-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.571470639748277
  episode_reward_mean: -9.9911930862922
  episode_reward_min: -44.849717708690555
  episodes_this_iter: 604
  episodes_total: 604
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.131868362426758
        mean_q: 2.804091215133667
        min_q: -16.912063598632812
    learner_queue:
      size_count: 6277
      size_mean: 0.24
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.47159304490206383
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 195
    num_steps_sampled: 209000
    num_steps_trained: 336384
    num_target_updates: 6
    num_weight_syncs: 521
    replay_shard_0:
      add_batch_time_ms: 15.39
      policy_default_policy:
        added_count: 52200
        est_size_bytes: 17800200
        num_entries: 52200
        sampled_count: 84992
      replay_time_ms: 24.121
      update_priorities_time_ms: 78.543
    sample_throughput: 0.0
    train_throughput: 24247.269
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.4025215914107836
    mean_inference_ms: 1.4446563668441785
    mean_processing_ms: 0.9927517913684166
  time_since_restore: 30.354692220687866
  time_this_iter_s: 30.354692220687866
  time_total_s: 30.354692220687866
  timestamp: 1563407900
  timesteps_since_restore: 209000
  timesteps_this_iter: 209000
  timesteps_total: 209000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11457], 30 s, 1 iter, 209000 ts, -9.99 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-58-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.36150557785507
  episode_reward_mean: 4.973921809552129
  episode_reward_min: -43.82336960122225
  episodes_this_iter: 556
  episodes_total: 1160
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 27.49911880493164
        mean_q: 5.249557018280029
        min_q: -31.675386428833008
    learner_queue:
      size_count: 7050
      size_mean: 0.32
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5075431016179808
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 201
    num_steps_sampled: 407400
    num_steps_trained: 732160
    num_target_updates: 14
    num_weight_syncs: 1016
    replay_shard_0:
      add_batch_time_ms: 15.863
      policy_default_policy:
        added_count: 99400
        est_size_bytes: 33895400
        num_entries: 99400
        sampled_count: 185856
      replay_time_ms: 28.157
      update_priorities_time_ms: 87.322
    sample_throughput: 4218.943
    train_throughput: 21600.986
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.436836274390129
    mean_inference_ms: 1.5162944892618562
    mean_processing_ms: 1.0213435535114315
  time_since_restore: 60.761701822280884
  time_this_iter_s: 30.407009601593018
  time_total_s: 60.761701822280884
  timestamp: 1563407930
  timesteps_since_restore: 407400
  timesteps_this_iter: 198400
  timesteps_total: 407400
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11457], 60 s, 2 iter, 407400 ts, 4.97 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-59-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.91653005215095
  episode_reward_mean: 14.154936076111726
  episode_reward_min: -13.865906620447818
  episodes_this_iter: 576
  episodes_total: 1736
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.75318908691406
        mean_q: 7.9816694259643555
        min_q: -5.553675174713135
    learner_queue:
      size_count: 7806
      size_mean: 0.46
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.6069596362197408
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 202
    num_steps_sampled: 608400
    num_steps_trained: 1118720
    num_target_updates: 22
    num_weight_syncs: 1520
    replay_shard_0:
      add_batch_time_ms: 20.2
      policy_default_policy:
        added_count: 150200
        est_size_bytes: 51218200
        num_entries: 150200
        sampled_count: 288768
      replay_time_ms: 26.098
      update_priorities_time_ms: 74.74
    sample_throughput: 9266.893
    train_throughput: 7907.749
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.4434779430664566
    mean_inference_ms: 1.5271853365243027
    mean_processing_ms: 1.0285044853938499
  time_since_restore: 91.09286689758301
  time_this_iter_s: 30.331165075302124
  time_total_s: 91.09286689758301
  timestamp: 1563407961
  timesteps_since_restore: 608400
  timesteps_this_iter: 201000
  timesteps_total: 608400
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11457], 91 s, 3 iter, 608400 ts, 14.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_01-59-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.41821066835636
  episode_reward_mean: 16.53412444608658
  episode_reward_min: -15.63268176132855
  episodes_this_iter: 576
  episodes_total: 2312
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.41727066040039
        mean_q: 8.61575698852539
        min_q: -12.402670860290527
    learner_queue:
      size_count: 8558
      size_mean: 0.28
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4915282290977803
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 202
    num_steps_sampled: 809800
    num_steps_trained: 1504256
    num_target_updates: 29
    num_weight_syncs: 2023
    replay_shard_0:
      add_batch_time_ms: 14.869
      policy_default_policy:
        added_count: 195600
        est_size_bytes: 66699600
        num_entries: 195600
        sampled_count: 386048
      replay_time_ms: 30.674
      update_priorities_time_ms: 80.855
    sample_throughput: 13036.518
    train_throughput: 11124.495
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.44067391942828
    mean_inference_ms: 1.530130356611245
    mean_processing_ms: 1.0296394413049355
  time_since_restore: 121.5056734085083
  time_this_iter_s: 30.412806510925293
  time_total_s: 121.5056734085083
  timestamp: 1563407991
  timesteps_since_restore: 809800
  timesteps_this_iter: 201400
  timesteps_total: 809800
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11457], 121 s, 4 iter, 809800 ts, 16.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

[2m[36m(pid=11457)[0m 2019-07-18 02:00:22,006	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_02-00-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.484324507126665
  episode_reward_mean: 17.987132324143545
  episode_reward_min: -6.118841887284307
  episodes_this_iter: 576
  episodes_total: 2888
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 38.051435417978915
    episode_reward_mean: 21.71305056947043
    episode_reward_min: 8.848920692590642
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.49896322831084605
      mean_inference_ms: 0.5011782496869762
      mean_processing_ms: 0.28297521526379865
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.59333038330078
        mean_q: 8.037371635437012
        min_q: -16.514225006103516
    learner_queue:
      size_count: 9306
      size_mean: 0.24
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4270831300812525
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 203
    num_steps_sampled: 1011200
    num_steps_trained: 1886720
    num_target_updates: 37
    num_weight_syncs: 2525
    replay_shard_0:
      add_batch_time_ms: 10.245
      policy_default_policy:
        added_count: 243800
        est_size_bytes: 83135800
        num_entries: 243800
        sampled_count: 486912
      replay_time_ms: 28.251
      update_priorities_time_ms: 76.45
    sample_throughput: 15605.116
    train_throughput: 0.0
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.4417557090919924
    mean_inference_ms: 1.5328575350817073
    mean_processing_ms: 1.0292083183933307
  time_since_restore: 151.90838885307312
  time_this_iter_s: 30.40271544456482
  time_total_s: 151.90838885307312
  timestamp: 1563408022
  timesteps_since_restore: 1011200
  timesteps_this_iter: 201400
  timesteps_total: 1011200
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11457], 151 s, 5 iter, 1011200 ts, 18 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_02-00-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.40848071457659
  episode_reward_mean: 17.86790099283454
  episode_reward_min: -14.838211810709993
  episodes_this_iter: 568
  episodes_total: 3456
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.94343185424805
        mean_q: 8.235358238220215
        min_q: -18.552228927612305
    learner_queue:
      size_count: 10066
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 207
    num_steps_sampled: 1211600
    num_steps_trained: 2275840
    num_target_updates: 45
    num_weight_syncs: 3028
    replay_shard_0:
      add_batch_time_ms: 16.671
      policy_default_policy:
        added_count: 293600
        est_size_bytes: 100117600
        num_entries: 293600
        sampled_count: 588288
      replay_time_ms: 24.262
      update_priorities_time_ms: 66.376
    sample_throughput: 8013.221
    train_throughput: 12308.307
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.4463355933028184
    mean_inference_ms: 1.5372447580906514
    mean_processing_ms: 1.0305951562094693
  time_since_restore: 182.31254124641418
  time_this_iter_s: 30.404152393341064
  time_total_s: 182.31254124641418
  timestamp: 1563408053
  timesteps_since_restore: 1211600
  timesteps_this_iter: 200400
  timesteps_total: 1211600
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 8/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 5, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	RUNNING, [8 CPUs, 1 GPUs], [pid=11457], 182 s, 6 iter, 1211600 ts, 17.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew

Result for APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:
  custom_metrics: {}
  date: 2019-07-18_02-01-23
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.130816260984375
  episode_reward_mean: 18.20658678532882
  episode_reward_min: -12.279203620500237
  episodes_this_iter: 580
  episodes_total: 4036
  experiment_id: 90859b3f17634888b5627c6c30fca344
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.9864387512207
        mean_q: 7.394440650939941
        min_q: -8.369593620300293
    learner_queue:
      size_count: 10815
      size_mean: 0.28
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4915282290977803
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 213
    num_steps_sampled: 1412600
    num_steps_trained: 2659840
    num_target_updates: 53
    num_weight_syncs: 3529
    replay_shard_0:
      add_batch_time_ms: 14.473
      policy_default_policy:
        added_count: 347600
        est_size_bytes: 118531600
        num_entries: 347600
        sampled_count: 685568
      replay_time_ms: 25.243
      update_priorities_time_ms: 72.915
    sample_throughput: 5794.838
    train_throughput: 29669.572
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 7
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 11457
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.4458012626492722
    mean_inference_ms: 1.5349179209261332
    mean_processing_ms: 1.027904490697604
  time_since_restore: 212.73670268058777
  time_this_iter_s: 30.424161434173584
  time_total_s: 212.73670268058777
  timestamp: 1563408083
  timesteps_since_restore: 1412600
  timesteps_this_iter: 201000
  timesteps_total: 1412600
  training_iteration: 7
  2019-07-18 02:01:23,794	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 11.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 6})
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11457], 212 s, 7 iter, 1412600 ts, 18.2 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 11.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 6 ({'TERMINATED': 6})
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9212], 371 s, 12 iter, 4655200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9736], 429 s, 14 iter, 4339200 ts, 18.2 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4,num_workers=11:	TERMINATED, [12 CPUs, 1 GPUs], [pid=9931], 366 s, 12 iter, 3039000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_3_num_envs_per_worker=16,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=10782], 214 s, 7 iter, 2176000 ts, 18.1 rew
 - APEX_DDPG_RoboschoolReacher-v1_4_num_envs_per_worker=8,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11103], 183 s, 6 iter, 1476800 ts, 18.3 rew
 - APEX_DDPG_RoboschoolReacher-v1_5_num_envs_per_worker=4,num_workers=7:	TERMINATED, [8 CPUs, 1 GPUs], [pid=11457], 212 s, 7 iter, 1412600 ts, 18.2 rew

[32m [  1839.07898s,  INFO] Experiment took 1838.86102 seconds | 30.64768 minutes | 0.51079 hours [0m
