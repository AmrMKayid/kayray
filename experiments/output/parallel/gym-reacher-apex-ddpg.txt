2019-07-24 02:32:59,708	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-24_02-32-59_707828_31592/logs.
2019-07-24 02:32:59,813	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:65340 to respond...
2019-07-24 02:32:59,922	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:35247 to respond...
2019-07-24 02:32:59,926	INFO services.py:806 -- Starting Redis shard with 3.33 GB max memory.
2019-07-24 02:32:59,948	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-24_02-32-59_707828_31592/logs.
2019-07-24 02:32:59,949	INFO services.py:1446 -- Starting the Plasma object store with 5.0 GB memory using /dev/shm.
2019-07-24 02:33:00,050	INFO tune.py:65 -- Did not find checkpoint file in /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg.
2019-07-24 02:33:00,050	INFO tune.py:233 -- Starting a new experiment.
2019-07-24 02:33:00,093	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
2019-07-24 02:33:00,241	WARNING util.py:64 -- The `start_trial` operation took 0.17787432670593262 seconds to complete, which may be a performance bottleneck.
[32m [     0.20870s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.20900s,  INFO] Experiment configs: 
 {
  "gym-reacher-apex-ddpg": {
    "env": "RoboschoolReacher-v1",
    "run": "APEX_DDPG",
    "local_dir": "~/kayray_results/parallel",
    "checkpoint_freq": 50,
    "checkpoint_at_end": true,
    "stop": {
      "episode_reward_mean": 21,
      "timesteps_total": 10000000
    },
    "config": {
      "env_config": {
        "env_type": "openai"
      },
      "use_huber": true,
      "clip_rewards": false,
      "num_gpus": 1,
      "num_workers": 11,
      "num_envs_per_worker": {
        "grid_search": [
          16,
          8,
          4
        ]
      },
      "n_step": 3,
      "exploration_ou_noise_scale": 1.0,
      "target_network_update_freq": 50000,
      "tau": 1.0,
      "evaluation_interval": 5,
      "evaluation_num_episodes": 10
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 1.1/16.7 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 1.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING

[2m[36m(pid=31643)[0m [32m [     0.01518s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m 2019-07-24 02:33:01.975592: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31643)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31643)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31643)[0m [32m [     0.58116s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58155s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58194s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58232s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58275s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58312s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58350s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58388s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58425s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58463s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58501s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58538s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58576s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m 2019-07-24 02:33:02,539	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fb5bad26f98>}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:02,539	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fb5bad874a8>}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:02,540	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fb5bad873c8>}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:02,548	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58615s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     0.58658s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m 2019-07-24 02:33:03,729	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=31643)[0m [32m [     2.22265s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09434s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09527s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09619s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09713s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09807s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09899s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.09990s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10082s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10173s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10266s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10357s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10458s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10549s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,053	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fb5b4321940>}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,053	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fb5b43215f8>}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,053	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fb5b431ae48>}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,070	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10636s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m [32m [     4.10724s,  INFO] TimeLimit:
[2m[36m(pid=31643)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31643)[0m - action_space = Box(2,)
[2m[36m(pid=31643)[0m - observation_space = Box(9,)
[2m[36m(pid=31643)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31643)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31643)[0m - _max_episode_steps = 150
[2m[36m(pid=31643)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,114	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,265	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.083, max=0.997, mean=0.171)},
[2m[36m(pid=31643)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.2, max=0.98, mean=0.122)},
[2m[36m(pid=31643)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.986, max=0.248, mean=-0.146)},
[2m[36m(pid=31643)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.924, max=0.211, mean=-0.145)},
[2m[36m(pid=31643)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.519, max=1.0, mean=0.049)},
[2m[36m(pid=31643)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.237, max=0.754, mean=0.241)},
[2m[36m(pid=31643)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.213, max=0.974, mean=0.136)},
[2m[36m(pid=31643)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.07, max=0.996, mean=0.152)},
[2m[36m(pid=31643)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.569, max=0.822, mean=0.009)},
[2m[36m(pid=31643)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.979, max=0.206, mean=-0.177)},
[2m[36m(pid=31643)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.963, max=0.269, mean=-0.099)},
[2m[36m(pid=31643)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.686, max=0.727, mean=0.007)},
[2m[36m(pid=31643)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.907, max=0.174, mean=-0.249)},
[2m[36m(pid=31643)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.744, max=0.192, mean=-0.194)},
[2m[36m(pid=31643)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.396, max=0.958, mean=0.109)},
[2m[36m(pid=31643)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.881, max=0.474, mean=-0.124)}}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,265	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=31643)[0m   1: {'agent0': None},
[2m[36m(pid=31643)[0m   2: {'agent0': None},
[2m[36m(pid=31643)[0m   3: {'agent0': None},
[2m[36m(pid=31643)[0m   4: {'agent0': None},
[2m[36m(pid=31643)[0m   5: {'agent0': None},
[2m[36m(pid=31643)[0m   6: {'agent0': None},
[2m[36m(pid=31643)[0m   7: {'agent0': None},
[2m[36m(pid=31643)[0m   8: {'agent0': None},
[2m[36m(pid=31643)[0m   9: {'agent0': None},
[2m[36m(pid=31643)[0m   10: {'agent0': None},
[2m[36m(pid=31643)[0m   11: {'agent0': None},
[2m[36m(pid=31643)[0m   12: {'agent0': None},
[2m[36m(pid=31643)[0m   13: {'agent0': None},
[2m[36m(pid=31643)[0m   14: {'agent0': None},
[2m[36m(pid=31643)[0m   15: {'agent0': None}}
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,266	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.083, max=0.997, mean=0.171)
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,266	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.083, max=0.997, mean=0.171)
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,277	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 0,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.083, max=0.997, mean=0.171),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 1,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.2, max=0.98, mean=0.122),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 2,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.986, max=0.248, mean=-0.146),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 3,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.924, max=0.211, mean=-0.145),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 4,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.519, max=1.0, mean=0.049),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 5,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.237, max=0.754, mean=0.241),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 6,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.213, max=0.974, mean=0.136),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 7,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.07, max=0.996, mean=0.152),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 8,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.569, max=0.822, mean=0.009),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 9,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.979, max=0.206, mean=-0.177),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 10,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.963, max=0.269, mean=-0.099),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 11,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.686, max=0.727, mean=0.007),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 12,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.907, max=0.174, mean=-0.249),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 13,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.744, max=0.192, mean=-0.194),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 14,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.396, max=0.958, mean=0.109),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31643)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31643)[0m                                   'env_id': 15,
[2m[36m(pid=31643)[0m                                   'info': None,
[2m[36m(pid=31643)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.881, max=0.474, mean=-0.124),
[2m[36m(pid=31643)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31643)[0m                                   'rnn_state': []},
[2m[36m(pid=31643)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,277	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=31643)[0m 2019-07-24 02:33:06,316	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-0.561, max=0.047, mean=-0.269),
[2m[36m(pid=31643)[0m                       [],
[2m[36m(pid=31643)[0m                       {})}
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31642)[0m 2019-07-24 02:33:06,545	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31642)[0m 2019-07-24 02:33:06.546420: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31642)[0m [32m [     0.03087s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31642)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31638)[0m [32m [     0.02714s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m 2019-07-24 02:33:06,811	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31638)[0m 2019-07-24 02:33:06.812414: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31645)[0m [32m [     0.03779s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     0.03543s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m 2019-07-24 02:33:06,815	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31645)[0m 2019-07-24 02:33:06.815750: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31639)[0m 2019-07-24 02:33:06,821	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31639)[0m 2019-07-24 02:33:06.821578: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31635)[0m [32m [     0.03643s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m 2019-07-24 02:33:06,863	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31635)[0m 2019-07-24 02:33:06.863951: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31640)[0m [32m [     0.02634s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m 2019-07-24 02:33:06,918	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31640)[0m 2019-07-24 02:33:06.918846: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31638)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31638)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31645)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31645)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31639)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31639)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31641)[0m 2019-07-24 02:33:06,935	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31641)[0m 2019-07-24 02:33:06.936376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31641)[0m [32m [     0.03753s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m 2019-07-24 02:33:06,939	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31637)[0m 2019-07-24 02:33:06.939938: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31637)[0m [32m [     0.03918s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31635)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31641)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31641)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31640)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31640)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31637)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31637)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31643)[0m 2019-07-24 02:33:07,294	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.0),
[2m[36m(pid=31643)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=31643)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1950757036.0, max=1950757036.0, mean=1950757036.0),
[2m[36m(pid=31643)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=31643)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-8.028, max=5.881, mean=0.035),
[2m[36m(pid=31643)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-8.028, max=5.881, mean=0.031),
[2m[36m(pid=31643)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=0.0),
[2m[36m(pid=31643)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-7.365, max=7.717, mean=-0.094),
[2m[36m(pid=31643)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-17.596, max=19.305, mean=-0.387),
[2m[36m(pid=31643)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=31643)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.001, max=19.358, mean=3.061)},
[2m[36m(pid=31643)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31904)[0m 2019-07-24 02:33:07,452	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31904)[0m 2019-07-24 02:33:07.453183: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31904)[0m [32m [     0.03488s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31643)[0m 2019-07-24 02:33:07,495	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m { 'data': { 'actions': np.ndarray((900, 2), dtype=float32, min=-1.0, max=1.0, mean=0.004),
[2m[36m(pid=31643)[0m             'agent_index': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m             'dones': np.ndarray((900,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=31643)[0m             'eps_id': np.ndarray((900,), dtype=int64, min=521240977.0, max=1950757036.0, mean=1361813557.167),
[2m[36m(pid=31643)[0m             'infos': np.ndarray((900,), dtype=object, head={}),
[2m[36m(pid=31643)[0m             'new_obs': np.ndarray((900, 9), dtype=float32, min=-10.0, max=10.0, mean=0.025),
[2m[36m(pid=31643)[0m             'obs': np.ndarray((900, 9), dtype=float32, min=-10.0, max=10.0, mean=0.019),
[2m[36m(pid=31643)[0m             'prev_actions': np.ndarray((900, 2), dtype=float32, min=-1.0, max=1.0, mean=0.004),
[2m[36m(pid=31643)[0m             'prev_rewards': np.ndarray((900,), dtype=float32, min=-19.769, max=16.629, mean=-0.176),
[2m[36m(pid=31643)[0m             'rewards': np.ndarray((900,), dtype=float32, min=-34.409, max=27.436, mean=-0.522),
[2m[36m(pid=31643)[0m             't': np.ndarray((900,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=31643)[0m             'unroll_id': np.ndarray((900,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31643)[0m             'weights': np.ndarray((900,), dtype=float32, min=0.001, max=33.572, mean=3.353)},
[2m[36m(pid=31643)[0m   'type': 'SampleBatch'}
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31904)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31904)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31903)[0m [32m [     0.03891s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m 2019-07-24 02:33:07,563	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31903)[0m 2019-07-24 02:33:07.563809: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31903)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31903)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31642)[0m [32m [     1.48478s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.48570s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.48665s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.48752s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.48839s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.48927s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49014s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49102s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49194s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49283s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49373s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49463s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49555s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49666s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m [32m [     1.49755s,  INFO] TimeLimit:
[2m[36m(pid=31642)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31642)[0m - action_space = Box(2,)
[2m[36m(pid=31642)[0m - observation_space = Box(9,)
[2m[36m(pid=31642)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31642)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31642)[0m - _max_episode_steps = 150
[2m[36m(pid=31642)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,048	INFO rollout_worker.py:428 -- Generating sample batch of size 800
[2m[36m(pid=31638)[0m [32m [     1.33262s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33343s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33434s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33535s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33627s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33716s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33802s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33896s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.33982s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.34066s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.34154s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.34234s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.34315s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.34404s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31638)[0m [32m [     1.34491s,  INFO] TimeLimit:
[2m[36m(pid=31638)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31638)[0m - action_space = Box(2,)
[2m[36m(pid=31638)[0m - observation_space = Box(9,)
[2m[36m(pid=31638)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31638)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31638)[0m - _max_episode_steps = 150
[2m[36m(pid=31638)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,197	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.313, max=0.961, mean=0.119)},
[2m[36m(pid=31642)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.8, max=0.191, mean=-0.163)},
[2m[36m(pid=31642)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.482, max=0.971, mean=0.052)},
[2m[36m(pid=31642)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.645, max=0.772, mean=0.087)},
[2m[36m(pid=31642)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.142, max=0.758, mean=0.2)},
[2m[36m(pid=31642)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.885, max=0.241, mean=-0.154)},
[2m[36m(pid=31642)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.984, max=0.223, mean=-0.136)},
[2m[36m(pid=31642)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.914, max=0.972, mean=-0.039)},
[2m[36m(pid=31642)[0m   8: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.538, max=0.925, mean=0.106)},
[2m[36m(pid=31642)[0m   9: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.285, max=0.957, mean=0.193)},
[2m[36m(pid=31642)[0m   10: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.814, max=0.581, mean=-0.006)},
[2m[36m(pid=31642)[0m   11: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.356, max=0.935, mean=0.155)},
[2m[36m(pid=31642)[0m   12: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.68, max=0.733, mean=-0.023)},
[2m[36m(pid=31642)[0m   13: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.966, max=0.928, mean=-0.036)},
[2m[36m(pid=31642)[0m   14: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.249, max=0.837, mean=0.234)},
[2m[36m(pid=31642)[0m   15: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.903, max=0.431, mean=-0.111)}}
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,198	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=31642)[0m   1: {'agent0': None},
[2m[36m(pid=31642)[0m   2: {'agent0': None},
[2m[36m(pid=31642)[0m   3: {'agent0': None},
[2m[36m(pid=31642)[0m   4: {'agent0': None},
[2m[36m(pid=31642)[0m   5: {'agent0': None},
[2m[36m(pid=31642)[0m   6: {'agent0': None},
[2m[36m(pid=31642)[0m   7: {'agent0': None},
[2m[36m(pid=31642)[0m   8: {'agent0': None},
[2m[36m(pid=31642)[0m   9: {'agent0': None},
[2m[36m(pid=31642)[0m   10: {'agent0': None},
[2m[36m(pid=31642)[0m   11: {'agent0': None},
[2m[36m(pid=31642)[0m   12: {'agent0': None},
[2m[36m(pid=31642)[0m   13: {'agent0': None},
[2m[36m(pid=31642)[0m   14: {'agent0': None},
[2m[36m(pid=31642)[0m   15: {'agent0': None}}
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,198	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.313, max=0.961, mean=0.119)
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,199	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.313, max=0.961, mean=0.119)
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,209	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31642)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 0,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.313, max=0.961, mean=0.119),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 1,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.8, max=0.191, mean=-0.163),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 2,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.482, max=0.971, mean=0.052),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 3,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.645, max=0.772, mean=0.087),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 4,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.142, max=0.758, mean=0.2),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 5,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.885, max=0.241, mean=-0.154),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 6,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.984, max=0.223, mean=-0.136),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 7,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.914, max=0.972, mean=-0.039),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 8,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.538, max=0.925, mean=0.106),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 9,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.285, max=0.957, mean=0.193),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 10,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.814, max=0.581, mean=-0.006),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 11,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.356, max=0.935, mean=0.155),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 12,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.68, max=0.733, mean=-0.023),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 13,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.966, max=0.928, mean=-0.036),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 14,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.249, max=0.837, mean=0.234),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31642)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31642)[0m                                   'env_id': 15,
[2m[36m(pid=31642)[0m                                   'info': None,
[2m[36m(pid=31642)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.903, max=0.431, mean=-0.111),
[2m[36m(pid=31642)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31642)[0m                                   'rnn_state': []},
[2m[36m(pid=31642)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,209	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,249	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31642)[0m { 'default_policy': ( np.ndarray((16, 2), dtype=float32, min=-0.463, max=0.02, mean=-0.252),
[2m[36m(pid=31642)[0m                       [],
[2m[36m(pid=31642)[0m                       {})}
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31639)[0m [32m [     1.50676s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.50760s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.50844s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.50931s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51014s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51102s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51196s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51282s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51365s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51446s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51525s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51609s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51703s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51799s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31639)[0m [32m [     1.51887s,  INFO] TimeLimit:
[2m[36m(pid=31639)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31639)[0m - action_space = Box(2,)
[2m[36m(pid=31639)[0m - observation_space = Box(9,)
[2m[36m(pid=31639)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31639)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31639)[0m - _max_episode_steps = 150
[2m[36m(pid=31639)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50450s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50548s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50631s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50709s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50792s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50880s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.50964s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51046s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51128s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51212s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51303s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51391s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51473s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51562s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31635)[0m [32m [     1.51656s,  INFO] TimeLimit:
[2m[36m(pid=31635)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31635)[0m - action_space = Box(2,)
[2m[36m(pid=31635)[0m - observation_space = Box(9,)
[2m[36m(pid=31635)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31635)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31635)[0m - _max_episode_steps = 150
[2m[36m(pid=31635)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48272s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48362s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48454s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48547s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48632s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48721s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48813s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48900s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.48985s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.49076s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.49165s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.49259s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.49348s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m [32m [     1.50595s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.50671s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.50730s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.50801s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.50873s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.50941s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51018s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51089s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51150s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59352s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59448s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59535s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59628s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59716s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59809s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59900s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.59987s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60092s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60242s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60390s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60486s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60580s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.49440s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31640)[0m [32m [     1.49544s,  INFO] TimeLimit:
[2m[36m(pid=31640)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31640)[0m - action_space = Box(2,)
[2m[36m(pid=31640)[0m - observation_space = Box(9,)
[2m[36m(pid=31640)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31640)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31640)[0m - _max_episode_steps = 150
[2m[36m(pid=31640)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51495s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51591s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60671s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31645)[0m [32m [     1.60763s,  INFO] TimeLimit:
[2m[36m(pid=31645)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31645)[0m - action_space = Box(2,)
[2m[36m(pid=31645)[0m - observation_space = Box(9,)
[2m[36m(pid=31645)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31645)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31645)[0m - _max_episode_steps = 150
[2m[36m(pid=31645)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51698s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51797s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51897s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31641)[0m [32m [     1.51990s,  INFO] TimeLimit:
[2m[36m(pid=31641)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31641)[0m - action_space = Box(2,)
[2m[36m(pid=31641)[0m - observation_space = Box(9,)
[2m[36m(pid=31641)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31641)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31641)[0m - _max_episode_steps = 150
[2m[36m(pid=31641)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.53802s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.53897s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54009s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54104s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54203s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54301s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54394s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54490s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54587s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54681s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54780s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54882s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.54982s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.55075s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31637)[0m [32m [     1.55169s,  INFO] TimeLimit:
[2m[36m(pid=31637)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31637)[0m - action_space = Box(2,)
[2m[36m(pid=31637)[0m - observation_space = Box(9,)
[2m[36m(pid=31637)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31637)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31637)[0m - _max_episode_steps = 150
[2m[36m(pid=31637)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,613	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31642)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-0.64, max=1.0, mean=0.302),
[2m[36m(pid=31642)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=812648236.0, max=812648236.0, mean=812648236.0),
[2m[36m(pid=31642)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=31642)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-1.264, max=3.491, mean=0.242),
[2m[36m(pid=31642)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-1.264, max=3.491, mean=0.228),
[2m[36m(pid=31642)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-0.64, max=1.0, mean=0.296),
[2m[36m(pid=31642)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-1.472, max=3.371, mean=0.056),
[2m[36m(pid=31642)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-3.703, max=7.311, mean=0.168),
[2m[36m(pid=31642)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=31642)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.034, max=7.383, mean=1.033)},
[2m[36m(pid=31642)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31642)[0m 2019-07-24 02:33:08,713	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31642)[0m { 'data': { 'actions': np.ndarray((800, 2), dtype=float32, min=-0.719, max=1.0, mean=0.289),
[2m[36m(pid=31642)[0m             'agent_index': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m             'dones': np.ndarray((800,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m             'eps_id': np.ndarray((800,), dtype=int64, min=220122904.0, max=1927181589.0, mean=1081512334.25),
[2m[36m(pid=31642)[0m             'infos': np.ndarray((800,), dtype=object, head={}),
[2m[36m(pid=31642)[0m             'new_obs': np.ndarray((800, 9), dtype=float32, min=-1.537, max=4.11, mean=0.176),
[2m[36m(pid=31642)[0m             'obs': np.ndarray((800, 9), dtype=float32, min=-1.537, max=4.11, mean=0.163),
[2m[36m(pid=31642)[0m             'prev_actions': np.ndarray((800, 2), dtype=float32, min=-0.719, max=1.0, mean=0.283),
[2m[36m(pid=31642)[0m             'prev_rewards': np.ndarray((800,), dtype=float32, min=-5.647, max=4.995, mean=0.094),
[2m[36m(pid=31642)[0m             'rewards': np.ndarray((800,), dtype=float32, min=-15.629, max=14.249, mean=0.281),
[2m[36m(pid=31642)[0m             't': np.ndarray((800,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=31642)[0m             'unroll_id': np.ndarray((800,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31642)[0m             'weights': np.ndarray((800,), dtype=float32, min=0.0, max=15.561, mean=0.961)},
[2m[36m(pid=31642)[0m   'type': 'SampleBatch'}
[2m[36m(pid=31642)[0m 
[2m[36m(pid=31904)[0m [32m [     1.59370s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.59744s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.59838s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.59930s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60252s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60355s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60448s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60540s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60629s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60715s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.60799s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.61160s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.61259s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.61360s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31904)[0m [32m [     1.61451s,  INFO] TimeLimit:
[2m[36m(pid=31904)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31904)[0m - action_space = Box(2,)
[2m[36m(pid=31904)[0m - observation_space = Box(9,)
[2m[36m(pid=31904)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31904)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31904)[0m - _max_episode_steps = 150
[2m[36m(pid=31904)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.53842s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.53939s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54035s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54133s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54229s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54322s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54412s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54517s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54612s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54706s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54798s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54887s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.54976s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.55065s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31903)[0m [32m [     1.55159s,  INFO] TimeLimit:
[2m[36m(pid=31903)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31903)[0m - action_space = Box(2,)
[2m[36m(pid=31903)[0m - observation_space = Box(9,)
[2m[36m(pid=31903)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31903)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31903)[0m - _max_episode_steps = 150
[2m[36m(pid=31903)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m 2019-07-24 02:33:13,046	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31958)[0m 2019-07-24 02:33:13.047561: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31958)[0m [32m [     0.05683s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31958)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31643)[0m 2019-07-24 02:33:13,974	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m { 'count': 512,
[2m[36m(pid=31643)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=0.065),
[2m[36m(pid=31643)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=47.0, max=12757.0, mean=5147.141),
[2m[36m(pid=31643)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.018),
[2m[36m(pid=31643)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.151),
[2m[36m(pid=31643)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.137),
[2m[36m(pid=31643)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-40.451, max=39.519, mean=-1.763),
[2m[36m(pid=31643)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.035, max=0.137, mean=0.057)},
[2m[36m(pid=31643)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=31643)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m 2019-07-24 02:33:14,312	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31643)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.057792895,
[2m[36m(pid=31643)[0m                                          'mean_q': -0.27424878,
[2m[36m(pid=31643)[0m                                          'min_q': -1.1090813},
[2m[36m(pid=31643)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-39.744, max=40.104, mean=1.732)}}
[2m[36m(pid=31643)[0m 
[2m[36m(pid=31958)[0m [32m [     2.61687s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.61776s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.61861s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.61947s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62046s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62148s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62244s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62341s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62434s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62522s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62618s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62709s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62800s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62895s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31958)[0m [32m [     2.62985s,  INFO] TimeLimit:
[2m[36m(pid=31958)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31958)[0m - action_space = Box(2,)
[2m[36m(pid=31958)[0m - observation_space = Box(9,)
[2m[36m(pid=31958)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31958)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31958)[0m - _max_episode_steps = 150
[2m[36m(pid=31958)[0m - _elapsed_steps = None [0m
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-33-41
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 27.583448937147942
  episode_reward_mean: -11.177823318451534
  episode_reward_min: -55.788985643861714
  episodes_this_iter: 944
  episodes_total: 944
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 16.619047164916992
        mean_q: 1.7020070552825928
        min_q: -15.627065658569336
    learner_queue:
      size_count: 1465
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 393600
    num_steps_trained: 195072
    num_target_updates: 3
    num_weight_syncs: 492
    replay_shard_0:
      add_batch_time_ms: 90.211
      policy_default_policy:
        added_count: 99200
        est_size_bytes: 33827200
        num_entries: 99200
        sampled_count: 51200
      replay_time_ms: 43.085
      update_priorities_time_ms: 108.462
    sample_throughput: 0.0
    train_throughput: 20649.675
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.983353022767701
    mean_inference_ms: 2.7653977894964283
    mean_processing_ms: 4.390546471111894
  time_since_restore: 30.95015788078308
  time_this_iter_s: 30.95015788078308
  time_total_s: 30.95015788078308
  timestamp: 1563928421
  timesteps_since_restore: 393600
  timesteps_this_iter: 393600
  timesteps_total: 393600
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 8.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 30 s, 1 iter, 393600 ts, -11.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-34-12
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 12.698981950089046
  episode_reward_mean: -17.801459482458178
  episode_reward_min: -116.24506906208472
  episodes_this_iter: 944
  episodes_total: 1888
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 22.79669189453125
        mean_q: 1.3596587181091309
        min_q: -20.82674217224121
    learner_queue:
      size_count: 1885
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 787200
    num_steps_trained: 410112
    num_target_updates: 8
    num_weight_syncs: 984
    replay_shard_0:
      add_batch_time_ms: 99.599
      policy_default_policy:
        added_count: 184800
        est_size_bytes: 63016800
        num_entries: 184800
        sampled_count: 108032
      replay_time_ms: 33.895
      update_priorities_time_ms: 102.32
    sample_throughput: 23769.654
    train_throughput: 0.0
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.024388228525113
    mean_inference_ms: 2.821440876065225
    mean_processing_ms: 4.47534305084757
  time_since_restore: 61.85831832885742
  time_this_iter_s: 30.90816044807434
  time_total_s: 61.85831832885742
  timestamp: 1563928452
  timesteps_since_restore: 787200
  timesteps_this_iter: 393600
  timesteps_total: 787200
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 10.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 61 s, 2 iter, 787200 ts, -17.8 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-34-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.700500578226853
  episode_reward_mean: -10.397619350365511
  episode_reward_min: -41.26773821016107
  episodes_this_iter: 960
  episodes_total: 2848
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.236783981323242
        mean_q: 1.1430518627166748
        min_q: -17.073753356933594
    learner_queue:
      size_count: 2296
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 1184000
    num_steps_trained: 621056
    num_target_updates: 12
    num_weight_syncs: 1480
    replay_shard_0:
      add_batch_time_ms: 82.546
      policy_default_policy:
        added_count: 284800
        est_size_bytes: 97116800
        num_entries: 284800
        sampled_count: 161280
      replay_time_ms: 33.521
      update_priorities_time_ms: 107.642
    sample_throughput: 32013.006
    train_throughput: 0.0
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.057352934101117
    mean_inference_ms: 2.809866601891281
    mean_processing_ms: 4.495445932216713
  time_since_restore: 92.8188009262085
  time_this_iter_s: 30.960482597351074
  time_total_s: 92.8188009262085
  timestamp: 1563928483
  timesteps_since_restore: 1184000
  timesteps_this_iter: 396800
  timesteps_total: 1184000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 11.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 92 s, 3 iter, 1184000 ts, -10.4 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-35-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 19.44888155617185
  episode_reward_mean: -2.917923181738712
  episode_reward_min: -40.65864051988193
  episodes_this_iter: 928
  episodes_total: 3776
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 23.106250762939453
        mean_q: -1.3647539615631104
        min_q: -32.23725891113281
    learner_queue:
      size_count: 2706
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 1581600
    num_steps_trained: 830464
    num_target_updates: 16
    num_weight_syncs: 1977
    replay_shard_0:
      add_batch_time_ms: 116.494
      policy_default_policy:
        added_count: 375200
        est_size_bytes: 127943200
        num_entries: 375200
        sampled_count: 212992
      replay_time_ms: 49.564
      update_priorities_time_ms: 115.841
    sample_throughput: 0.0
    train_throughput: 25434.777
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.063872025293749
    mean_inference_ms: 2.8321389933803247
    mean_processing_ms: 4.489301438184462
  time_since_restore: 123.72788310050964
  time_this_iter_s: 30.909082174301147
  time_total_s: 123.72788310050964
  timestamp: 1563928514
  timesteps_since_restore: 1581600
  timesteps_this_iter: 397600
  timesteps_total: 1581600
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 123 s, 4 iter, 1581600 ts, -2.92 rew

[2m[36m(pid=31643)[0m 2019-07-24 02:35:45,744	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-35-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 20.413740040713456
  episode_reward_mean: -2.6016808790919574
  episode_reward_min: -35.746729908411055
  episodes_this_iter: 992
  episodes_total: 4768
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 15.481934763974957
    episode_reward_mean: -5.338812848828755
    episode_reward_min: -77.44457563560934
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.8398721839466465
      mean_inference_ms: 1.0976757633993766
      mean_processing_ms: 1.6499750818638477
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 22.02944564819336
        mean_q: -0.23918871581554413
        min_q: -32.1002082824707
    learner_queue:
      size_count: 3098
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 1983200
    num_steps_trained: 1031680
    num_target_updates: 20
    num_weight_syncs: 2479
    replay_shard_0:
      add_batch_time_ms: 73.956
      policy_default_policy:
        added_count: 480800
        est_size_bytes: 163952800
        num_entries: 480800
        sampled_count: 265216
      replay_time_ms: 32.611
      update_priorities_time_ms: 95.119
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.036966709370718
    mean_inference_ms: 2.792462192927227
    mean_processing_ms: 4.468518122773682
  time_since_restore: 154.72420692443848
  time_this_iter_s: 30.996323823928833
  time_total_s: 154.72420692443848
  timestamp: 1563928545
  timesteps_since_restore: 1983200
  timesteps_this_iter: 401600
  timesteps_total: 1983200
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 154 s, 5 iter, 1983200 ts, -2.6 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-36-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 29.793888288375417
  episode_reward_mean: -1.8247549486852983
  episode_reward_min: -35.95718560987025
  episodes_this_iter: 992
  episodes_total: 5760
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 17.0301570892334
        mean_q: 0.5824036002159119
        min_q: -15.065671920776367
    learner_queue:
      size_count: 3519
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 2380000
    num_steps_trained: 1246720
    num_target_updates: 24
    num_weight_syncs: 2975
    replay_shard_0:
      add_batch_time_ms: 80.37
      policy_default_policy:
        added_count: 584800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 317440
      replay_time_ms: 35.088
      update_priorities_time_ms: 121.522
    sample_throughput: 0.0
    train_throughput: 10899.826
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.023999311843206
    mean_inference_ms: 2.7695025002249882
    mean_processing_ms: 4.47424498018947
  time_since_restore: 185.67153143882751
  time_this_iter_s: 30.947324514389038
  time_total_s: 185.67153143882751
  timestamp: 1563928578
  timesteps_since_restore: 2380000
  timesteps_this_iter: 396800
  timesteps_total: 2380000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 185 s, 6 iter, 2380000 ts, -1.82 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-36-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.694435964562004
  episode_reward_mean: 1.274627600237961
  episode_reward_min: -35.36280484592441
  episodes_this_iter: 960
  episodes_total: 6720
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.703140258789062
        mean_q: 1.6589356660842896
        min_q: -17.53734588623047
    learner_queue:
      size_count: 3929
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 2780000
    num_steps_trained: 1457152
    num_target_updates: 29
    num_weight_syncs: 3475
    replay_shard_0:
      add_batch_time_ms: 86.553
      policy_default_policy:
        added_count: 682400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 372736
      replay_time_ms: 41.421
      update_priorities_time_ms: 96.96
    sample_throughput: 0.0
    train_throughput: 31540.752
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.018084588769023
    mean_inference_ms: 2.7623743417004554
    mean_processing_ms: 4.4615882021539885
  time_since_restore: 216.72792887687683
  time_this_iter_s: 31.056397438049316
  time_total_s: 216.72792887687683
  timestamp: 1563928609
  timesteps_since_restore: 2780000
  timesteps_this_iter: 400000
  timesteps_total: 2780000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 216 s, 7 iter, 2780000 ts, 1.27 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-37-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 34.69282769024221
  episode_reward_mean: 4.145250761032814
  episode_reward_min: -33.37773757420503
  episodes_this_iter: 944
  episodes_total: 7664
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.568140029907227
        mean_q: 1.7720983028411865
        min_q: -14.918519973754883
    learner_queue:
      size_count: 4342
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 3179200
    num_steps_trained: 1668608
    num_target_updates: 33
    num_weight_syncs: 3974
    replay_shard_0:
      add_batch_time_ms: 72.264
      policy_default_policy:
        added_count: 795200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 420352
      replay_time_ms: 45.4
      update_priorities_time_ms: 116.926
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.019508603038479
    mean_inference_ms: 2.770160906986746
    mean_processing_ms: 4.461929556450018
  time_since_restore: 247.7143692970276
  time_this_iter_s: 30.986440420150757
  time_total_s: 247.7143692970276
  timestamp: 1563928640
  timesteps_since_restore: 3179200
  timesteps_this_iter: 399200
  timesteps_total: 3179200
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 247 s, 8 iter, 3179200 ts, 4.15 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-37-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.8611420024927
  episode_reward_mean: 10.243240268047078
  episode_reward_min: -26.77170543116778
  episodes_this_iter: 1008
  episodes_total: 8672
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 20.790170669555664
        mean_q: 1.7744529247283936
        min_q: -14.628541946411133
    learner_queue:
      size_count: 4745
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 3581600
    num_steps_trained: 1874944
    num_target_updates: 37
    num_weight_syncs: 4477
    replay_shard_0:
      add_batch_time_ms: 84.024
      policy_default_policy:
        added_count: 888000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 474112
      replay_time_ms: 40.406
      update_priorities_time_ms: 115.211
    sample_throughput: 34821.831
    train_throughput: 14857.314
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.01329793936188
    mean_inference_ms: 2.7533644363253327
    mean_processing_ms: 4.45939292824997
  time_since_restore: 278.7028212547302
  time_this_iter_s: 30.988451957702637
  time_total_s: 278.7028212547302
  timestamp: 1563928671
  timesteps_since_restore: 3581600
  timesteps_this_iter: 402400
  timesteps_total: 3581600
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 278 s, 9 iter, 3581600 ts, 10.2 rew

[2m[36m(pid=31643)[0m 2019-07-24 02:38:22,594	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-38-22
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.55012626409473
  episode_reward_mean: 12.502279001001542
  episode_reward_min: -27.80327901363907
  episodes_this_iter: 960
  episodes_total: 9632
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 35.04027537452158
    episode_reward_mean: 11.969889388818213
    episode_reward_min: -24.73957493926724
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.446400463945134
      mean_inference_ms: 0.9154693607751824
      mean_processing_ms: 1.4031149161517253
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 25.821941375732422
        mean_q: 2.4862258434295654
        min_q: -10.729113578796387
    learner_queue:
      size_count: 5137
      size_mean: 0.06
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.23748684174075835
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 3985600
    num_steps_trained: 2075648
    num_target_updates: 41
    num_weight_syncs: 4982
    replay_shard_0:
      add_batch_time_ms: 68.146
      policy_default_policy:
        added_count: 989600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 526336
      replay_time_ms: 35.726
      update_priorities_time_ms: 113.57
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 6.002216236516641
    mean_inference_ms: 2.751049926786374
    mean_processing_ms: 4.458090261918562
  time_since_restore: 309.6607098579407
  time_this_iter_s: 30.95788860321045
  time_total_s: 309.6607098579407
  timestamp: 1563928702
  timesteps_since_restore: 3985600
  timesteps_this_iter: 404000
  timesteps_total: 3985600
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 309 s, 10 iter, 3985600 ts, 12.5 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-38-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.41395445241428
  episode_reward_mean: 15.709780668065982
  episode_reward_min: -15.217950057095061
  episodes_this_iter: 976
  episodes_total: 10608
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.276397705078125
        mean_q: 2.891653060913086
        min_q: -8.643415451049805
    learner_queue:
      size_count: 5549
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 4384800
    num_steps_trained: 2286592
    num_target_updates: 45
    num_weight_syncs: 5481
    replay_shard_0:
      add_batch_time_ms: 90.206
      policy_default_policy:
        added_count: 1077600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 584704
      replay_time_ms: 38.52
      update_priorities_time_ms: 107.211
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.995992458159869
    mean_inference_ms: 2.7456692042710205
    mean_processing_ms: 4.452068244055533
  time_since_restore: 340.5806052684784
  time_this_iter_s: 30.91989541053772
  time_total_s: 340.5806052684784
  timestamp: 1563928735
  timesteps_since_restore: 4384800
  timesteps_this_iter: 399200
  timesteps_total: 4384800
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 340 s, 11 iter, 4384800 ts, 15.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-39-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.75971159584199
  episode_reward_mean: 17.211420061395593
  episode_reward_min: -12.99024652619598
  episodes_this_iter: 960
  episodes_total: 11568
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 25.755205154418945
        mean_q: 3.5321121215820312
        min_q: -7.479418754577637
    learner_queue:
      size_count: 5959
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 4781600
    num_steps_trained: 2496000
    num_target_updates: 49
    num_weight_syncs: 5977
    replay_shard_0:
      add_batch_time_ms: 109.974
      policy_default_policy:
        added_count: 1180800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 637440
      replay_time_ms: 40.64
      update_priorities_time_ms: 111.405
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.9938890274842755
    mean_inference_ms: 2.7556844843757213
    mean_processing_ms: 4.454090867957276
  time_since_restore: 371.4871189594269
  time_this_iter_s: 30.906513690948486
  time_total_s: 371.4871189594269
  timestamp: 1563928766
  timesteps_since_restore: 4781600
  timesteps_this_iter: 396800
  timesteps_total: 4781600
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 371 s, 12 iter, 4781600 ts, 17.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-39-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.55955282074174
  episode_reward_mean: 16.890485112718224
  episode_reward_min: -15.768602424672638
  episodes_this_iter: 976
  episodes_total: 12544
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.5054931640625
        mean_q: 3.8211870193481445
        min_q: -6.571774959564209
    learner_queue:
      size_count: 6370
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 5180000
    num_steps_trained: 2706944
    num_target_updates: 53
    num_weight_syncs: 6475
    replay_shard_0:
      add_batch_time_ms: 99.827
      policy_default_policy:
        added_count: 1283200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 692736
      replay_time_ms: 38.484
      update_priorities_time_ms: 96.817
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.986701462865297
    mean_inference_ms: 2.7481918995674977
    mean_processing_ms: 4.452634440138729
  time_since_restore: 402.45144748687744
  time_this_iter_s: 30.96432852745056
  time_total_s: 402.45144748687744
  timestamp: 1563928797
  timesteps_since_restore: 5180000
  timesteps_this_iter: 398400
  timesteps_total: 5180000
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 402 s, 13 iter, 5180000 ts, 16.9 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-40-28
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.148814801529966
  episode_reward_mean: 17.31305064912231
  episode_reward_min: -15.859476902092359
  episodes_this_iter: 976
  episodes_total: 13520
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.17042350769043
        mean_q: 3.099510908126831
        min_q: -5.050930976867676
    learner_queue:
      size_count: 6777
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5219195340279955
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 5577600
    num_steps_trained: 2915328
    num_target_updates: 58
    num_weight_syncs: 6972
    replay_shard_0:
      add_batch_time_ms: 65.185
      policy_default_policy:
        added_count: 1386400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 741376
      replay_time_ms: 35.405
      update_priorities_time_ms: 98.672
    sample_throughput: 0.0
    train_throughput: 25374.07
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.987034245529249
    mean_inference_ms: 2.746667607646302
    mean_processing_ms: 4.453360982578486
  time_since_restore: 433.31319522857666
  time_this_iter_s: 30.86174774169922
  time_total_s: 433.31319522857666
  timestamp: 1563928828
  timesteps_since_restore: 5577600
  timesteps_this_iter: 397600
  timesteps_total: 5577600
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 433 s, 14 iter, 5577600 ts, 17.3 rew

[2m[36m(pid=31643)[0m 2019-07-24 02:40:59,046	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-40-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.60213472380723
  episode_reward_mean: 17.15841032611014
  episode_reward_min: -9.49123337218229
  episodes_this_iter: 944
  episodes_total: 14464
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 35.883364982757854
    episode_reward_mean: 15.861227626006821
    episode_reward_min: -2.77030960303372
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.2877769859988444
      mean_inference_ms: 0.8443107291466921
      mean_processing_ms: 1.3063647703613934
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.07976150512695
        mean_q: 3.0691463947296143
        min_q: -4.093217849731445
    learner_queue:
      size_count: 7182
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.46
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 5977600
    num_steps_trained: 3122176
    num_target_updates: 62
    num_weight_syncs: 7472
    replay_shard_0:
      add_batch_time_ms: 86.492
      policy_default_policy:
        added_count: 1476800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 797696
      replay_time_ms: 33.159
      update_priorities_time_ms: 95.771
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.9917822044878495
    mean_inference_ms: 2.747602258586314
    mean_processing_ms: 4.460397797646401
  time_since_restore: 464.1876275539398
  time_this_iter_s: 30.87443232536316
  time_total_s: 464.1876275539398
  timestamp: 1563928859
  timesteps_since_restore: 5977600
  timesteps_this_iter: 400000
  timesteps_total: 5977600
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 464 s, 15 iter, 5977600 ts, 17.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-41-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.88593475321909
  episode_reward_mean: 17.789844031874136
  episode_reward_min: -11.961368802011974
  episodes_this_iter: 992
  episodes_total: 15456
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.81388854980469
        mean_q: 3.720677375793457
        min_q: -2.9641103744506836
    learner_queue:
      size_count: 7586
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.32496153618543844
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 6377600
    num_steps_trained: 3329024
    num_target_updates: 66
    num_weight_syncs: 7972
    replay_shard_0:
      add_batch_time_ms: 102.715
      policy_default_policy:
        added_count: 1584000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 844800
      replay_time_ms: 40.157
      update_priorities_time_ms: 104.007
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.981108531245604
    mean_inference_ms: 2.7366353004467747
    mean_processing_ms: 4.456130996134603
  time_since_restore: 495.16719818115234
  time_this_iter_s: 30.979570627212524
  time_total_s: 495.16719818115234
  timestamp: 1563928891
  timesteps_since_restore: 6377600
  timesteps_this_iter: 400000
  timesteps_total: 6377600
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 495 s, 16 iter, 6377600 ts, 17.8 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-42-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.429391938493794
  episode_reward_mean: 18.19890003405733
  episode_reward_min: -14.230033417713358
  episodes_this_iter: 976
  episodes_total: 16432
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.33953857421875
        mean_q: 3.836341381072998
        min_q: -2.644686222076416
    learner_queue:
      size_count: 7987
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.32496153618543844
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 6778400
    num_steps_trained: 3534336
    num_target_updates: 70
    num_weight_syncs: 8473
    replay_shard_0:
      add_batch_time_ms: 77.352
      policy_default_policy:
        added_count: 1678400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 897024
      replay_time_ms: 30.25
      update_priorities_time_ms: 99.51
    sample_throughput: 26211.943
    train_throughput: 0.0
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.977478026108695
    mean_inference_ms: 2.739266395843784
    mean_processing_ms: 4.456879805399618
  time_since_restore: 526.0949342250824
  time_this_iter_s: 30.927736043930054
  time_total_s: 526.0949342250824
  timestamp: 1563928922
  timesteps_since_restore: 6778400
  timesteps_this_iter: 400800
  timesteps_total: 6778400
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 526 s, 17 iter, 6778400 ts, 18.2 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-42-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.03752669369662
  episode_reward_mean: 17.440382903963595
  episode_reward_min: -22.587441604245125
  episodes_this_iter: 960
  episodes_total: 17392
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.1185417175293
        mean_q: 4.207344055175781
        min_q: -2.2917656898498535
    learner_queue:
      size_count: 8404
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 7174400
    num_steps_trained: 3748352
    num_target_updates: 74
    num_weight_syncs: 8968
    replay_shard_0:
      add_batch_time_ms: 103.538
      policy_default_policy:
        added_count: 1765600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 951296
      replay_time_ms: 48.306
      update_priorities_time_ms: 108.891
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.980139506666388
    mean_inference_ms: 2.7419111678197576
    mean_processing_ms: 4.463192419475062
  time_since_restore: 556.9943242073059
  time_this_iter_s: 30.89938998222351
  time_total_s: 556.9943242073059
  timestamp: 1563928953
  timesteps_since_restore: 7174400
  timesteps_this_iter: 396000
  timesteps_total: 7174400
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 556 s, 18 iter, 7174400 ts, 17.4 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-43-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.85111929653452
  episode_reward_mean: 18.83889026624862
  episode_reward_min: -8.63489004299124
  episodes_this_iter: 976
  episodes_total: 18368
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.391944885253906
        mean_q: 4.225975036621094
        min_q: -1.3961412906646729
    learner_queue:
      size_count: 8805
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5219195340279956
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 7574400
    num_steps_trained: 3953152
    num_target_updates: 78
    num_weight_syncs: 9468
    replay_shard_0:
      add_batch_time_ms: 92.858
      policy_default_policy:
        added_count: 1861600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1007616
      replay_time_ms: 37.178
      update_priorities_time_ms: 118.829
    sample_throughput: 0.0
    train_throughput: 15531.538
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.979526791058915
    mean_inference_ms: 2.7453318153853443
    mean_processing_ms: 4.462293679965789
  time_since_restore: 587.9007029533386
  time_this_iter_s: 30.906378746032715
  time_total_s: 587.9007029533386
  timestamp: 1563928984
  timesteps_since_restore: 7574400
  timesteps_this_iter: 400000
  timesteps_total: 7574400
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 587 s, 19 iter, 7574400 ts, 18.8 rew

[2m[36m(pid=31643)[0m 2019-07-24 02:43:35,162	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-43-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.002750373442
  episode_reward_mean: 18.687545054167575
  episode_reward_min: -12.586857487819321
  episodes_this_iter: 960
  episodes_total: 19328
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 36.44784843720851
    episode_reward_mean: 17.45966770016751
    episode_reward_min: -13.10376094695651
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.150691462917856
      mean_inference_ms: 0.7834945465965634
      mean_processing_ms: 1.223479350881801
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.90962600708008
        mean_q: 4.247777938842773
        min_q: -1.1982674598693848
    learner_queue:
      size_count: 9209
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.41424630354415964
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 7974400
    num_steps_trained: 4160512
    num_target_updates: 82
    num_weight_syncs: 9968
    replay_shard_0:
      add_batch_time_ms: 89.135
      policy_default_policy:
        added_count: 1962400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1060864
      replay_time_ms: 40.613
      update_priorities_time_ms: 113.668
    sample_throughput: 0.0
    train_throughput: 46533.698
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.976311777859753
    mean_inference_ms: 2.745946937030563
    mean_processing_ms: 4.458860188393108
  time_since_restore: 618.840854883194
  time_this_iter_s: 30.940151929855347
  time_total_s: 618.840854883194
  timestamp: 1563929015
  timesteps_since_restore: 7974400
  timesteps_this_iter: 400000
  timesteps_total: 7974400
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 618 s, 20 iter, 7974400 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-44-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.61490904846593
  episode_reward_mean: 17.978470186450163
  episode_reward_min: -13.577072847726491
  episodes_this_iter: 960
  episodes_total: 20288
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.97649002075195
        mean_q: 4.862871170043945
        min_q: -0.7685731053352356
    learner_queue:
      size_count: 9612
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 8375200
    num_steps_trained: 4366848
    num_target_updates: 87
    num_weight_syncs: 10469
    replay_shard_0:
      add_batch_time_ms: 65.774
      policy_default_policy:
        added_count: 2063200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1112064
      replay_time_ms: 39.671
      update_priorities_time_ms: 88.902
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.977435336929587
    mean_inference_ms: 2.742772665342705
    mean_processing_ms: 4.462180211345594
  time_since_restore: 649.7751216888428
  time_this_iter_s: 30.934266805648804
  time_total_s: 649.7751216888428
  timestamp: 1563929047
  timesteps_since_restore: 8375200
  timesteps_this_iter: 400800
  timesteps_total: 8375200
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 649 s, 21 iter, 8375200 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-44-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.39990790428746
  episode_reward_mean: 18.476394364683117
  episode_reward_min: -12.639643032759063
  episodes_this_iter: 976
  episodes_total: 21264
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.744468688964844
        mean_q: 4.76255464553833
        min_q: -0.26214832067489624
    learner_queue:
      size_count: 10009
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 8777600
    num_steps_trained: 4570112
    num_target_updates: 91
    num_weight_syncs: 10972
    replay_shard_0:
      add_batch_time_ms: 73.586
      policy_default_policy:
        added_count: 2166400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1166848
      replay_time_ms: 39.0
      update_priorities_time_ms: 89.694
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.9683946966985175
    mean_inference_ms: 2.7460150406002075
    mean_processing_ms: 4.458844378164339
  time_since_restore: 680.8063282966614
  time_this_iter_s: 31.031206607818604
  time_total_s: 680.8063282966614
  timestamp: 1563929079
  timesteps_since_restore: 8777600
  timesteps_this_iter: 402400
  timesteps_total: 8777600
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 680 s, 22 iter, 8777600 ts, 18.5 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-45-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.123238567958545
  episode_reward_mean: 17.911562839522855
  episode_reward_min: -16.0426475978106
  episodes_this_iter: 992
  episodes_total: 22256
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.220523834228516
        mean_q: 4.1265950202941895
        min_q: -1.0186223983764648
    learner_queue:
      size_count: 10401
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 9180000
    num_steps_trained: 4770816
    num_target_updates: 95
    num_weight_syncs: 11475
    replay_shard_0:
      add_batch_time_ms: 79.655
      policy_default_policy:
        added_count: 2266400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1218048
      replay_time_ms: 36.271
      update_priorities_time_ms: 111.404
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.9636388165650995
    mean_inference_ms: 2.743569676610097
    mean_processing_ms: 4.460218938283005
  time_since_restore: 711.8281450271606
  time_this_iter_s: 31.021816730499268
  time_total_s: 711.8281450271606
  timestamp: 1563929110
  timesteps_since_restore: 9180000
  timesteps_this_iter: 402400
  timesteps_total: 9180000
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 711 s, 23 iter, 9180000 ts, 17.9 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-45-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.82313158037881
  episode_reward_mean: 18.682671027684453
  episode_reward_min: -6.829944935784475
  episodes_this_iter: 960
  episodes_total: 23216
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.845001220703125
        mean_q: 4.0157551765441895
        min_q: -0.6685022115707397
    learner_queue:
      size_count: 10809
      size_mean: 0.24
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.47159304490206383
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 9576800
    num_steps_trained: 4979200
    num_target_updates: 99
    num_weight_syncs: 11971
    replay_shard_0:
      add_batch_time_ms: 80.796
      policy_default_policy:
        added_count: 2372800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1267712
      replay_time_ms: 30.067
      update_priorities_time_ms: 84.535
    sample_throughput: 0.0
    train_throughput: 29273.622
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.963498884436692
    mean_inference_ms: 2.7442999505986916
    mean_processing_ms: 4.463245666407287
  time_since_restore: 742.6849093437195
  time_this_iter_s: 30.856764316558838
  time_total_s: 742.6849093437195
  timestamp: 1563929140
  timesteps_since_restore: 9576800
  timesteps_this_iter: 396800
  timesteps_total: 9576800
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 742 s, 24 iter, 9576800 ts, 18.7 rew

[2m[36m(pid=31643)[0m 2019-07-24 02:46:11,907	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-46-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.077130336728096
  episode_reward_mean: 18.312361955124512
  episode_reward_min: -8.242039655991006
  episodes_this_iter: 992
  episodes_total: 24208
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 38.30383420199526
    episode_reward_mean: 19.211489300893813
    episode_reward_min: -1.5521908696579187
    episodes_this_iter: 60
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 2.06313206471764
      mean_inference_ms: 0.7440081474505379
      mean_processing_ms: 1.1706931872217254
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.09005355834961
        mean_q: 4.270931243896484
        min_q: -0.6309820413589478
    learner_queue:
      size_count: 11217
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3249615361854384
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 9975200
    num_steps_trained: 5188608
    num_target_updates: 103
    num_weight_syncs: 12469
    replay_shard_0:
      add_batch_time_ms: 105.428
      policy_default_policy:
        added_count: 2472000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1318400
      replay_time_ms: 35.969
      update_priorities_time_ms: 108.91
    sample_throughput: 32032.259
    train_throughput: 0.0
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.9612877300289435
    mean_inference_ms: 2.7455224662088624
    mean_processing_ms: 4.460456486809422
  time_since_restore: 773.6732954978943
  time_this_iter_s: 30.988386154174805
  time_total_s: 773.6732954978943
  timestamp: 1563929171
  timesteps_since_restore: 9975200
  timesteps_this_iter: 398400
  timesteps_total: 9975200
  training_iteration: 25
  2019-07-24 02:46:44,862	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-24 02:46:44,869	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 14.0/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	RUNNING, [12 CPUs, 1 GPUs], [pid=31643], 773 s, 25 iter, 9975200 ts, 18.3 rew

Result for APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:
  custom_metrics: {}
  date: 2019-07-24_02-46-44
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.477019611484856
  episode_reward_mean: 18.749704967053674
  episode_reward_min: -3.4712231496433317
  episodes_this_iter: 960
  episodes_total: 25168
  experiment_id: 50b9abe80ebd4847b5c9daa3f3748c89
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 37.0822868347168
        mean_q: 4.810225486755371
        min_q: -0.13969048857688904
    learner_queue:
      size_count: 11626
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4142463035441596
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 11
    num_steps_sampled: 10373600
    num_steps_trained: 5398016
    num_target_updates: 107
    num_weight_syncs: 12967
    replay_shard_0:
      add_batch_time_ms: 50.27
      policy_default_policy:
        added_count: 2567200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1373696
      replay_time_ms: 39.733
      update_priorities_time_ms: 110.008
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31643
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 5.961906240163904
    mean_inference_ms: 2.746716165078093
    mean_processing_ms: 4.459286470282686
  time_since_restore: 804.7192833423615
  time_this_iter_s: 31.045987844467163
  time_total_s: 804.7192833423615
  timestamp: 1563929204
  timesteps_since_restore: 10373600
  timesteps_this_iter: 398400
  timesteps_total: 10373600
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 14.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'PENDING': 2})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	PENDING
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m [32m [     0.03177s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m 2019-07-24 02:46:45.003459: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31967)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31967)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31967)[0m [32m [     1.43412s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     1.43460s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     1.43505s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     1.43563s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     1.43612s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     1.43657s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     1.43704s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m 2019-07-24 02:46:46,403	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fab16dfec18>}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:46,403	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fab16dde198>}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:46,403	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fab16e25b00>}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:46,409	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=31967)[0m 2019-07-24 02:46:46,418	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=31905)[0m 2019-07-24 02:46:46,536	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31905)[0m 2019-07-24 02:46:46.536857: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31905)[0m [32m [     0.05340s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     0.04091s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m 2019-07-24 02:46:46,525	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31959)[0m 2019-07-24 02:46:46.525744: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31965)[0m 2019-07-24 02:46:46,504	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=31965)[0m 2019-07-24 02:46:46.504716: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=31965)[0m [32m [     0.04121s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31965)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31905)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31905)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31959)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31959)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=31967)[0m [32m [     1.99918s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.79808s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.79906s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.79994s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.80080s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.80186s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.80275s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m [32m [     2.80351s,  INFO] TimeLimit:
[2m[36m(pid=31965)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31965)[0m - action_space = Box(2,)
[2m[36m(pid=31965)[0m - observation_space = Box(9,)
[2m[36m(pid=31965)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31965)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31965)[0m - _max_episode_steps = 150
[2m[36m(pid=31965)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,305	INFO rollout_worker.py:428 -- Generating sample batch of size 400
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,393	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.866, max=0.205, mean=-0.161)},
[2m[36m(pid=31965)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.056, max=0.788, mean=0.248)},
[2m[36m(pid=31965)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.392, max=0.92, mean=0.074)},
[2m[36m(pid=31965)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.982, max=0.317, mean=-0.108)},
[2m[36m(pid=31965)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.994, max=0.659, mean=-0.043)},
[2m[36m(pid=31965)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.982, max=0.221, mean=-0.101)},
[2m[36m(pid=31965)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.574, max=0.819, mean=0.087)},
[2m[36m(pid=31965)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.084, max=0.994, mean=0.19)}}
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,394	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=31965)[0m   1: {'agent0': None},
[2m[36m(pid=31965)[0m   2: {'agent0': None},
[2m[36m(pid=31965)[0m   3: {'agent0': None},
[2m[36m(pid=31965)[0m   4: {'agent0': None},
[2m[36m(pid=31965)[0m   5: {'agent0': None},
[2m[36m(pid=31965)[0m   6: {'agent0': None},
[2m[36m(pid=31965)[0m   7: {'agent0': None}}
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,394	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.866, max=0.205, mean=-0.161)
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,394	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.866, max=0.205, mean=-0.161)
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,401	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31965)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 0,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.866, max=0.205, mean=-0.161),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 1,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.056, max=0.788, mean=0.248),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 2,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.392, max=0.92, mean=0.074),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 3,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.982, max=0.317, mean=-0.108),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 4,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.994, max=0.659, mean=-0.043),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 5,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.982, max=0.221, mean=-0.101),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 6,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.574, max=0.819, mean=0.087),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31965)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31965)[0m                                   'env_id': 7,
[2m[36m(pid=31965)[0m                                   'info': None,
[2m[36m(pid=31965)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.084, max=0.994, mean=0.19),
[2m[36m(pid=31965)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31965)[0m                                   'rnn_state': []},
[2m[36m(pid=31965)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,401	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,444	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31965)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=0.002, max=0.821, mean=0.432),
[2m[36m(pid=31965)[0m                       [],
[2m[36m(pid=31965)[0m                       {})}
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31905)[0m [32m [     3.02907s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31905)[0m [32m [     3.03006s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31905)[0m [32m [     3.03096s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31905)[0m [32m [     3.03183s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31905)[0m [32m [     3.03269s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31905)[0m [32m [     3.03357s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31905)[0m [32m [     3.03445s,  INFO] TimeLimit:
[2m[36m(pid=31905)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31905)[0m - action_space = Box(2,)
[2m[36m(pid=31905)[0m - observation_space = Box(9,)
[2m[36m(pid=31905)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31905)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31905)[0m - _max_episode_steps = 150
[2m[36m(pid=31905)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.23742s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.23828s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.23911s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.23991s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.25658s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.25753s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31959)[0m [32m [     3.25841s,  INFO] TimeLimit:
[2m[36m(pid=31959)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31959)[0m - action_space = Box(2,)
[2m[36m(pid=31959)[0m - observation_space = Box(9,)
[2m[36m(pid=31959)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31959)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31959)[0m - _max_episode_steps = 150
[2m[36m(pid=31959)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,745	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31965)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.1),
[2m[36m(pid=31965)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=174979954.0, max=174979954.0, mean=174979954.0),
[2m[36m(pid=31965)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=31965)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-1.0, max=2.154, mean=0.102),
[2m[36m(pid=31965)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-1.0, max=2.154, mean=0.086),
[2m[36m(pid=31965)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.109),
[2m[36m(pid=31965)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-0.2, max=2.699, mean=0.38),
[2m[36m(pid=31965)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-0.571, max=6.279, mean=1.216),
[2m[36m(pid=31965)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=31965)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.03, max=6.304, mean=1.475)},
[2m[36m(pid=31965)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31965)[0m 2019-07-24 02:46:49,807	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31965)[0m { 'data': { 'actions': np.ndarray((400, 2), dtype=float32, min=-1.0, max=1.0, mean=0.085),
[2m[36m(pid=31965)[0m             'agent_index': np.ndarray((400,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m             'dones': np.ndarray((400,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m             'eps_id': np.ndarray((400,), dtype=int64, min=152267880.0, max=1947688905.0, mean=1102316046.125),
[2m[36m(pid=31965)[0m             'infos': np.ndarray((400,), dtype=object, head={}),
[2m[36m(pid=31965)[0m             'new_obs': np.ndarray((400, 9), dtype=float32, min=-1.241, max=2.491, mean=0.07),
[2m[36m(pid=31965)[0m             'obs': np.ndarray((400, 9), dtype=float32, min=-1.241, max=2.491, mean=0.075),
[2m[36m(pid=31965)[0m             'prev_actions': np.ndarray((400, 2), dtype=float32, min=-1.0, max=1.0, mean=0.094),
[2m[36m(pid=31965)[0m             'prev_rewards': np.ndarray((400,), dtype=float32, min=-4.243, max=2.699, mean=-0.041),
[2m[36m(pid=31965)[0m             'rewards': np.ndarray((400,), dtype=float32, min=-10.141, max=6.279, mean=-0.124),
[2m[36m(pid=31965)[0m             't': np.ndarray((400,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=31965)[0m             'unroll_id': np.ndarray((400,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31965)[0m             'weights': np.ndarray((400,), dtype=float32, min=0.001, max=10.213, mean=1.094)},
[2m[36m(pid=31965)[0m   'type': 'SampleBatch'}
[2m[36m(pid=31965)[0m 
[2m[36m(pid=31967)[0m [32m [     4.86954s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     4.87580s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     4.87675s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     4.87765s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     4.87864s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     4.87948s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m [32m [     4.88038s,  INFO] TimeLimit:
[2m[36m(pid=31967)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=31967)[0m - action_space = Box(2,)
[2m[36m(pid=31967)[0m - observation_space = Box(9,)
[2m[36m(pid=31967)[0m - reward_range = (-inf, inf)
[2m[36m(pid=31967)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=31967)[0m - _max_episode_steps = 150
[2m[36m(pid=31967)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m 2019-07-24 02:46:49,839	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7fab0e1e9a58>}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:49,839	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fab0e1e9710>}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:49,839	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fab0e1ddac8>}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:49,854	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=31967)[0m 2019-07-24 02:46:49,954	INFO rollout_worker.py:428 -- Generating sample batch of size 400
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,155	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.045, max=0.999, mean=0.176)},
[2m[36m(pid=31967)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.773, max=0.262, mean=-0.235)},
[2m[36m(pid=31967)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.712, max=1.024, mean=0.114)},
[2m[36m(pid=31967)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.899, max=0.437, mean=-0.063)},
[2m[36m(pid=31967)[0m   4: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.799, max=0.601, mean=-0.011)},
[2m[36m(pid=31967)[0m   5: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.111, max=0.998, mean=0.151)},
[2m[36m(pid=31967)[0m   6: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.982, max=0.199, mean=-0.138)},
[2m[36m(pid=31967)[0m   7: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.564, max=0.826, mean=0.023)}}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,156	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=31967)[0m   1: {'agent0': None},
[2m[36m(pid=31967)[0m   2: {'agent0': None},
[2m[36m(pid=31967)[0m   3: {'agent0': None},
[2m[36m(pid=31967)[0m   4: {'agent0': None},
[2m[36m(pid=31967)[0m   5: {'agent0': None},
[2m[36m(pid=31967)[0m   6: {'agent0': None},
[2m[36m(pid=31967)[0m   7: {'agent0': None}}
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,156	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.045, max=0.999, mean=0.176)
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,156	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.045, max=0.999, mean=0.176)
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,162	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 0,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.045, max=0.999, mean=0.176),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 1,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.773, max=0.262, mean=-0.235),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 2,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.712, max=1.024, mean=0.114),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 3,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.899, max=0.437, mean=-0.063),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 4,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.799, max=0.601, mean=-0.011),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 5,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.111, max=0.998, mean=0.151),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 6,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.982, max=0.199, mean=-0.138),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=31967)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=31967)[0m                                   'env_id': 7,
[2m[36m(pid=31967)[0m                                   'info': None,
[2m[36m(pid=31967)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.564, max=0.826, mean=0.023),
[2m[36m(pid=31967)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=31967)[0m                                   'rnn_state': []},
[2m[36m(pid=31967)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,172	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=31967)[0m 2019-07-24 02:46:50,253	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m { 'default_policy': ( np.ndarray((8, 2), dtype=float32, min=-0.252, max=0.21, mean=-0.01),
[2m[36m(pid=31967)[0m                       [],
[2m[36m(pid=31967)[0m                       {})}
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m 2019-07-24 02:46:51,144	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.193),
[2m[36m(pid=31967)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=31967)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=1365973770.0, max=1365973770.0, mean=1365973770.0),
[2m[36m(pid=31967)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=31967)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-4.75, max=4.088, mean=-0.129),
[2m[36m(pid=31967)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-4.75, max=4.088, mean=-0.123),
[2m[36m(pid=31967)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.195),
[2m[36m(pid=31967)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-4.226, max=2.801, mean=-0.233),
[2m[36m(pid=31967)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-11.766, max=7.832, mean=-0.71),
[2m[36m(pid=31967)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=31967)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.022, max=11.53, mean=2.323)},
[2m[36m(pid=31967)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m 2019-07-24 02:46:51,251	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m { 'data': { 'actions': np.ndarray((450, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.192),
[2m[36m(pid=31967)[0m             'agent_index': np.ndarray((450,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m             'dones': np.ndarray((450,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=31967)[0m             'eps_id': np.ndarray((450,), dtype=int64, min=1365973770.0, max=1989261635.0, mean=1643580915.0),
[2m[36m(pid=31967)[0m             'infos': np.ndarray((450,), dtype=object, head={}),
[2m[36m(pid=31967)[0m             'new_obs': np.ndarray((450, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.257),
[2m[36m(pid=31967)[0m             'obs': np.ndarray((450, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.247),
[2m[36m(pid=31967)[0m             'prev_actions': np.ndarray((450, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.193),
[2m[36m(pid=31967)[0m             'prev_rewards': np.ndarray((450,), dtype=float32, min=-29.917, max=25.544, mean=-0.306),
[2m[36m(pid=31967)[0m             'rewards': np.ndarray((450,), dtype=float32, min=-40.802, max=35.459, mean=-1.0),
[2m[36m(pid=31967)[0m             't': np.ndarray((450,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=31967)[0m             'unroll_id': np.ndarray((450,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=31967)[0m             'weights': np.ndarray((450,), dtype=float32, min=0.019, max=40.922, mean=5.573)},
[2m[36m(pid=31967)[0m   'type': 'SampleBatch'}
[2m[36m(pid=31967)[0m 
[2m[36m(pid=32202)[0m 2019-07-24 02:46:51,952	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32202)[0m 2019-07-24 02:46:51.953000: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32202)[0m [32m [     0.03798s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     0.06610s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m 2019-07-24 02:46:52,137	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32198)[0m 2019-07-24 02:46:52.138297: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32202)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32202)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32198)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32198)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32200)[0m 2019-07-24 02:46:52,272	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32200)[0m 2019-07-24 02:46:52.272992: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32200)[0m [32m [     0.04344s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32200)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32197)[0m 2019-07-24 02:46:52,399	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32197)[0m 2019-07-24 02:46:52.400404: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32199)[0m 2019-07-24 02:46:52,381	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32199)[0m 2019-07-24 02:46:52.382198: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32199)[0m [32m [     0.06702s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     0.03955s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     0.04403s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m 2019-07-24 02:46:52,465	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32201)[0m 2019-07-24 02:46:52.466582: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32197)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32197)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32199)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32199)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32201)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32201)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32203)[0m [32m [     0.03665s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m 2019-07-24 02:46:52,913	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32203)[0m 2019-07-24 02:46:52.914126: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32203)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32203)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32202)[0m [32m [     1.86231s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32202)[0m [32m [     1.86288s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32202)[0m [32m [     1.86345s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32202)[0m [32m [     1.86409s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32202)[0m [32m [     1.86465s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32202)[0m [32m [     1.86517s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32202)[0m [32m [     1.86572s,  INFO] TimeLimit:
[2m[36m(pid=32202)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32202)[0m - action_space = Box(2,)
[2m[36m(pid=32202)[0m - observation_space = Box(9,)
[2m[36m(pid=32202)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32202)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32202)[0m - _max_episode_steps = 150
[2m[36m(pid=32202)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.53729s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.53812s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.53898s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.53980s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.54063s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.54148s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32199)[0m [32m [     1.54243s,  INFO] TimeLimit:
[2m[36m(pid=32199)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32199)[0m - action_space = Box(2,)
[2m[36m(pid=32199)[0m - observation_space = Box(9,)
[2m[36m(pid=32199)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32199)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32199)[0m - _max_episode_steps = 150
[2m[36m(pid=32199)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57256s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57339s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57422s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57505s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57590s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57675s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32197)[0m [32m [     1.57762s,  INFO] TimeLimit:
[2m[36m(pid=32197)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32197)[0m - action_space = Box(2,)
[2m[36m(pid=32197)[0m - observation_space = Box(9,)
[2m[36m(pid=32197)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32197)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32197)[0m - _max_episode_steps = 150
[2m[36m(pid=32197)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.88882s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.88968s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.89054s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.89141s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.89234s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.89312s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32198)[0m [32m [     1.89392s,  INFO] TimeLimit:
[2m[36m(pid=32198)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32198)[0m - action_space = Box(2,)
[2m[36m(pid=32198)[0m - observation_space = Box(9,)
[2m[36m(pid=32198)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32198)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32198)[0m - _max_episode_steps = 150
[2m[36m(pid=32198)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75071s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75138s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75207s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75265s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75337s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75422s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32200)[0m [32m [     1.75515s,  INFO] TimeLimit:
[2m[36m(pid=32200)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32200)[0m - action_space = Box(2,)
[2m[36m(pid=32200)[0m - observation_space = Box(9,)
[2m[36m(pid=32200)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32200)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32200)[0m - _max_episode_steps = 150
[2m[36m(pid=32200)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m 2019-07-24 02:46:54,042	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32334)[0m 2019-07-24 02:46:54.042656: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32334)[0m [32m [     0.04182s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65162s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65262s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65353s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65439s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65524s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65613s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32201)[0m [32m [     1.65704s,  INFO] TimeLimit:
[2m[36m(pid=32201)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32201)[0m - action_space = Box(2,)
[2m[36m(pid=32201)[0m - observation_space = Box(9,)
[2m[36m(pid=32201)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32201)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32201)[0m - _max_episode_steps = 150
[2m[36m(pid=32201)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32334)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32203)[0m [32m [     1.51688s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m [32m [     1.51789s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m [32m [     1.51878s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m [32m [     1.51969s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m [32m [     1.52057s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m [32m [     1.52146s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32203)[0m [32m [     1.52236s,  INFO] TimeLimit:
[2m[36m(pid=32203)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32203)[0m - action_space = Box(2,)
[2m[36m(pid=32203)[0m - observation_space = Box(9,)
[2m[36m(pid=32203)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32203)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32203)[0m - _max_episode_steps = 150
[2m[36m(pid=32203)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91170s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91269s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91369s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91462s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91587s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91686s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32334)[0m [32m [     1.91797s,  INFO] TimeLimit:
[2m[36m(pid=32334)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32334)[0m - action_space = Box(2,)
[2m[36m(pid=32334)[0m - observation_space = Box(9,)
[2m[36m(pid=32334)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32334)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32334)[0m - _max_episode_steps = 150
[2m[36m(pid=32334)[0m - _elapsed_steps = None [0m
[2m[36m(pid=31967)[0m 2019-07-24 02:46:57,761	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m { 'count': 512,
[2m[36m(pid=31967)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=0.019),
[2m[36m(pid=31967)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=118.0, max=12753.0, mean=6002.488),
[2m[36m(pid=31967)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.023),
[2m[36m(pid=31967)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.025),
[2m[36m(pid=31967)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=-0.016),
[2m[36m(pid=31967)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-25.66, max=34.75, mean=-0.133),
[2m[36m(pid=31967)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.024, max=0.123, mean=0.045)},
[2m[36m(pid=31967)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=31967)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m 2019-07-24 02:46:58,089	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=31967)[0m 
[2m[36m(pid=31967)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.8381804,
[2m[36m(pid=31967)[0m                                          'mean_q': 0.03154075,
[2m[36m(pid=31967)[0m                                          'min_q': -0.18675944},
[2m[36m(pid=31967)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-33.899, max=25.404, mean=0.144)}}
[2m[36m(pid=31967)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-47-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 24.064860521920792
  episode_reward_mean: -12.750466254503223
  episode_reward_min: -70.2145326507187
  episodes_this_iter: 736
  episodes_total: 736
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 15.334921836853027
        mean_q: 1.911912441253662
        min_q: -11.329916000366211
    learner_queue:
      size_count: 2050
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 315600
    num_steps_trained: 200192
    num_target_updates: 3
    num_weight_syncs: 789
    replay_shard_0:
      add_batch_time_ms: 43.197
      policy_default_policy:
        added_count: 76800
        est_size_bytes: 26188800
        num_entries: 76800
        sampled_count: 48128
      replay_time_ms: 32.619
      update_priorities_time_ms: 104.873
    sample_throughput: 21143.842
    train_throughput: 0.0
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.052253652489703
    mean_inference_ms: 2.809027879636065
    mean_processing_ms: 2.242904062145027
  time_since_restore: 30.68790316581726
  time_this_iter_s: 30.68790316581726
  time_total_s: 30.68790316581726
  timestamp: 1563929244
  timesteps_since_restore: 315600
  timesteps_this_iter: 315600
  timesteps_total: 315600
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 30 s, 1 iter, 315600 ts, -12.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-47-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 16.70150948799714
  episode_reward_mean: -8.899133197308327
  episode_reward_min: -99.63913227762691
  episodes_this_iter: 760
  episodes_total: 1496
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.165035247802734
        mean_q: 1.1798975467681885
        min_q: -20.406911849975586
    learner_queue:
      size_count: 2496
      size_mean: 0.6
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 2.0
      - 5.0
      size_std: 1.0770329614269007
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 627600
    num_steps_trained: 428544
    num_target_updates: 8
    num_weight_syncs: 1569
    replay_shard_0:
      add_batch_time_ms: 45.8
      policy_default_policy:
        added_count: 151600
        est_size_bytes: 51695600
        num_entries: 151600
        sampled_count: 101888
      replay_time_ms: 45.775
      update_priorities_time_ms: 116.028
    sample_throughput: 15343.553
    train_throughput: 9819.874
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0652900784859196
    mean_inference_ms: 2.8404190549684536
    mean_processing_ms: 2.2659172370226877
  time_since_restore: 61.38058376312256
  time_this_iter_s: 30.692680597305298
  time_total_s: 61.38058376312256
  timestamp: 1563929275
  timesteps_since_restore: 627600
  timesteps_this_iter: 312000
  timesteps_total: 627600
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 61 s, 2 iter, 627600 ts, -8.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-48-26
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.48484229273088
  episode_reward_mean: 3.3954553476681992
  episode_reward_min: -45.096762703351516
  episodes_this_iter: 776
  episodes_total: 2272
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 21.92201805114746
        mean_q: 2.4851067066192627
        min_q: -19.35301399230957
    learner_queue:
      size_count: 2918
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 2.0
      size_std: 0.38157568056677826
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 945200
    num_steps_trained: 645120
    num_target_updates: 12
    num_weight_syncs: 2363
    replay_shard_0:
      add_batch_time_ms: 22.501
      policy_default_policy:
        added_count: 236400
        est_size_bytes: 80612400
        num_entries: 236400
        sampled_count: 154624
      replay_time_ms: 47.176
      update_priorities_time_ms: 108.078
    sample_throughput: 12228.024
    train_throughput: 15651.871
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.042998947559808
    mean_inference_ms: 2.8222233621107398
    mean_processing_ms: 2.25268683201142
  time_since_restore: 92.11121582984924
  time_this_iter_s: 30.730632066726685
  time_total_s: 92.11121582984924
  timestamp: 1563929306
  timesteps_since_restore: 945200
  timesteps_this_iter: 317600
  timesteps_total: 945200
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 92 s, 3 iter, 945200 ts, 3.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-48-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 36.77628366846452
  episode_reward_mean: 14.997956389393453
  episode_reward_min: -14.02007001008959
  episodes_this_iter: 768
  episodes_total: 3040
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 29.036174774169922
        mean_q: 5.543166637420654
        min_q: -15.528973579406738
    learner_queue:
      size_count: 3334
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.48989794855663565
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 1260800
    num_steps_trained: 858112
    num_target_updates: 17
    num_weight_syncs: 3152
    replay_shard_0:
      add_batch_time_ms: 40.75
      policy_default_policy:
        added_count: 320000
        est_size_bytes: 109120000
        num_entries: 320000
        sampled_count: 211456
      replay_time_ms: 37.121
      update_priorities_time_ms: 103.496
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.033630536334981
    mean_inference_ms: 2.7989297341779817
    mean_processing_ms: 2.2618540479826206
  time_since_restore: 122.56390023231506
  time_this_iter_s: 30.45268440246582
  time_total_s: 122.56390023231506
  timestamp: 1563929336
  timesteps_since_restore: 1260800
  timesteps_this_iter: 315600
  timesteps_total: 1260800
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 122 s, 4 iter, 1260800 ts, 15 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m 2019-07-24 02:49:27,290	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-49-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.55218224357748
  episode_reward_mean: 15.329844565158819
  episode_reward_min: -10.12599976068749
  episodes_this_iter: 776
  episodes_total: 3816
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 36.26051484254395
    episode_reward_mean: 13.138884148716905
    episode_reward_min: -41.65456924220517
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.7389296294251246
      mean_inference_ms: 1.560386670419914
      mean_processing_ms: 0.9423066535460559
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 25.869138717651367
        mean_q: 4.768791675567627
        min_q: -22.18219757080078
    learner_queue:
      size_count: 3756
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.43863424398922624
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 1577200
    num_steps_trained: 1074176
    num_target_updates: 21
    num_weight_syncs: 3943
    replay_shard_0:
      add_batch_time_ms: 32.472
      policy_default_policy:
        added_count: 401600
        est_size_bytes: 136945600
        num_entries: 401600
        sampled_count: 260608
      replay_time_ms: 40.696
      update_priorities_time_ms: 109.106
    sample_throughput: 15384.602
    train_throughput: 19692.29
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0317581828206506
    mean_inference_ms: 2.8007369201380308
    mean_processing_ms: 2.260257760331117
  time_since_restore: 153.31170654296875
  time_this_iter_s: 30.747806310653687
  time_total_s: 153.31170654296875
  timestamp: 1563929367
  timesteps_since_restore: 1577200
  timesteps_this_iter: 316400
  timesteps_total: 1577200
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 153 s, 5 iter, 1577200 ts, 15.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-49-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.9167624207049
  episode_reward_mean: 17.513103712008103
  episode_reward_min: -12.61460657831763
  episodes_this_iter: 776
  episodes_total: 4592
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.76030731201172
        mean_q: 6.836397647857666
        min_q: -14.22203540802002
    learner_queue:
      size_count: 4179
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 1894800
    num_steps_trained: 1290240
    num_target_updates: 25
    num_weight_syncs: 4737
    replay_shard_0:
      add_batch_time_ms: 33.883
      policy_default_policy:
        added_count: 484800
        est_size_bytes: 165316800
        num_entries: 484800
        sampled_count: 311296
      replay_time_ms: 32.438
      update_priorities_time_ms: 101.777
    sample_throughput: 22087.781
    train_throughput: 0.0
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0268974588080684
    mean_inference_ms: 2.7926267641363745
    mean_processing_ms: 2.2615590505076217
  time_since_restore: 184.0343165397644
  time_this_iter_s: 30.722609996795654
  time_total_s: 184.0343165397644
  timestamp: 1563929399
  timesteps_since_restore: 1894800
  timesteps_this_iter: 317600
  timesteps_total: 1894800
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 184 s, 6 iter, 1894800 ts, 17.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-50-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.20128375069308
  episode_reward_mean: 16.646400023887807
  episode_reward_min: -15.163989709295276
  episodes_this_iter: 760
  episodes_total: 5352
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.400775909423828
        mean_q: 7.163127422332764
        min_q: -12.013483047485352
    learner_queue:
      size_count: 4596
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 2213200
    num_steps_trained: 1504256
    num_target_updates: 29
    num_weight_syncs: 5533
    replay_shard_0:
      add_batch_time_ms: 31.523
      policy_default_policy:
        added_count: 573200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 356864
      replay_time_ms: 49.052
      update_priorities_time_ms: 118.907
    sample_throughput: 17146.378
    train_throughput: 0.0
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0287165772131726
    mean_inference_ms: 2.798280440839084
    mean_processing_ms: 2.265240720987411
  time_since_restore: 214.69264197349548
  time_this_iter_s: 30.65832543373108
  time_total_s: 214.69264197349548
  timestamp: 1563929429
  timesteps_since_restore: 2213200
  timesteps_this_iter: 318400
  timesteps_total: 2213200
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 214 s, 7 iter, 2213200 ts, 16.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-51-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 37.96641564424336
  episode_reward_mean: 16.33207643485258
  episode_reward_min: -9.872271975906571
  episodes_this_iter: 784
  episodes_total: 6136
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.54033279418945
        mean_q: 6.441648006439209
        min_q: -2.097724676132202
    learner_queue:
      size_count: 5022
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4472135954999579
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 2530400
    num_steps_trained: 1721856
    num_target_updates: 34
    num_weight_syncs: 6326
    replay_shard_0:
      add_batch_time_ms: 22.358
      policy_default_policy:
        added_count: 663200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 404992
      replay_time_ms: 35.477
      update_priorities_time_ms: 112.606
    sample_throughput: 0.0
    train_throughput: 20974.592
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0228385283894665
    mean_inference_ms: 2.7928700930201384
    mean_processing_ms: 2.25791332565136
  time_since_restore: 245.42934584617615
  time_this_iter_s: 30.736703872680664
  time_total_s: 245.42934584617615
  timestamp: 1563929460
  timesteps_since_restore: 2530400
  timesteps_this_iter: 317200
  timesteps_total: 2530400
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 245 s, 8 iter, 2530400 ts, 16.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-51-31
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.72643844496646
  episode_reward_mean: 16.244879142409925
  episode_reward_min: -15.890232408759635
  episodes_this_iter: 768
  episodes_total: 6904
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.36003875732422
        mean_q: 4.903264999389648
        min_q: -1.9510217905044556
    learner_queue:
      size_count: 5457
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5385164807134504
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 2844800
    num_steps_trained: 1945088
    num_target_updates: 38
    num_weight_syncs: 7112
    replay_shard_0:
      add_batch_time_ms: 27.861
      policy_default_policy:
        added_count: 731600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 466432
      replay_time_ms: 36.212
      update_priorities_time_ms: 98.691
    sample_throughput: 18618.182
    train_throughput: 0.0
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.021069742274531
    mean_inference_ms: 2.7963093606094627
    mean_processing_ms: 2.256211252292769
  time_since_restore: 276.1426639556885
  time_this_iter_s: 30.71331810951233
  time_total_s: 276.1426639556885
  timestamp: 1563929491
  timesteps_since_restore: 2844800
  timesteps_this_iter: 314400
  timesteps_total: 2844800
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 276 s, 9 iter, 2844800 ts, 16.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m 2019-07-24 02:52:02,032	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-52-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.13044027911083
  episode_reward_mean: 17.867564409751854
  episode_reward_min: -13.588431979238734
  episodes_this_iter: 768
  episodes_total: 7672
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 38.50359251892466
    episode_reward_mean: 22.38679861396465
    episode_reward_min: -1.9287886635655742
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.4629868386124063
      mean_inference_ms: 1.2159086743174232
      mean_processing_ms: 0.8019032179150962
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.57529067993164
        mean_q: 4.608517169952393
        min_q: -1.2180404663085938
    learner_queue:
      size_count: 5886
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4331281565541543
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 3161600
    num_steps_trained: 2164224
    num_target_updates: 43
    num_weight_syncs: 7904
    replay_shard_0:
      add_batch_time_ms: 24.669
      policy_default_policy:
        added_count: 814000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 518656
      replay_time_ms: 35.455
      update_priorities_time_ms: 110.749
    sample_throughput: 15154.646
    train_throughput: 14548.46
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0258277794381025
    mean_inference_ms: 2.804266581761579
    mean_processing_ms: 2.2552312646971147
  time_since_restore: 306.880158662796
  time_this_iter_s: 30.737494707107544
  time_total_s: 306.880158662796
  timestamp: 1563929522
  timesteps_since_restore: 3161600
  timesteps_this_iter: 316800
  timesteps_total: 3161600
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 306 s, 10 iter, 3161600 ts, 17.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-52-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.73238688898639
  episode_reward_mean: 17.47316871967891
  episode_reward_min: -8.437574066642183
  episodes_this_iter: 776
  episodes_total: 8448
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.156856536865234
        mean_q: 4.864163875579834
        min_q: -1.0189902782440186
    learner_queue:
      size_count: 6305
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4142463035441596
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 3480400
    num_steps_trained: 2378752
    num_target_updates: 47
    num_weight_syncs: 8701
    replay_shard_0:
      add_batch_time_ms: 34.409
      policy_default_policy:
        added_count: 891600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 571904
      replay_time_ms: 36.683
      update_priorities_time_ms: 94.163
    sample_throughput: 16181.0
    train_throughput: 6903.893
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0245223652836275
    mean_inference_ms: 2.7994338890731116
    mean_processing_ms: 2.256805356311106
  time_since_restore: 337.6149072647095
  time_this_iter_s: 30.734748601913452
  time_total_s: 337.6149072647095
  timestamp: 1563929553
  timesteps_since_restore: 3480400
  timesteps_this_iter: 318800
  timesteps_total: 3480400
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 337 s, 11 iter, 3480400 ts, 17.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-53-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.4497367319718
  episode_reward_mean: 16.71264426705661
  episode_reward_min: -14.79036217250886
  episodes_this_iter: 752
  episodes_total: 9200
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.02531433105469
        mean_q: 5.9055681228637695
        min_q: -0.5574324727058411
    learner_queue:
      size_count: 6735
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5385164807134504
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 3795600
    num_steps_trained: 2598912
    num_target_updates: 51
    num_weight_syncs: 9489
    replay_shard_0:
      add_batch_time_ms: 41.184
      policy_default_policy:
        added_count: 969600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 627712
      replay_time_ms: 38.343
      update_priorities_time_ms: 117.019
    sample_throughput: 14941.769
    train_throughput: 0.0
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0271011831003927
    mean_inference_ms: 2.811555606009852
    mean_processing_ms: 2.258167982232536
  time_since_restore: 368.3147358894348
  time_this_iter_s: 30.699828624725342
  time_total_s: 368.3147358894348
  timestamp: 1563929584
  timesteps_since_restore: 3795600
  timesteps_this_iter: 315200
  timesteps_total: 3795600
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 368 s, 12 iter, 3795600 ts, 16.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-53-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.70623135738045
  episode_reward_mean: 17.712242766632414
  episode_reward_min: -9.961794471602499
  episodes_this_iter: 776
  episodes_total: 9976
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 39.07472610473633
        mean_q: 6.222780227661133
        min_q: -0.47819751501083374
    learner_queue:
      size_count: 7155
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 4112800
    num_steps_trained: 2814464
    num_target_updates: 56
    num_weight_syncs: 10282
    replay_shard_0:
      add_batch_time_ms: 27.042
      policy_default_policy:
        added_count: 1052400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 680448
      replay_time_ms: 42.278
      update_priorities_time_ms: 110.89
    sample_throughput: 16732.957
    train_throughput: 0.0
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0237245847926912
    mean_inference_ms: 2.8029976266169196
    mean_processing_ms: 2.2576759391537355
  time_since_restore: 398.9838261604309
  time_this_iter_s: 30.669090270996094
  time_total_s: 398.9838261604309
  timestamp: 1563929615
  timesteps_since_restore: 4112800
  timesteps_this_iter: 317200
  timesteps_total: 4112800
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 398 s, 13 iter, 4112800 ts, 17.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-54-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.37754394568022
  episode_reward_mean: 18.355794763536743
  episode_reward_min: -12.424823814163934
  episodes_this_iter: 792
  episodes_total: 10768
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.454036712646484
        mean_q: 5.173807144165039
        min_q: -0.8771763443946838
    learner_queue:
      size_count: 7571
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 4431200
    num_steps_trained: 3027456
    num_target_updates: 60
    num_weight_syncs: 11078
    replay_shard_0:
      add_batch_time_ms: 40.877
      policy_default_policy:
        added_count: 1128000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 733184
      replay_time_ms: 39.443
      update_priorities_time_ms: 105.116
    sample_throughput: 12546.621
    train_throughput: 8029.837
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0188565659436786
    mean_inference_ms: 2.7977025229204884
    mean_processing_ms: 2.255826080066979
  time_since_restore: 429.588990688324
  time_this_iter_s: 30.605164527893066
  time_total_s: 429.588990688324
  timestamp: 1563929645
  timesteps_since_restore: 4431200
  timesteps_this_iter: 318400
  timesteps_total: 4431200
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 429 s, 14 iter, 4431200 ts, 18.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m 2019-07-24 02:54:36,660	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-54-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.407897336116314
  episode_reward_mean: 17.66000380988531
  episode_reward_min: -7.078079051617524
  episodes_this_iter: 768
  episodes_total: 11536
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 38.25754592418333
    episode_reward_mean: 16.2356474744623
    episode_reward_min: -2.6563124802560507
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.3655310257547757
      mean_inference_ms: 1.0856753823387524
      mean_processing_ms: 0.7439492173535941
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.88287353515625
        mean_q: 5.506468296051025
        min_q: -0.8321861624717712
    learner_queue:
      size_count: 8003
      size_mean: 0.08
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      size_std: 0.2712931993250107
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 4745200
    num_steps_trained: 3248128
    num_target_updates: 64
    num_weight_syncs: 11863
    replay_shard_0:
      add_batch_time_ms: 33.639
      policy_default_policy:
        added_count: 1201200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 789504
      replay_time_ms: 36.91
      update_priorities_time_ms: 82.106
    sample_throughput: 0.0
    train_throughput: 15086.683
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.020678081295379
    mean_inference_ms: 2.801088708758186
    mean_processing_ms: 2.255284457974948
  time_since_restore: 460.2791407108307
  time_this_iter_s: 30.690150022506714
  time_total_s: 460.2791407108307
  timestamp: 1563929676
  timesteps_since_restore: 4745200
  timesteps_this_iter: 314000
  timesteps_total: 4745200
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 460 s, 15 iter, 4745200 ts, 17.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-55-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.641430390429555
  episode_reward_mean: 18.2054952726182
  episode_reward_min: -16.665517216922403
  episodes_this_iter: 776
  episodes_total: 12312
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.33566665649414
        mean_q: 4.869346618652344
        min_q: -0.5699486136436462
    learner_queue:
      size_count: 8432
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.40049968789001567
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 5061200
    num_steps_trained: 3467776
    num_target_updates: 69
    num_weight_syncs: 12653
    replay_shard_0:
      add_batch_time_ms: 19.444
      policy_default_policy:
        added_count: 1289200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 837632
      replay_time_ms: 40.273
      update_priorities_time_ms: 104.583
    sample_throughput: 5441.353
    train_throughput: 13929.865
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0226736547993127
    mean_inference_ms: 2.797783189166245
    mean_processing_ms: 2.253751535343404
  time_since_restore: 490.9872660636902
  time_this_iter_s: 30.708125352859497
  time_total_s: 490.9872660636902
  timestamp: 1563929708
  timesteps_since_restore: 5061200
  timesteps_this_iter: 316000
  timesteps_total: 5061200
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 490 s, 16 iter, 5061200 ts, 18.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-55-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.44347057636297
  episode_reward_mean: 17.614658921827893
  episode_reward_min: -10.265627599681181
  episodes_this_iter: 776
  episodes_total: 13088
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.96390151977539
        mean_q: 4.705175399780273
        min_q: 0.12446659058332443
    learner_queue:
      size_count: 8862
      size_mean: 0.24
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4270831300812525
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 5376800
    num_steps_trained: 3688448
    num_target_updates: 73
    num_weight_syncs: 13442
    replay_shard_0:
      add_batch_time_ms: 33.073
      policy_default_policy:
        added_count: 1371600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 889856
      replay_time_ms: 47.741
      update_priorities_time_ms: 96.412
    sample_throughput: 10233.317
    train_throughput: 13098.646
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0190167077375283
    mean_inference_ms: 2.794917392700738
    mean_processing_ms: 2.253076954156812
  time_since_restore: 521.7245378494263
  time_this_iter_s: 30.737271785736084
  time_total_s: 521.7245378494263
  timestamp: 1563929739
  timesteps_since_restore: 5376800
  timesteps_this_iter: 315600
  timesteps_total: 5376800
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 521 s, 17 iter, 5376800 ts, 17.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-56-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.07697207567703
  episode_reward_mean: 17.353282182458692
  episode_reward_min: -23.9387643643375
  episodes_this_iter: 776
  episodes_total: 13864
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.994178771972656
        mean_q: 4.935375213623047
        min_q: 0.27679553627967834
    learner_queue:
      size_count: 9297
      size_mean: 0.58
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 2.1000000000000014
      - 5.0
      size_std: 1.2975361266646877
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 5692000
    num_steps_trained: 3910656
    num_target_updates: 77
    num_weight_syncs: 14230
    replay_shard_0:
      add_batch_time_ms: 37.426
      policy_default_policy:
        added_count: 1451600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 944128
      replay_time_ms: 39.962
      update_priorities_time_ms: 89.707
    sample_throughput: 14607.53
    train_throughput: 11218.583
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0209763414899644
    mean_inference_ms: 2.7954689717174594
    mean_processing_ms: 2.253001670240683
  time_since_restore: 552.4600374698639
  time_this_iter_s: 30.735499620437622
  time_total_s: 552.4600374698639
  timestamp: 1563929769
  timesteps_since_restore: 5692000
  timesteps_this_iter: 315200
  timesteps_total: 5692000
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 552 s, 18 iter, 5692000 ts, 17.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-56-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.50530193076545
  episode_reward_mean: 17.95491495303824
  episode_reward_min: -17.149407485963817
  episodes_this_iter: 776
  episodes_total: 14640
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.995037078857422
        mean_q: 4.7822771072387695
        min_q: -0.5303513407707214
    learner_queue:
      size_count: 9726
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5291502622129182
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 6006800
    num_steps_trained: 4130304
    num_target_updates: 82
    num_weight_syncs: 15017
    replay_shard_0:
      add_batch_time_ms: 44.345
      policy_default_policy:
        added_count: 1524000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 997888
      replay_time_ms: 30.18
      update_priorities_time_ms: 101.08
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.020821590383588
    mean_inference_ms: 2.7935992276771118
    mean_processing_ms: 2.254025311822785
  time_since_restore: 583.0715141296387
  time_this_iter_s: 30.61147665977478
  time_total_s: 583.0715141296387
  timestamp: 1563929800
  timesteps_since_restore: 6006800
  timesteps_this_iter: 314800
  timesteps_total: 6006800
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 583 s, 19 iter, 6006800 ts, 18 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m 2019-07-24 02:57:11,126	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-57-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.63380319207865
  episode_reward_mean: 17.832378065209674
  episode_reward_min: -8.877710658894786
  episodes_this_iter: 760
  episodes_total: 15400
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 39.49647012124653
    episode_reward_mean: 16.981172237263085
    episode_reward_min: 2.1541135132688933
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.2699445478960485
      mean_inference_ms: 0.9687043875403591
      mean_processing_ms: 0.6922813349964326
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.992919921875
        mean_q: 4.670891284942627
        min_q: -0.7752942442893982
    learner_queue:
      size_count: 10146
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.41424630354415964
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 6322800
    num_steps_trained: 4345856
    num_target_updates: 86
    num_weight_syncs: 15807
    replay_shard_0:
      add_batch_time_ms: 32.839
      policy_default_policy:
        added_count: 1602000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1050112
      replay_time_ms: 48.248
      update_priorities_time_ms: 119.073
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0223791315336586
    mean_inference_ms: 2.7961627496324963
    mean_processing_ms: 2.254770923651721
  time_since_restore: 613.7712469100952
  time_this_iter_s: 30.699732780456543
  time_total_s: 613.7712469100952
  timestamp: 1563929831
  timesteps_since_restore: 6322800
  timesteps_this_iter: 316000
  timesteps_total: 6322800
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 613 s, 20 iter, 6322800 ts, 17.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-57-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.0022117539352
  episode_reward_mean: 17.813514344911432
  episode_reward_min: -16.330469354067652
  episodes_this_iter: 760
  episodes_total: 16160
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.30398941040039
        mean_q: 5.493635654449463
        min_q: -0.07562213391065598
    learner_queue:
      size_count: 10571
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5589275444992848
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 6639600
    num_steps_trained: 4562944
    num_target_updates: 90
    num_weight_syncs: 16599
    replay_shard_0:
      add_batch_time_ms: 38.561
      policy_default_policy:
        added_count: 1679600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1105408
      replay_time_ms: 38.118
      update_priorities_time_ms: 99.328
    sample_throughput: 12017.374
    train_throughput: 0.0
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0235428437597864
    mean_inference_ms: 2.794845305509549
    mean_processing_ms: 2.2546381959874897
  time_since_restore: 644.4579339027405
  time_this_iter_s: 30.686686992645264
  time_total_s: 644.4579339027405
  timestamp: 1563929862
  timesteps_since_restore: 6639600
  timesteps_this_iter: 316800
  timesteps_total: 6639600
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 644 s, 21 iter, 6639600 ts, 17.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-58-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.034181068666214
  episode_reward_mean: 18.43290495327591
  episode_reward_min: -6.3302459810160805
  episodes_this_iter: 768
  episodes_total: 16928
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.02981185913086
        mean_q: 5.118494033813477
        min_q: -0.09747264534235
    learner_queue:
      size_count: 11004
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.34698703145794946
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 6953600
    num_steps_trained: 4785152
    num_target_updates: 95
    num_weight_syncs: 17384
    replay_shard_0:
      add_batch_time_ms: 28.831
      policy_default_policy:
        added_count: 1760800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1158144
      replay_time_ms: 42.961
      update_priorities_time_ms: 114.554
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.024119602329758
    mean_inference_ms: 2.7948285042539065
    mean_processing_ms: 2.2558879097065825
  time_since_restore: 675.1395654678345
  time_this_iter_s: 30.681631565093994
  time_total_s: 675.1395654678345
  timestamp: 1563929893
  timesteps_since_restore: 6953600
  timesteps_this_iter: 314000
  timesteps_total: 6953600
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 675 s, 22 iter, 6953600 ts, 18.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-58-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 40.134551244814695
  episode_reward_mean: 18.927524412660897
  episode_reward_min: -10.396403525118012
  episodes_this_iter: 760
  episodes_total: 17688
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.86504364013672
        mean_q: 5.317958831787109
        min_q: -0.02065790444612503
    learner_queue:
      size_count: 11425
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.41761226035642196
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 7271200
    num_steps_trained: 5000704
    num_target_updates: 99
    num_weight_syncs: 18178
    replay_shard_0:
      add_batch_time_ms: 33.072
      policy_default_policy:
        added_count: 1848800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1206784
      replay_time_ms: 39.74
      update_priorities_time_ms: 116.315
    sample_throughput: 15198.681
    train_throughput: 6484.771
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0257833311420392
    mean_inference_ms: 2.799965656644074
    mean_processing_ms: 2.2566515077186864
  time_since_restore: 705.9421019554138
  time_this_iter_s: 30.802536487579346
  time_total_s: 705.9421019554138
  timestamp: 1563929924
  timesteps_since_restore: 7271200
  timesteps_this_iter: 317600
  timesteps_total: 7271200
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 705 s, 23 iter, 7271200 ts, 18.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-59-15
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.47527729555933
  episode_reward_mean: 17.350966179024095
  episode_reward_min: -12.557577164652972
  episodes_this_iter: 768
  episodes_total: 18456
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.17324447631836
        mean_q: 5.361876010894775
        min_q: -0.060889832675457
    learner_queue:
      size_count: 11842
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.417612260356422
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 7589200
    num_steps_trained: 5214208
    num_target_updates: 103
    num_weight_syncs: 18973
    replay_shard_0:
      add_batch_time_ms: 36.968
      policy_default_policy:
        added_count: 1930800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1257472
      replay_time_ms: 38.38
      update_priorities_time_ms: 105.409
    sample_throughput: 10223.215
    train_throughput: 0.0
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.024989905690614
    mean_inference_ms: 2.8015183258552496
    mean_processing_ms: 2.257611985872084
  time_since_restore: 736.6708314418793
  time_this_iter_s: 30.728729486465454
  time_total_s: 736.6708314418793
  timestamp: 1563929955
  timesteps_since_restore: 7589200
  timesteps_this_iter: 318000
  timesteps_total: 7589200
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 736 s, 24 iter, 7589200 ts, 17.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m 2019-07-24 02:59:45,939	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_02-59-45
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.55204746335142
  episode_reward_mean: 18.560667478538132
  episode_reward_min: -15.800018097552543
  episodes_this_iter: 768
  episodes_total: 19224
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 37.38056726051287
    episode_reward_mean: 20.55489114140062
    episode_reward_min: 5.9470311218375755
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.204631875059426
      mean_inference_ms: 0.8915548426833441
      mean_processing_ms: 0.658349070608426
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.831214904785156
        mean_q: 4.943934440612793
        min_q: -0.3158523738384247
    learner_queue:
      size_count: 12255
      size_mean: 0.44
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 5.0
      size_std: 1.022936948203554
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 7907200
    num_steps_trained: 5425664
    num_target_updates: 108
    num_weight_syncs: 19768
    replay_shard_0:
      add_batch_time_ms: 47.289
      policy_default_policy:
        added_count: 2008000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1308672
      replay_time_ms: 40.097
      update_priorities_time_ms: 131.964
    sample_throughput: 15843.705
    train_throughput: 20279.942
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0259337750519903
    mean_inference_ms: 2.7986587810007504
    mean_processing_ms: 2.2570682273801945
  time_since_restore: 767.3707566261292
  time_this_iter_s: 30.699925184249878
  time_total_s: 767.3707566261292
  timestamp: 1563929985
  timesteps_since_restore: 7907200
  timesteps_this_iter: 318000
  timesteps_total: 7907200
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 767 s, 25 iter, 7907200 ts, 18.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-00-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.92505284835105
  episode_reward_mean: 18.1424247033524
  episode_reward_min: -13.644972293855783
  episodes_this_iter: 792
  episodes_total: 20016
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.646446228027344
        mean_q: 4.7605743408203125
        min_q: 0.03665850684046745
    learner_queue:
      size_count: 12672
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5385164807134504
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 8225200
    num_steps_trained: 5638144
    num_target_updates: 112
    num_weight_syncs: 20563
    replay_shard_0:
      add_batch_time_ms: 34.33
      policy_default_policy:
        added_count: 2085200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1362944
      replay_time_ms: 40.274
      update_priorities_time_ms: 111.97
    sample_throughput: 12336.279
    train_throughput: 7895.219
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.02437594630522
    mean_inference_ms: 2.7971601445134673
    mean_processing_ms: 2.2551628837857884
  time_since_restore: 798.0401186943054
  time_this_iter_s: 30.66936206817627
  time_total_s: 798.0401186943054
  timestamp: 1563930017
  timesteps_since_restore: 8225200
  timesteps_this_iter: 318000
  timesteps_total: 8225200
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 798 s, 26 iter, 8225200 ts, 18.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-00-48
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.745996938815715
  episode_reward_mean: 18.514912896264775
  episode_reward_min: -9.833490436097376
  episodes_this_iter: 768
  episodes_total: 20784
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.37413024902344
        mean_q: 4.719064712524414
        min_q: -0.2651236355304718
    learner_queue:
      size_count: 13093
      size_mean: 0.56
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 2.0
      - 4.0
      size_std: 1.0983624174196784
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 8541600
    num_steps_trained: 5854720
    num_target_updates: 116
    num_weight_syncs: 21354
    replay_shard_0:
      add_batch_time_ms: 30.347
      policy_default_policy:
        added_count: 2164400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1415680
      replay_time_ms: 42.743
      update_priorities_time_ms: 101.62
    sample_throughput: 13229.574
    train_throughput: 0.0
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0250719972902806
    mean_inference_ms: 2.796541183686825
    mean_processing_ms: 2.2544550317300573
  time_since_restore: 828.754896402359
  time_this_iter_s: 30.71477770805359
  time_total_s: 828.754896402359
  timestamp: 1563930048
  timesteps_since_restore: 8541600
  timesteps_this_iter: 316400
  timesteps_total: 8541600
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 828 s, 27 iter, 8541600 ts, 18.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-01-19
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.68403601071602
  episode_reward_mean: 18.238589605289718
  episode_reward_min: -13.386564653917267
  episodes_this_iter: 768
  episodes_total: 21552
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.94240951538086
        mean_q: 4.7251081466674805
        min_q: -0.5019024014472961
    learner_queue:
      size_count: 13515
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5589275444992848
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 8858000
    num_steps_trained: 6070272
    num_target_updates: 120
    num_weight_syncs: 22145
    replay_shard_0:
      add_batch_time_ms: 27.761
      policy_default_policy:
        added_count: 2235600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1473536
      replay_time_ms: 40.03
      update_priorities_time_ms: 114.984
    sample_throughput: 13492.338
    train_throughput: 8635.096
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.026065448994735
    mean_inference_ms: 2.7977867888707113
    mean_processing_ms: 2.255638940816889
  time_since_restore: 859.4699623584747
  time_this_iter_s: 30.715065956115723
  time_total_s: 859.4699623584747
  timestamp: 1563930079
  timesteps_since_restore: 8858000
  timesteps_this_iter: 316400
  timesteps_total: 8858000
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 859 s, 28 iter, 8858000 ts, 18.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-01-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.153261047453924
  episode_reward_mean: 18.310857601655417
  episode_reward_min: -12.069908509334997
  episodes_this_iter: 768
  episodes_total: 22320
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.654197692871094
        mean_q: 4.36640739440918
        min_q: -0.18219439685344696
    learner_queue:
      size_count: 13939
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 9172800
    num_steps_trained: 6287872
    num_target_updates: 125
    num_weight_syncs: 22932
    replay_shard_0:
      add_batch_time_ms: 31.232
      policy_default_policy:
        added_count: 2317600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1526784
      replay_time_ms: 38.843
      update_priorities_time_ms: 93.302
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.026640053833598
    mean_inference_ms: 2.798431187934428
    mean_processing_ms: 2.256628088281705
  time_since_restore: 890.1377942562103
  time_this_iter_s: 30.667831897735596
  time_total_s: 890.1377942562103
  timestamp: 1563930109
  timesteps_since_restore: 9172800
  timesteps_this_iter: 314800
  timesteps_total: 9172800
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 890 s, 29 iter, 9172800 ts, 18.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

[2m[36m(pid=31967)[0m 2019-07-24 03:02:20,659	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-02-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.30762241671234
  episode_reward_mean: 18.679694333451845
  episode_reward_min: -5.507553626017286
  episodes_this_iter: 760
  episodes_total: 23080
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 36.556460836353736
    episode_reward_mean: 20.98079264893993
    episode_reward_min: 3.679966736098076
    episodes_this_iter: 30
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 1.1766460082525678
      mean_inference_ms: 0.8436770711054601
      mean_processing_ms: 0.6399387137385482
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.20463943481445
        mean_q: 4.144797325134277
        min_q: -0.09805970638990402
    learner_queue:
      size_count: 14356
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 9490400
    num_steps_trained: 6501376
    num_target_updates: 129
    num_weight_syncs: 23726
    replay_shard_0:
      add_batch_time_ms: 30.093
      policy_default_policy:
        added_count: 2396000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1580032
      replay_time_ms: 38.53
      update_priorities_time_ms: 103.597
    sample_throughput: 14669.117
    train_throughput: 0.0
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.027812637009339
    mean_inference_ms: 2.7994106749594145
    mean_processing_ms: 2.257351092346761
  time_since_restore: 920.8913164138794
  time_this_iter_s: 30.753522157669067
  time_total_s: 920.8913164138794
  timestamp: 1563930140
  timesteps_since_restore: 9490400
  timesteps_this_iter: 317600
  timesteps_total: 9490400
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 920 s, 30 iter, 9490400 ts, 18.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-02-52
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.61493907405358
  episode_reward_mean: 18.1199867533879
  episode_reward_min: -10.844830143571222
  episodes_this_iter: 760
  episodes_total: 23840
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.54581832885742
        mean_q: 3.9978084564208984
        min_q: -0.7815407514572144
    learner_queue:
      size_count: 14786
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4000000000000001
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 9804800
    num_steps_trained: 6721536
    num_target_updates: 133
    num_weight_syncs: 24512
    replay_shard_0:
      add_batch_time_ms: 41.979
      policy_default_policy:
        added_count: 2464800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1637376
      replay_time_ms: 38.445
      update_priorities_time_ms: 105.14
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.0291369367724226
    mean_inference_ms: 2.801364710692367
    mean_processing_ms: 2.257937469775855
  time_since_restore: 951.5365200042725
  time_this_iter_s: 30.645203590393066
  time_total_s: 951.5365200042725
  timestamp: 1563930172
  timesteps_since_restore: 9804800
  timesteps_this_iter: 314400
  timesteps_total: 9804800
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 1, 'RUNNING': 1, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	RUNNING, [12 CPUs, 1 GPUs], [pid=31967], 951 s, 31 iter, 9804800 ts, 18.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
2019-07-24 03:03:23,269	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2019-07-24 03:03:23,296	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.

Result for APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:
  custom_metrics: {}
  date: 2019-07-24_03-03-23
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 38.41642982547979
  episode_reward_mean: 17.995450811342543
  episode_reward_min: -6.237923726214236
  episodes_this_iter: 768
  episodes_total: 24608
  experiment_id: 3f59094c6326489792423c24350469d0
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.98657989501953
        mean_q: 4.410200119018555
        min_q: 0.059821635484695435
    learner_queue:
      size_count: 15206
      size_mean: 0.38
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 5.0
      size_std: 1.0176443386566842
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 16
    num_steps_sampled: 10120800
    num_steps_trained: 6936576
    num_target_updates: 138
    num_weight_syncs: 25302
    replay_shard_0:
      add_batch_time_ms: 45.358
      policy_default_policy:
        added_count: 2540800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1689088
      replay_time_ms: 41.88
      update_priorities_time_ms: 115.196
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 31967
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 3.027359550401265
    mean_inference_ms: 2.8021479724466007
    mean_processing_ms: 2.2571376332709607
  time_since_restore: 982.1825325489044
  time_this_iter_s: 30.646012544631958
  time_total_s: 982.1825325489044
  timestamp: 1563930203
  timesteps_since_restore: 10120800
  timesteps_this_iter: 316000
  timesteps_total: 10120800
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'PENDING': 1})
PENDING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	PENDING
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:03:23.509275: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32336)[0m [32m [     0.07305s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32336)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32336)[0m 2019-07-24 03:03:24,896	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f1fc6d7cc18>}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:24,896	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f1fc6d5c160>}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:24,896	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1fc6da5b38>}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:24,901	INFO actors.py:108 -- Trying to create 4 colocated actors
[2m[36m(pid=32336)[0m 2019-07-24 03:03:24,910	INFO actors.py:101 -- Got 4 colocated actors of 4
[2m[36m(pid=32336)[0m [32m [     1.46279s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m [32m [     1.46334s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m [32m [     1.46383s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32339)[0m [32m [     0.04135s,  INFO] TimeLimit:
[2m[36m(pid=32339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32339)[0m - action_space = Box(2,)
[2m[36m(pid=32339)[0m - observation_space = Box(9,)
[2m[36m(pid=32339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32339)[0m - _max_episode_steps = 150
[2m[36m(pid=32339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32339)[0m 2019-07-24 03:03:25,039	INFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32339)[0m 2019-07-24 03:03:25.040406: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32328)[0m 2019-07-24 03:03:25,013	INFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32328)[0m 2019-07-24 03:03:25.014398: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32328)[0m [32m [     0.03852s,  INFO] TimeLimit:
[2m[36m(pid=32328)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32328)[0m - action_space = Box(2,)
[2m[36m(pid=32328)[0m - observation_space = Box(9,)
[2m[36m(pid=32328)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32328)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32328)[0m - _max_episode_steps = 150
[2m[36m(pid=32328)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32326)[0m [32m [     0.03898s,  INFO] TimeLimit:
[2m[36m(pid=32326)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32326)[0m - action_space = Box(2,)
[2m[36m(pid=32326)[0m - observation_space = Box(9,)
[2m[36m(pid=32326)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32326)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32326)[0m - _max_episode_steps = 150
[2m[36m(pid=32326)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32326)[0m 2019-07-24 03:03:25,018	INFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32326)[0m 2019-07-24 03:03:25.018942: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32204)[0m 2019-07-24 03:03:25,090	INFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32204)[0m 2019-07-24 03:03:25.090897: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32204)[0m [32m [     0.04707s,  INFO] TimeLimit:
[2m[36m(pid=32204)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32204)[0m - action_space = Box(2,)
[2m[36m(pid=32204)[0m - observation_space = Box(9,)
[2m[36m(pid=32204)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32204)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32204)[0m - _max_episode_steps = 150
[2m[36m(pid=32204)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32327)[0m [32m [     0.04679s,  INFO] TimeLimit:
[2m[36m(pid=32327)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32327)[0m - action_space = Box(2,)
[2m[36m(pid=32327)[0m - observation_space = Box(9,)
[2m[36m(pid=32327)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32327)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32327)[0m - _max_episode_steps = 150
[2m[36m(pid=32327)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32327)[0m 2019-07-24 03:03:25,050	INFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32327)[0m 2019-07-24 03:03:25.051427: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32329)[0m 2019-07-24 03:03:25,056	INFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32329)[0m 2019-07-24 03:03:25.057345: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32329)[0m [32m [     0.04290s,  INFO] TimeLimit:
[2m[36m(pid=32329)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32329)[0m - action_space = Box(2,)
[2m[36m(pid=32329)[0m - observation_space = Box(9,)
[2m[36m(pid=32329)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32329)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32329)[0m - _max_episode_steps = 150
[2m[36m(pid=32329)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32328)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32328)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32326)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32326)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32327)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32327)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32339)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32339)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32329)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32329)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32204)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32204)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32336)[0m [32m [     2.00983s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32339)[0m [32m [     2.55174s,  INFO] TimeLimit:
[2m[36m(pid=32339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32339)[0m - action_space = Box(2,)
[2m[36m(pid=32339)[0m - observation_space = Box(9,)
[2m[36m(pid=32339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32339)[0m - _max_episode_steps = 150
[2m[36m(pid=32339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32339)[0m [32m [     2.55264s,  INFO] TimeLimit:
[2m[36m(pid=32339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32339)[0m - action_space = Box(2,)
[2m[36m(pid=32339)[0m - observation_space = Box(9,)
[2m[36m(pid=32339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32339)[0m - _max_episode_steps = 150
[2m[36m(pid=32339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32339)[0m [32m [     2.55350s,  INFO] TimeLimit:
[2m[36m(pid=32339)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32339)[0m - action_space = Box(2,)
[2m[36m(pid=32339)[0m - observation_space = Box(9,)
[2m[36m(pid=32339)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32339)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32339)[0m - _max_episode_steps = 150
[2m[36m(pid=32339)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32204)[0m [32m [     2.74003s,  INFO] TimeLimit:
[2m[36m(pid=32204)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32204)[0m - action_space = Box(2,)
[2m[36m(pid=32204)[0m - observation_space = Box(9,)
[2m[36m(pid=32204)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32204)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32204)[0m - _max_episode_steps = 150
[2m[36m(pid=32204)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32204)[0m [32m [     2.74094s,  INFO] TimeLimit:
[2m[36m(pid=32204)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32204)[0m - action_space = Box(2,)
[2m[36m(pid=32204)[0m - observation_space = Box(9,)
[2m[36m(pid=32204)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32204)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32204)[0m - _max_episode_steps = 150
[2m[36m(pid=32204)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32204)[0m [32m [     2.74181s,  INFO] TimeLimit:
[2m[36m(pid=32204)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32204)[0m - action_space = Box(2,)
[2m[36m(pid=32204)[0m - observation_space = Box(9,)
[2m[36m(pid=32204)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32204)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32204)[0m - _max_episode_steps = 150
[2m[36m(pid=32204)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32327)[0m [32m [     2.94260s,  INFO] TimeLimit:
[2m[36m(pid=32327)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32327)[0m - action_space = Box(2,)
[2m[36m(pid=32327)[0m - observation_space = Box(9,)
[2m[36m(pid=32327)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32327)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32327)[0m - _max_episode_steps = 150
[2m[36m(pid=32327)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32327)[0m [32m [     2.94353s,  INFO] TimeLimit:
[2m[36m(pid=32327)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32327)[0m - action_space = Box(2,)
[2m[36m(pid=32327)[0m - observation_space = Box(9,)
[2m[36m(pid=32327)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32327)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32327)[0m - _max_episode_steps = 150
[2m[36m(pid=32327)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32327)[0m [32m [     2.94444s,  INFO] TimeLimit:
[2m[36m(pid=32327)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32327)[0m - action_space = Box(2,)
[2m[36m(pid=32327)[0m - observation_space = Box(9,)
[2m[36m(pid=32327)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32327)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32327)[0m - _max_episode_steps = 150
[2m[36m(pid=32327)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32326)[0m [32m [     3.03773s,  INFO] TimeLimit:
[2m[36m(pid=32326)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32326)[0m - action_space = Box(2,)
[2m[36m(pid=32326)[0m - observation_space = Box(9,)
[2m[36m(pid=32326)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32326)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32326)[0m - _max_episode_steps = 150
[2m[36m(pid=32326)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32326)[0m [32m [     3.03861s,  INFO] TimeLimit:
[2m[36m(pid=32326)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32326)[0m - action_space = Box(2,)
[2m[36m(pid=32326)[0m - observation_space = Box(9,)
[2m[36m(pid=32326)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32326)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32326)[0m - _max_episode_steps = 150
[2m[36m(pid=32326)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32326)[0m [32m [     3.03946s,  INFO] TimeLimit:
[2m[36m(pid=32326)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32326)[0m - action_space = Box(2,)
[2m[36m(pid=32326)[0m - observation_space = Box(9,)
[2m[36m(pid=32326)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32326)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32326)[0m - _max_episode_steps = 150
[2m[36m(pid=32326)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32328)[0m [32m [     3.07867s,  INFO] TimeLimit:
[2m[36m(pid=32328)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32328)[0m - action_space = Box(2,)
[2m[36m(pid=32328)[0m - observation_space = Box(9,)
[2m[36m(pid=32328)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32328)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32328)[0m - _max_episode_steps = 150
[2m[36m(pid=32328)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32328)[0m [32m [     3.07960s,  INFO] TimeLimit:
[2m[36m(pid=32328)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32328)[0m - action_space = Box(2,)
[2m[36m(pid=32328)[0m - observation_space = Box(9,)
[2m[36m(pid=32328)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32328)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32328)[0m - _max_episode_steps = 150
[2m[36m(pid=32328)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32328)[0m [32m [     3.08052s,  INFO] TimeLimit:
[2m[36m(pid=32328)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32328)[0m - action_space = Box(2,)
[2m[36m(pid=32328)[0m - observation_space = Box(9,)
[2m[36m(pid=32328)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32328)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32328)[0m - _max_episode_steps = 150
[2m[36m(pid=32328)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32329)[0m [32m [     3.03040s,  INFO] TimeLimit:
[2m[36m(pid=32329)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32329)[0m - action_space = Box(2,)
[2m[36m(pid=32329)[0m - observation_space = Box(9,)
[2m[36m(pid=32329)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32329)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32329)[0m - _max_episode_steps = 150
[2m[36m(pid=32329)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32329)[0m [32m [     3.03123s,  INFO] TimeLimit:
[2m[36m(pid=32329)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32329)[0m - action_space = Box(2,)
[2m[36m(pid=32329)[0m - observation_space = Box(9,)
[2m[36m(pid=32329)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32329)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32329)[0m - _max_episode_steps = 150
[2m[36m(pid=32329)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32329)[0m [32m [     3.03918s,  INFO] TimeLimit:
[2m[36m(pid=32329)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32329)[0m - action_space = Box(2,)
[2m[36m(pid=32329)[0m - observation_space = Box(9,)
[2m[36m(pid=32329)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32329)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32329)[0m - _max_episode_steps = 150
[2m[36m(pid=32329)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,067	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,139	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.631, max=0.776, mean=0.096)},
[2m[36m(pid=32326)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.719, max=0.199, mean=-0.23)},
[2m[36m(pid=32326)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.844, max=0.537, mean=-0.038)},
[2m[36m(pid=32326)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.8, max=0.6, mean=-0.011)}}
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,146	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=32326)[0m   1: {'agent0': None},
[2m[36m(pid=32326)[0m   2: {'agent0': None},
[2m[36m(pid=32326)[0m   3: {'agent0': None}}
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,146	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.631, max=0.776, mean=0.096)
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,146	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.631, max=0.776, mean=0.096)
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,153	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32326)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32326)[0m                                   'env_id': 0,
[2m[36m(pid=32326)[0m                                   'info': None,
[2m[36m(pid=32326)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.631, max=0.776, mean=0.096),
[2m[36m(pid=32326)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32326)[0m                                   'rnn_state': []},
[2m[36m(pid=32326)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32326)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32326)[0m                                   'env_id': 1,
[2m[36m(pid=32326)[0m                                   'info': None,
[2m[36m(pid=32326)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.719, max=0.199, mean=-0.23),
[2m[36m(pid=32326)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32326)[0m                                   'rnn_state': []},
[2m[36m(pid=32326)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32326)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32326)[0m                                   'env_id': 2,
[2m[36m(pid=32326)[0m                                   'info': None,
[2m[36m(pid=32326)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.844, max=0.537, mean=-0.038),
[2m[36m(pid=32326)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32326)[0m                                   'rnn_state': []},
[2m[36m(pid=32326)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32326)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32326)[0m                                   'env_id': 3,
[2m[36m(pid=32326)[0m                                   'info': None,
[2m[36m(pid=32326)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.8, max=0.6, mean=-0.011),
[2m[36m(pid=32326)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32326)[0m                                   'rnn_state': []},
[2m[36m(pid=32326)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,156	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,205	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32326)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=0.049, max=0.16, mean=0.103),
[2m[36m(pid=32326)[0m                       [],
[2m[36m(pid=32326)[0m                       {})}
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,308	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x7f1fbf39f5f8>}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,318	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f1fbf39f2b0>}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,318	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1fbf399ef0>}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,324	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=32336)[0m [32m [     4.88420s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m [32m [     4.88511s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m [32m [     4.88600s,  INFO] TimeLimit:
[2m[36m(pid=32336)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32336)[0m - action_space = Box(2,)
[2m[36m(pid=32336)[0m - observation_space = Box(9,)
[2m[36m(pid=32336)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32336)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32336)[0m - _max_episode_steps = 150
[2m[36m(pid=32336)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,407	INFO rollout_worker.py:428 -- Generating sample batch of size 200
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,507	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.94, max=0.167, mean=-0.173)},
[2m[36m(pid=32336)[0m   1: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.917, max=0.398, mean=-0.05)},
[2m[36m(pid=32336)[0m   2: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.834, max=0.877, mean=0.057)},
[2m[36m(pid=32336)[0m   3: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.88, max=0.122, mean=-0.169)}}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,507	INFO sampler.py:309 -- Info return from env: { 0: {'agent0': None},
[2m[36m(pid=32336)[0m   1: {'agent0': None},
[2m[36m(pid=32336)[0m   2: {'agent0': None},
[2m[36m(pid=32336)[0m   3: {'agent0': None}}
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,508	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.94, max=0.167, mean=-0.173)
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,508	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.94, max=0.167, mean=-0.173)
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,511	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32336)[0m                                   'env_id': 0,
[2m[36m(pid=32336)[0m                                   'info': None,
[2m[36m(pid=32336)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.94, max=0.167, mean=-0.173),
[2m[36m(pid=32336)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32336)[0m                                   'rnn_state': []},
[2m[36m(pid=32336)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32336)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32336)[0m                                   'env_id': 1,
[2m[36m(pid=32336)[0m                                   'info': None,
[2m[36m(pid=32336)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.917, max=0.398, mean=-0.05),
[2m[36m(pid=32336)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32336)[0m                                   'rnn_state': []},
[2m[36m(pid=32336)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32336)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32336)[0m                                   'env_id': 2,
[2m[36m(pid=32336)[0m                                   'info': None,
[2m[36m(pid=32336)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.834, max=0.877, mean=0.057),
[2m[36m(pid=32336)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32336)[0m                                   'rnn_state': []},
[2m[36m(pid=32336)[0m                         'type': 'PolicyEvalData'},
[2m[36m(pid=32336)[0m                       { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=32336)[0m                                   'env_id': 3,
[2m[36m(pid=32336)[0m                                   'info': None,
[2m[36m(pid=32336)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.88, max=0.122, mean=-0.169),
[2m[36m(pid=32336)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=32336)[0m                                   'rnn_state': []},
[2m[36m(pid=32336)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,511	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,557	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32326)[0m { 'agent0': { 'data': { 'actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.019),
[2m[36m(pid=32326)[0m                         'agent_index': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                         'dones': np.ndarray((50,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                         'eps_id': np.ndarray((50,), dtype=int64, min=331773579.0, max=331773579.0, mean=331773579.0),
[2m[36m(pid=32326)[0m                         'infos': np.ndarray((50,), dtype=object, head={}),
[2m[36m(pid=32326)[0m                         'new_obs': np.ndarray((50, 9), dtype=float32, min=-3.554, max=2.4, mean=0.087),
[2m[36m(pid=32326)[0m                         'obs': np.ndarray((50, 9), dtype=float32, min=-3.554, max=2.4, mean=0.094),
[2m[36m(pid=32326)[0m                         'prev_actions': np.ndarray((50, 2), dtype=float32, min=-1.0, max=1.0, mean=0.026),
[2m[36m(pid=32326)[0m                         'prev_rewards': np.ndarray((50,), dtype=float32, min=-3.487, max=2.083, mean=-0.261),
[2m[36m(pid=32326)[0m                         'rewards': np.ndarray((50,), dtype=float32, min=-9.726, max=6.019, mean=-0.623),
[2m[36m(pid=32326)[0m                         't': np.ndarray((50,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=32326)[0m                         'unroll_id': np.ndarray((50,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m                         'weights': np.ndarray((50,), dtype=float32, min=0.282, max=9.595, mean=3.112)},
[2m[36m(pid=32326)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32336)[0m 2019-07-24 03:03:28,599	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m { 'default_policy': ( np.ndarray((4, 2), dtype=float32, min=-0.203, max=-0.007, mean=-0.13),
[2m[36m(pid=32336)[0m                       [],
[2m[36m(pid=32336)[0m                       {})}
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32326)[0m 2019-07-24 03:03:28,584	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32326)[0m { 'data': { 'actions': np.ndarray((200, 2), dtype=float32, min=-1.0, max=1.0, mean=0.023),
[2m[36m(pid=32326)[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m             'eps_id': np.ndarray((200,), dtype=int64, min=10656006.0, max=831515439.0, mean=353179185.5),
[2m[36m(pid=32326)[0m             'infos': np.ndarray((200,), dtype=object, head={}),
[2m[36m(pid=32326)[0m             'new_obs': np.ndarray((200, 9), dtype=float32, min=-5.237, max=4.269, mean=0.038),
[2m[36m(pid=32326)[0m             'obs': np.ndarray((200, 9), dtype=float32, min=-5.237, max=4.269, mean=0.048),
[2m[36m(pid=32326)[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=-1.0, max=1.0, mean=0.028),
[2m[36m(pid=32326)[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-5.959, max=4.076, mean=-0.151),
[2m[36m(pid=32326)[0m             'rewards': np.ndarray((200,), dtype=float32, min=-16.351, max=11.795, mean=-0.371),
[2m[36m(pid=32326)[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=49.0, mean=24.5),
[2m[36m(pid=32326)[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32326)[0m             'weights': np.ndarray((200,), dtype=float32, min=0.054, max=16.101, mean=3.18)},
[2m[36m(pid=32326)[0m   'type': 'SampleBatch'}
[2m[36m(pid=32326)[0m 
[2m[36m(pid=32336)[0m 2019-07-24 03:03:29,201	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.023),
[2m[36m(pid=32336)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=32336)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=146262035.0, max=146262035.0, mean=146262035.0),
[2m[36m(pid=32336)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=32336)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-10.0, max=10.0, mean=0.623),
[2m[36m(pid=32336)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-10.0, max=10.0, mean=0.597),
[2m[36m(pid=32336)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.022),
[2m[36m(pid=32336)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-30.437, max=30.419, mean=-0.692),
[2m[36m(pid=32336)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-39.542, max=36.44, mean=-1.996),
[2m[36m(pid=32336)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32336)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m                         'weights': np.ndarray((150,), dtype=float32, min=0.015, max=39.342, mean=15.849)},
[2m[36m(pid=32336)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m 2019-07-24 03:03:29,304	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m { 'data': { 'actions': np.ndarray((300, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.016),
[2m[36m(pid=32336)[0m             'agent_index': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m             'dones': np.ndarray((300,), dtype=bool, min=0.0, max=1.0, mean=0.02),
[2m[36m(pid=32336)[0m             'eps_id': np.ndarray((300,), dtype=int64, min=146262035.0, max=156577472.0, mean=151419753.5),
[2m[36m(pid=32336)[0m             'infos': np.ndarray((300,), dtype=object, head={}),
[2m[36m(pid=32336)[0m             'new_obs': np.ndarray((300, 9), dtype=float32, min=-10.0, max=10.0, mean=0.59),
[2m[36m(pid=32336)[0m             'obs': np.ndarray((300, 9), dtype=float32, min=-10.0, max=10.0, mean=0.571),
[2m[36m(pid=32336)[0m             'prev_actions': np.ndarray((300, 2), dtype=float32, min=-1.0, max=1.0, mean=-0.016),
[2m[36m(pid=32336)[0m             'prev_rewards': np.ndarray((300,), dtype=float32, min=-30.526, max=30.419, mean=-0.625),
[2m[36m(pid=32336)[0m             'rewards': np.ndarray((300,), dtype=float32, min=-40.9, max=36.44, mean=-1.877),
[2m[36m(pid=32336)[0m             't': np.ndarray((300,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=32336)[0m             'unroll_id': np.ndarray((300,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=32336)[0m             'weights': np.ndarray((300,), dtype=float32, min=0.013, max=41.021, mean=16.005)},
[2m[36m(pid=32336)[0m   'type': 'SampleBatch'}
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32616)[0m [32m [     0.03747s,  INFO] TimeLimit:
[2m[36m(pid=32616)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32616)[0m - action_space = Box(2,)
[2m[36m(pid=32616)[0m - observation_space = Box(9,)
[2m[36m(pid=32616)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32616)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32616)[0m - _max_episode_steps = 150
[2m[36m(pid=32616)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32616)[0m 2019-07-24 03:03:29,937	INFO rollout_worker.py:301 -- Creating policy evaluation worker 10 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32616)[0m 2019-07-24 03:03:29.938274: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32616)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32616)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32612)[0m 2019-07-24 03:03:30,242	INFO rollout_worker.py:301 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32612)[0m 2019-07-24 03:03:30.243207: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32612)[0m [32m [     0.05781s,  INFO] TimeLimit:
[2m[36m(pid=32612)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32612)[0m - action_space = Box(2,)
[2m[36m(pid=32612)[0m - observation_space = Box(9,)
[2m[36m(pid=32612)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32612)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32612)[0m - _max_episode_steps = 150
[2m[36m(pid=32612)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32614)[0m [32m [     0.08144s,  INFO] TimeLimit:
[2m[36m(pid=32614)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32614)[0m - action_space = Box(2,)
[2m[36m(pid=32614)[0m - observation_space = Box(9,)
[2m[36m(pid=32614)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32614)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32614)[0m - _max_episode_steps = 150
[2m[36m(pid=32614)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32614)[0m 2019-07-24 03:03:30,301	INFO rollout_worker.py:301 -- Creating policy evaluation worker 8 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32614)[0m 2019-07-24 03:03:30.302368: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32612)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32612)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32615)[0m 2019-07-24 03:03:30,315	INFO rollout_worker.py:301 -- Creating policy evaluation worker 9 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32615)[0m 2019-07-24 03:03:30.315643: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32615)[0m [32m [     0.04355s,  INFO] TimeLimit:
[2m[36m(pid=32615)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32615)[0m - action_space = Box(2,)
[2m[36m(pid=32615)[0m - observation_space = Box(9,)
[2m[36m(pid=32615)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32615)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32615)[0m - _max_episode_steps = 150
[2m[36m(pid=32615)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32614)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32614)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32615)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32615)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32616)[0m [32m [     1.64732s,  INFO] TimeLimit:
[2m[36m(pid=32616)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32616)[0m - action_space = Box(2,)
[2m[36m(pid=32616)[0m - observation_space = Box(9,)
[2m[36m(pid=32616)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32616)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32616)[0m - _max_episode_steps = 150
[2m[36m(pid=32616)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32616)[0m [32m [     1.64785s,  INFO] TimeLimit:
[2m[36m(pid=32616)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32616)[0m - action_space = Box(2,)
[2m[36m(pid=32616)[0m - observation_space = Box(9,)
[2m[36m(pid=32616)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32616)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32616)[0m - _max_episode_steps = 150
[2m[36m(pid=32616)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32616)[0m [32m [     1.64834s,  INFO] TimeLimit:
[2m[36m(pid=32616)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32616)[0m - action_space = Box(2,)
[2m[36m(pid=32616)[0m - observation_space = Box(9,)
[2m[36m(pid=32616)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32616)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32616)[0m - _max_episode_steps = 150
[2m[36m(pid=32616)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32713)[0m 2019-07-24 03:03:31,690	INFO rollout_worker.py:301 -- Creating policy evaluation worker 11 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=32713)[0m 2019-07-24 03:03:31.690801: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=32713)[0m [32m [     0.05694s,  INFO] TimeLimit:
[2m[36m(pid=32713)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32713)[0m - action_space = Box(2,)
[2m[36m(pid=32713)[0m - observation_space = Box(9,)
[2m[36m(pid=32713)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32713)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32713)[0m - _max_episode_steps = 150
[2m[36m(pid=32713)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32612)[0m [32m [     1.53903s,  INFO] TimeLimit:
[2m[36m(pid=32612)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32612)[0m - action_space = Box(2,)
[2m[36m(pid=32612)[0m - observation_space = Box(9,)
[2m[36m(pid=32612)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32612)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32612)[0m - _max_episode_steps = 150
[2m[36m(pid=32612)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32612)[0m [32m [     1.53995s,  INFO] TimeLimit:
[2m[36m(pid=32612)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32612)[0m - action_space = Box(2,)
[2m[36m(pid=32612)[0m - observation_space = Box(9,)
[2m[36m(pid=32612)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32612)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32612)[0m - _max_episode_steps = 150
[2m[36m(pid=32612)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32612)[0m [32m [     1.54089s,  INFO] TimeLimit:
[2m[36m(pid=32612)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32612)[0m - action_space = Box(2,)
[2m[36m(pid=32612)[0m - observation_space = Box(9,)
[2m[36m(pid=32612)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32612)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32612)[0m - _max_episode_steps = 150
[2m[36m(pid=32612)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32614)[0m [32m [     1.62972s,  INFO] TimeLimit:
[2m[36m(pid=32614)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32614)[0m - action_space = Box(2,)
[2m[36m(pid=32614)[0m - observation_space = Box(9,)
[2m[36m(pid=32614)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32614)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32614)[0m - _max_episode_steps = 150
[2m[36m(pid=32614)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32614)[0m [32m [     1.63083s,  INFO] TimeLimit:
[2m[36m(pid=32614)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32614)[0m - action_space = Box(2,)
[2m[36m(pid=32614)[0m - observation_space = Box(9,)
[2m[36m(pid=32614)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32614)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32614)[0m - _max_episode_steps = 150
[2m[36m(pid=32614)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32614)[0m [32m [     1.63181s,  INFO] TimeLimit:
[2m[36m(pid=32614)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32614)[0m - action_space = Box(2,)
[2m[36m(pid=32614)[0m - observation_space = Box(9,)
[2m[36m(pid=32614)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32614)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32614)[0m - _max_episode_steps = 150
[2m[36m(pid=32614)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32713)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32713)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=32615)[0m [32m [     1.66011s,  INFO] TimeLimit:
[2m[36m(pid=32615)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32615)[0m - action_space = Box(2,)
[2m[36m(pid=32615)[0m - observation_space = Box(9,)
[2m[36m(pid=32615)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32615)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32615)[0m - _max_episode_steps = 150
[2m[36m(pid=32615)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32615)[0m [32m [     1.66109s,  INFO] TimeLimit:
[2m[36m(pid=32615)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32615)[0m - action_space = Box(2,)
[2m[36m(pid=32615)[0m - observation_space = Box(9,)
[2m[36m(pid=32615)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32615)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32615)[0m - _max_episode_steps = 150
[2m[36m(pid=32615)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32615)[0m [32m [     1.66201s,  INFO] TimeLimit:
[2m[36m(pid=32615)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32615)[0m - action_space = Box(2,)
[2m[36m(pid=32615)[0m - observation_space = Box(9,)
[2m[36m(pid=32615)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32615)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32615)[0m - _max_episode_steps = 150
[2m[36m(pid=32615)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32713)[0m [32m [     1.94398s,  INFO] TimeLimit:
[2m[36m(pid=32713)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32713)[0m - action_space = Box(2,)
[2m[36m(pid=32713)[0m - observation_space = Box(9,)
[2m[36m(pid=32713)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32713)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32713)[0m - _max_episode_steps = 150
[2m[36m(pid=32713)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32713)[0m [32m [     1.94498s,  INFO] TimeLimit:
[2m[36m(pid=32713)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32713)[0m - action_space = Box(2,)
[2m[36m(pid=32713)[0m - observation_space = Box(9,)
[2m[36m(pid=32713)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32713)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32713)[0m - _max_episode_steps = 150
[2m[36m(pid=32713)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32713)[0m [32m [     1.94591s,  INFO] TimeLimit:
[2m[36m(pid=32713)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=32713)[0m - action_space = Box(2,)
[2m[36m(pid=32713)[0m - observation_space = Box(9,)
[2m[36m(pid=32713)[0m - reward_range = (-inf, inf)
[2m[36m(pid=32713)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=32713)[0m - _max_episode_steps = 150
[2m[36m(pid=32713)[0m - _elapsed_steps = None [0m
[2m[36m(pid=32336)[0m 2019-07-24 03:03:36,171	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m { 'count': 512,
[2m[36m(pid=32336)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((512, 2), dtype=float32, min=-1.0, max=1.0, mean=0.036),
[2m[36m(pid=32336)[0m                                                     'batch_indexes': np.ndarray((512,), dtype=int64, min=40.0, max=12760.0, mean=5878.303),
[2m[36m(pid=32336)[0m                                                     'dones': np.ndarray((512,), dtype=bool, min=0.0, max=1.0, mean=0.012),
[2m[36m(pid=32336)[0m                                                     'new_obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.107),
[2m[36m(pid=32336)[0m                                                     'obs': np.ndarray((512, 9), dtype=float32, min=-10.0, max=10.0, mean=0.111),
[2m[36m(pid=32336)[0m                                                     'rewards': np.ndarray((512,), dtype=float32, min=-38.975, max=37.019, mean=-0.427),
[2m[36m(pid=32336)[0m                                                     'weights': np.ndarray((512,), dtype=float64, min=0.058, max=0.543, mean=0.113)},
[2m[36m(pid=32336)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=32336)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m 2019-07-24 03:03:36,532	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=32336)[0m 
[2m[36m(pid=32336)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.7034478,
[2m[36m(pid=32336)[0m                                          'mean_q': 0.054017387,
[2m[36m(pid=32336)[0m                                          'min_q': -0.22586973},
[2m[36m(pid=32336)[0m                       'td_error': np.ndarray((512,), dtype=float32, min=-36.942, max=38.965, mean=0.432)}}
[2m[36m(pid=32336)[0m 
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-04-02
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 23.05090587158013
  episode_reward_mean: -11.363555219530015
  episode_reward_min: -80.22771346929711
  episodes_this_iter: 632
  episodes_total: 632
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 17.39934730529785
        mean_q: 2.28593111038208
        min_q: -11.13478946685791
    learner_queue:
      size_count: 2459
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4386342439892262
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 263000
    num_steps_trained: 207872
    num_target_updates: 4
    num_weight_syncs: 655
    replay_shard_0:
      add_batch_time_ms: 7.92
      policy_default_policy:
        added_count: 65000
        est_size_bytes: 22165000
        num_entries: 65000
        sampled_count: 56320
      replay_time_ms: 33.548
      update_priorities_time_ms: 86.205
    sample_throughput: 8624.695
    train_throughput: 14719.479
  iterations_since_restore: 1
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5227558762800475
    mean_inference_ms: 2.0202948974262473
    mean_processing_ms: 1.1557396567070561
  time_since_restore: 30.54563593864441
  time_this_iter_s: 30.54563593864441
  time_total_s: 30.54563593864441
  timestamp: 1563930242
  timesteps_since_restore: 263000
  timesteps_this_iter: 263000
  timesteps_total: 263000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.3/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 30 s, 1 iter, 263000 ts, -11.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-04-32
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 14.570192101993554
  episode_reward_mean: -11.627082487931693
  episode_reward_min: -99.01647040988375
  episodes_this_iter: 624
  episodes_total: 1256
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 25.76807975769043
        mean_q: 3.1105875968933105
        min_q: -14.40117359161377
    learner_queue:
      size_count: 2926
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.4331281565541544
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 519800
    num_steps_trained: 446464
    num_target_updates: 8
    num_weight_syncs: 1297
    replay_shard_0:
      add_batch_time_ms: 8.847
      policy_default_policy:
        added_count: 132600
        est_size_bytes: 45216600
        num_entries: 132600
        sampled_count: 117248
      replay_time_ms: 36.9
      update_priorities_time_ms: 116.995
    sample_throughput: 4498.83
    train_throughput: 0.0
  iterations_since_restore: 2
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5321281291620175
    mean_inference_ms: 2.1118208328025743
    mean_processing_ms: 1.1550875058679775
  time_since_restore: 61.04975724220276
  time_this_iter_s: 30.50412130355835
  time_total_s: 61.04975724220276
  timestamp: 1563930272
  timesteps_since_restore: 519800
  timesteps_this_iter: 256800
  timesteps_total: 519800
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 61 s, 2 iter, 519800 ts, -11.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-05-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 31.684558949167283
  episode_reward_mean: 4.117119495973449
  episode_reward_min: -42.00903686673566
  episodes_this_iter: 632
  episodes_total: 1888
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 24.154687881469727
        mean_q: 4.683382987976074
        min_q: -10.017040252685547
    learner_queue:
      size_count: 3382
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.32496153618543844
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 779600
    num_steps_trained: 679936
    num_target_updates: 13
    num_weight_syncs: 1948
    replay_shard_0:
      add_batch_time_ms: 6.078
      policy_default_policy:
        added_count: 196400
        est_size_bytes: 66972400
        num_entries: 196400
        sampled_count: 176128
      replay_time_ms: 31.758
      update_priorities_time_ms: 109.318
    sample_throughput: 9998.52
    train_throughput: 12798.105
  iterations_since_restore: 3
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5350696185306405
    mean_inference_ms: 2.1311864282941984
    mean_processing_ms: 1.1595658480445155
  time_since_restore: 91.63270449638367
  time_this_iter_s: 30.582947254180908
  time_total_s: 91.63270449638367
  timestamp: 1563930303
  timesteps_since_restore: 779600
  timesteps_this_iter: 259800
  timesteps_total: 779600
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 91 s, 3 iter, 779600 ts, 4.12 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-05-33
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.58379685745347
  episode_reward_mean: 13.218373128533905
  episode_reward_min: -20.41425170384568
  episodes_this_iter: 620
  episodes_total: 2508
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.1675968170166
        mean_q: 6.299330234527588
        min_q: -11.789386749267578
    learner_queue:
      size_count: 3857
      size_mean: 1.48
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 7.0
      - 9.0
      size_std: 2.624804754643667
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 1034400
    num_steps_trained: 923136
    num_target_updates: 18
    num_weight_syncs: 2583
    replay_shard_0:
      add_batch_time_ms: 8.409
      policy_default_policy:
        added_count: 261200
        est_size_bytes: 89069200
        num_entries: 261200
        sampled_count: 238080
      replay_time_ms: 42.547
      update_priorities_time_ms: 82.47
    sample_throughput: 14141.043
    train_throughput: 0.0
  iterations_since_restore: 4
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5370846845120292
    mean_inference_ms: 2.1551520904054873
    mean_processing_ms: 1.1639139601495487
  time_since_restore: 122.18870115280151
  time_this_iter_s: 30.555996656417847
  time_total_s: 122.18870115280151
  timestamp: 1563930333
  timesteps_since_restore: 1034400
  timesteps_this_iter: 254800
  timesteps_total: 1034400
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.8/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 122 s, 4 iter, 1034400 ts, 13.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:06:04,424	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-06-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.32311904040879
  episode_reward_mean: 14.417647697837825
  episode_reward_min: -10.728351353459873
  episodes_this_iter: 628
  episodes_total: 3136
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 36.421781675234364
    episode_reward_mean: 13.891748456141565
    episode_reward_min: -10.06409788678237
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.8670823086427897
      mean_inference_ms: 1.2976391008899342
      mean_processing_ms: 0.5345597098780666
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 30.140512466430664
        mean_q: 7.124833583831787
        min_q: -18.773483276367188
    learner_queue:
      size_count: 4302
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.46
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 1295200
    num_steps_trained: 1151488
    num_target_updates: 22
    num_weight_syncs: 3235
    replay_shard_0:
      add_batch_time_ms: 7.892
      policy_default_policy:
        added_count: 325800
        est_size_bytes: 111097800
        num_entries: 325800
        sampled_count: 292864
      replay_time_ms: 37.723
      update_priorities_time_ms: 92.81
    sample_throughput: 8004.881
    train_throughput: 6830.832
  iterations_since_restore: 5
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.538476142798748
    mean_inference_ms: 2.150757086256645
    mean_processing_ms: 1.1627629446904313
  time_since_restore: 152.79841661453247
  time_this_iter_s: 30.609715461730957
  time_total_s: 152.79841661453247
  timestamp: 1563930364
  timesteps_since_restore: 1295200
  timesteps_this_iter: 260800
  timesteps_total: 1295200
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 12.9/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 152 s, 5 iter, 1295200 ts, 14.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-06-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.31625367789644
  episode_reward_mean: 16.08382922888711
  episode_reward_min: -9.06400700609724
  episodes_this_iter: 624
  episodes_total: 3760
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.902549743652344
        mean_q: 8.340381622314453
        min_q: -5.989341735839844
    learner_queue:
      size_count: 4763
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.3469870314579494
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 1552800
    num_steps_trained: 1387520
    num_target_updates: 27
    num_weight_syncs: 3879
    replay_shard_0:
      add_batch_time_ms: 13.397
      policy_default_policy:
        added_count: 387000
        est_size_bytes: 131967000
        num_entries: 387000
        sampled_count: 354304
      replay_time_ms: 37.283
      update_priorities_time_ms: 85.151
    sample_throughput: 5874.622
    train_throughput: 15039.033
  iterations_since_restore: 6
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.541004502155606
    mean_inference_ms: 2.1566345539657967
    mean_processing_ms: 1.1654132148205076
  time_since_restore: 183.24151730537415
  time_this_iter_s: 30.443100690841675
  time_total_s: 183.24151730537415
  timestamp: 1563930395
  timesteps_since_restore: 1552800
  timesteps_this_iter: 257600
  timesteps_total: 1552800
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.1/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 183 s, 6 iter, 1552800 ts, 16.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-07-06
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.57126316120806
  episode_reward_mean: 16.545725491915473
  episode_reward_min: -26.19116396958546
  episodes_this_iter: 628
  episodes_total: 4388
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.978878021240234
        mean_q: 7.926055431365967
        min_q: -3.2893550395965576
    learner_queue:
      size_count: 5221
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5219195340279956
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 1810000
    num_steps_trained: 1621504
    num_target_updates: 32
    num_weight_syncs: 4521
    replay_shard_0:
      add_batch_time_ms: 8.707
      policy_default_policy:
        added_count: 446200
        est_size_bytes: 152154200
        num_entries: 446200
        sampled_count: 414720
      replay_time_ms: 36.436
      update_priorities_time_ms: 85.387
    sample_throughput: 10307.505
    train_throughput: 0.0
  iterations_since_restore: 7
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5393919521455293
    mean_inference_ms: 2.1614319449694026
    mean_processing_ms: 1.1648626298579892
  time_since_restore: 213.7776973247528
  time_this_iter_s: 30.536180019378662
  time_total_s: 213.7776973247528
  timestamp: 1563930426
  timesteps_since_restore: 1810000
  timesteps_this_iter: 257200
  timesteps_total: 1810000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.2/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 213 s, 7 iter, 1810000 ts, 16.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-07-36
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.004305173716375
  episode_reward_mean: 15.752574774930123
  episode_reward_min: -13.126802605365384
  episodes_this_iter: 632
  episodes_total: 5020
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.76227951049805
        mean_q: 8.840719223022461
        min_q: -17.265243530273438
    learner_queue:
      size_count: 5659
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.482078831727758
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 2071600
    num_steps_trained: 1846272
    num_target_updates: 36
    num_weight_syncs: 5177
    replay_shard_0:
      add_batch_time_ms: 5.887
      policy_default_policy:
        added_count: 512200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 470016
      replay_time_ms: 33.736
      update_priorities_time_ms: 103.368
    sample_throughput: 9861.912
    train_throughput: 8415.498
  iterations_since_restore: 8
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.537786550022671
    mean_inference_ms: 2.158682287998192
    mean_processing_ms: 1.1644166184541596
  time_since_restore: 244.34269738197327
  time_this_iter_s: 30.56500005722046
  time_total_s: 244.34269738197327
  timestamp: 1563930456
  timesteps_since_restore: 2071600
  timesteps_this_iter: 261600
  timesteps_total: 2071600
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 244 s, 8 iter, 2071600 ts, 15.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-08-07
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.41643393486862
  episode_reward_mean: 17.15636227678882
  episode_reward_min: -16.579116285999735
  episodes_this_iter: 628
  episodes_total: 5648
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.71221923828125
        mean_q: 6.536410808563232
        min_q: -3.3349640369415283
    learner_queue:
      size_count: 6130
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.447213595499958
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 2327200
    num_steps_trained: 2087424
    num_target_updates: 41
    num_weight_syncs: 5816
    replay_shard_0:
      add_batch_time_ms: 15.519
      policy_default_policy:
        added_count: 570600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 534528
      replay_time_ms: 36.109
      update_priorities_time_ms: 109.049
    sample_throughput: 11076.848
    train_throughput: 18904.488
  iterations_since_restore: 9
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.538125488231679
    mean_inference_ms: 2.1577066844593733
    mean_processing_ms: 1.1631781158199737
  time_since_restore: 274.86107087135315
  time_this_iter_s: 30.518373489379883
  time_total_s: 274.86107087135315
  timestamp: 1563930487
  timesteps_since_restore: 2327200
  timesteps_this_iter: 255600
  timesteps_total: 2327200
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 274 s, 9 iter, 2327200 ts, 17.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:08:37,981	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-08-37
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.078132373044575
  episode_reward_mean: 17.996174451876072
  episode_reward_min: -9.652673871062484
  episodes_this_iter: 632
  episodes_total: 6280
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 34.57112265143189
    episode_reward_mean: 19.16524175985931
    episode_reward_min: 1.875729417332315
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.7327022366076774
      mean_inference_ms: 1.0157792317184116
      mean_processing_ms: 0.44413261125480685
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 29.188758850097656
        mean_q: 5.616798400878906
        min_q: -1.7562838792800903
    learner_queue:
      size_count: 6585
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 2585600
    num_steps_trained: 2320384
    num_target_updates: 46
    num_weight_syncs: 6461
    replay_shard_0:
      add_batch_time_ms: 10.042
      policy_default_policy:
        added_count: 634000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 592384
      replay_time_ms: 40.299
      update_priorities_time_ms: 93.492
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 10
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5386896422260685
    mean_inference_ms: 2.161346486369108
    mean_processing_ms: 1.1619604698053398
  time_since_restore: 305.403023481369
  time_this_iter_s: 30.54195261001587
  time_total_s: 305.403023481369
  timestamp: 1563930517
  timesteps_since_restore: 2585600
  timesteps_this_iter: 258400
  timesteps_total: 2585600
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 305 s, 10 iter, 2585600 ts, 18 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-09-09
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.19309821293682
  episode_reward_mean: 17.769916727738963
  episode_reward_min: -15.076650657019158
  episodes_this_iter: 624
  episodes_total: 6904
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.22154998779297
        mean_q: 5.7682647705078125
        min_q: -0.732412576675415
    learner_queue:
      size_count: 7048
      size_mean: 0.36
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5919459434779497
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 2842200
    num_steps_trained: 2555904
    num_target_updates: 50
    num_weight_syncs: 7103
    replay_shard_0:
      add_batch_time_ms: 15.319
      policy_default_policy:
        added_count: 699400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 650752
      replay_time_ms: 37.554
      update_priorities_time_ms: 94.492
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 11
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5395130163865183
    mean_inference_ms: 2.162872936908632
    mean_processing_ms: 1.1618481494340982
  time_since_restore: 335.8595154285431
  time_this_iter_s: 30.456491947174072
  time_total_s: 335.8595154285431
  timestamp: 1563930549
  timesteps_since_restore: 2842200
  timesteps_this_iter: 256600
  timesteps_total: 2842200
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 335 s, 11 iter, 2842200 ts, 17.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-09-39
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.25734125290658
  episode_reward_mean: 17.598706332561115
  episode_reward_min: -23.11242261578964
  episodes_this_iter: 628
  episodes_total: 7532
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.370872497558594
        mean_q: 6.477190017700195
        min_q: -0.4278132915496826
    learner_queue:
      size_count: 7492
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 3102800
    num_steps_trained: 2784768
    num_target_updates: 55
    num_weight_syncs: 7753
    replay_shard_0:
      add_batch_time_ms: 7.777
      policy_default_policy:
        added_count: 762200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 709120
      replay_time_ms: 34.917
      update_priorities_time_ms: 102.908
    sample_throughput: 23071.36
    train_throughput: 0.0
  iterations_since_restore: 12
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5402317835746744
    mean_inference_ms: 2.164946646960287
    mean_processing_ms: 1.1649317092866818
  time_since_restore: 366.41615772247314
  time_this_iter_s: 30.556642293930054
  time_total_s: 366.41615772247314
  timestamp: 1563930579
  timesteps_since_restore: 3102800
  timesteps_this_iter: 260600
  timesteps_total: 3102800
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 366 s, 12 iter, 3102800 ts, 17.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-10-10
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.14002836563628
  episode_reward_mean: 17.482322325435447
  episode_reward_min: -19.26268923825778
  episodes_this_iter: 616
  episodes_total: 8148
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.385398864746094
        mean_q: 5.430259704589844
        min_q: -0.8891973495483398
    learner_queue:
      size_count: 7956
      size_mean: 0.7
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 3.0
      - 5.0
      size_std: 1.3304134695650067
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 3359800
    num_steps_trained: 3022336
    num_target_updates: 60
    num_weight_syncs: 8396
    replay_shard_0:
      add_batch_time_ms: 13.304
      policy_default_policy:
        added_count: 823200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 766464
      replay_time_ms: 35.753
      update_priorities_time_ms: 101.356
    sample_throughput: 6202.344
    train_throughput: 15878.0
  iterations_since_restore: 13
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5396387493164276
    mean_inference_ms: 2.1660404728207854
    mean_processing_ms: 1.1656811792185857
  time_since_restore: 396.97309827804565
  time_this_iter_s: 30.55694055557251
  time_total_s: 396.97309827804565
  timestamp: 1563930610
  timesteps_since_restore: 3359800
  timesteps_this_iter: 257000
  timesteps_total: 3359800
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.4/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 396 s, 13 iter, 3359800 ts, 17.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-10-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.96842698058699
  episode_reward_mean: 17.813608293230942
  episode_reward_min: -10.222675544071228
  episodes_this_iter: 632
  episodes_total: 8780
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.34682083129883
        mean_q: 5.896936893463135
        min_q: -0.47737282514572144
    learner_queue:
      size_count: 8414
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 3618400
    num_steps_trained: 3256320
    num_target_updates: 64
    num_weight_syncs: 9045
    replay_shard_0:
      add_batch_time_ms: 14.513
      policy_default_policy:
        added_count: 883400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 826368
      replay_time_ms: 39.382
      update_priorities_time_ms: 90.702
    sample_throughput: 6185.742
    train_throughput: 0.0
  iterations_since_restore: 14
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5405554426534822
    mean_inference_ms: 2.163909064985077
    mean_processing_ms: 1.1664182221610597
  time_since_restore: 427.4416129589081
  time_this_iter_s: 30.468514680862427
  time_total_s: 427.4416129589081
  timestamp: 1563930640
  timesteps_since_restore: 3618400
  timesteps_this_iter: 258600
  timesteps_total: 3618400
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 427 s, 14 iter, 3618400 ts, 17.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:11:11,506	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-11-11
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.76926423473594
  episode_reward_mean: 18.305545610315054
  episode_reward_min: -6.085725507887218
  episodes_this_iter: 624
  episodes_total: 9404
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 36.77254808182972
    episode_reward_mean: 17.850280702639676
    episode_reward_min: -8.705398161336468
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.6613273773142513
      mean_inference_ms: 0.8747332177929621
      mean_processing_ms: 0.3980241271504876
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.13456344604492
        mean_q: 5.5167059898376465
        min_q: -0.5762825608253479
    learner_queue:
      size_count: 8881
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5744562646538028
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 3874600
    num_steps_trained: 3495936
    num_target_updates: 69
    num_weight_syncs: 9683
    replay_shard_0:
      add_batch_time_ms: 6.32
      policy_default_policy:
        added_count: 948200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 886784
      replay_time_ms: 37.37
      update_priorities_time_ms: 91.007
    sample_throughput: 5197.403
    train_throughput: 13305.351
  iterations_since_restore: 15
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5423755167707667
    mean_inference_ms: 2.1661242810201764
    mean_processing_ms: 1.165508688388191
  time_since_restore: 457.99351811408997
  time_this_iter_s: 30.551905155181885
  time_total_s: 457.99351811408997
  timestamp: 1563930671
  timesteps_since_restore: 3874600
  timesteps_this_iter: 256200
  timesteps_total: 3874600
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 457 s, 15 iter, 3874600 ts, 18.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-11-42
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.429179763801415
  episode_reward_mean: 18.54636366385301
  episode_reward_min: -10.612916702656648
  episodes_this_iter: 636
  episodes_total: 10040
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.411901473999023
        mean_q: 4.930852890014648
        min_q: -0.6112639307975769
    learner_queue:
      size_count: 9343
      size_mean: 0.22
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.46
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 4132800
    num_steps_trained: 3731968
    num_target_updates: 74
    num_weight_syncs: 10330
    replay_shard_0:
      add_batch_time_ms: 14.575
      policy_default_policy:
        added_count: 1013800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 942592
      replay_time_ms: 37.049
      update_priorities_time_ms: 111.785
    sample_throughput: 8042.461
    train_throughput: 0.0
  iterations_since_restore: 16
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5424751093572462
    mean_inference_ms: 2.163820745778208
    mean_processing_ms: 1.165388908716326
  time_since_restore: 488.5490961074829
  time_this_iter_s: 30.555577993392944
  time_total_s: 488.5490961074829
  timestamp: 1563930702
  timesteps_since_restore: 4132800
  timesteps_this_iter: 258200
  timesteps_total: 4132800
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 488 s, 16 iter, 4132800 ts, 18.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-12-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.82132243623693
  episode_reward_mean: 18.84651283482465
  episode_reward_min: -12.6880364358524
  episodes_this_iter: 624
  episodes_total: 10664
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.171363830566406
        mean_q: 5.203509330749512
        min_q: -0.5951030850410461
    learner_queue:
      size_count: 9795
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 4390000
    num_steps_trained: 3963904
    num_target_updates: 79
    num_weight_syncs: 10972
    replay_shard_0:
      add_batch_time_ms: 18.582
      policy_default_policy:
        added_count: 1078000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1000960
      replay_time_ms: 39.95
      update_priorities_time_ms: 102.772
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 17
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5424223314848375
    mean_inference_ms: 2.1647876372194372
    mean_processing_ms: 1.1640627528227403
  time_since_restore: 518.8554856777191
  time_this_iter_s: 30.306389570236206
  time_total_s: 518.8554856777191
  timestamp: 1563930733
  timesteps_since_restore: 4390000
  timesteps_this_iter: 257200
  timesteps_total: 4390000
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 518 s, 17 iter, 4390000 ts, 18.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-12-43
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.532616586577966
  episode_reward_mean: 18.262770127006757
  episode_reward_min: -6.905477229186718
  episodes_this_iter: 612
  episodes_total: 11276
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.37483596801758
        mean_q: 5.722394943237305
        min_q: -0.6119973063468933
    learner_queue:
      size_count: 10251
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.447213595499958
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 4648400
    num_steps_trained: 4197376
    num_target_updates: 83
    num_weight_syncs: 11618
    replay_shard_0:
      add_batch_time_ms: 13.705
      policy_default_policy:
        added_count: 1144000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1057280
      replay_time_ms: 35.439
      update_priorities_time_ms: 87.073
    sample_throughput: 0.0
    train_throughput: 23235.13
  iterations_since_restore: 18
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.545103610054202
    mean_inference_ms: 2.168118011322831
    mean_processing_ms: 1.1646208444276016
  time_since_restore: 549.3385367393494
  time_this_iter_s: 30.48305106163025
  time_total_s: 549.3385367393494
  timestamp: 1563930763
  timesteps_since_restore: 4648400
  timesteps_this_iter: 258400
  timesteps_total: 4648400
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 549 s, 18 iter, 4648400 ts, 18.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-13-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.025318467992484
  episode_reward_mean: 18.765165059988107
  episode_reward_min: -15.205212498266372
  episodes_this_iter: 620
  episodes_total: 11896
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.83788299560547
        mean_q: 5.201657295227051
        min_q: -0.47058558464050293
    learner_queue:
      size_count: 10710
      size_mean: 0.24
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4270831300812525
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 4906400
    num_steps_trained: 4431872
    num_target_updates: 88
    num_weight_syncs: 12264
    replay_shard_0:
      add_batch_time_ms: 9.961
      policy_default_policy:
        added_count: 1206000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1116672
      replay_time_ms: 30.583
      update_priorities_time_ms: 98.649
    sample_throughput: 12261.896
    train_throughput: 10463.484
  iterations_since_restore: 19
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5461429162008395
    mean_inference_ms: 2.169148374810556
    mean_processing_ms: 1.1657011093130394
  time_since_restore: 579.8159217834473
  time_this_iter_s: 30.4773850440979
  time_total_s: 579.8159217834473
  timestamp: 1563930794
  timesteps_since_restore: 4906400
  timesteps_this_iter: 258000
  timesteps_total: 4906400
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 579 s, 19 iter, 4906400 ts, 18.8 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:13:44,791	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-13-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.29030308136971
  episode_reward_mean: 18.562407781823914
  episode_reward_min: -11.145462147001993
  episodes_this_iter: 624
  episodes_total: 12520
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 32.3003550089312
    episode_reward_mean: 18.585847250347992
    episode_reward_min: -1.6073780033131404
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.6195456464905257
      mean_inference_ms: 0.7917827620438911
      mean_processing_ms: 0.3700770558881238
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.31998062133789
        mean_q: 4.556972026824951
        min_q: -0.7136208415031433
    learner_queue:
      size_count: 11161
      size_mean: 0.3
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.49999999999999994
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 5165200
    num_steps_trained: 4663296
    num_target_updates: 92
    num_weight_syncs: 12909
    replay_shard_0:
      add_batch_time_ms: 11.63
      policy_default_policy:
        added_count: 1266000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1176064
      replay_time_ms: 39.99
      update_priorities_time_ms: 101.893
    sample_throughput: 17994.354
    train_throughput: 0.0
  iterations_since_restore: 20
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5456047961665114
    mean_inference_ms: 2.1693571611481794
    mean_processing_ms: 1.165685133734719
  time_since_restore: 610.355560541153
  time_this_iter_s: 30.53963875770569
  time_total_s: 610.355560541153
  timestamp: 1563930824
  timesteps_since_restore: 5165200
  timesteps_this_iter: 258800
  timesteps_total: 5165200
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 610 s, 20 iter, 5165200 ts, 18.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-14-16
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.79611894558813
  episode_reward_mean: 19.216495371443333
  episode_reward_min: -11.528384349965322
  episodes_this_iter: 628
  episodes_total: 13148
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.54323196411133
        mean_q: 5.1031646728515625
        min_q: -0.5657997131347656
    learner_queue:
      size_count: 11622
      size_mean: 0.12
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.32496153618543844
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 5422200
    num_steps_trained: 4899328
    num_target_updates: 97
    num_weight_syncs: 13553
    replay_shard_0:
      add_batch_time_ms: 5.986
      policy_default_policy:
        added_count: 1333200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1233408
      replay_time_ms: 44.367
      update_priorities_time_ms: 102.291
    sample_throughput: 19991.976
    train_throughput: 6397.432
  iterations_since_restore: 21
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.545863478188306
    mean_inference_ms: 2.1691151520622594
    mean_processing_ms: 1.1660244697475743
  time_since_restore: 640.9410302639008
  time_this_iter_s: 30.585469722747803
  time_total_s: 640.9410302639008
  timestamp: 1563930856
  timesteps_since_restore: 5422200
  timesteps_this_iter: 257000
  timesteps_total: 5422200
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.5/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 640 s, 21 iter, 5422200 ts, 19.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-14-46
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.44757744660398
  episode_reward_mean: 18.48465904254937
  episode_reward_min: -12.41211480097824
  episodes_this_iter: 616
  episodes_total: 13764
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.254905700683594
        mean_q: 4.837931156158447
        min_q: -0.42007899284362793
    learner_queue:
      size_count: 12082
      size_mean: 0.4
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 3.0
      size_std: 0.7483314773547883
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 5678400
    num_steps_trained: 5134336
    num_target_updates: 102
    num_weight_syncs: 14193
    replay_shard_0:
      add_batch_time_ms: 9.519
      policy_default_policy:
        added_count: 1397800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1291264
      replay_time_ms: 34.205
      update_priorities_time_ms: 103.049
    sample_throughput: 6616.7
    train_throughput: 11292.501
  iterations_since_restore: 22
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5464014290179606
    mean_inference_ms: 2.1710557260597527
    mean_processing_ms: 1.16685085424867
  time_since_restore: 671.4629714488983
  time_this_iter_s: 30.52194118499756
  time_total_s: 671.4629714488983
  timestamp: 1563930886
  timesteps_since_restore: 5678400
  timesteps_this_iter: 256200
  timesteps_total: 5678400
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 671 s, 22 iter, 5678400 ts, 18.5 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-15-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.74373225091092
  episode_reward_mean: 18.626073769116708
  episode_reward_min: -5.7086703469782885
  episodes_this_iter: 624
  episodes_total: 14388
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.56420135498047
        mean_q: 5.0531463623046875
        min_q: -0.48519280552864075
    learner_queue:
      size_count: 12549
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 5934600
    num_steps_trained: 5373952
    num_target_updates: 107
    num_weight_syncs: 14834
    replay_shard_0:
      add_batch_time_ms: 17.096
      policy_default_policy:
        added_count: 1453600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1353216
      replay_time_ms: 34.175
      update_priorities_time_ms: 96.187
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 23
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.547003141582665
    mean_inference_ms: 2.171733605739394
    mean_processing_ms: 1.1666987650982867
  time_since_restore: 701.9591734409332
  time_this_iter_s: 30.496201992034912
  time_total_s: 701.9591734409332
  timestamp: 1563930917
  timesteps_since_restore: 5934600
  timesteps_this_iter: 256200
  timesteps_total: 5934600
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 701 s, 23 iter, 5934600 ts, 18.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-15-47
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.632011523064286
  episode_reward_mean: 19.219548880095186
  episode_reward_min: -15.03352940195245
  episodes_this_iter: 624
  episodes_total: 15012
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.50022506713867
        mean_q: 4.477167129516602
        min_q: -0.889823317527771
    learner_queue:
      size_count: 13003
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 6193000
    num_steps_trained: 5606400
    num_target_updates: 111
    num_weight_syncs: 15480
    replay_shard_0:
      add_batch_time_ms: 9.37
      policy_default_policy:
        added_count: 1523000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1412096
      replay_time_ms: 41.993
      update_priorities_time_ms: 91.974
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 24
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5470634019459957
    mean_inference_ms: 2.1713820093450287
    mean_processing_ms: 1.166368155779202
  time_since_restore: 732.4807844161987
  time_this_iter_s: 30.521610975265503
  time_total_s: 732.4807844161987
  timestamp: 1563930947
  timesteps_since_restore: 6193000
  timesteps_this_iter: 258400
  timesteps_total: 6193000
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 732 s, 24 iter, 6193000 ts, 19.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:16:18,392	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-16-18
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.587452284429716
  episode_reward_mean: 19.338583636394162
  episode_reward_min: -5.443007815311495
  episodes_this_iter: 632
  episodes_total: 15644
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 38.034813949237595
    episode_reward_mean: 20.07705384784967
    episode_reward_min: 7.803289573659945
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.591857622422475
      mean_inference_ms: 0.7352125006393707
      mean_processing_ms: 0.35123363173662153
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 35.106910705566406
        mean_q: 4.625044822692871
        min_q: -1.0741002559661865
    learner_queue:
      size_count: 13451
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.34698703145794946
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 6453000
    num_steps_trained: 5835776
    num_target_updates: 116
    num_weight_syncs: 16129
    replay_shard_0:
      add_batch_time_ms: 7.823
      policy_default_policy:
        added_count: 1593400
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1465344
      replay_time_ms: 40.053
      update_priorities_time_ms: 108.383
    sample_throughput: 12155.113
    train_throughput: 0.0
  iterations_since_restore: 25
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.547314783450907
    mean_inference_ms: 2.1713226982608447
    mean_processing_ms: 1.166194319582925
  time_since_restore: 763.0269002914429
  time_this_iter_s: 30.54611587524414
  time_total_s: 763.0269002914429
  timestamp: 1563930978
  timesteps_since_restore: 6453000
  timesteps_this_iter: 260000
  timesteps_total: 6453000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 763 s, 25 iter, 6453000 ts, 19.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-16-49
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.81350278565593
  episode_reward_mean: 19.053890582329092
  episode_reward_min: -12.199809516050252
  episodes_this_iter: 632
  episodes_total: 16276
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.62922668457031
        mean_q: 4.338108062744141
        min_q: -0.05103064328432083
    learner_queue:
      size_count: 13911
      size_mean: 1.5
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 6.0
      - 8.0
      size_std: 2.4020824298928627
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 6710600
    num_steps_trained: 6071296
    num_target_updates: 120
    num_weight_syncs: 16774
    replay_shard_0:
      add_batch_time_ms: 18.036
      policy_default_policy:
        added_count: 1655200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1525760
      replay_time_ms: 40.323
      update_priorities_time_ms: 83.399
    sample_throughput: 16543.022
    train_throughput: 21175.068
  iterations_since_restore: 26
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.546207418201578
    mean_inference_ms: 2.170030377647902
    mean_processing_ms: 1.165908721094644
  time_since_restore: 793.5463709831238
  time_this_iter_s: 30.519470691680908
  time_total_s: 793.5463709831238
  timestamp: 1563931009
  timesteps_since_restore: 6710600
  timesteps_this_iter: 257600
  timesteps_total: 6710600
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 793 s, 26 iter, 6710600 ts, 19.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-17-20
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.657128126317204
  episode_reward_mean: 18.280128687131054
  episode_reward_min: -7.834201180482253
  episodes_this_iter: 616
  episodes_total: 16892
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.620853424072266
        mean_q: 3.9073314666748047
        min_q: -0.39086291193962097
    learner_queue:
      size_count: 14366
      size_mean: 0.14
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.34698703145794946
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 6969000
    num_steps_trained: 6304256
    num_target_updates: 125
    num_weight_syncs: 17418
    replay_shard_0:
      add_batch_time_ms: 9.013
      policy_default_policy:
        added_count: 1718000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1578496
      replay_time_ms: 37.665
      update_priorities_time_ms: 107.496
    sample_throughput: 16920.874
    train_throughput: 21658.719
  iterations_since_restore: 27
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5465952379287329
    mean_inference_ms: 2.171364578073646
    mean_processing_ms: 1.1662390860356504
  time_since_restore: 824.024073600769
  time_this_iter_s: 30.477702617645264
  time_total_s: 824.024073600769
  timestamp: 1563931040
  timesteps_since_restore: 6969000
  timesteps_this_iter: 258400
  timesteps_total: 6969000
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 824 s, 27 iter, 6969000 ts, 18.3 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-17-50
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.79229917175385
  episode_reward_mean: 19.130632457475826
  episode_reward_min: -6.234480336888059
  episodes_this_iter: 620
  episodes_total: 17512
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.5370979309082
        mean_q: 3.768812894821167
        min_q: -0.7690778374671936
    learner_queue:
      size_count: 14824
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 7226400
    num_steps_trained: 6538752
    num_target_updates: 130
    num_weight_syncs: 18063
    replay_shard_0:
      add_batch_time_ms: 5.418
      policy_default_policy:
        added_count: 1787200
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1634304
      replay_time_ms: 41.168
      update_priorities_time_ms: 96.345
    sample_throughput: 16185.085
    train_throughput: 0.0
  iterations_since_restore: 28
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5477661823965847
    mean_inference_ms: 2.1726343538462136
    mean_processing_ms: 1.1671550456043163
  time_since_restore: 854.6190614700317
  time_this_iter_s: 30.594987869262695
  time_total_s: 854.6190614700317
  timestamp: 1563931070
  timesteps_since_restore: 7226400
  timesteps_this_iter: 257400
  timesteps_total: 7226400
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 854 s, 28 iter, 7226400 ts, 19.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-18-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.90333443190701
  episode_reward_mean: 19.56819353333309
  episode_reward_min: -3.3372259465222056
  episodes_this_iter: 616
  episodes_total: 18128
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.04735565185547
        mean_q: 4.541081428527832
        min_q: -0.41563576459884644
    learner_queue:
      size_count: 15280
      size_mean: 0.36
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 3.0
      size_std: 0.6248199740725324
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 7484000
    num_steps_trained: 6772224
    num_target_updates: 134
    num_weight_syncs: 18707
    replay_shard_0:
      add_batch_time_ms: 5.984
      policy_default_policy:
        added_count: 1853000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1689600
      replay_time_ms: 37.733
      update_priorities_time_ms: 100.385
    sample_throughput: 13723.181
    train_throughput: 10037.527
  iterations_since_restore: 29
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5476406257442425
    mean_inference_ms: 2.1733871042414084
    mean_processing_ms: 1.1675767585465577
  time_since_restore: 885.1901490688324
  time_this_iter_s: 30.57108759880066
  time_total_s: 885.1901490688324
  timestamp: 1563931101
  timesteps_since_restore: 7484000
  timesteps_this_iter: 257600
  timesteps_total: 7484000
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 885 s, 29 iter, 7484000 ts, 19.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:18:51,986	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-18-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.26063115287019
  episode_reward_mean: 19.149134036130825
  episode_reward_min: -1.53928091970674
  episodes_this_iter: 628
  episodes_total: 18756
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 35.33611726261982
    episode_reward_mean: 19.53899887401733
    episode_reward_min: 7.577108754273052
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.5729710617603019
      mean_inference_ms: 0.6969573043274031
      mean_processing_ms: 0.339093468934326
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.23698043823242
        mean_q: 3.510434627532959
        min_q: -0.12354157865047455
    learner_queue:
      size_count: 15729
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.417612260356422
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 7743000
    num_steps_trained: 7002112
    num_target_updates: 139
    num_weight_syncs: 19356
    replay_shard_0:
      add_batch_time_ms: 12.221
      policy_default_policy:
        added_count: 1912600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1744896
      replay_time_ms: 36.857
      update_priorities_time_ms: 98.743
    sample_throughput: 4558.357
    train_throughput: 11669.394
  iterations_since_restore: 30
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5479550974135394
    mean_inference_ms: 2.1739261571039186
    mean_processing_ms: 1.1674696625960423
  time_since_restore: 915.700023651123
  time_this_iter_s: 30.50987458229065
  time_total_s: 915.700023651123
  timestamp: 1563931131
  timesteps_since_restore: 7743000
  timesteps_this_iter: 259000
  timesteps_total: 7743000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 915 s, 30 iter, 7743000 ts, 19.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-19-23
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.540434645120406
  episode_reward_mean: 18.559170897728343
  episode_reward_min: -13.141400780747567
  episodes_this_iter: 616
  episodes_total: 19372
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.462730407714844
        mean_q: 3.569460391998291
        min_q: -0.32169443368911743
    learner_queue:
      size_count: 16198
      size_mean: 0.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.38418745424597095
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 7998400
    num_steps_trained: 7241728
    num_target_updates: 144
    num_weight_syncs: 19993
    replay_shard_0:
      add_batch_time_ms: 11.957
      policy_default_policy:
        added_count: 1975000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1803264
      replay_time_ms: 33.045
      update_priorities_time_ms: 95.751
    sample_throughput: 5345.071
    train_throughput: 13683.382
  iterations_since_restore: 31
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5482460819044548
    mean_inference_ms: 2.174645479219623
    mean_processing_ms: 1.1678264391292263
  time_since_restore: 946.2094728946686
  time_this_iter_s: 30.509449243545532
  time_total_s: 946.2094728946686
  timestamp: 1563931163
  timesteps_since_restore: 7998400
  timesteps_this_iter: 255400
  timesteps_total: 7998400
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.6/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 946 s, 31 iter, 7998400 ts, 18.6 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-19-53
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.97389727827144
  episode_reward_mean: 18.854699675910297
  episode_reward_min: -3.6406063652871183
  episodes_this_iter: 624
  episodes_total: 19996
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.146095275878906
        mean_q: 3.8755974769592285
        min_q: -0.9199343919754028
    learner_queue:
      size_count: 16649
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 8257400
    num_steps_trained: 7472128
    num_target_updates: 148
    num_weight_syncs: 20640
    replay_shard_0:
      add_batch_time_ms: 6.095
      policy_default_policy:
        added_count: 2038600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1864704
      replay_time_ms: 38.584
      update_priorities_time_ms: 111.081
    sample_throughput: 7995.623
    train_throughput: 0.0
  iterations_since_restore: 32
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5487093562522032
    mean_inference_ms: 2.1752883469345106
    mean_processing_ms: 1.1681086558534015
  time_since_restore: 976.726719379425
  time_this_iter_s: 30.51724648475647
  time_total_s: 976.726719379425
  timestamp: 1563931193
  timesteps_since_restore: 8257400
  timesteps_this_iter: 259000
  timesteps_total: 8257400
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 976 s, 32 iter, 8257400 ts, 18.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-20-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.1237960334364
  episode_reward_mean: 19.133469539559837
  episode_reward_min: -6.183872946317083
  episodes_this_iter: 636
  episodes_total: 20632
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.664119720458984
        mean_q: 3.553753614425659
        min_q: -0.2424691915512085
    learner_queue:
      size_count: 17110
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 8514400
    num_steps_trained: 7709184
    num_target_updates: 153
    num_weight_syncs: 21282
    replay_shard_0:
      add_batch_time_ms: 7.685
      policy_default_policy:
        added_count: 2096800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1926656
      replay_time_ms: 38.08
      update_priorities_time_ms: 102.296
    sample_throughput: 14798.899
    train_throughput: 0.0
  iterations_since_restore: 33
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5476743125379093
    mean_inference_ms: 2.173957893077949
    mean_processing_ms: 1.1681500018687894
  time_since_restore: 1007.2570328712463
  time_this_iter_s: 30.53031349182129
  time_total_s: 1007.2570328712463
  timestamp: 1563931224
  timesteps_since_restore: 8514400
  timesteps_this_iter: 257000
  timesteps_total: 8514400
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 1007 s, 33 iter, 8514400 ts, 19.1 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-20-55
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.581038097908554
  episode_reward_mean: 18.728850715294456
  episode_reward_min: -5.158144730478022
  episodes_this_iter: 632
  episodes_total: 21264
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 32.257415771484375
        mean_q: 3.685546398162842
        min_q: -0.2671630084514618
    learner_queue:
      size_count: 17553
      size_mean: 0.1
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 0.10000000000000142
      - 1.0
      size_std: 0.3
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 8774600
    num_steps_trained: 7936000
    num_target_updates: 158
    num_weight_syncs: 21933
    replay_shard_0:
      add_batch_time_ms: 6.901
      policy_default_policy:
        added_count: 2164000
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 1981952
      replay_time_ms: 36.109
      update_priorities_time_ms: 98.346
    sample_throughput: 4231.072
    train_throughput: 0.0
  iterations_since_restore: 34
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5469276308122077
    mean_inference_ms: 2.173057049803118
    mean_processing_ms: 1.1676743430157641
  time_since_restore: 1037.782649755478
  time_this_iter_s: 30.525616884231567
  time_total_s: 1037.782649755478
  timestamp: 1563931255
  timesteps_since_restore: 8774600
  timesteps_this_iter: 260200
  timesteps_total: 8774600
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 1037 s, 34 iter, 8774600 ts, 18.7 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

[2m[36m(pid=32336)[0m 2019-07-24 03:21:25,569	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-21-25
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.06475641200601
  episode_reward_mean: 18.865690281848906
  episode_reward_min: -14.382868542011122
  episodes_this_iter: 628
  episodes_total: 21892
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 37.985378578733005
    episode_reward_mean: 21.96783624466927
    episode_reward_min: 4.277418117210285
    episodes_this_iter: 20
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.559281397016818
      mean_inference_ms: 0.6683533478927421
      mean_processing_ms: 0.32938692772751993
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 36.04867172241211
        mean_q: 3.672820806503296
        min_q: -0.33448806405067444
    learner_queue:
      size_count: 18010
      size_mean: 0.82
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 4.0
      - 6.0
      size_std: 1.5058552387264852
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 9032600
    num_steps_trained: 8169472
    num_target_updates: 162
    num_weight_syncs: 22578
    replay_shard_0:
      add_batch_time_ms: 7.89
      policy_default_policy:
        added_count: 2228600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 2037760
      replay_time_ms: 46.039
      update_priorities_time_ms: 90.554
    sample_throughput: 24935.174
    train_throughput: 0.0
  iterations_since_restore: 35
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.547386477065594
    mean_inference_ms: 2.172243531497373
    mean_processing_ms: 1.167664653280994
  time_since_restore: 1068.334742307663
  time_this_iter_s: 30.55209255218506
  time_total_s: 1068.334742307663
  timestamp: 1563931285
  timesteps_since_restore: 9032600
  timesteps_this_iter: 258000
  timesteps_total: 9032600
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 1068 s, 35 iter, 9032600 ts, 18.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-21-56
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 39.17968575243415
  episode_reward_mean: 18.86686493954927
  episode_reward_min: -1.222210280217636
  episodes_this_iter: 632
  episodes_total: 22524
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 34.48878479003906
        mean_q: 3.1841509342193604
        min_q: -0.3351408541202545
    learner_queue:
      size_count: 18472
      size_mean: 0.16
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.36660605559646714
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 9289400
    num_steps_trained: 8406016
    num_target_updates: 167
    num_weight_syncs: 23222
    replay_shard_0:
      add_batch_time_ms: 10.616
      policy_default_policy:
        added_count: 2288800
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 2096128
      replay_time_ms: 27.362
      update_priorities_time_ms: 99.125
    sample_throughput: 7513.913
    train_throughput: 0.0
  iterations_since_restore: 36
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5470405830982041
    mean_inference_ms: 2.1711951741777074
    mean_processing_ms: 1.1676806475754478
  time_since_restore: 1098.7951817512512
  time_this_iter_s: 30.460439443588257
  time_total_s: 1098.7951817512512
  timestamp: 1563931316
  timesteps_since_restore: 9289400
  timesteps_this_iter: 256800
  timesteps_total: 9289400
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 1098 s, 36 iter, 9289400 ts, 18.9 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew
2019-07-24 03:23:28,502	INFO ray_trial_executor.py:187 -- Destroying actor for trial APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-22-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.9661925214796
  episode_reward_mean: 19.37872140680791
  episode_reward_min: -1.2800157788285709
  episodes_this_iter: 620
  episodes_total: 23144
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.607242584228516
        mean_q: 3.7038376331329346
        min_q: -0.9101136326789856
    learner_queue:
      size_count: 18921
      size_mean: 0.2
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 1.0
      size_std: 0.4
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 9548600
    num_steps_trained: 8635904
    num_target_updates: 172
    num_weight_syncs: 23868
    replay_shard_0:
      add_batch_time_ms: 8.418
      policy_default_policy:
        added_count: 2355600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 2154496
      replay_time_ms: 38.234
      update_priorities_time_ms: 97.493
    sample_throughput: 4654.494
    train_throughput: 0.0
  iterations_since_restore: 37
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5476080742181695
    mean_inference_ms: 2.1710452750331655
    mean_processing_ms: 1.1680604532729477
  time_since_restore: 1129.29802775383
  time_this_iter_s: 30.502846002578735
  time_total_s: 1129.29802775383
  timestamp: 1563931347
  timesteps_since_restore: 9548600
  timesteps_this_iter: 259200
  timesteps_total: 9548600
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 1129 s, 37 iter, 9548600 ts, 19.4 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-22-57
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 38.83747688824374
  episode_reward_mean: 19.16807096037859
  episode_reward_min: -10.457059970447732
  episodes_this_iter: 628
  episodes_total: 23772
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 33.17731857299805
        mean_q: 3.890939235687256
        min_q: -0.41526344418525696
    learner_queue:
      size_count: 19375
      size_mean: 0.26
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 1.0
      - 2.0
      size_std: 0.5219195340279955
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 9807200
    num_steps_trained: 8868864
    num_target_updates: 176
    num_weight_syncs: 24515
    replay_shard_0:
      add_batch_time_ms: 18.616
      policy_default_policy:
        added_count: 2418600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 2212864
      replay_time_ms: 31.07
      update_priorities_time_ms: 91.587
    sample_throughput: 14563.556
    train_throughput: 0.0
  iterations_since_restore: 38
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.5476923952831025
    mean_inference_ms: 2.170392594024445
    mean_processing_ms: 1.1683603798680304
  time_since_restore: 1159.8185722827911
  time_this_iter_s: 30.52054452896118
  time_total_s: 1159.8185722827911
  timestamp: 1563931377
  timesteps_since_restore: 9807200
  timesteps_this_iter: 258600
  timesteps_total: 9807200
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 1/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})
RUNNING trials:
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	RUNNING, [12 CPUs, 1 GPUs], [pid=32336], 1159 s, 38 iter, 9807200 ts, 19.2 rew
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew

Result for APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:
  custom_metrics: {}
  date: 2019-07-24_03-23-28
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 39.023729370213736
  episode_reward_mean: 19.698224500568795
  episode_reward_min: -5.30790579545751
  episodes_this_iter: 620
  episodes_total: 24392
  experiment_id: 551d10f630db4ab0a22be0a16a50447a
  hostname: navel-notebook-1
  info:
    learner:
      default_policy:
        max_q: 31.414316177368164
        mean_q: 3.1024181842803955
        min_q: -0.2740628123283386
    learner_queue:
      size_count: 19837
      size_mean: 1.18
      size_quantiles:
      - 0.0
      - 0.0
      - 0.0
      - 5.0
      - 7.0
      size_std: 2.075475849052453
    max_exploration: 0.4
    min_exploration: 0.0
    num_samples_dropped: 55
    num_steps_sampled: 10063400
    num_steps_trained: 9105408
    num_target_updates: 181
    num_weight_syncs: 25157
    replay_shard_0:
      add_batch_time_ms: 14.096
      policy_default_policy:
        added_count: 2481600
        est_size_bytes: 170500000
        num_entries: 500000
        sampled_count: 2272768
      replay_time_ms: 35.026
      update_priorities_time_ms: 94.5
    sample_throughput: 0.0
    train_throughput: 0.0
  iterations_since_restore: 39
  node_ip: 10.16.128.63
  num_healthy_workers: 11
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 32336
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 1.547802406925541
    mean_inference_ms: 2.1705401778371787
    mean_processing_ms: 1.1685933614191468
  time_since_restore: 1190.3168189525604
  time_this_iter_s: 30.498246669769287
  time_total_s: 1190.3168189525604
  timestamp: 1563931408
  timesteps_since_restore: 10063400
  timesteps_this_iter: 256200
  timesteps_total: 10063400
  training_iteration: 39
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 3})
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	TERMINATED, [12 CPUs, 1 GPUs], [pid=32336], 1190 s, 39 iter, 10063400 ts, 19.7 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs
Memory usage on this node: 13.7/16.7 GB
Result logdir: /home/amr/kayray_results/parallel/gym-reacher-apex-ddpg
Number of trials: 3 ({'TERMINATED': 3})
TERMINATED trials:
 - APEX_DDPG_RoboschoolReacher-v1_0_num_envs_per_worker=16:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31643], 804 s, 26 iter, 10373600 ts, 18.7 rew
 - APEX_DDPG_RoboschoolReacher-v1_1_num_envs_per_worker=8:	TERMINATED, [12 CPUs, 1 GPUs], [pid=31967], 982 s, 32 iter, 10120800 ts, 18 rew
 - APEX_DDPG_RoboschoolReacher-v1_2_num_envs_per_worker=4:	TERMINATED, [12 CPUs, 1 GPUs], [pid=32336], 1190 s, 39 iter, 10063400 ts, 19.7 rew

[32m [  3029.00872s,  INFO] Experiment took 3028.80371 seconds | 50.48006 minutes | 0.84133 hours [0m
