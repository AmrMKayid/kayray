2019-07-14 15:47:48,840	INFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-14_15-47-48_838819_20157/logs.
2019-07-14 15:47:48,950	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:45770 to respond...
2019-07-14 15:47:49,067	INFO services.py:409 -- Waiting for redis server at 127.0.0.1:39294 to respond...
2019-07-14 15:47:49,072	INFO services.py:806 -- Starting Redis shard with 1.72 GB max memory.
2019-07-14 15:47:49,113	INFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-14_15-47-48_838819_20157/logs.
2019-07-14 15:47:49,116	INFO services.py:1446 -- Starting the Plasma object store with 2.58 GB memory using /tmp.
2019-07-14 15:47:49,751	INFO tune.py:65 -- Did not find checkpoint file in /Users/amrmkayid/kayray_results/gym-reacher-ddpg.
2019-07-14 15:47:49,751	INFO tune.py:233 -- Starting a new experiment.
2019-07-14 15:47:51,236	WARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.
[32m [     0.06022s,  INFO] Registering env:  RoboschoolReacher-v1 [0m
[32m [     0.06090s,  INFO] Experiment configs: 
 {
  "gym-reacher-ddpg": {
    "env": "RoboschoolReacher-v1",
    "run": "DDPG",
    "local_dir": "~/kayray_results",
    "checkpoint_freq": 100,
    "checkpoint_at_end": true,
    "stop": {
      "time_total_s": 7200,
      "episode_reward_mean": 21,
      "training_iteration": 500
    },
    "config": {
      "actor_hiddens": [
        256,
        128
      ],
      "critic_hiddens": [
        256,
        128,
        128
      ],
      "n_step": 1,
      "model": {},
      "gamma": 0.99,
      "env_config": {},
      "exploration_should_anneal": true,
      "schedule_max_timesteps": 100000,
      "timesteps_per_iteration": 1000,
      "exploration_fraction": 0.1,
      "exploration_final_scale": 0.02,
      "exploration_ou_noise_scale": 0.1,
      "exploration_ou_theta": 0.15,
      "exploration_ou_sigma": 0.2,
      "target_network_update_freq": 0,
      "tau": 0.0001,
      "buffer_size": 1000000,
      "prioritized_replay": true,
      "prioritized_replay_alpha": 0.6,
      "prioritized_replay_beta": 0.4,
      "prioritized_replay_eps": 1e-06,
      "clip_rewards": false,
      "actor_lr": 1e-05,
      "critic_lr": 3e-05,
      "use_huber": false,
      "huber_threshold": 1.0,
      "l2_reg": 1e-06,
      "learning_starts": 500,
      "sample_batch_size": 1,
      "train_batch_size": 1024,
      "num_workers": 0,
      "num_gpus_per_worker": 0,
      "per_worker_exploration": false,
      "worker_side_prioritization": false,
      "evaluation_interval": 5,
      "evaluation_num_episodes": 10
    }
  }
} [0m
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING

[2m[36m(pid=20180)[0m [32m [     0.08646s,  INFO] TimeLimit:
[2m[36m(pid=20180)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=20180)[0m - action_space = Box(2,)
[2m[36m(pid=20180)[0m - observation_space = Box(9,)
[2m[36m(pid=20180)[0m - reward_range = (-inf, inf)
[2m[36m(pid=20180)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=20180)[0m - _max_episode_steps = 150
[2m[36m(pid=20180)[0m - _elapsed_steps = None [0m
[2m[36m(pid=20180)[0m 2019-07-14 15:47:54,021	INFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=20180)[0m 2019-07-14 15:47:54.030340: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[2m[36m(pid=20180)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=20180)[0m WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
[2m[36m(pid=20180)[0m [32m [     1.87027s,  INFO] TimeLimit:
[2m[36m(pid=20180)[0m - env = <RoboschoolReacher<RoboschoolReacher-v1>>
[2m[36m(pid=20180)[0m - action_space = Box(2,)
[2m[36m(pid=20180)[0m - observation_space = Box(9,)
[2m[36m(pid=20180)[0m - reward_range = (-inf, inf)
[2m[36m(pid=20180)[0m - metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 60}
[2m[36m(pid=20180)[0m - _max_episode_steps = 150
[2m[36m(pid=20180)[0m - _elapsed_steps = None [0m
[2m[36m(pid=20180)[0m 2019-07-14 15:47:55,789	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x1cfa2bac8>}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:55,790	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x1cf563e10>}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:55,791	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x1cf56b978>}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:55,805	INFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,040	INFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.agents.ddpg.ddpg_policy.DDPGTFPolicy object at 0x1d03bc320>}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,040	INFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x1d03bc048>}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,040	INFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x1d03a3ef0>}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,043	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,303	INFO rollout_worker.py:428 -- Generating sample batch of size 1
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,326	INFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float64, min=-0.132, max=0.864, mean=0.197)}}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,326	INFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,327	INFO sampler.py:407 -- Preprocessed obs: np.ndarray((9,), dtype=float64, min=-0.132, max=0.864, mean=0.197)
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,327	INFO sampler.py:411 -- Filtered obs: np.ndarray((9,), dtype=float64, min=-0.132, max=0.864, mean=0.197)
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,328	INFO sampler.py:525 -- Inputs to compute_actions():
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=20180)[0m                                   'env_id': 0,
[2m[36m(pid=20180)[0m                                   'info': None,
[2m[36m(pid=20180)[0m                                   'obs': np.ndarray((9,), dtype=float64, min=-0.132, max=0.864, mean=0.197),
[2m[36m(pid=20180)[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=20180)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=20180)[0m                                   'rnn_state': []},
[2m[36m(pid=20180)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,328	INFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,368	INFO sampler.py:552 -- Outputs of compute_actions():
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.107, max=-0.01, mean=-0.059),
[2m[36m(pid=20180)[0m                       [],
[2m[36m(pid=20180)[0m                       {})}
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,528	INFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m { 'agent0': { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-0.705, max=0.585, mean=-0.065),
[2m[36m(pid=20180)[0m                         'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=20180)[0m                         'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=20180)[0m                         'eps_id': np.ndarray((150,), dtype=int64, min=110817912.0, max=110817912.0, mean=110817912.0),
[2m[36m(pid=20180)[0m                         'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=20180)[0m                         'new_obs': np.ndarray((150, 9), dtype=float32, min=-4.544, max=1.094, mean=-0.088),
[2m[36m(pid=20180)[0m                         'obs': np.ndarray((150, 9), dtype=float32, min=-4.516, max=1.094, mean=-0.084),
[2m[36m(pid=20180)[0m                         'prev_actions': np.ndarray((150, 2), dtype=float32, min=-0.705, max=0.585, mean=-0.064),
[2m[36m(pid=20180)[0m                         'prev_rewards': np.ndarray((150,), dtype=float32, min=-1.033, max=1.676, mean=0.031),
[2m[36m(pid=20180)[0m                         'rewards': np.ndarray((150,), dtype=float32, min=-1.033, max=1.676, mean=0.032),
[2m[36m(pid=20180)[0m                         't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=20180)[0m                         'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=20180)[0m                         'weights': np.ndarray((150,), dtype=float32, min=1.0, max=1.0, mean=1.0)},
[2m[36m(pid=20180)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m 2019-07-14 15:47:57,533	INFO rollout_worker.py:462 -- Completed sample batch:
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m { 'data': { 'actions': np.ndarray((150, 2), dtype=float32, min=-0.705, max=0.585, mean=-0.065),
[2m[36m(pid=20180)[0m             'agent_index': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=20180)[0m             'dones': np.ndarray((150,), dtype=bool, min=0.0, max=1.0, mean=0.007),
[2m[36m(pid=20180)[0m             'eps_id': np.ndarray((150,), dtype=int64, min=110817912.0, max=110817912.0, mean=110817912.0),
[2m[36m(pid=20180)[0m             'infos': np.ndarray((150,), dtype=object, head={}),
[2m[36m(pid=20180)[0m             'new_obs': np.ndarray((150, 9), dtype=float32, min=-4.544, max=1.094, mean=-0.088),
[2m[36m(pid=20180)[0m             'obs': np.ndarray((150, 9), dtype=float32, min=-4.516, max=1.094, mean=-0.084),
[2m[36m(pid=20180)[0m             'prev_actions': np.ndarray((150, 2), dtype=float32, min=-0.705, max=0.585, mean=-0.064),
[2m[36m(pid=20180)[0m             'prev_rewards': np.ndarray((150,), dtype=float32, min=-1.033, max=1.676, mean=0.031),
[2m[36m(pid=20180)[0m             'rewards': np.ndarray((150,), dtype=float32, min=-1.033, max=1.676, mean=0.032),
[2m[36m(pid=20180)[0m             't': np.ndarray((150,), dtype=int64, min=0.0, max=149.0, mean=74.5),
[2m[36m(pid=20180)[0m             'unroll_id': np.ndarray((150,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=20180)[0m             'weights': np.ndarray((150,), dtype=float32, min=1.0, max=1.0, mean=1.0)},
[2m[36m(pid=20180)[0m   'type': 'SampleBatch'}
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m 2019-07-14 15:48:00,757	INFO rollout_worker.py:552 -- Training on concatenated sample batches:
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m { 'count': 1024,
[2m[36m(pid=20180)[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((1024, 2), dtype=float32, min=-1.0, max=0.995, mean=-0.058),
[2m[36m(pid=20180)[0m                                                     'batch_indexes': np.ndarray((1024,), dtype=int64, min=0.0, max=500.0, mean=254.266),
[2m[36m(pid=20180)[0m                                                     'dones': np.ndarray((1024,), dtype=bool, min=0.0, max=1.0, mean=0.006),
[2m[36m(pid=20180)[0m                                                     'new_obs': np.ndarray((1024, 9), dtype=float32, min=-3.517, max=3.438, mean=-0.014),
[2m[36m(pid=20180)[0m                                                     'obs': np.ndarray((1024, 9), dtype=float32, min=-3.517, max=3.438, mean=-0.015),
[2m[36m(pid=20180)[0m                                                     'rewards': np.ndarray((1024,), dtype=float32, min=-2.986, max=4.309, mean=-0.016),
[2m[36m(pid=20180)[0m                                                     'weights': np.ndarray((1024,), dtype=float64, min=1.0, max=1.0, mean=1.0)},
[2m[36m(pid=20180)[0m                                           'type': 'SampleBatch'}},
[2m[36m(pid=20180)[0m   'type': 'MultiAgentBatch'}
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m 2019-07-14 15:48:00,965	INFO rollout_worker.py:574 -- Training output:
[2m[36m(pid=20180)[0m 
[2m[36m(pid=20180)[0m { 'default_policy': { 'learner_stats': { 'max_q': 0.31267962,
[2m[36m(pid=20180)[0m                                          'mean_q': 0.07042807,
[2m[36m(pid=20180)[0m                                          'min_q': -0.20330356},
[2m[36m(pid=20180)[0m                       'td_error': np.ndarray((1024,), dtype=float32, min=-4.181, max=2.947, mean=0.03)}}
[2m[36m(pid=20180)[0m 
Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_15-50-51
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -6.97190622922717
  episode_reward_min: -22.1456121966197
  episodes_this_iter: 6
  episodes_total: 6
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 409.027
    learner:
      default_policy:
        max_q: 2.146395444869995
        mean_q: 0.05572110414505005
        min_q: -0.7907300591468811
    max_exploration: 1.0
    min_exploration: 1.0
    num_steps_sampled: 1000
    num_steps_trained: 512000
    num_target_updates: 1000
    opt_peak_throughput: 2503.502
    opt_samples: 1024.0
    replay_time_ms: 423.45
    sample_time_ms: 4.943
    update_time_ms: 0.006
  iterations_since_restore: 1
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.3961621226368846
    mean_inference_ms: 1.0167316242412372
    mean_processing_ms: 0.514866231562017
  time_since_restore: 172.7270278930664
  time_this_iter_s: 172.7270278930664
  time_total_s: 172.7270278930664
  timestamp: 1563112251
  timesteps_since_restore: 1000
  timesteps_this_iter: 1000
  timesteps_total: 1000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 172 s, 1 iter, 1000 ts, -6.97 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_15-56-44
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -16.419009837221378
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 13
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 175.002
    learner:
      default_policy:
        max_q: 9.122196197509766
        mean_q: -0.00983920507133007
        min_q: -8.81088924407959
    max_exploration: 0.902
    min_exploration: 0.902
    num_steps_sampled: 2000
    num_steps_trained: 1536000
    num_target_updates: 2000
    opt_peak_throughput: 5851.369
    opt_samples: 1024.0
    replay_time_ms: 223.019
    sample_time_ms: 3.051
    update_time_ms: 0.004
  iterations_since_restore: 2
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4158852807640096
    mean_inference_ms: 1.1387360045191508
    mean_processing_ms: 0.5414962618887338
  time_since_restore: 524.9371900558472
  time_this_iter_s: 352.21016216278076
  time_total_s: 524.9371900558472
  timestamp: 1563112604
  timesteps_since_restore: 2000
  timesteps_this_iter: 1000
  timesteps_total: 2000
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 524 s, 2 iter, 2000 ts, -16.4 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-03-08
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -10.442624216939109
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 20
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 146.614
    learner:
      default_policy:
        max_q: 9.910628318786621
        mean_q: -0.009285543113946915
        min_q: -10.881840705871582
    max_exploration: 0.804
    min_exploration: 0.804
    num_steps_sampled: 3000
    num_steps_trained: 2560000
    num_target_updates: 3000
    opt_peak_throughput: 6984.345
    opt_samples: 1024.0
    replay_time_ms: 205.674
    sample_time_ms: 3.136
    update_time_ms: 0.004
  iterations_since_restore: 3
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4388876459837324
    mean_inference_ms: 1.2095975972539628
    mean_processing_ms: 0.5578052006533584
  time_since_restore: 909.0603790283203
  time_this_iter_s: 384.12318897247314
  time_total_s: 909.0603790283203
  timestamp: 1563112988
  timesteps_since_restore: 3000
  timesteps_this_iter: 1000
  timesteps_total: 3000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 909 s, 3 iter, 3000 ts, -10.4 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-08-34
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -11.086117213603107
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 6
  episodes_total: 26
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 134.241
    learner:
      default_policy:
        max_q: 11.863973617553711
        mean_q: 0.02291332557797432
        min_q: -12.055598258972168
    max_exploration: 0.706
    min_exploration: 0.706
    num_steps_sampled: 4000
    num_steps_trained: 3584000
    num_target_updates: 4000
    opt_peak_throughput: 7628.081
    opt_samples: 1024.0
    replay_time_ms: 166.677
    sample_time_ms: 2.922
    update_time_ms: 0.004
  iterations_since_restore: 4
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4499227321184686
    mean_inference_ms: 1.2386453358319462
    mean_processing_ms: 0.5665731727025661
  time_since_restore: 1235.6934943199158
  time_this_iter_s: 326.63311529159546
  time_total_s: 1235.6934943199158
  timestamp: 1563113314
  timesteps_since_restore: 4000
  timesteps_this_iter: 1000
  timesteps_total: 4000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 1235 s, 4 iter, 4000 ts, -11.1 rew

[2m[36m(pid=20180)[0m 2019-07-14 16:13:58,928	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-13-58
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -9.068309158437414
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 33
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 7.853886570305821
    episode_reward_mean: -10.733345344873861
    episode_reward_min: -37.465544371281645
    episodes_this_iter: 10
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.3563721392083986
      mean_inference_ms: 0.8985037328560563
      mean_processing_ms: 0.19730650874147093
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 142.904
    learner:
      default_policy:
        max_q: 12.304708480834961
        mean_q: 0.13273575901985168
        min_q: -11.68110466003418
    max_exploration: 0.608
    min_exploration: 0.608
    num_steps_sampled: 5000
    num_steps_trained: 4608000
    num_target_updates: 5000
    opt_peak_throughput: 7165.663
    opt_samples: 1024.0
    replay_time_ms: 160.434
    sample_time_ms: 5.293
    update_time_ms: 0.004
  iterations_since_restore: 5
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.45751514186590186
    mean_inference_ms: 1.2595561781336833
    mean_processing_ms: 0.574261285903867
  time_since_restore: 1559.643167257309
  time_this_iter_s: 323.9496729373932
  time_total_s: 1559.643167257309
  timestamp: 1563113638
  timesteps_since_restore: 5000
  timesteps_this_iter: 1000
  timesteps_total: 5000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 1559 s, 5 iter, 5000 ts, -9.07 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-19-27
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -8.068032113840342
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 40
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 139.581
    learner:
      default_policy:
        max_q: 11.771451950073242
        mean_q: 0.14385297894477844
        min_q: -13.833908081054688
    max_exploration: 0.51
    min_exploration: 0.51
    num_steps_sampled: 6000
    num_steps_trained: 5632000
    num_target_updates: 6000
    opt_peak_throughput: 7336.25
    opt_samples: 1024.0
    replay_time_ms: 155.438
    sample_time_ms: 2.685
    update_time_ms: 0.007
  iterations_since_restore: 6
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.462432384549591
    mean_inference_ms: 1.2719295934404895
    mean_processing_ms: 0.5790490371476569
  time_since_restore: 1885.632648229599
  time_this_iter_s: 325.98948097229004
  time_total_s: 1885.632648229599
  timestamp: 1563113967
  timesteps_since_restore: 6000
  timesteps_this_iter: 1000
  timesteps_total: 6000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 1885 s, 6 iter, 6000 ts, -8.07 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-25-03
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 10.585776418983187
  episode_reward_mean: -7.708583595044521
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 6
  episodes_total: 46
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 350.66
    learner:
      default_policy:
        max_q: 11.093783378601074
        mean_q: 0.08246725052595139
        min_q: -13.616634368896484
    max_exploration: 0.41200000000000003
    min_exploration: 0.41200000000000003
    num_steps_sampled: 7000
    num_steps_trained: 6656000
    num_target_updates: 7000
    opt_peak_throughput: 2920.208
    opt_samples: 1024.0
    replay_time_ms: 456.841
    sample_time_ms: 3.647
    update_time_ms: 0.007
  iterations_since_restore: 7
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4654200981369751
    mean_inference_ms: 1.2793538135369242
    mean_processing_ms: 0.5816110790115927
  time_since_restore: 2221.730102300644
  time_this_iter_s: 336.0974540710449
  time_total_s: 2221.730102300644
  timestamp: 1563114303
  timesteps_since_restore: 7000
  timesteps_this_iter: 1000
  timesteps_total: 7000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 2221 s, 7 iter, 7000 ts, -7.71 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-30-21
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.59028803649081
  episode_reward_mean: -7.624289740679377
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 53
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 125.377
    learner:
      default_policy:
        max_q: 12.42038631439209
        mean_q: 0.22723239660263062
        min_q: -14.540274620056152
    max_exploration: 0.31400000000000006
    min_exploration: 0.31400000000000006
    num_steps_sampled: 8000
    num_steps_trained: 7680000
    num_target_updates: 8000
    opt_peak_throughput: 8167.36
    opt_samples: 1024.0
    replay_time_ms: 154.304
    sample_time_ms: 2.578
    update_time_ms: 0.005
  iterations_since_restore: 8
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.46809251955834724
    mean_inference_ms: 1.2860738826258082
    mean_processing_ms: 0.5851530038599468
  time_since_restore: 2539.5518701076508
  time_this_iter_s: 317.82176780700684
  time_total_s: 2539.5518701076508
  timestamp: 1563114621
  timesteps_since_restore: 8000
  timesteps_this_iter: 1000
  timesteps_total: 8000
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 2539 s, 8 iter, 8000 ts, -7.62 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-35-29
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.59028803649081
  episode_reward_mean: -6.314073876872599
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 60
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 122.582
    learner:
      default_policy:
        max_q: 9.990490913391113
        mean_q: 0.240644633769989
        min_q: -13.520766258239746
    max_exploration: 0.21599999999999997
    min_exploration: 0.21599999999999997
    num_steps_sampled: 9000
    num_steps_trained: 8704000
    num_target_updates: 9000
    opt_peak_throughput: 8353.565
    opt_samples: 1024.0
    replay_time_ms: 149.534
    sample_time_ms: 2.947
    update_time_ms: 0.004
  iterations_since_restore: 9
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.46977098226416214
    mean_inference_ms: 1.2898775707698604
    mean_processing_ms: 0.5869534415848482
  time_since_restore: 2847.995105266571
  time_this_iter_s: 308.4432351589203
  time_total_s: 2847.995105266571
  timestamp: 1563114929
  timesteps_since_restore: 9000
  timesteps_this_iter: 1000
  timesteps_total: 9000
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 2847 s, 9 iter, 9000 ts, -6.31 rew

[2m[36m(pid=20180)[0m 2019-07-14 16:39:14,207	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-39-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.59028803649081
  episode_reward_mean: -5.932412745064224
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 6
  episodes_total: 66
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 19.768651743034418
    episode_reward_mean: -1.4046496032805884
    episode_reward_min: -21.17191764368681
    episodes_this_iter: 10
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.33327612021954106
      mean_inference_ms: 0.8578555262425136
      mean_processing_ms: 0.18447061507761733
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 99.308
    learner:
      default_policy:
        max_q: 12.07966423034668
        mean_q: 0.19186878204345703
        min_q: -15.773914337158203
    max_exploration: 0.118
    min_exploration: 0.118
    num_steps_sampled: 10000
    num_steps_trained: 9728000
    num_target_updates: 10000
    opt_peak_throughput: 10311.343
    opt_samples: 1024.0
    replay_time_ms: 116.626
    sample_time_ms: 1.85
    update_time_ms: 0.004
  iterations_since_restore: 10
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.469494595070127
    mean_inference_ms: 1.2880080669019855
    mean_processing_ms: 0.5863146703792671
  time_since_restore: 3072.2675631046295
  time_this_iter_s: 224.27245783805847
  time_total_s: 3072.2675631046295
  timestamp: 1563115154
  timesteps_since_restore: 10000
  timesteps_this_iter: 1000
  timesteps_total: 10000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 3072 s, 10 iter, 10000 ts, -5.93 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-43-04
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 15.863225782166877
  episode_reward_mean: -5.122761207173023
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 73
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 96.04
    learner:
      default_policy:
        max_q: 13.540728569030762
        mean_q: 0.3068974018096924
        min_q: -15.908334732055664
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 11000
    num_steps_trained: 10752000
    num_target_updates: 11000
    opt_peak_throughput: 10662.22
    opt_samples: 1024.0
    replay_time_ms: 115.402
    sample_time_ms: 1.821
    update_time_ms: 0.004
  iterations_since_restore: 11
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4678949019817293
    mean_inference_ms: 1.2822614435935857
    mean_processing_ms: 0.5839676597356166
  time_since_restore: 3300.8500549793243
  time_this_iter_s: 228.58249187469482
  time_total_s: 3300.8500549793243
  timestamp: 1563115384
  timesteps_since_restore: 11000
  timesteps_this_iter: 1000
  timesteps_total: 11000
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 3300 s, 11 iter, 11000 ts, -5.12 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-47-59
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.453953554731501
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 80
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 124.401
    learner:
      default_policy:
        max_q: 15.008402824401855
        mean_q: 0.3746235966682434
        min_q: -16.074748992919922
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 12000
    num_steps_trained: 11776000
    num_target_updates: 12000
    opt_peak_throughput: 8231.412
    opt_samples: 1024.0
    replay_time_ms: 152.367
    sample_time_ms: 2.957
    update_time_ms: 0.003
  iterations_since_restore: 12
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4657606591108442
    mean_inference_ms: 1.2754543655091877
    mean_processing_ms: 0.5810603180974978
  time_since_restore: 3595.914895057678
  time_this_iter_s: 295.0648400783539
  time_total_s: 3595.914895057678
  timestamp: 1563115679
  timesteps_since_restore: 12000
  timesteps_this_iter: 1000
  timesteps_total: 12000
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.1/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 3595 s, 12 iter, 12000 ts, -5.45 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-53-00
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.523254845817843
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 6
  episodes_total: 86
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 134.319
    learner:
      default_policy:
        max_q: 15.25931167602539
        mean_q: 0.3032699525356293
        min_q: -16.33083724975586
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 13000
    num_steps_trained: 12800000
    num_target_updates: 13000
    opt_peak_throughput: 7623.622
    opt_samples: 1024.0
    replay_time_ms: 153.056
    sample_time_ms: 2.775
    update_time_ms: 0.004
  iterations_since_restore: 13
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4641929878952759
    mean_inference_ms: 1.2701187619373502
    mean_processing_ms: 0.578843553898532
  time_since_restore: 3896.516018152237
  time_this_iter_s: 300.6011230945587
  time_total_s: 3896.516018152237
  timestamp: 1563115980
  timesteps_since_restore: 13000
  timesteps_this_iter: 1000
  timesteps_total: 13000
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 3896 s, 13 iter, 13000 ts, -5.52 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_16-57-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.8062163264908175
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 93
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 106.994
    learner:
      default_policy:
        max_q: 14.145246505737305
        mean_q: 0.4374814033508301
        min_q: -9.344890594482422
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 14000
    num_steps_trained: 13824000
    num_target_updates: 14000
    opt_peak_throughput: 9570.606
    opt_samples: 1024.0
    replay_time_ms: 117.907
    sample_time_ms: 1.755
    update_time_ms: 0.004
  iterations_since_restore: 14
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.46212707946356446
    mean_inference_ms: 1.2633496458444096
    mean_processing_ms: 0.5760328117766628
  time_since_restore: 4150.446930170059
  time_this_iter_s: 253.93091201782227
  time_total_s: 4150.446930170059
  timestamp: 1563116234
  timesteps_since_restore: 14000
  timesteps_this_iter: 1000
  timesteps_total: 14000
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 4150 s, 14 iter, 14000 ts, -5.81 rew

[2m[36m(pid=20180)[0m 2019-07-14 17:02:24,953	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-02-24
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.465273613281203
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 7
  episodes_total: 100
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 14.786911645607145
    episode_reward_mean: -7.30945984269169
    episode_reward_min: -35.45479917279939
    episodes_this_iter: 10
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.3231231658940632
      mean_inference_ms: 0.8320547783102639
      mean_processing_ms: 0.17810880968996215
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 103.81
    learner:
      default_policy:
        max_q: 16.272260665893555
        mean_q: 0.4565529227256775
        min_q: -13.620047569274902
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 15000
    num_steps_trained: 14848000
    num_target_updates: 15000
    opt_peak_throughput: 9864.15
    opt_samples: 1024.0
    replay_time_ms: 120.49
    sample_time_ms: 2.435
    update_time_ms: 0.004
  iterations_since_restore: 15
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4604816495270576
    mean_inference_ms: 1.2578874406325178
    mean_processing_ms: 0.5736689377853462
  time_since_restore: 4461.079735040665
  time_this_iter_s: 310.63280487060547
  time_total_s: 4461.079735040665
  timestamp: 1563116544
  timesteps_since_restore: 15000
  timesteps_this_iter: 1000
  timesteps_total: 15000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 4461 s, 15 iter, 15000 ts, -5.47 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-07-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.4513542315977475
  episode_reward_min: -44.92158985641269
  episodes_this_iter: 6
  episodes_total: 106
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 117.628
    learner:
      default_policy:
        max_q: 14.083730697631836
        mean_q: 0.5129307508468628
        min_q: -10.88002872467041
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 16000
    num_steps_trained: 15872000
    num_target_updates: 16000
    opt_peak_throughput: 8705.444
    opt_samples: 1024.0
    replay_time_ms: 146.633
    sample_time_ms: 2.496
    update_time_ms: 0.004
  iterations_since_restore: 16
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4630679468337878
    mean_inference_ms: 1.2677356053573734
    mean_processing_ms: 0.5751928429318199
  time_since_restore: 4748.344877004623
  time_this_iter_s: 287.26514196395874
  time_total_s: 4748.344877004623
  timestamp: 1563116834
  timesteps_since_restore: 16000
  timesteps_this_iter: 1000
  timesteps_total: 16000
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.7/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 4748 s, 16 iter, 16000 ts, -5.45 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-12-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -4.282332973258506
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 7
  episodes_total: 113
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 108.973
    learner:
      default_policy:
        max_q: 15.707786560058594
        mean_q: 0.46502384543418884
        min_q: -17.524303436279297
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 17000
    num_steps_trained: 16896000
    num_target_updates: 17000
    opt_peak_throughput: 9396.833
    opt_samples: 1024.0
    replay_time_ms: 123.52
    sample_time_ms: 1.867
    update_time_ms: 0.003
  iterations_since_restore: 17
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.46359318361699037
    mean_inference_ms: 1.2631163687009277
    mean_processing_ms: 0.5733840937596955
  time_since_restore: 5039.480726957321
  time_this_iter_s: 291.13584995269775
  time_total_s: 5039.480726957321
  timestamp: 1563117125
  timesteps_since_restore: 17000
  timesteps_this_iter: 1000
  timesteps_total: 17000
  training_iteration: 17
  2019-07-14 17:52:00,865	INFO ray_trial_executor.py:187 -- Destroying actor for trial DDPG_RoboschoolReacher-v1_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 5039 s, 17 iter, 17000 ts, -4.28 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-17-40
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.3431700266665345
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 7
  episodes_total: 120
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 98.078
    learner:
      default_policy:
        max_q: 16.076370239257812
        mean_q: 0.34104329347610474
        min_q: -15.311452865600586
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 18000
    num_steps_trained: 17920000
    num_target_updates: 18000
    opt_peak_throughput: 10440.643
    opt_samples: 1024.0
    replay_time_ms: 115.592
    sample_time_ms: 2.273
    update_time_ms: 0.003
  iterations_since_restore: 18
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4607793746080827
    mean_inference_ms: 1.251699215472484
    mean_processing_ms: 0.5702346910111861
  time_since_restore: 5375.137771844864
  time_this_iter_s: 335.6570448875427
  time_total_s: 5375.137771844864
  timestamp: 1563117460
  timesteps_since_restore: 18000
  timesteps_this_iter: 1000
  timesteps_total: 18000
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.2/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 5375 s, 18 iter, 18000 ts, -5.34 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-22-17
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.197797236081912
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 6
  episodes_total: 126
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 98.405
    learner:
      default_policy:
        max_q: 18.565343856811523
        mean_q: 0.5469175577163696
        min_q: -14.26695442199707
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 19000
    num_steps_trained: 18944000
    num_target_updates: 19000
    opt_peak_throughput: 10405.975
    opt_samples: 1024.0
    replay_time_ms: 117.017
    sample_time_ms: 1.839
    update_time_ms: 0.003
  iterations_since_restore: 19
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4578290916720347
    mean_inference_ms: 1.242082927052912
    mean_processing_ms: 0.5669640695252748
  time_since_restore: 5651.349511623383
  time_this_iter_s: 276.2117397785187
  time_total_s: 5651.349511623383
  timestamp: 1563117737
  timesteps_since_restore: 19000
  timesteps_this_iter: 1000
  timesteps_total: 19000
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 5651 s, 19 iter, 19000 ts, -5.2 rew

[2m[36m(pid=20180)[0m 2019-07-14 17:28:01,981	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-28-01
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.083159172333153
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 7
  episodes_total: 133
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 7.352839067057718
    episode_reward_mean: -3.0652386140149246
    episode_reward_min: -18.85411290353465
    episodes_this_iter: 10
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.323125385090663
      mean_inference_ms: 0.8317770409339301
      mean_processing_ms: 0.17690801601412454
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 114.591
    learner:
      default_policy:
        max_q: 17.1959171295166
        mean_q: 0.6240522265434265
        min_q: -15.564935684204102
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 20000
    num_steps_trained: 19968000
    num_target_updates: 20000
    opt_peak_throughput: 8936.094
    opt_samples: 1024.0
    replay_time_ms: 140.491
    sample_time_ms: 2.334
    update_time_ms: 0.004
  iterations_since_restore: 20
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.45505468067105737
    mean_inference_ms: 1.2308792548171923
    mean_processing_ms: 0.5626857227284564
  time_since_restore: 5996.1814057827
  time_this_iter_s: 344.831894159317
  time_total_s: 5996.1814057827
  timestamp: 1563118081
  timesteps_since_restore: 20000
  timesteps_this_iter: 1000
  timesteps_total: 20000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 5996 s, 20 iter, 20000 ts, -5.08 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-32-35
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.659658890499379
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 7
  episodes_total: 140
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 103.526
    learner:
      default_policy:
        max_q: 19.199556350708008
        mean_q: 0.5221933126449585
        min_q: -18.876968383789062
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 21000
    num_steps_trained: 20992000
    num_target_updates: 21000
    opt_peak_throughput: 9891.251
    opt_samples: 1024.0
    replay_time_ms: 120.796
    sample_time_ms: 2.551
    update_time_ms: 0.003
  iterations_since_restore: 21
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4522375850138044
    mean_inference_ms: 1.219823609066032
    mean_processing_ms: 0.5582465650277202
  time_since_restore: 6267.788005590439
  time_this_iter_s: 271.60659980773926
  time_total_s: 6267.788005590439
  timestamp: 1563118355
  timesteps_since_restore: 21000
  timesteps_this_iter: 1000
  timesteps_total: 21000
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 6267 s, 21 iter, 21000 ts, -5.66 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-38-13
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.95236482023002
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 6
  episodes_total: 146
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 135.065
    learner:
      default_policy:
        max_q: 19.76409339904785
        mean_q: 0.6800740361213684
        min_q: -18.585365295410156
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 22000
    num_steps_trained: 22016000
    num_target_updates: 22000
    opt_peak_throughput: 7581.537
    opt_samples: 1024.0
    replay_time_ms: 154.535
    sample_time_ms: 3.419
    update_time_ms: 0.012
  iterations_since_restore: 22
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.45003331475618374
    mean_inference_ms: 1.2107086528478472
    mean_processing_ms: 0.5547190571718659
  time_since_restore: 6605.245633602142
  time_this_iter_s: 337.4576280117035
  time_total_s: 6605.245633602142
  timestamp: 1563118693
  timesteps_since_restore: 22000
  timesteps_this_iter: 1000
  timesteps_total: 22000
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 6.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 6605 s, 22 iter, 22000 ts, -5.95 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-42-14
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -5.38980921197618
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 7
  episodes_total: 153
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 99.343
    learner:
      default_policy:
        max_q: 19.473730087280273
        mean_q: 0.6573741436004639
        min_q: -19.439184188842773
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 23000
    num_steps_trained: 23040000
    num_target_updates: 23000
    opt_peak_throughput: 10307.701
    opt_samples: 1024.0
    replay_time_ms: 117.152
    sample_time_ms: 1.843
    update_time_ms: 0.005
  iterations_since_restore: 23
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4471120138794928
    mean_inference_ms: 1.199035365458889
    mean_processing_ms: 0.5495110898687874
  time_since_restore: 6846.165349721909
  time_this_iter_s: 240.91971611976624
  time_total_s: 6846.165349721909
  timestamp: 1563118934
  timesteps_since_restore: 23000
  timesteps_this_iter: 1000
  timesteps_total: 23000
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.3/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 6846 s, 23 iter, 23000 ts, -5.39 rew

Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-46-05
  done: false
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -6.345788659714575
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 7
  episodes_total: 160
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 117.651
    learner:
      default_policy:
        max_q: 21.71531105041504
        mean_q: 0.695499062538147
        min_q: -15.039834022521973
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 24000
    num_steps_trained: 24064000
    num_target_updates: 24000
    opt_peak_throughput: 8703.715
    opt_samples: 1024.0
    replay_time_ms: 138.92
    sample_time_ms: 2.561
    update_time_ms: 0.004
  iterations_since_restore: 24
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.4440610722284045
    mean_inference_ms: 1.1871677976108084
    mean_processing_ms: 0.5444435638903597
  time_since_restore: 7077.93043255806
  time_this_iter_s: 231.76508283615112
  time_total_s: 7077.93043255806
  timestamp: 1563119165
  timesteps_since_restore: 24000
  timesteps_this_iter: 1000
  timesteps_total: 24000
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.4/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - DDPG_RoboschoolReacher-v1_0:	RUNNING, [1 CPUs, 0 GPUs], [pid=20180], 7077 s, 24 iter, 24000 ts, -6.35 rew

[2m[36m(pid=20180)[0m 2019-07-14 17:51:59,036	INFO trainer.py:507 -- Evaluating current policy for 10 episodes
Result for DDPG_RoboschoolReacher-v1_0:
  custom_metrics: {}
  date: 2019-07-14_17-51-59
  done: true
  episode_len_mean: 150.0
  episode_reward_max: 18.16050682010583
  episode_reward_mean: -6.34553571974597
  episode_reward_min: -41.71124112288666
  episodes_this_iter: 6
  episodes_total: 166
  evaluation:
    custom_metrics: {}
    episode_len_mean: 150.0
    episode_reward_max: 18.59734363523221
    episode_reward_mean: -4.036109826591969
    episode_reward_min: -23.7291113628113
    episodes_this_iter: 10
    num_metric_batches_dropped: 0
    off_policy_estimator: {}
    policy_reward_mean: {}
    sampler_perf:
      mean_env_wait_ms: 0.31634219923677376
      mean_inference_ms: 0.8174617109583717
      mean_processing_ms: 0.17369488797814508
  experiment_id: 13d2b166c1cb4e6787b63ab8203902a9
  hostname: KayidmacOS
  info:
    grad_time_ms: 102.574
    learner:
      default_policy:
        max_q: 17.920564651489258
        mean_q: 0.6615831851959229
        min_q: -18.7943172454834
    max_exploration: 0.020000000000000018
    min_exploration: 0.020000000000000018
    num_steps_sampled: 25000
    num_steps_trained: 25088000
    num_target_updates: 25000
    opt_peak_throughput: 9983.017
    opt_samples: 1024.0
    replay_time_ms: 114.314
    sample_time_ms: 2.107
    update_time_ms: 0.003
  iterations_since_restore: 25
  node_ip: 192.168.0.3
  num_healthy_workers: 0
  num_metric_batches_dropped: 0
  off_policy_estimator: {}
  pid: 20180
  policy_reward_mean: {}
  sampler_perf:
    mean_env_wait_ms: 0.44213183626220515
    mean_inference_ms: 1.1792321817280247
    mean_processing_ms: 0.5410140346963338
  time_since_restore: 7431.079563617706
  time_this_iter_s: 353.1491310596466
  time_total_s: 7431.079563617706
  timestamp: 1563119519
  timesteps_since_restore: 25000
  timesteps_this_iter: 1000
  timesteps_total: 25000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'TERMINATED': 1})
TERMINATED trials:
 - DDPG_RoboschoolReacher-v1_0:	TERMINATED, [1 CPUs, 0 GPUs], [pid=20180], 7431 s, 25 iter, 25000 ts, -6.35 rew

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs
Memory usage on this node: 5.6/8.6 GB
Result logdir: /Users/amrmkayid/kayray_results/gym-reacher-ddpg
Number of trials: 1 ({'TERMINATED': 1})
TERMINATED trials:
 - DDPG_RoboschoolReacher-v1_0:	TERMINATED, [1 CPUs, 0 GPUs], [pid=20180], 7431 s, 25 iter, 25000 ts, -6.35 rew

[32m [  7452.09525s,  INFO] Experiment took 7452.04450 seconds | 124.20074 minutes | 2.07001 hours [0m
